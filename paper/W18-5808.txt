Automatically Tailoring Unsupervised Morphological Segmentation to the

Language

Ramy Eskander†

Owen Rambow(cid:93)

Smaranda Muresan†‡

†Department of Computer Science, Columbia University

‡Data Science Institute, Columbia University

{rnd2110,smara}@columbia.edu

(cid:93)Elemental Cognition, Inc.

owenr@elementalcognition.com

Abstract

languages,

Morphological segmentation is beneﬁcial for
several natural
language processing tasks
dealing with large vocabularies. Unsuper-
vised methods for morphological segmen-
tation are essential for handling a diverse
set of
including low-resource
languages.
Eskander et al. (2016) intro-
duced a Language Independent Morpholog-
ical Segmenter (LIMS) using Adaptor Gram-
mars (AG) based on the best-on-average per-
forming AG conﬁguration. However, while
LIMS worked best on average and outper-
forms other state-of-the-art unsupervised
morphological segmentation approaches, it
did not provide the optimal AG conﬁgura-
tion for ﬁve out of the six languages. We
propose two language-independent classi-
ﬁers that enable the selection of the opti-
mal or nearly-optimal conﬁguration for the
morphological segmentation of unseen lan-
guages.

1 Introduction

As natural language processing becomes more
interested in many languages,
including low-
resource languages, unsupervised morphologi-
cal segmentation remains an important area of
study. For most of the languages of the world,
we do not have morphologically annotated re-
sources. However, many human language tech-
nologies proﬁt from morphological segmenta-
tion, for example machine translation (Nguyen
et al., 2010; Ataman et al., 2017) and speech
recognition (Narasimhan et al., 2014).

In this paper, we build on previous work on
unsupervised morphological segmentation us-
ing Adaptor Grammars (AGs) (Johnson, 2008;
Sirts and Goldwater, 2013; Eskander et al., 2016),
a type of nonparametric Bayesian models that
generalize probabilistic context-free grammars
(PCFGs) (Johnson et al., 2007), where the PCFG
is typically a morphological grammar that spec-

Speciﬁcally, we ex-
iﬁes the word structure.
tend the research proposed by Eskander et al.
(2016), who investigate a large space of param-
eters when using Adaptor Grammars related to
(i) the underlying context-free grammar and (ii)
the use of a “Cascaded” system in which one
grammar chooses afﬁxes to be seeded into an-
other in order to simulate the situation where
scholar-knowledge is available. Their results
on a development set of 6 languages (English,
German, Finish, Turkish, Estonian and Zulu)
show that the best performing AG-based con-
ﬁguration (grammar and learning setup) differ
from language to language. For processing un-
seen languages, Eskander et al. (2016) proposed
the Language-Independent Morphological Seg-
menter (LIMS) based on the best-on-average
performing conﬁguration when running leave-
one-out cross validation on the development
languages.

However, while LIMS works best on average
and has been shown to outperform other state-
of-the-art unsupervised morphological segmen-
tation systems (Eskander et al., 2016), it is not
the optimal conﬁguration for any of the devel-
opment languages except Zulu. Thus, in this
paper we propose an approach to automatically
select the optimal or nearly-optimal language-
independent conﬁguration for the morphologi-
cal segmentation of unseen languages. We train
two classiﬁers on the development languages
used by Eskander et al. (2016) to make choices
for unseen languages (Section 3). We show that
we can choose the best parameter settings for
the six development languages in a leave-one-
out cross validation, and also on an unseen test
language (Arabic).

2 Problem Deﬁnition and Dataset

Adaptor Grammars (AGs) have been used suc-
cessfully for unsupervised morphological seg-

Proceedingsofthe15thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages78–83Brussels,Belgium,October31,2018.c(cid:13)2018TheSpecialInterestGrouponComputationalMorphologyandPhonologyhttps://doi.org/10.18653/v1/P1778Main Representation

Compound Morph

SubMorph

Segmentation Level

Grammar
Morph+SM

Morph+

PrStSu

Simple

PrStSu+SM

Simple+SM

Preﬁx?+Stem+Sufﬁx?
Preﬁx?+Stem+Sufﬁx?
Preﬁx+Stem+Sufﬁx
Preﬁx+Stem+Sufﬁx
Preﬁx+Stem+Sufﬁx
Preﬁx?+(Stem+Sufﬁx)
(Preﬁx-Stem)+Sufﬁx?
PrStSu2b++Co+SM (Preﬁx-Stem)+Sufﬁx?

PrStSu+Co+SM
PrStSu2a+SM
PrStSu2b+SM

No
No
No
No
No
Yes
No
No
Yes

Yes
No
No
Yes
Yes
Yes
Yes
Yes
Yes

Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes

Morph

Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx
Preﬁx-Stem-Sufﬁx

Table 1: Grammar Representations. Compound = Upper level representation of the word as a sequence of compounds;
Morph = Afﬁx/Morph representation as a sequence of morphs. SubMorph (SM) = Lower level representation of characters
as a sequence of sub-morphs. “+" denotes one or more and “?" denotes optional.

mentation (Johnson, 2008; Sirts and Goldwater,
2013; Eskander et al., 2016), which is the task of
breaking down words in a language into a se-
quence of morphs. An AG model typically has
two main components: a PCFG and an adap-
tor that adapts the probabilities assigned to in-
dividual subtrees in the grammar. For the task
of morphological segmentation, a PCFG is typ-
ically a morphological grammar that speciﬁes
word structure. Given a list of input strings, AGs
can learn latent tree structures.

Eskander et al. (2016) developed several AG
models based on different underlying context-
free grammars and learning settings, which we
brieﬂy introduce below.

Grammars. Eskander et al. (2016) introduce a
set of 9 grammars (see Table 1) designed based
on three dimensions: 1) how the grammar gen-
erates the preﬁx, stem and sufﬁx (morph vs.
tripartite), 2) the levels which are represented
in nonterminals (e.g., compounds, morphs and
sub-morphs) and 3) the levels at which the seg-
mentation into output morphs is produced. For
example,
in the PrStSu+SM grammar a word
is modeled as a preﬁx, a stem and a sufﬁx,
where the preﬁx and sufﬁx are sequences of
zero or more morphs, while a morph is a se-
quence of sub-morphs, and the segmentation is
based on the preﬁx, sufﬁx and stem level. The
PrStSu2a+SM grammar is similar, but a word is
modeled as a preﬁx and stem-sufﬁx sequence,
where the preﬁx is optional, and stem-sufﬁx is
either a stem or a stem and a sufﬁx (see Eskan-
der et al. (2016) for more details). Figure 1 shows
the trees for segmenting the word replayings us-
ing the PrStSu+SM and PrStSu2a+SM grammars.

Learning Settings. Eskander et al. (2016) con-
sider three learning settings: Standard (Std),
Scholar-Seeded Knowledge (Sch) and Cascaded
(Casc).
In the Standard setting, no scholar
knowledge is introduced in the grammars, while

in the Scholar-Seeded Knowledge setting the
grammars are augmented with scholar knowl-
edge in the form of information about afﬁxes
gathered from grammar books (before learning
happens). The Cascaded setting approximates
the effect of scholar-seeded knowledge by ﬁrst
using a high-precision AG to derive a set of af-
ﬁxes and then insert those afﬁxes into the gram-
mars used in a second learning step.

Eskander et al. (2016) show that the segmenta-
tion performance differs signiﬁcantly across the
different grammars, learning settings and lan-
guages. For instance, the best performance for
German is obtained by running the Standard
PrStSu+SM conﬁguration, while the Cascaded
PrStSu2a+SM conﬁguration produces the best
segmentation for Finnish. That means, there
is no setup that yields the optimal segmenta-
tion for all languages. For the processing of an
unseen language (i.e., not part of the develop-
ment), Eskander et al. (2016) recommend using
the Cascaded PrStSu+SM conﬁguration (referred
to as LIMS: Language-Independent Morpholog-
ical Segmenter), as it is the best-on-average per-
forming one when running leave-one-out cross
validation on the development languages.

Problem deﬁnition. While LIMS works best
on average,
it is not the optimal conﬁgura-
tion for any of the development languages ex-
cept Zulu. Thus, in this paper, we address the
problem of automatically selecting the optimal
or nearly-optimal language-independent (Stan-
dard or Cascaded) conﬁguration for the mor-
phological segmentation of unseen languages.

We use the 6 development languages used by
Eskander et al. (2016) as well as Arabic as a fully
unseen language. The data for English, German,
Finnish, Turkish and Estonian is from Morpho
Challenge1, and the data for Zulu is from the Uk-
wabelana corpus (Spiegler et al., 2010). For the

1http://research.ics.aalto.ﬁ/events/morphochallenge/

79Figure 1: Grammar trees for the word replayings: (a) PrStSu+SM, (b) PrStSu2a+SM

TRAIN DEV TEST

Source

Lang.
English Morpho Challenge 50,046 1,212
German Morpho Challenge 50,086 540
Finnish Morpho Challenge 49,909 1,494
Turkish Morpho Challenge 49,765 1,531
Estonian Morpho Challenge 49,909 1,500
50,000 1,000
50,000

Zulu
Arabic

Ukwabelana

PATB

–

–
–
–
–
–
–

1,000

Table 2: Data source and size information. TRAIN
= training corpus, DEV = development corpus
and TEST = test corpus.

unseen language we choose Arabic as it belongs
to the Semitic family, while none of the develop-
ment languages does. We obtain the Arabic data
by randomly selecting 50K words from the PATB
corpus (Maamourio et al., 2004). Table 2 lists the
sources and sizes of our corpora.

3 Method

Since we have nine grammars to choose from
(see Table 1) with two possible learning set-
ting (Standard and Cascaded),
for a total of
18 possible conﬁgurations, we restrict our pool
of selection to the four conﬁgurations that
yield the best-on-average performance across
the development languages, namely Cascaded
PrStSu+SM, Cascaded PrStSu2a+SM, Standard
PrStSu+SM and Standard PrStSu2a+SM, with av-
erage EMMA F-scores (Spiegler and Monson,
2010) of 0.720, 0.695, 0.684 and 0.683, respec-
tively (see Section 2 and Table 1 for grammar
descriptions). EMMA stands for the Evaluation
Metric for Morphological Analysis (Spiegler and

Monson, 2010), and is a metric that has been
shown to be particularly adequate for evaluating
unsupervised methods for morphological seg-
mentation and superior to the metric used in the
Morpho Challenge competition series.

We use a supervised machine learning ap-
proach to select the best conﬁguration. Since we
only have six development languages, we split
the classiﬁcation task into two binary classiﬁ-
cation ones: Approach Classiﬁcation (Standard
(Std) vs. Cascaded (Casc)) and Grammar Classi-
ﬁcation (PrStSu+SM vs. PrStSu2a+SM), and run
leave-one-out cross validation on the develop-
ment languages for both tasks. Table 3 lists the
best conﬁgurations and the gold class labels (for
both Approach and Grammar) for the six devel-
opment languages.

Language

Conﬁguration

class

class

Best

Approach

Grammar

Std PrStSu+SM
Std PrStSu+SM

Std
English
German
Std
Finnish Casc PrStSu2a+SM Casc
Turkish
Std
Estonian Casc PrStSu2a+SM Casc
Casc

Casc PrStSu+SM

Std PrStSu+SM

Zulu

PrStSu+SM
PrStSu+SM
PrStSu2a+SM
PrStSu+SM
PrStSu2a+SM
PrStSu+SM

Table 3: The best conﬁgurations and the gold class labels
for both the Approach classiﬁcation and Grammar classiﬁ-
cation for the six development languages.

3.1 Feature Generation
In order to generate morphological features for
the classiﬁcation tasks, we run a phase of AG seg-
mentation using the Standard PrStSu+SM con-
ﬁguration, where we only run 50 optimization
iterations (i.e., one tenth of the number of it-
erations in a complete segmentation process as

80Feature ID

Description

F01
F02
F03
F04
F05
F06
F07
F08
F09
F10
F11
F12
F13
F14

Average no. of simple afﬁxes per word
Average no. of simple preﬁxes per word
Average no. of simple sufﬁxes per word

Average no. of characters per afﬁx

No. of distinct simple afﬁxes
No. of distinct simple preﬁxes
No. of distinct simple sufﬁxes

Average no. of complex afﬁxes per word
Average no. of complex preﬁxes per word
Average no. of complex sufﬁxes per word

Average no. of characters per afﬁx

No. of distinct complex afﬁxes
No. of distinct complex preﬁxes
No. of distinct complex sufﬁxes

Table 4: Classiﬁcation features

Language

KNN

NB

RF

Std PrStSu+SM
Std PrStSu+SM

English
Std PrStSu+SM
German
Casc PrStSu+SM
Finnish Casc PrStSu2a+SM Casc PrStSu+SM (x) Casc PrStSu2a+SM
Turkish
Estonian Casc PrStSu2a+SM Casc PrStSu+SM (x) Std PrStSu2a+SM (x)

Std PrStSu+SM
Std PrStSu+SM

Std PrStSu+SM

Std PrStSu+SM

Std PrStSu+SM

Zulu

Casc PrStSu+SM Casc PrStSu+SM Casc PrStSu+SM

Accuracy

100.0%

66.7%

88.3%

Table 5: Overall system output. KNN = K-Nearest Neigh-
bors, NB = Naive Bayes and RF = Random Forest. Wrong
predictions are denoted by (x).

reported by Eskander et al. (2016)), as the pur-
pose is to quickly generate morphological clues
that help the classiﬁcation rather than to ob-
tain highly optimized segmentation. We choose
this particular conﬁguration due to its high efﬁ-
ciency across all languages in addition to its rela-
tively small execution time. Upon generating the
initial segmentation, we extract 14 morpholog-
ical features for classiﬁcation. The features are
listed in Table 4. We only consider afﬁxes that
appear more than 10 times in the segmentation
output, where a simple afﬁx contains only one
morpheme, while a complex afﬁx contains one
or more simple afﬁxes.

3.2 Classiﬁcation

We experiment with three classiﬁcation meth-
ods; K-Nearest Neighbors (KNN), Naive Bayes
(NB) and Random Forest (RF) for both the Ap-
proach (Std vs. Casc) and Grammar (PrStSu+SM
vs. PrStSu2a+SM) classiﬁcation tasks. We con-
duct the two classiﬁcation tasks separately, and
then we combine the outcome to obtain the best
conﬁguration.

In the training phase, we perform leave-one-
out cross validation on the six development lan-
guages.
In each of the six folds of the cross
validation, we choose one language in turn as
the test language. We use the training and de-

velopment corpora listed in table 2 for training
the models and evaluating the classiﬁers, respec-
tively.

Table 5 shows the ﬁnal system output af-
ter combining the outcomes from the Approach
classiﬁcation and Grammar Classiﬁcation. KNN
predicts the right conﬁguration consistently,
while NB picks the wrong grammars for Finnish
and Estonian, and RF predicts the wrong ap-
proach and grammar for Estonian. Thus, the
overall accuracies of KNN, NB and RF are 100%,
66.7% and 88.3%, respectively, which suggests
using KNN for classiﬁcation. So for an unseen
language, we ﬁrst run the Standard PrTuSu+SM
conﬁguration for 50 optimization iterations to
obtain the morphological features. We then run
the KNN classiﬁer on those features in order to
obtain the ﬁnal AG conﬁguration.

Studying the correlation between the mor-
phological features and the output shows that
features F14, F07, F11 and F03 in table 4, are
the most signiﬁcant ones for the selection of the
best conﬁguration. This illustrates the high re-
liance on information about sufﬁxes as three out
of the four features, namely F14, F07 and F03, are
sufﬁx-related.

4 Evaluation

We report results using the EMMA F-measure
score (Spiegler and Monson, 2010).

Results on an unseen language. We evaluate
our system on Arabic, a language that is not
part of the development of the system. Ara-
bic also belongs to the Semitic family, where
none of the development languages does. For
an unseen language, we ﬁrst run the Standard
PrStSu+SM conﬁguration for 50 optimization it-
erations to obtain the morphological features.
We then run the KNN classiﬁer on those features
in order to obtain the ﬁnal AG conﬁguration. Ta-
ble 6 lists the EMMA F-scores for Arabic for all
grammars in both the Standard and Cascaded
setups. Our KNN classiﬁer picks the Standard
PrStSu+SM conﬁguration, which yields the best
segmentation among all the conﬁgurations with
an EMMA F-score of 0.701.

Comparison with existing unsupervised ap-
proaches. Table 7 compares the performance
of the selected conﬁgurations of our system (Ta-
ble 5) to three other systems; Morfessor (Creutz
and Lagus, 2007), MorphoChain (Narasimhan
et al., 2015) and LIMS (Eskander et al., 2016)
(where the cascaded PrStSu+SM conﬁguration is

81Grammar
Morph+SM

Simple

Simple+SM

PrStSu

PrStSu+SM

PrStSu+Co+SM
PrStSu2a+SM
PrStSu2b+SM

PrStSu2b+Co+SM

Standard

Cascaded

0.647
0.651
0.680
0.642
0.701
0.648
0.676
0.682
0.532

0.642
0.593
0.631
0.646
0.692
0.628
0.682
0.688
0.532

Table 6: Adaptor-grammar results (Emma F-scores) for the
Standard and Cascaded setups for Arabic. Boldface indi-
cates the best conﬁguration and the choice of our system.

English
German
Finnish
Turkish

Grammar Morfessor MorphoChain LIMS Ours Best
0.809 0.821 0.826
0.777 0.790 0.790
0.727 0.733 0.733
0.591 0.647 0.647
0.611 0.611 0.611
0.805 0.828 0.847
0.682 0.701 0.701
0.715 0.733 0.736

0.805
0.740
0.675
0.551
0.414
0.779
0.779
0.678

0.746
0.625
0.621
0.551
0.390
0.679
0.751
0.623

Estonian
Arabic
Avg.

Zulu

Table 7: The performance of our system (Ours) compared
to Morfessor, MorphoChain, LIMS and an upper-bound
system (Best), using EMMA F-scores.

chosen). Our system has EMMA F-score error re-
ductions of 17.1%, 29.2% and 6.3% over Morfes-
sor, MorphoChain2 and LIMS, respectively, on
average across the development languages and
Arabic. It is also only 0.003 of average EMMA F-
score behind an oracle system, where the best
conﬁguration is always selected (indicated as
Best). We are not able to compare versus the sys-
tem presented by Wang et al. (2016) as neither
their system nor their data is currently available.

5 Related Work

The ﬁrst work that utilizes AGs for unsupervised
morphological segmentation is introduced by
Johnson (2008), while Sirts and Goldwater (2013)
propose minimally supervised AG models of dif-
ferent tree structures for morphological segmen-
tation. The most recent work on using AGs for
morphological segmentation is proposed by Es-
kander et al. (2016), where they experiment with
several AG models based on different underly-
ing grammars and learning settings. They also
research the use of scholar knowledge seeded
in the grammar trees. This knowledge could be
gathered from grammar books or automatically
generated via bootstrapping. This paper extends
their work by proposing a machine learning ap-

2Since MorphoChain expects large corpora in order to
learn the morphological chains, it does not perform well
on the small corpora we use in our setup, where we experi-
ment with real conditions of low-resource languages.

proach to select the best language-independent
model for each language.

In addition to the use of AGs, several mod-
els have been successfully used for unsupervised
morphological segmentation such as genera-
tive probabilistic models (utilized by Morfessor
(Creutz and Lagus, 2007)), and log-linear models
using contextual and global features (Poon et al.,
2009). Narasimhan et al. (2015) use a discrimina-
tive model for unsupervised morphological seg-
mentation that integrates orthographic and se-
mantic properties of words. The model learns
morphological chains, where a chain extends a
base form available in the lexicon.

Another recent notable work is introduced by
Wang et al. (2016), who use neural networks for
unsupervised segmentation, where they build
LSTM (Hochreiter and Schmidhuber, 1997) ar-
chitectures to learn word structures in order
to predict morphological boundaries. Another
variation of the this approach is presented by
Yang et al. (2017), where they use partial-word
information as character bigram embeddings
and evaluate their work on Chinese.

6 Conclusion and Future Work

We have shown that our language-independent
classiﬁers improve the state-of-the-art unsuper-
vised morphological segmentation proposed by
Eskander et al. (2016) by making choices that op-
timize for a given language, rather than choosing
parameters for all languages based on averages
on the development languages.

In future work, we plan to conduct an extrinsic
evaluation on tasks that could beneﬁt from mor-
phological segmentation such as machine trans-
lation, information retrieval and summarization.
We also plan to optimize the segmentation mod-
els for those speciﬁc tasks.

Acknowledgements

This research is based upon work supported in
part by the Ofﬁce of the Director of National
Intelligence (ODNI), Intelligence Advanced Re-
search Projects Activity (IARPA), via contract #
FA8650-17-C-9117. The views and conclusions
contained herein are those of the authors and
should not be interpreted as necessarily repre-
senting the ofﬁcial policies, either expressed or
implied, of ODNI, IARPA, or the U.S. Govern-
ment. The U.S. Government is authorized to re-
produce and distribute reprints for governmen-
tal purposes notwithstanding any copyright an-
notation therein.

82References

Duygu Ataman, Matteo Negri, Marco Turchi, and
Marcello Federico. 2017. Linguistically mo-
tivated vocabulary reduction for neural ma-
chine translation from turkish to english. 108.

Mathias Creutz and Krista Lagus. 2007. Unsu-
pervised models for morpheme segmentation
and morphology learning. ACM Trans. Speech
Lang. Process., 4(1):3:1–3:34.

Ramy Eskander, Owen Rambow, and Tianchun
Extending the use of adap-
Yang. 2016.
tor grammars for unsupervised morphologi-
cal segmentation of unseen languages. In Pro-
ceedings of he Twenty-Sixth International Con-
ference on Computational Linguistics (COL-
ING), Osaka, Japan.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computa-
tion, 9(8):1735–1780.

Mark Johnson. 2008. Unsupervised word seg-
mentation for Sesotho using adaptor gram-
mars.
In Proceedings of the Tenth Meeting
of ACL Special Interest Group on Computa-
tional Morphology and Phonology, pages 20–
27, Columbus, Ohio. Association for Compu-
tational Linguistics.

Mark Johnson, Thomas L. Grifﬁths, and Sharon
Goldwater. 2007. Adaptor grammars: a frame-
work for specifying compositional nonpara-
metric bayesian models. In Advances in Neu-
ral Information Processing Systems 19, pages
641–648, Cambridge, MA. MIT Press.

Mohamed Maamourio, Ann Bies, Tim Buckwal-
ter, and Wigdan Mekki. 2004. Arabic treebank:
Building a large-scale annotated arabic cor-
pus.
In Proceedings of the NEMLAR Confer-
ence on Arabic Language Resources and Tools,
Cairo, Egypt.

Karthik Narasimhan, Regina Barzilay,

and
Tommi Jaakkola. 2015.
An unsupervised
method for uncovering morphological chains.
In Twelfth AAAI Conference on Artiﬁcial
Intelligence.

ThuyLinh Nguyen, Stephan Vogel, and Noah A.
Smith. 2010. Nonparametric word segmenta-
tion for machine translation. In Proceedings of
the 23rd International Conference on Compu-
tational Linguistics, COLING ’10, pages 815–
823, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Hoifung Poon, Colin Cherry, and Kristina
Toutanova. 2009. Unsupervised morpholog-
ical segmentation with log-linear models.
In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of
the North American Chapter of the Associ-
ation for Computational Linguistics, pages
209–217, Boulder, Colorado. Association for
Computational Linguistics.

Kairit Sirts and Sharon Goldwater. 2013.
Minimally-supervised morphological
seg-
mentation using adaptor grammars. Trans-
actions of the Association for Computational
Linguistics, 1(May):231–242.

Sebastian Spiegler and Christian Monson. 2010.
Emma: A novel evaluation metric for morpho-
logical analysis. In Proceedings of the 23rd In-
ternational Conference on Computational Lin-
guistics (Coling 2010), pages 1029–1037, Bei-
jing, China. Coling 2010 Organizing Commit-
tee.

Sebastian Spiegler, Andrew van der Spuy, and Pe-
ter A. Flach. 2010. Ukwabelana - an open-
source morphological zulu corpus.
In Pro-
ceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010),
pages 1020–1028, Beijing, China. Coling 2010
Organizing Committee.

Linlin Wang, Zhu Cao, Yu Xia, and Gerard
de Melo. 2016. Morphological segmentation
with window LSTM neural networks. In Thir-
tieth AAAI Conference on Artiﬁcial Intelligence.

Karthik Narasimhan, Damianos Karakos,
Richard M. Schwartz, Stavros Tsakalidis,
and Regina Barzilay. 2014. Morphologi-
cal segmentation for keyword spotting.
In
EMNLP.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neu-
ral word segmentation with rich pretraining.
In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics,
ACL, Vancouver, Canada.

83
Assessing the Impact of Incremental Error Detection and Correction.

A Case Study on the Italian Universal Dependency Treebank

Chiara Alzetta(cid:63), Felice Dell’Orletta(cid:5), Simonetta Montemagni(cid:5), Maria Simi•, Giulia Venturi(cid:5)

(cid:5)Istituto di Linguistica Computazionale “Antonio Zampolli” (ILC-CNR), Pisa

(cid:63)Universit`a degli Studi di Genova

ItaliaNLP Lab - www.italianlp.it

•Dipartimento di Informatica, Universit`a di Pisa

chiara.alzetta@edu.unige.it,

{felice.dellorletta,simonetta.montemagni,giulia.venturi}@ilc.cnr.it

simi@di.unipi.it

Abstract

Detection and correction of errors and incon-
sistencies in “gold treebanks” are becoming
more and more central topics of corpus anno-
tation. The paper illustrates a new incremental
method for enhancing treebanks, with partic-
ular emphasis on the extension of error pat-
terns across different textual genres and reg-
isters.
Impact and role of corrections have
been assessed in a dependency parsing exper-
iment carried out with four different parsers,
whose results are promising. For both eval-
uation datasets, the performance of parsers in-
creases, in terms of the standard LAS and UAS
measures and of a more focused measure tak-
ing into account only relations involved in er-
ror patterns, and at the level of individual de-
pendencies.

Introduction

1
Over the last years, many approaches to detect er-
rors and inconsistencies in treebanks have been
devised (Dickinson, 2015). They can be catego-
rized in two main groups, depending on whether
the proposed quality check procedure relies on
heuristic patterns (Dickinson and Meurers, 2003,
2005; Boyd et al., 2008) or on statistical methods
(Ambati et al., 2011). More recently, the Univer-
sal Dependencies (UD) initiative (Nivre, 2015) has
yielded a renewed interest as shown by the meth-
ods and tools introduced by de Marneffe et al.
(2017); Alzetta et al. (2018); Wisniewski (2018).
A number of reasons prompted the importance of
these methods:
they can be useful to check the
internal coherence of the newly created treebanks
with respect to other treebanks created for a same
language or to the annotation guidelines. The risk
of inconsistencies or errors is considerable if we
consider that 70% of the released UD treebanks
originate from a conversion process and only 29%
of them has been manually revised after automatic

conversion. In this paper, we extend the method
proposed by Alzetta et al. (2018) for error detec-
tion and correction in “gold treebanks” and we
evaluate its impact on parsing results.

2

Incremental Approach to Error
Detection

Detection of annotation errors is often depicted as
a two–stage static process, which consists in ﬁnd-
ing errors in a corpus and correcting them. Dick-
inson and Tuﬁs (2017) provide a broader view of
the task of improving the annotation of corpora,
referred to as iterative enhancement: “iterative en-
hancement encompasses techniques that can be it-
erated, improving the resource with every pass”.
Surveyed methods for iterative enhancement are
applied to both corpora with (mostly) completed
annotation and corpora with in–progress annota-
tion. In our opinion, the strategy of iterative en-
hancement is particularly relevant in the construc-
tion of treebanks which result from the conversion
of pre-existing resources, as it is more often the
case, and/or whose annotation scheme is continu-
ously evolving e.g. to accommodate new linguis-
tic phenomena or to increase cross-lingual consis-
tency, as it happens in the Universal Dependencies
(UD) initiative1. In this paper, the error detection
method proposed by Alzetta et al. (2018) is incre-
mentally extended to deal with other corpus sec-
tions from other domains and registers: this can be
seen as a ﬁrst step of an iterative enhancement ap-
proach, which represents one of the currently ex-
plored lines of research.

Alzetta et al. (2018) proposed an original error
detection and correction method which represents
the starting point for the case study reported in this
paper. The method, tested against the Italian Uni-
versal Dependency Treebank (henceforth IUDT)

1http://universaldependencies.org/

ProceedingsoftheSecondWorkshoponUniversalDependencies(UDW2018),pages1–7Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics1(Bosco et al., 2013), mainly targets systematic er-
rors, which represent potentially “dangerous” re-
lations providing systematic but misleading evi-
dence to a parser. Note that with systematic er-
rors we refer here to both real errors as well as an-
notation inconsistencies internal to the treebank,
whose origin can be traced back to different anno-
tation guidelines underlying the source treebanks,
or that are connected with substantial changes in
the annotation guidelines (e.g. from version 1.4 to
2.0).

This error detection methodology is based on
an algorithm, LISCA (LInguiStically–driven Se-
lection of Correct Arcs) (Dell’Orletta et al., 2013),
originally developed to measure the reliability of
automatically produced dependency relations that
are ranked from correct to anomalous ones, with
the latter potentially including incorrect ones. The
process is carried out through the following steps:
• LISCA collects statistics about a wide range of
linguistic features extracted from a large refer-
ence corpus of automatically parsed sentences.
These features are both local, corresponding to
the characteristics of the syntactic arc consid-
ered (e.g. the linear distance in terms of tokens
between a dependent d and its syntactic head h),
and global, locating the considered arc within
the overall syntactic structure, with respect to
both hierarchical structure and linear ordering
of words (e.g. the number of “siblings” and
“children” nodes of d, recurring respectively to
its right or left in the linear order of the sen-
tence; the distance from the root node, the closer
and furthest leaf node);

• collected statistics are used to assign a qual-
ity score to each arc contained in a target cor-
pus (e.g. a treebank). To avoid possible inter-
ferences in detecting anomalies which are due
to the variety of language taken into account
rather than erroneous annotations, both refer-
ence and target corpora should belong to the
same textual genre or register. On the basis of
the assigned score, arcs are ranked by decreas-
ing quality scores;

• the resulting ranking of arcs in the target cor-
pus is partitioned into 10 groups of equivalent
size. Starting from the assumption that anoma-
lous annotations (i.e. dependencies which to-
gether with their context occurrence are de-
viant from the “linguistic norm” computed by

LISCA on the basis of the evidence acquired
from the reference corpus) concentrate in the
bottom groups of the ranking, the manual search
of error patterns is restricted to the last groups.
Detected anomalous annotations include both
systematic and random errors. Systematic er-
rors, formalized as error patterns, are looked for
in the whole target corpus, matching contexts
are manually revised and, if needed, corrected.

The methodology was tested against the news-
paper section of the Italian Universal Dependency
Treebank (henceforth IUDT–news), which is com-
posed by 10,891 sentences, for a total of 154,784
tokens. In this paper, the error detection and cor-
rection method depicted above is extended to other
sections of the IUDT treebank, containing texts
belonging to different genres (namely, legal and
encyclopedic texts).

3

Incremental Enhancement of IUDT

The incremental error detection strategy depicted
in Section 2 was used to improve IUDT version
2.0 (ofﬁcially released in March 2017).
IUDT
2.0 is the result of an automatic conversion pro-
cess from the previous version (IUDT 1.4), which
was needed because of major changes in the an-
notation guidelines for speciﬁc constructions and
new dependencies in the Universal Dependencies
(UD) tagset2.
In spite of the fact that this pro-
cess was followed by a manual revision target-
ing speciﬁc constructions, the resulting treebank
needed a quality check in order to guarantee ho-
mogeneity and coherence to the resource: it is a
widely acknowledged fact that automatic conver-
sion may cause internal inconsistencies, typically
corresponding to systematic errors.

The ﬁrst step of this revision process is de-
scribed in Alzetta et al. (2018), which led to IUDT
version 2.1, released in November 2017. At this
stage, 0.51% dependency relations of IUDT–news
were modiﬁed (789 arcs): among them, 286 arcs
(36.01%) turned out to be random errors, while
503 (63.99%) represent systematic errors.

For the latest published version of IUDT (i.e.
2.2, released in July 2018), error patterns identi-
ﬁed in IUDT–news were matched against the other
sections of IUDT, which contain legal texts and
Wikipedia pages. Although error patterns were ac-
quired from IUDT–news, their occurrence in the

2http://universaldependencies.org/v2/summary.html

2other two sections of the treebank turned out to
be equivalent. In particular, modiﬁed arcs corre-
sponding to systematic errors are 0.36% in IUDT–
news, 0.34% in IUDT–Wikipedia and 0.35% in
IUDT–legal, for a total amount of 1028 deprels,
525 of which were modiﬁed in the passage from
version 2.0 to version 2.1. This result proves the
effectiveness of the methodology: despite of the
fact that error patterns were retrieved in a signif-
icantly limited search space of the news section
of the treebank (covering about 25% of the to-
tal number of arcs in IUDT–news), they turned
out to be general enough to be valid for the other
language registers represented by the other IUDT
sub–corpora.

Version 2.2 of IUDT has been further improved:
the result is IUDT version 2.3, still unpublished.
In this version, residual cases instantiating error
patterns were corrected and instances of one of the
six error patterns (concerned with nonﬁnite ver-
bal constructions functioning as nominals) were
reported to the original annotation, since we ob-
served that the proposed annotation was no longer
convincing on the basis of some of the new in-
stances that were found.

Overall, from IUDT version 2.0 to 2.3, a to-
tal of 2,237 dependency relations was modiﬁed:
50.91% of them (corresponding to 1,139 arcs)
represented systematic errors, while 49.08% (i.e.
1,098 arcs) contained non–pattern errors. Among
the latter, 25.77% are random errors (286 arcs),
while 74.22% are structural errors (i.e. 815 erro-
neous non-projective arcs).

4 Experiments

In order to test the impact of the result of our incre-
mental treebank enhancement approach, we com-
pared the dependency parsing results achieved us-
ing IUDT versions 2.0 vs 2.3 for training.

4.1 Experimental Setup
Data. Although the overall size of IUDT changed
across the 2.0 and 2.3 versions, we used two
equivalent training sets of 265,554 tokens to train
the parsers, containing exactly the same texts but
different annotations. For both sets of experi-
ments, parser performances were tested against a
dev(elopment) set of 10,490 tokens and a test set
of 7,545 tokens, differing again at the annotation
level only. Parsers. Four different parsers were
selected for the experiments, differing at the level

of the used parsing algorithm. The conﬁgurations
of the parsers were kept the same across all exper-
iments.
DeSR MLP is a transition-based parser that uses
a Multi-Layer Perceptron (Attardi, 2006; At-
tardi et al., 2009), selected as representative of
transition-based parsers. The best conﬁguration
for UD, which uses a rich set of features including
third order ones and a graph score, is described in
Attardi et al. (2015). We trained it on 300 hidden
variables, with a learning rate of 0.01, and early
stopping when validation accuracy reaches 99.5%.
TurboParser (Martins et al., 2013) is a graph-
based parser that uses third-order feature models
and a specialized accelerated dual decomposition
algorithm for making non-projective parsing com-
putationally feasible. It was used in conﬁguration
“full”, enabling all third-order features.
Mate is a graph-based parser that uses passive ag-
gressive perceptron and exploits a rich feature set
(Bohnet, 2010). Among the conﬁgurable parame-
ters, we set to 25 the numbers of iterations. Mate
was used in the pure graph version.
UDPipe is a trainable pipeline for tokenization,
tagging, lemmatization and dependency parsing
(Straka and Strakov´a, 2017). The transition-based
parser provided with the pipeline is based on a
non-recurrent neural network, with just one hidden
layer, with locally normalized scores. We used the
parser in the basic conﬁguration provided for the
CoNLL 2017 Shared Task on Dependency Pars-
ing.
Evaluation Metrics. The performance of parsers
was assessed in terms of the standard evaluation
metrics of dependency parsing, i.e. Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (UAS). To assess the impact of the correc-
tion of systematic errors, we devised a new metric
inspired by the Content-word Labeled Attachment
Score (CLAS) introduced for the CoNLL 2017
Shared Task (Zeman and al., 2017). Similarly to
CLAS, the new metric focuses on a selection of
dependencies: whereas CLAS focuses on relations
between content words only, our metric is com-
puted by only considering those dependencies di-
rectly or indirectly involved in the pattern–based
error correction process. Table 2 reports the list of
UD dependencies involved in error patterns: it in-
cludes both modiﬁed and modifying dependencies
occurring in the rewriting rules formalizing error
patterns. Henceforth, we will refer to this metric

3as Selected Labeled Attachment Score (SLAS).

4.2 Parsing Results

The experiments were carried out to assess the im-
pact on parsing of the corrections in the IUDT
version 2.3 with respect to version 2.0. Table 1
reports the results of the four parsers in terms of
LAS, UAS and SLAS achieved against the IUDT
dev and test
sets of the corresponding releases
(2.0 vs 2.3).
It can be noticed that all parsers
improve their performance when trained on ver-
sion 2.3, against both the test set and the dev set.
The only exception is represented by UDPipe for
which a slightly LAS decrease is recorded for the
dev set, i.e.
-0.12%; note, however, that for the
same dev set UAS increases (+0.12%). The aver-
age improvement for LAS and UAS measures is
higher for the test set than for the dev set: +0.38%
vs +0.17% for LAS, and +0.35% vs +0.23% for
UAS. The higher improvement is obtained by UD-
Pipe (+0.91% LAS, +0.69% UAS) on the test set.
Besides standard measures such as LAS and
UAS, we devised an additional evaluation measure
aimed at investigating the impact of the pattern–
based error correction, SLAS, described in Section
4.1. As it can be seen in Table 1, for all parsers
the gain in terms of SLAS is signiﬁcantly higher:
the average improvement for the test set and the
dev set is +0.57% and +0.47% respectively. It is
also interesting to note that the SLAS values for
the two data sets are much closer than in the case
of LAS and UAS, suggesting that the higher differ-
ence recorded for the general LAS and UAS mea-
sures possibly originates in other relations types
and corrections (we are currently investigating this
hypothesis). This result shows that SLAS is able
to intercept the higher accuracy in the prediction of
dependency types involved in the error patterns.

To better assess the impact of pattern–based er-
ror correction we focused on individual dependen-
cies involved in the error patterns, both modiﬁed
and modifying ones. This analysis is restricted to
the output of the MATE parser, for which a lower
average SLAS improvement is recorded (0.34).
For both dev and test sets versions 2.0 and 2.3, Ta-
ble 2 reports, for each relation type, the number of
occurrences in the gold dataset (column “gold”),
the number of correct predictions by the parser
(column “correct”) and the number of predicted
dependencies, including erroneous ones (column
“sys”). For this dependency subset, an overall re-

duction of the number of errors can be observed
for both evaluation sets. The picture is more artic-
ulated if we consider individual dependencies. For
most of them, both precision and recall increase
from version 2.0 to 2.3. There are however few
exceptions: e.g. in the 2.3 version, the number of
errors is slightly higher for the aux relation in both
dev and test datasets (+4 and +1 respectively), or
the acl relation in the dev set (+3).

Table 3 reports, for the same set of relations, the
recorded F-measure (F1), accounting for both pre-
cision and recall achieved by the MATE parser for
individual dependencies:
interesting differences
can be noted at the level of the distribution of F1
values in column “Diff”, where positive values re-
fer to a gain. Out of the 14 selected dependen-
cies, a F1 gain is reported for 10 relations in the
dev set, and for 8 in the test set. Typically, a gain
in F1 corresponds to a reduction in the number
of errors. Consider, for example, the cc depen-
dency involved in a head identiﬁcation error pat-
tern (conj head), where in speciﬁc construc-
tions a coordinating conjunction was erroneously
headed by the ﬁrst conjunct (coordination head)
rather than by the second one (this follows from
a change in the UD guidelines from version 1.4
to 2.0): in this case, F1 increases for both eval-
uation datasets (+1.55 and +2.77) and errors de-
crease (-5 and -6). However, it is not always the
case that a decrease of the F1 value is accompa-
nied by a higher number of errors for the same re-
lation. Consider, for example, the acl relation for
which F1 decreases signiﬁcantly in version 2.3 of
both dev and test datasets (-6.97 and -4.59). The
acl relation is involved in a labeling error pattern
(acl4amod), where adjectival modiﬁers of nouns
(amod) were originally annotated as clausal mod-
iﬁers. Whereas in the dev set 2.3 the F1 value for
acl decreases and the number of errors increase,
in the test set 2.3 we observe a decrease in F1 (-
4.59%) accompanied by a reduction of the number
of errors (-1). The latter case combines apparently
contrasting facts: note, however, that the loss in
F1 is also inﬂuenced by the reduction of acl oc-
currences, some of which were transformed into
amod in version 2.3.

Last but not least, we carried out the same type
of evaluation on the subset of sentences in the de-
velopment dataset which contain at least one in-
stance of the error patterns: we call it Pattern Cor-
pus. For this subset the values of LAS, UAS and

4Dev 2.0
Dev 2.3
Diff.
Test 2.0
Test 2.3
Diff.

LAS
87.89
87.92
0.03
89.00
89.16
0.16

DeSR MLP

UAS
91.18
91.23
0.05
91.99
92.07
0.08

SLAS
81.10
81.48
0.38
82.59
83.14
0.55

MATE
UAS
92.95
93.28
0.33
93.25
93.70
0.45

SLAS
85.82
86.28
0.46
86.08
86.30
0.22

LAS
90.73
90.99
0.26
91.13
91.41
0.28

TurboParser

LAS
89.83
90.34
0.51
90.39
90.54
0.15

UAS
92.72
93.14
0.42
93.33
93.49
0.16

SLAS
84.10
84.98
0.88
84.78
85.00
0.22

UDPipe

UAS
90.14
90.26
0.12
90.38
91.07
0.69

SLAS
79.11
79.25
0.14
79.66
80.95
1.29

LAS
87.02
86.90
-0.12
87.21
88.12
0.91

Table 1: Evaluation of the parsers against the IUDT test and development sets version 2.0 and 2.3.

deprel

acl
acl:relcl
amod
aux
aux:pass
cc
ccomp
conj
cop
nmod
obj
obl
obl:agent
xcomp
TOTAL

gold
151
137
637
218
78
325
62
372
126
977
412
678
43
92
4308

Development

correct
118
106
606
208
69
305
43
289
100
828
372
541
39
73
3697

IUDT 2.0

sys
146
131
636
229
84
323
61
403
117
986
438
640
46
84
4324

gold
83
100
455
172
69
217
29
253
85
710
275
523
39
58
3068

Test
correct
71
77
439
162
64
194
19
175
79
612
247
427
36
39
2641

sys
86
100
455
167
74
217
32
251
87
723
291
504
38
47
3072

gold
115
137
667
217
79
326
62
370
126
976
413
681
43
96
4308

Development

correct
83
112
641
206
71
311
46
281
101
827
374
551
40
73
3717

IUDT 2.3

sys
114
138
669
231
85
324
63
394
113
976
433
648
45
86
4319

gold
71
101
460
172
69
217
30
257
85
705
275
523
39
62
3066

Test
correct
56
80
445
159
63
200
19
178
80
615
247
425
36
43
2646

sys
70
100
464
165
76
217
27
252
89
725
288
503
39
53
3068

Table 2: Statistics of individual dependencies involved in an error pattern in the test and development sets of
IUDT 2.0 and 2.3 (gold). sys refers to the number of predicted dependencies by the MATE parser and correct to
the correct predictions.

deprel
acl
acl:relcl
amod
aux
aux:pass
cc
ccomp
conj
cop
nmod
obj
obl
obl:agent
xcomp

Development

F1 2.0
79.46
79.11
95.20
93.06
85.18
94.14
69.92
74.58
82.31
84.36
87.53
82.09
87.64
82.95

F1 2.3
72.49
81.45
95.95
91.97
86.58
95.69
73.60
73.56
84.52
84.73
88.42
82.92
90.91
80.22

Diff
-6.97
2.35
0.75
-1.10
1.40
1.55
3.68
-1.02
2.21
0.37
0.89
0.83
3.27
-2.74

F1 2.0
84.02
77.00
96.48
95.58
89.51
89.40
62.30
69.44
91.86
85.42
87.28
83.15
93.51
74.29

Test
F1 2.3
79.43
79.60
96.32
94.36
86.89
92.17
66.66
69.94
91.96
86.01
87.74
82.84
92.31
74.78

Diff
-4.59
2.60
-0.16
-1.22
-2.62
2.77
4.37
0.49
0.10
0.60
0.46
-0.31
-1.20
0.49

Table 3: F1 scores and differences for a selection of individual dependencies involved in error patterns by the
MATE parser trained on IUDT 2.0 and 2.3.

SLAS for the MATE parser are much higher, rang-
ing between 98.17 and 98.93 for the Pattern cor-
pus 2.0, and between 98.58 and 99.38 for the Pat-
tern corpus 2.3. The gain is in line with what re-
ported in Table 1 for MATE, higher for what con-
cerns LAS (+0.36) and UAS (+0.45), and slightly
lower for SLAS (+0.41). Trends similar to the
full evaluation datasets are reported also for the

dependency-based analysis, which shows however
higher F1 values.
5 Conclusion
In this paper, the treebank enhancement method
proposed by Alzetta et al. (2018) was further ex-
tended and the annotation quality of the resulting
treebank was assessed in a parsing experiment car-

5ried out with IUDT version 2.0 vs 2.3.

Error patterns identiﬁed in the news section of
the IUDT treebank were looked for in the other
IUDT sections, representative of other domains
and language registers. Interestingly, however, er-
ror patters acquired from IUDT-news turned out
to be characterized by a similar distribution across
different treebank sections, which demonstrates
their generality.

The resulting treebank was used to train and
test four different parsers with the ﬁnal aim of
assessing quality and consistency of the annota-
tion. Achieved results are promising:
for both
evaluation datasets all parsers show a performance
increase (with a minor exception only), in terms
of the standard LAS and UAS as well as of the
more focused SLAS measure. A dependency-
based analysis was also carried out for the rela-
tions involved in error patterns: for most of them,
a more or less signiﬁcant gain in the F-measure is
reported.

Current developments include: i) extension of
the incremental treebank enhancement method by
iterating the basic steps reported in the paper to
identify new error patterns in the other treebank
subsections using LISCA; ii) extension of the in-
cremental treebank enhancement method to other
UD treebanks for different languages; iii) exten-
sion of the treebank enhancement method to iden-
tify and correct random errors.

Acknowledgements

We thank the two anonymous reviewers whose
comments and suggestions helped us to improve
and clarify the submitted version of the paper. The
work reported in the paper was partially supported
by the 2–year project (2016-2018) Smart News,
Social sensing for breaking news, funded by Re-
gione Toscana (BANDO FAR-FAS 2014).

References

C. Alzetta, F. Dell’Orletta, S. Montemagni, and G. Ven-
turi. 2018. Dangerous relations in dependency tree-
banks. In Proceedings of 16th International Work-
shop on Treebanks and Linguistic Theories (TLT16),
pages 201–210, Prague, Czech Republic.

B. R. Ambati, R. Agarwal, M. Gupta, S. Husain, and
D. M. Sharma. 2011. Error Detection for Tree-
bank Validation. In Proceedings of 9th International
Workshop on Asian Language Resources (ALR).

Giuseppe Attardi. 2006. Experiments with a multilan-
In Pro-
guage non-projective dependency parser.
ceedings of the Tenth Conference on Computational
Natural Language Learning, CoNLL-X ’06, pages
166–170, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Giuseppe Attardi, Felice Dell’Orletta, Maria Simi, and
Joseph Turian. 2009. Accurate dependency parsing
with a stacked multilayer perceptron. In Proceeding
of Evalita 2009, LNCS. Springer.

Giuseppe Attardi, Simone Saletti, and Maria Simi.
2015. Evolution of italian treebank and depen-
dency parsing towards universal dependencies.
In
Proceedings of the Second Italian Conference on
Computational Linguistics, CLIC-it 2015, pages 23–
30, Torino, Italy. Accademia University Press/Open
Editions.

Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 89–97,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

C. Bosco, S. Montemagni, and M. Simi. 2013. Con-
verting Italian Treebanks: Towards an Italian Stan-
ford Dependency Treebank. In Proceedings of the
ACL Linguistic Annotation Workshop & Interoper-
ability with Discourse, Soﬁa, Bulgaria.

A. Boyd, M. Dickinson, and W. D. Meurers. 2008.
On Detecting Errors in Dependency Treebanks. Re-
search on Language & Computation, 6(2):113–137.

F. Dell’Orletta, G. Venturi, and S. Montemagni. 2013.
Linguistically-driven Selection of Correct Arcs for
Computaci`on y Sistemas,
Dependency Parsing.
2:125–136.

M. Dickinson. 2015. Detection of Annotation Errors
in Corpora. Language and Linguistics Compass,
9(3):119–138.

M. Dickinson and W. D. Meurers. 2003. Detecting
Inconsistencies in Treebank. In Proceedings of the
Second Workshop on Treebanks and Linguistic The-
ories (TLT 2003).

M. Dickinson and W. D. Meurers. 2005. Detecting
Errors in Discontinuous Structural Annotation.
In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 322–329.

M. Dickinson and D. Tuﬁs. 2017.

Iterative enhance-
ment. In Handbook of Linguistic Annotation, pages
257–276. Springer, Berlin, Germany.

M.C. de Marneffe, M. Grioni, J. Kanerva, and F. Gin-
ter. 2017. Assessing the Annotation Consistency of
In Proceed-
the Universal Dependencies Corpora.
ings of the 4th International Conference on Depen-
dency Linguistics (Depling 2007), pages 108–115,
Pisa, Italy.

6A. Martins, M. Almeida, and N. A. Smith. 2013. ”turn-
ing on the turbo: Fast third-order non-projective
turbo parsers”. In Annual Meeting of the Associa-
tion for Computational Linguistics - ACL, volume -,
pages 617–622.

J. Nivre. 2015. Towards a Universal Grammar for Nat-
In Computational Lin-
ural Language Processing.
guistics and Intelligent Text Processing - Proceed-
ings of the 16th International Conference, CICLing
2015, Part I, pages 3–16, Cairo, Egypt.

Milan Straka and Jana Strakov´a. 2017. Tokenizing,
pos tagging, lemmatizing and parsing ud 2.0 with
udpipe. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Univer-
sal Dependencies, pages 88–99, Vancouver, Canada.
Association for Computational Linguistics.

G. Wisniewski. 2018. Errator: a tool to help detect an-
notation errors in the universal dependencies project.
In Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), pages 4489–4493, Miyazaki, Japan.

D. Zeman and al. 2017. CoNLL 2017 shared task:
Multilingual parsing from raw text to universal de-
In Proceedings of the CoNLL 2017
pendencies.
Shared Task: Multilingual Parsing from Raw Text
to Universal Dependencies, pages 1–19, Vancouver,
Canada.

7
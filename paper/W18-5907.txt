Identiﬁcation of Emergency Blood Donation Request on Twitter

Puneet Mathur1, Meghna Ayyar2, Sahil Chopra3,

Simra Shahid4, Laiba Mehnaz 4, and Rajiv Ratn Shah2

1Netaji Subhas Institute of Technology, NSIT-Delhi

pmathur3k6@gmail.com

2Indraprastha Institute of Information Technology, IIIT-Delhi

{meghnaa,rajivratn}@iiitd.ac.in

3Maharaja Surajmal Institute of Technology, MSIT-Delhi

sahilc.msit@gmail.com

4Delhi Technolological University, DTU-Delhi

{simrashahid_bt2k16,laibamehnaz}@dtu.ac.in

Abstract

Social media-based text mining in healthcare
has received special attention in recent times
due to the enhanced accessibility of social me-
dia sites like Twitter. The increasing trend
of spreading important
information in dis-
tress can help patients reach out to prospective
blood donors in a time bound manner. How-
ever such manual efforts are mostly inefﬁcient
due to the limited network of a user. In a novel
step to solve this problem, we present an an-
notated Emergency Blood Donation Request
(EBDR) dataset1 to classify tweets referring to
the necessity of urgent blood donation require-
ment. Additionally, we also present an auto-
mated feature-based SVM classiﬁcation tech-
nique that can help selective EBDR tweets
reach relevant personals as well as medical
authorities. Our experiments also present a
quantitative evidence that linguistic along with
handcrafted heuristics can act as the most rep-
resentative set of signals this task with an ac-
curacy of 97.89%.
Introduction

1
Sufﬁciency in the availability of blood in emer-
gency situations can dramatically improve the life
expectancy and quality of lives of patients in
chronic medical conditions. However, many pa-
tients still suffer due to the dual challenges of
timely availability and shortfall of required whole
blood and blood components.
In the case of
countries with low rates of blood donation record,
blood donation is largely dependent on the fam-
ilies and friends of patients, usually through the
word of mouth or peer to peer networking. With
the increasing accessibility of social media web-
sites, several instances have emerged where the
1The dataset and code are available for research purposes

at https://github.com/pmathur5k10/EBDR

friends and the family of the patients in need of
a blood transfusion have tried to voice their urgent
need of blood donation through social media chan-
nels. They have reached out to the online com-
munity through tweets, Facebook posts, and status
updates on popular social media platforms. The
effect of such tweets in an emergency situation is
largely limited to a user’s ﬁrst few degrees of con-
nections. Thus, it fails to reach the desired donor
within the stipulated critical time.
Problem Statement: Emergency Blood Donation
Request (EBDR) detection is the task of identify-
ing tweets that explicitly or implicitly mention a
necessity for an urgent blood donor.
1.1 Challenges
The key challenges in preparing the corpus are:

1. More than one topic categorization: The
task of EBDR tweets prediction is not lim-
ited to segregation into binary classes on the
basis of certain keywords (like urgent need,
blood required, etc.). It involves identiﬁca-
tion of multiple textual modalities such as
blood group, quantity of blood required, the
disease being treated and presence of per-
sonal details for authentication.

2. Multiple instances of retweets: It is difﬁ-
cult to obtain a unique set of EBDR tweets
as many of the instances of such tweets ex-
tracted through the search API are retweets
by immediate connections of the original
tweet author.
the tweets
describing the same events are spread by
rephrasing, morphing or editing the original
tweets causing duplication of tweet instances.

In many cases,

1.2 Contributions
The main contributions can be summarized as:

Proceedingsofthe3rdSocialMediaMiningforHealthApplications(SMM4H)Workshop&SharedTask,pages27–31Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics27• Building a corpus of annotated emergency
blood donation request tweets divided into
three separate datasets using different tweet
extraction methodologies.

• Extraction of ancillary handcrafted features
from tweets pertaining to the speciﬁcations of
the requested blood donation.

• Feature modeling using four independent sets
of tweet features:
linguistic, handcrafted,
user speciﬁc metadata and textual metadata
for the purpose of tweet classiﬁcation fol-
lowed by determination of the most relevant
set of auxiliary features for SVM based clas-
siﬁcation.

2 Related Work
Several successful attempts in health text mining
have shown that social media can act as a rich
source of information for public health monitoring
(Broniatowski et al., 2015). MA and Eldredge de-
veloped an annotated dataset from consumer drug
review posts on social media. Twitter data has
been used previously for identifying mentions of
medication intake (Mahata et al., 2018a,b), mon-
itoring prescription drug abuse (Hanson et al.,
2013). The domain of health text mining also ex-
tends to include mental health. Past work on iden-
tifying hateful behaviour (Mathur et al., 2018a,b)
and suicidal behavior on Twitter (Sawhney et al.,
2018a).
3 Dataset
3.1 Dataset Creation
The tweets were collected between 10 May 2018
to 10 July 2018 using Twitter Streaming API. The
major problem of extracting tweets for solicitation
of blood donation was the infrequent and sporadic
nature of their occurrence. Due to the limitation of
time restriction imposed on the search query based
retrieval, two parallel strategies were developed to
build three separate datasets:

1. Personal Donation Requests(PDR) Dataset
(1311 EBDR, 1511 non-EBDR): A curated
list of 53 medical phrases was extracted
from selected online blood donation informa-
tion portals such as American Red Cross2,
Australian Red Cross3 and NHS Blood and
transplant4 by using TF/IDF (Ramos et al.,

2http://www.redcross.org/
3https://www.donateblood.com.au/
4https://www.blood.co.uk/

2003) to identify the most frequently occur-
ring terms. A few such phrases have been
depicted in Fig.1 which were used to query
tweets related to blood donation after remov-
ing the stop words. Apart from them, tweets
mined by using general medical terms were
incorporated to form the complete dataset.

2. Blood

Donation

Community(BDC)
Dataset (1889 EBDR, 3268 non-EBDR):
The list of users present in the tweets in
PDR dataset was obtained. Speciﬁc Twit-
ter handles of community blood donation
groups were identiﬁed from these users and
historical tweets from their timeline were
extracted for the positive class.
For the
negative class, past tweets from extraneous
users were collected.

3. Dataset HO: (741 EBDR, 1072 non-EBDR):
This represents the held-out dataset, with
tweets collected using both approaches stated
above and no overlap with PDR and BDC.

We utilized the traditional query based tweet min-
ing practice which made the PDR dataset more
generic in nature.
In addition, we employed
a Twitter handle supervision technique in BDC
dataset to focus on a larger tweet corpus heav-
ily speciﬁc to the positive class. Lastly, the held
out dataset was created using both the techniques
in conjunction for a fair assessment of real world
cases. The collected corpus of tweets was ﬁltered
down to remove tweets involving non-English text
using Ling-Pipe, non-Unicode characters, dupli-
cate tweets, and tweets containing only URL’s, im-
ages, videos or having less than 3 words.

Figure 1: Word cloud of EBDR tweets

3.2 Data Annotation
The datasets were annotated by two independent
human annotators. In the case of conﬂict amongst
the annotators, an NLP expert ﬁnally assigned the
ground truth annotation for ambiguous tweets. A
satisfactory agreement between the annotators was
inferred from Cohen’s Kappa score of 0.86 and the

28Feature
Category
Linguistic
features (L)

User metadata
(U)

Textual
metadata (T)

Handcrafted
features (H)

Attributes
Unigram & Bigram presence and count,
TF-IDF vector
Retweet count, presence of source of
posting, presence of place of posting,
user friends count, user followers count,
user favorites count, user status count
Count of URL’s, hashtags, user mentions
and special symbols
Presence of name of reference contact,
name of place of requirement, contact
number, name of hospital/blood bank,
blood group required, quantity of blood
required, patient disease information

Table 1: Features of tweets in the EBDR dataset.

inter-annotator agreement for the complete EBDR
dataset was 89.20%. The classiﬁcation was done
on account of observations stated below:

1. EBDR Tweets :

• Tweets describing a personal critical
medical situation indicative of urgent
blood requirement within a stipulated
timeframe; For instance, “@BDonors:
MOST URGENT B+ve Blood Donors
urgently required for a serious cancer
patient...”

• Tweets appealing for blood donation
due to a major crisis or mishap involving
casualties and loss of life; e.g., “Twin
blasts in city leave hundreds injured.
Request nrby residents having any of
A+, B- or O type blood to save precious
lives. Contact @username at 99123...”

2. Non-EBDR Tweets:

• Tweets with no motive to discuss blood
donation; e.g., “We will have to urgently
counter this bloody war to sustain our
basic living requirements”.

• Tweets related to general medical termi-
nology or about general awareness; e.g.,
“Iron helps in blood clotting...”.

• Promotional content highlighting the
usefulness of blood donation to reach
e.g., “Lets
out to a target audience.
pledge to donate blood every 2 months
to help people ﬁghting Leukemia”.

• Tweets such as “We thank @user for
registering as #AB- blood donor...” that
portray gratitude for blood donation.

• Tweets publicizing an offer to donate
blood might give rise to contextual bias;
e.g., “I will donate my rare O- blood...”

Handcrafted Features

Name of Reference contact

Place of requirement

Contact number

Hospital/Blood bank
Required blood group

Patient Disease Description
Quantity of blood required

Total EBDR tweets

PDR BDC HO
171
1117
1188
522
541
1142
525
1059
701
1227
109
80
156
64
1311
741

1513
1844
1783
1832
1829
267
842
1889

Table 2: Distribution of hand crafted features
of EBDR tweets across Personal Donation Re-
quest, Blood Donation Community and Held-Out
datasets

4 Feature Modeling
Table 1 presents the complete set of features cor-
responding to each annotated tweet. The feature
set is composed of four constituents: (i) linguis-
tic features, (ii) user metadata, (iii) textual meta-
data and (iv) handcrafted features. Linguistic fea-
tures consist of standard unigrams and bigrams
as n-grams features along with TF-IDF frequen-
cies that capture the syntactic as well as seman-
tic information. Tweet virality (Cha et al., 2010)
and user’s network worth (Recuero et al., 2011),
measured by the count of friends, followers, fa-
vorites and status effect, are necessary parameters
to gauge the ability to broadcast emergency mes-
sages through the social media network. A com-
mon observation during the tweet mining has been
the presence of hashtags, URL’s and user mentions
related to blood donation in EBDR tweets. For in-
stance, hashtags similar to #SaveLife, #BloodMat-
ters, #HelpEmergency were prominently present
in the positive category of EBDR dataset. Lastly,
several handcrafted elements including presence
of blood group, blood quantity required, the name
of the hospital or blood bank soliciting blood do-
nation on behalf of a patient, disease for which
blood transfusion is desired, name, place and con-
tact number of the patient; were extracted by hu-
man annotators. Personal details such as user
mentions, name, address and phone numbers of
patients and tweet posters were anonymized due
to privacy concerns of individuals. This resulted in
the accumulation of blood donation speciﬁc traits
as depicted in Table 2.
5 Evaluation
Table 3 shows the performance of datasets BDC,
PDR and HO, where the three datasets have been
trained using SVM classiﬁer (Chang and Lin,
2011) by taking a combination of one or more fea-
ture sets mentioned in Section 4. The train-test
split in each case was ﬁxed to 70:30 and stratiﬁed
ﬁve-fold cross-validation performance is reported

29PDR

Dataset
Feature Set Accuracy (%)
L
U
T
H
L+H
U+T
U+H
T+H
All

96.22
81.88
86.62
96.19
96.91
64.92
87.22
89.49
75.67

BDC

HO

F1-score
0.974
0.775
0.853
0.975
0.983
0.691
0.879
0.879
0.759

Precision Recall Accuracy (%)
0.979
0.759
0.801
0.971
0.921
0.780
0.814
0.801
0.761

97.01
51.67
81.62
96.59
96.99
77.48
80.80
88.26
77.11

0.968
0.817
0.887
0.982
0.986
0.649
0.873
0.836
0.723

F1-score
0.958
0.564
0.814
0.981
0.983
0.732
0.885
0.879
0.683

Precision Recall Accuracy (%)
0.986
0.512
0.812
0.985
0.985
0.744
0.885
0.823
0.824

0.945
0.598
0.816
0.979
0.979
0.774
0.896
0.884
0.771

97.12
70.18
85.58
97.01
97.89
48.48
78.59
89.99
76.96

F1-score
0.974
0.699
0.855
0.970
0.980
0.431
0.786
0.875
0.770

Precision Recall
0.986
0.973
0.701
0.698
0.858
0.861
0.983
0.920
0.982
0.971
0.484
0.647
0.853
0.785
0.870
0.830
0.840
0.769

Table 3: Results of SVM classiﬁer on PDR, BDC and HO datasets

to account for any imbalance of tweet classes in
the datasets that may occur.In each case, linguis-
tic features achieve a marginally better accuracy as
compared to the handcrafted features when trained
separately, but outperform all other combinations
of features when utilized in pair. The extensive
under-performance due to inclusion of textual and
user metadata prove that these feature sets poorly
correlate with the positive class. Dataset BDC
consists of a more number of samples having a
direct correlation with emergency blood donation
requests, as opposed to dataset PDR having a
greater abundance of samples relevant to the topic
of blood donation. This leads to a higher preci-
sion but lower recall in evaluation of Linguistic
and Handcrafted feature based PDR dataset.
In
contrast, the datasets PDR and HO, show a better
score, implying the ability to effectively identify
posts of EBDR class, thereby reducing the false
positive cases. Also, despite the downside of PDR
dataset in terms of accuracy, the evaluation met-
rics follow a similar trend. The best performance
in terms of F1-score is shown by using linguistic
and handcrafted features in all the three datasets.
The HO dataset performs better in terms of accu-
racy (97.89%) as compared to both PDR and BDC,
implying that training classiﬁers with tweets cov-
ering various other topics and aspects increases its
robustness towards noise.
5.1 Error Analysis
Some categories of errors that were noticed are:

1. Rants due to non-availability of blood
donors: Tweets like “Can’t believe we live
in a pathetic world, no one came forward to
donate a single bottle of B+ve blood ...#Hu-
manityIsDead” are an example of reactionary
posts. Such posts do not belong to EBDR.
However the supervised classiﬁers classify
such tweets into the same, making it difﬁcult
to separate false requests from genuine cases.
2. Acknowledgment of blood donation: The
tweet “We thank @user for registering as

#AB- blood donor ...” was correctly identi-
ﬁed by the human annotators but misclassi-
ﬁed by the automated classiﬁers. This can be
attributed to the inefﬁciency of the classiﬁers
to derive contextual meaning from the tweets.

6 Conclusion and Future Work
In this paper, we introduced a robust feature based
classiﬁcation system in addition to an annotated
corpus to accurately identify Emergency Blood
Donation Request (EBDR) tweets and separate
them from other unrelated blood donation commu-
nication, referred to as non-EBDR tweets. Given
the diverse nature of emergency request tweets,
we adopted a two-way corpus construction strat-
egy. We mine three datasets to probe various as-
pects such as robustness and accuracy and man-
ually annotated them to validate the performance
of the proposed classiﬁcation system. In addition
we also perform an analysis of the efﬁciency of
four independent feature sets extracted from the
tweets. The results point out that the linguistic
features like n-grams and TF-IDF statistics along
with handcrafted features related to blood dona-
tion requirement are best suited for classiﬁcation.
The EBDR data corpus can beneﬁt researchers in
various aspects including but not limited to (i) au-
tomatic evaluation of emergency blood donation
requests from health posts, (ii) named entity ex-
traction of patient details, blood group and quan-
tity requirement statistics with the help of hand-
crafted features provided with the tweets, (iii) cri-
sis assessment and management through social
media monitoring of medical emergency events
and (iv) feature modelling using genetic algo-
rithms as done by Sawhney et al. (2018b,c).
References
David Andre Broniatowski, Mark Dredze, Michael J
Paul, and Andrea Dugas. 2015. Using social media
to perform local inﬂuenza surveillance in an inner-
city hospital: a retrospective observational study.
JMIR public health and surveillance, 1(1).

Meeyoung Cha, Hamed Haddadi, Fabricio Ben-

30in binary evolutionary algorithms for single objec-
In International Symposium on
tive optimization.
Distributed Computing and Artiﬁcial Intelligence,
pages 27–35. Springer.

evenuto, P Krishna Gummadi, et al. 2010. Measur-
ing user inﬂuence in twitter: The million follower
fallacy. Icwsm, 10(10-17):30.

Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a
library for support vector machines. ACM transac-
tions on intelligent systems and technology (TIST),
2(3):27.

Carl Lee Hanson, Ben Cannon, Scott Burton, and
Christophe Giraud-Carrier. 2013. An exploration of
social circles and prescription drug abuse through
twitter. Journal of medical Internet research, 15(9).

Paul Fontelo MA and MD Eldredge. Development
of an adverse drug reaction corpus from consumer
health posts.

Debanjan Mahata, Jasper Friedrichs, Rajiv Ratn Shah,
and Jing Jiang. 2018a. Did you take the pill?-
detecting personal intake of medicine from twitter.
arXiv preprint arXiv:1808.02082.

Debanjan Mahata, Jasper Friedrichs, Rajiv Ratn Shah,
et al. 2018b. # phramacovigilance-exploring deep
learning techniques for identifying mentions of
arXiv preprint
medication intake from twitter.
arXiv:1805.06375.

Puneet Mathur, Ramit Sawhney, Meghna Ayyar, and
Rajiv Shah. 2018a. Did you offend me? classiﬁca-
tion of offensive tweets in hinglish language. In Pro-
ceedings of the Second Workshop on Abusive Lan-
guage Online.

Puneet Mathur, Rajiv Shah, Ramit Sawhney, and De-
banjan Mahata. 2018b. Detecting offensive tweets
In Pro-
in hindi-english code-switched language.
ceedings of the Sixth International Workshop on
Natural Language Processing for Social Media,
pages 18–26.

Juan Ramos et al. 2003. Using tf-idf to determine word
In Proceedings of
relevance in document queries.
the ﬁrst instructional conference on machine learn-
ing, volume 242, pages 133–142.

Raquel Recuero, Ricardo Araujo, and Gabriela Zago.
2011. How does social capital affect retweets? In
ICWSM.

Ramit Sawhney, Prachi Manchanda, Raj Singh, and
Swati Aggarwal. 2018a. A computational approach
to feature extraction for identiﬁcation of suicidal
ideation in tweets. In Proceedings of ACL 2018, Stu-
dent Research Workshop, pages 91–98.

Ramit Sawhney, Puneet Mathur, and Ravi Shankar.
2018b. A ﬁreﬂy algorithm based wrapper-penalty
feature selection method for cancer diagnosis.
In
International Conference on Computational Science
and Its Applications, pages 438–449. Springer.

Ramit Sawhney, Ravi Shankar, and Roopal Jain.
2018c. A comparative study of transfer functions

31
Unsupervised Token-wise Alignment to

Improve Interpretation of Encoder-Decoder Models

Shun Kiyono 1∗ Sho Takase 2 Jun Suzuki 1,2,4

Naoaki Okazaki 3 Kentaro Inui 1,4 Masaaki Nagata 2

3 Tokyo Institute of Technology 4 RIKEN Center for Advanced Intelligence Project

1 Tohoku University 2 NTT Communication Science Laboratories
{kiyono,jun.suzuki,inui}@ecei.tohoku.ac.jp,
{takase.sho, nagata.masaaki}@lab.ntt.co.jp,

okazaki@c.titech.ac.jp

Abstract

Developing a method for understanding the in-
ner workings of black-box neural methods is
an important research endeavor. Convention-
ally, many studies have used an attention ma-
trix to interpret how Encoder-Decoder-based
models translate a given source sentence to the
corresponding target sentence. However, re-
cent studies have empirically revealed that an
attention matrix is not optimal for token-wise
translation analyses. We propose a method
that explicitly models the token-wise align-
ment between the source and target sequences
to provide a better analysis. Experiments show
that our method can acquire token-wise align-
ments that are superior to those of an attention
mechanism1.
Introduction

1
The Encoder-Decoder model with an attention
mechanism (EncDec) (Sutskever et al., 2014; Cho
et al., 2014; Bahdanau et al., 2015; Luong et al.,
2015) has been an epoch-making development that
has led to great progress in many natural language
generation tasks, such as machine translation (Bah-
danau et al., 2015), dialog generation (Shang et al.,
2015), and headline generation (Rush et al., 2015).
An enormous number of studies have attempted to
enhance the ability of EncDec.

Furthermore, several intensive studies have also
attempted to analyze and interpret the inside of
black-box EncDec models, especially how they
translate a given source sentence to the correspond-
ing target sentence (Ding et al., 2017). One typical
approach to this is to visualize an attention matrix,
which is a collection of attention vectors (Bahdanau
et al., 2015; Luong et al., 2015; Tu et al., 2016).
∗ This work is a product of collaborative research pro-
gram of Tohoku University and NTT Communication Science
Laboratories.

1Our code for reproducing the experiments is available at

https://github.com/butsugiri/UAM

The assumption behind this interpretation is that
the attention matrix has a “soft” token-wise align-
ment between the source and target sequences, and
thus we can use EncDec models to skim which to-
kens in the source are converted to which tokens in
the target.

However, recent studies have empirically re-
vealed that an attention model can operate not only
for token-wise alignment but also for other func-
tionalities, such as reordering (Ghader and Monz,
2017; Liu et al., 2016). In addition, Luong et al.
(2015) reported that the quality of attention matrix-
based alignment is quantitatively inferior to that of
the Berkeley aligner (Liang et al., 2006). Koehn
and Knowles (2017) also reported that attention
matrix-based alignment is signiﬁcantly different
from that acquired from an off-the-shelf aligner
for English-German language pairs. From these re-
cent ﬁndings, the goal of this paper is to provide a
method that can offer a better interpretation of how
EncDec models translate a given source sentence
to the corresponding target sentence.

In this paper, we focus exclusively on the head-
line generation task, which is categorized as a lossy-
compression generation (lossy-gen) task (Nallapati
et al., 2016). Compared with a machine translation
task, which is categorized as a loss-less genera-
tion (lossless-gen) task, the headline generation
task additionally requires EncDec models to ap-
propriately select salient ideas in given source sen-
tences (Suzuki and Nagata, 2017). Therefore, the
lossy-gen task seems to make modeling by EncDec
much harder. In fact, our preliminary experiments
revealed that the attention mechanism in EncDec
models largely fails to capture token-wise align-
ments, e.g., less than 10 percent accuracy, even if
we use one of the current state-of-the-art EncDec
models (Table 3).

To obtain a better analysis of how EncDec mod-
els translate a given source sentence to the corre-

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages74–81Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics74sponding target sentence in the headline generation
task, this paper introduces the Unsupervised token-
wise Alignment Module (UAM), a novel compo-
nent that can be plugged into EncDec models. Un-
like a conventional attention model, the proposed
UAM explicitly captures token-wise alignments
between the source and target sequences on the
ﬁnal hidden layer. One can plug the UAM into a
EncDec model during a training phase and easily
understand the EncDec model’s behavior by analyz-
ing the UAM’s token-wise alignments. Moreover,
the UAM does not require any gold alignment data.
To demonstrate the effectiveness of the UAM,
we evaluate EncDec models with the UAM in the
headline generation task (Rush et al., 2015), a
widely used benchmark for EncDec models. Our
experiments show that (i) EncDec models with a
UAM achieve comparable (or even superior) per-
formance to the current state-of-the-art headline
generation model, and (ii) the produced token-wise
alignment is practical regardless of the absence of
gold alignment during its training phase.

2 Headline Generation Task

We address the headline generation task introduced
in Rush et al. (2015). The source (input) is the ﬁrst
sentence of a news article, and the target (output)
is the article’s headline. We say I and J represent
the numbers of tokens in the source and target,
respectively. An important assumption in headline
generation is that the target must be shorter than
the source (I > J).
Here, we denote a source sequence as sequence
X of one-hot vectors. Let xi ∈ {0, 1}Vs repre-
sent the one-hot vector of the i-th token in X,
where Vs represents the number of tokens in the
source-side vocabulary Vs. We use x1:I to repre-
sent (x1, . . . , xI ); namely, X = x1:I. Similarly,
let yj ∈ {0, 1}Vt represent the one-hot vector of
the j-th token in target sequence Y , where Vt is
the number of tokens in the target-side vocabulary
Vt. Here, we deﬁne Y as always containing two
additional one-hot vectors of special tokens (cid:104)bos(cid:105)
for y0 and (cid:104)eos(cid:105) for yJ+1, namely, Y = y0:J+1.
3 Encoder-Decoder Model with

Attention Mechanism (EncDec)

J+1(cid:89)

3.1 Model Deﬁnition
EncDec models the following conditional probabil-
ity:

p(Y |X) =

p(yj|y0:j−1, X).

(1)

j=1

Encoder EncDec encodes the source one-hot
vector sequence x1:I and generates the hidden state
sequence h1:I, where hi ∈ RH for all i and H is
the size of the hidden state.

We employ bidirectional RNN (BiRNN) as the
encoder of the base model. BiRNN is composed of
−−−→
two separate RNNs for the forward (
RNNsrc) and
←−−−
backward (
RNNsrc) directions. The forward RNN
reads the source sequence X from left to right
and constructs hidden states ((cid:126)h1:I ). Similarly, the
backward RNN reads the input in the reverse order
to obtain another sequence of hidden states ( (cid:126)h1:I ).
Finally, we take the summation of hidden states in
each direction to construct the ﬁnal representation
of the source sequence (h1:I ).

Concretely, for a given time step i, the represen-

tation hi is constructed as follows:

−−−→
RNNsrc(Esxi, (cid:126)hi−1),
←−−−
RNNsrc(Esxi, (cid:126)hi+1),

(cid:126)hi =
(cid:126)hi =
hi = (cid:126)hi + (cid:126)hi

(2)
(3)
(4)
where Es ∈ RD×Vs denotes the word embedding
matrix of the source-side and D denotes the size of
word embedding.

Decoder The decoder is the unidirectional RNN
in the input-feeding approach (Luong et al., 2015).
Concretely, decoder RNN takes the output of the
previous time step yj−1, decoder hidden state (cid:126)zj−1
and ﬁnal hidden state zj−1 to derive the hidden
state of current time step zj:

−−−→
RNNtrg(Etyj−1, zj−1, (cid:126)zj−1),

(cid:126)zj =
(cid:126)z0 = (cid:126)hI + (cid:126)h1

(5)
(6)
where Et ∈ RD×Vt denotes the word embedding
matrix of the decoder. Here, z0 is deﬁned as a zero
vector.

This section brieﬂy describes EncDec as the base
model of our method2.

2Our EncDec conﬁguration follows the model described

in Luong et al. (2015).

Attention Mechanism The attention architec-
ture of the base model is the same as that of the
Global Attention model proposed by Luong et al.
(2015). Attention is responsible for constructing

75I(cid:88)

the ﬁnal hidden state zj from the decoder hidden
state (cid:126)zj and encoder hidden states (h1:I ).
First, the model computes the attention vector
αj ∈ RI from the decoder hidden state (cid:126)zj and
encoder hidden states (h1:I ). From among three
attention scoring functions proposed in Luong et al.
(2015), we employ a general function. This func-
tion calculates the attention score in bilinear form.
Speciﬁcally, the attention score between the i-th
source hidden state and the j-th decoder hidden
state is computed by the following equation:

(cid:80)I
exp(h(cid:62)
i=1 exp(h(cid:62)

i Wα(cid:126)zj)

i Wα(cid:126)zj)

αj[i] =

(7)

where Wα ∈ RH×H is a parameter matrix, and
αj[i] denotes the i-th element of αj.

αj is then used for collecting the source-side
information that is relevant for predicting the target
token. This is done by taking the weighted sum on
the encoder hidden states:

cj =

αj[i]hi

(8)

i=1

Next, the source-side information is mixed with
the decoder hidden state to derive ﬁnal hidden state
zj. Concretely, the context vector cj is concate-
nated with (cid:126)zj to form vector uj ∈ R2H. uj is then
fed into a single fully-connected layer with tanh
nonlinearity:

zj = tanh(Wsuj)

(9)

where Ws ∈ RH×2H is a parameter matrix.

Finally, zj is fed into the softmax layer. The
model generates a target-side token based on the
probability distribution oj ∈ RVt as

oj = softmax(Wozj + bo),

(10)
where Wo ∈ RVt×H is a parameter matrix and
bo ∈ RVt is a bias term.
3.2 Training of EncDec
To train EncDec, let D be training data for head-
line generation, which consists of source-headline
sentence pairs. Let θ represent all parameters in
EncDec. Our goal is to ﬁnd the optimal param-
eter set ˆθ that minimizes the following objective
function G0(θ) for the given training data D:

G0(θ) =

1
|D|
(cid:96)trg(Y , X, θ) = − log

(X,Y )∈D

p(Y |X, θ)

(cid:17)

(cid:96)trg(Y , X, θ),

(cid:88)
(cid:16)

Figure 1: Overview of EncDec with UAM. UAM
predicts the probability distribution over the source
vocabulary qj at each time step j. After predicting
all the time steps, the SPM compares the sum of
the predictions ˜q with the sum of the source-side
tokens ˜x as an objective function (cid:96)src.

Since oj for each j is a vector representation of the
probabilities of p( ˆy|y0:j−1, X, θ) over the target
vocabularies ˆy ∈ Vt, we can calculate (cid:96)trg as

(cid:96)trg(Y , X, θ) = − J+1(cid:88)

j · log(cid:0)oj

y(cid:62)

(cid:1).

(12)

j=1

Inference of EncDec

3.3
In the inference step, we use the trained param-
eters to search for the best target sequence. We
use beam search to ﬁnd the target sequence that
maximizes the product of the conditional proba-
bilities as described in Equation 1. From among
several stopping criteria for beam search (Huang
et al., 2017), we adopt the widely used “shrink-
ing beam” implemented in RNNsearch (Bahdanau
et al., 2015)3.

4 Proposed Method: Unsupervised

Alignment Module (UAM)

Figure 1 illustrates the proposed method, UAM.
UAM is the module attached on top of the decoder
output layer of an EncDec model, and it explicitly
represents a token-wise alignment by predicting
source tokens simultaneously with target tokens.

Speciﬁcally, during the training of EncDec, the
decoder estimates the probability distribution over
the source-side vocabulary, qj ∈ RVs, simulta-
neously with that of the target-side vocabulary,

.

(11)

3https://github.com/lisa-groundhog/

GroundHog

76Target Pred.(𝒐":$)sharepricesexchangethursday.londonshare<eos><null><null>++++𝒙&𝒒&Sentence-wise Loss (Equation 18)ℓ)*+	Final Hidden(𝒛":$)Source Pred.(𝒒":$)Source(𝑿)Encoder-Decoder (EncDec)UAMComputationAlignmentPairThis way of

oj ∈ RVt, for each time step j. In Figure 1, when
the decoder predicts the target-side token “share”,
we want to predict its corresponding source-side
token “share.” If we can correctly predict the cor-
responding source-side token for each target-side
token, we can obtain token-wise alignment. As
shown below, we can train the model for this pre-
diction without gold token-wise alignment signals.
representing alignments can
be extended to (cid:104)null(cid:105) alignments. We ﬁrst
expand a given target sequence with a se-
quence of (cid:104)null(cid:105)s so that its length is the same
as that of the source sequence (in Figure 1,
we extend “london share. . .(cid:104)eos(cid:105)” to “london
share. . .(cid:104)eos(cid:105)(cid:104)null(cid:105). . .(cid:104)null(cid:105)”). We then train the
model so that it can predict a discarded source-
side token (e.g., “thursday”) when it predicts (cid:104)null(cid:105)
for the target-side. Through the task of predicting
source-side tokens corresponding to (cid:104)null(cid:105)s, we
expect the model to effectively learn to identify
unimportant information in the source sequence.

4.1 Model Deﬁnition
EncDec with UAM jointly estimates the probabil-
ity distributions over the source and target vocab-
ularies. Speciﬁcally, UAM estimates a probability
distribution over the source vocabulary qj ∈ RVs
at each time step j in the decoding process by:

qj = softmax(Wqzj + bq),

(13)
where Wq ∈ RVs×H is a parameter matrix, zj ∈
RH is a decoder’s ﬁnal hidden layer, and bq ∈ RVs
is a bias term. H is the size of the hidden state.
EncDec also calculates a probability distribution
over target vocabulary oj from the same vector zj.
Next, we deﬁne Y (cid:48) as a concatenation of the one-
hot vectors of the target-side sequence Y and those
of the special token (cid:104)null(cid:105) of length I − (J + 1).
Here, yJ+1 is a one-hot vector of (cid:104)eos(cid:105), and yj
for each j ∈ {J + 2, . . . , I} is a one-hot vector
of (cid:104)null(cid:105). Note that the length of Y (cid:48) is always no
shorter than that of Y , that is, |Y (cid:48)| ≥ |Y | because
headline generation always assumes I > J.

Based on this setting, we train the model in an un-
supervised manner without gold alignment signals.
To this end, we consider a sentence-wise loss func-
tion instead of token-wise loss. Speciﬁcally, we
deﬁne the sentence-wise loss as the degree of differ-
ence between the sum of all one-hot vectors in the
source sequence and the sum of UAM predictions.
i=1 xi

Namely, we take the difference of ˜x = (cid:80)I

and ˜q = (cid:80)I

j=1 qj. Note that ˜x is a vector rep-
resentation of the occurrences (or bag-of-words
representation) of the source-side tokens. To min-
imize sentence-wise loss, the model must predict
the bag-of-words representation. Through this opti-
mization, the model is expected to eventually ﬁnd
the token-wise alignment.

EncDec with UAM models the following condi-

tional probability:

p(Y (cid:48), ˜x|X) = p( ˜x|Y (cid:48), X)p(Y (cid:48)|X).

(14)

We deﬁne p(Y (cid:48)|X) as follows:

p(Y (cid:48)|X) =

p(yj|y0:j−1, X),

(15)

I(cid:89)

j=1

which is identical to the conditional probability
modeled by the base EncDec, except for consid-
ering (cid:104)null(cid:105) as part of the probability. Next, we
deﬁne p( ˜x|Y (cid:48), X) as follows:

(cid:18)−(cid:107) ˜q − ˜x(cid:107)2

(cid:19)

2

C

,

(16)

p( ˜x|Y (cid:48), X) =

1
Z

exp

where Z is a normalization term and C is a hyper-
parameter that controls the sensitivity of the distri-
bution.

4.2 Training of UAM
We optimize the UAM combined with EncDec by
minimizing the negative log-likelihood of Equation
14 as a loss function. Let γ represent the parameter
set of SPM Then, we deﬁne the UAM loss (cid:96)src as
follows;
(cid:96)src( ˜x, X, Y (cid:48), γ, θ) = − log(p( ˜x|Y (cid:48), X, γ, θ))
(17)

From Equation 16, we can derive (cid:96)src as

(cid:96)src( ˜x,X,Y (cid:48),γ, θ) =

(cid:107) ˜q − ˜x(cid:107)2

2 + log(Z).

1
C

(18)
We can discard log(Z) from the RHS, since it is
independent of γ and θ.

Here, we regard the sum of (cid:96)src and (cid:96)trg as an
objective loss function of multi-task training. For-
mally, we train EncDec with the UAM by minimiz-
ing the following objective function G1:

G1(θ, γ) =

(cid:96)trg(Y (cid:48), X, θ)

1
|D|
+ (cid:96)src( ˜x, X, Y (cid:48), γ, θ)

(X,Y )∈D

(cid:17)

(19)
where D is training data for headline generation,
which consists of source-headline sentence pairs.

,

(cid:88)

(cid:16)

774.3

Inference of UAM

In the inference time, the goal is only to search for
the best target sequence. Thus, we do not need to
compute the UAM during the inference. Similarly,
it is unnecessary to produce (cid:104)null(cid:105) after generating
(cid:104)eos(cid:105). Thus, we can utilize the beam search proce-
dure described in Section 3.3, and as a result the
actual computation cost for the inference remains
unchanged from the base EncDec.

5 Experiment

5.1 Dataset

The origin of the headline generation dataset used
in our experiments is identical to that used in Rush
et al. (2015). The dataset consists of pairs of the
ﬁrst sentence of each article and its headline from
the annotated English Gigaword corpus (Napoles
et al., 2012). Rush et al. (2015) deﬁned the train-
ing, validation and test splits, which contain ap-
proximately 3.8M, 200K and 400K source-headline
pairs, respectively

We used the entire training split for training as
in the previous studies. We randomly sampled 8K
instances as validation data and 10K instances as
test data from the validation split. Moreover, we
experimented on the test data provided by Zhou
et al. (2017) and Toutanova et al. (2016) for com-
parison with the reported state-of-the-art perfor-
mance (Zhou et al., 2017). We refer to those test
data sets as Test (Ours), Test (Zhou), and MSR-
ATC respectively. Among these test sets, MSR-
ATC is the only dataset created by a human worker.

5.2

Implementation Details

In the experiment, we selected the hyper-parameter
settings commonly used in previous studies
e.g., (Rush et al., 2015; Nallapati et al., 2016;
Suzuki and Nagata, 2017) We constructed the vo-
cabulary set using byte pair encoding4 (BPE) (Sen-
nrich et al., 2016) to handle low-frequency words,
since this is now a common practice in the ﬁeld of
neural machine translation. The BPE merge oper-
ations are jointly learned from the source and the
target. We set the number of BPE merge operations
at 5, 000.

4https://github.com/rsennrich/

subword-nmt

Source Vocab. Size Vs
Target Vocab. Size Vt
Word Embed. Size D
Hidden State Size H
RNN Cell

Encoder RNN Unit
Decoder RNN Unit

5131
5131
200
400
Long Short-Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997)
2-layer bidirectional-LSTM
2-layer LSTM with attention (Luong
et al., 2015)
Adam (Kingma and Ba, 2015)

Optimizer
Initial Learning Rate 0.001
Learning Rate Decay 0.5 for each epoch (after epoch 9)
Weight C of (cid:96)src
Mini-batch Size
Gradient Clipping
Stopping Criterion Max 15 epochs with early stopping
Regularization
Beam Search

10
256 (shufﬂed at each epoch)
5

Dropout (rate 0.3)
Beam size 20 with the length normal-
ization

Table 1: Conﬁgurations used in our experiments

5.3 Evaluation Metric
We evaluated the performance by ROUGE-1 (RG-
1), ROUGE-2 (RG-2) and ROUGE-L (RG-L)5. We
report the F1 value as given in a previous study6.
We computed the ROUGE scores using the ofﬁcial
ROUGE script (version 1.5.5).

5.4 Results
To investigate the effectiveness of the UAM
quantitatively, we chose a very strong baseline,
SEASS (Zhou et al., 2017), which is the cur-
rent state-of-the-art model. We reimplemented
SEASS, hereafter EncDec+sGate, and compared
EncDec+sGate with the model incorporating UAM
into EncDec+sGate.

test data.

The table shows

Table 2 summarizes ROUGE-F1 results
for all
that
EncDec+sGate+UAM achieved a huge gain
particularly for MSR-ATC and a performance
comparable to EncDec+sGate in Test (Ours) and
Test (Zhou). Considering that the MSR-ATC
dataset was created by a human worker, we believe
that the improvement in MSR-ATC is the most
remarkable result among the three test sets, since
it indicates that our model most closely ﬁts the
human-generated summary.

6 Discussion
We investigated whether the UAM improves token-
wise alignment between the source and target se-

5We restored sub-words to the standard token split for the

evaluation.

6 ROUGE script option is: “-n2 -m -w 1.2”

78Test (Ours)

Test (Zhou)

MSR-ATC

RG-1
46.80
46.91

Model
EncDec+sGate
EncDec+sGate+UAM†
SEASS (Zhou et al., 2017)

RG-2
15.45
17.02
10.63
Table 2: Full-length ROUGE F1 evaluation results. † is the proposed model.

RG-L
43.62
43.68
43.53

RG-2
24.75
24.93
24.58

RG-1
46.79
46.89
46.86

RG-1
29.73
32.32
25.75

RG-L
43.74
43.87

RG-2
24.48
24.86

-

-

-

RG-L
27.30
29.76
22.90

quences. We compared the alignments acquired by
the UAM and the attention model, since attention
has been implicitly considered as alignment (Bah-
danau et al., 2015; Luong et al., 2015; Tu et al.,
2016).

6.1 Visualizing UAM and Attention

We visualized UAM prediction and the attention
matrix to see the acquired token-wise alignment be-
tween the source and target. Speciﬁcally, we fed the
source-target pair (X, Y ) to EncDec+sGate and
EncDec+sGate+UAM, and then collected UAM
predictions (q1:I) of EncDec+sGate+UAM and the
attention vectors (α1:J) of EncDec+sGate. We
computed the attention vectors using Equation 7.
For UAM prediction, we extracted the probability
of each token xi ∈ X from q1:I.

Figure 2 and 3 show an example of the heat map.
We used Test (Ours) as the input. The brackets
in the y-axis represent the source-side tokens that
are aligned with target-side tokens. We obtained
the aligned tokens as follows: For attention (Fig-
ure 2a, 3a), we select the token with the largest
attention value. For the UAM (Figure 2b, 3b), we
select the token with the largest probability over
the vocabulary Vs.

Figure 2a indicates that attention provides poor
token-wise alignment. For example, the target-
side token “kong” is incorrectly aligned with the
source-side sentence period.
In contrast, Fig-
ure 2b shows that the UAM captures reasonable
alignments. Here, the source-side token “rose” is
aligned with the target-side token “higher.” UAM
also correctly aligned unimportant tokens such as
“##.##” with (cid:104)null(cid:105).

In Figure 3a, the attention model repeatedly pays
attention to the source-side token “positive.” As a
result, the attention model aligned the target-side
token “egyptian” to “positive.” On the other hand,
in 3b, the UAM correctly aligns “egyptian.” In addi-
tion, the UAM aligned source-side token “foreign”
with target-side token “fm.”

(a) Attention matrix of EncDec+sGate

(b) UAM prediction of EncDec+sGate+UAM

Figure 2: Visualization of models. The x-axis and
y-axis correspond to the source and the target se-
quence respectively. Tokens in the brackets are
source-side tokens aligned at that time step.

6.2 Alignment Accuracy
In order to quantitatively investigate the quality of
the alignment, we evaluated the alignment accuracy
of both EncDec+sGate and EncDec+sGate+UAM.
We randomly sampled 40, 30 and 30 instances
from Test (Ours), Test (Zhou) and MSR-ATC re-
spectively. We acquired the alignment of the data
by applying the UAM and the attention model, in
the same manner as described in Section 6.1. A
single annotator then evaluated the accuracy of the
alignment by hand.

Table 3 summarizes the results. The alignment
of the UAM is signiﬁcantly more accurate than

79hongkongstocksrose##.##points,or#.##percent,toopenat#,###.##monday.<eos>: (.)higher: (stocks)open: (to)stocks: (kong)kong: (.)hong: (hong)0.00.20.40.60.81.0hongkongstocksrose##.##points,or#.##percent,toopenat#,###.##monday.<null>: (at)<null>: (,)<null>: (points)<null>: (#,###.##)<null>: (percent)<null>: (to)<null>: (or)<null>: (monday)<null>: (percent)<null>: (,)<null>: (##.##)<eos>: (.)higher: (rose)open: (open)stocks: (stocks)kong: (kong)hong: (hong)0.00.20.40.60.81.0Model
EncDec+sGate
EncDec+sGate+UAM

Test (Ours)
8.60
52.52

Test (Zhou) MSR-ATC Overall
6.71
51.41

5.97
50.91

6.11
51.07

Table 3: Alignment Macro Accuracy (%)

7 Conclusion

In this paper, we introduced the Unsupervised
token-wise Alignment Module (UAM), which
learns to predict the token-wise alignment of to-
kens in the source and the target. Experiments on
the headline generation task showed that the UAM
can achieve comparable performance to the current
state-of-the-art sGate model. In addition, the UAM
obtained token-wise alignment that is superior to
that of the attention model. This ﬁnding suggests
we can use the UAM as an alternative to the atten-
tion matrix to attain a better understanding of the
token-wise alignment of EncDec-based model.

Acknowledgment

We are grateful to anonymous reviewers for their
insightful comments. We thank Sosuke Kobayashi
for providing helpful comments. We also thank
Qingyu Zhou for providing a dataset and informa-
tion for a fair comparison.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR 2015).

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation.
In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014), pages 1724–1734.

Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Visualizing and Understanding Neural
Machine Translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2017), pages 1150–1159.

Hamidreza Ghader and Christof Monz. 2017. What
does Attention in Neural Machine Translation Pay
Attention to? In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2017), pages 30–39.

(a) Attention matrix of EncDec+sGate

(b) UAM prediction of EncDec+sGate+UAM

Figure 3: Visualization of models. The x-axis and
y-axis correspond to the source and the target se-
quence respectively. Tokens in the brackets are
source-side tokens aligned at that time step.

that of the attention model. This result is consis-
tent with the empirical results reported by Koehn
and Knowles (2017) and Luong et al. (2015). The
attention alignment mistakes are mostly due to pay-
ing attention to either the sentence period or to the
token decoded in the previous time step. It is note-
worthy that the accuracy of the UAM alignment
exceeds 50% even though we trained the model in
an unsupervised manner. The fact that the UAM
prediction acquires reasonable alignment suggests
that the UAM has the potential to provide us a
better understanding of the model’s behavior.

80positivedevelop@@mentsreg@@ar@@dingpalestinianslivinginlibyaareexpected,saidegyptianforeignministeram@@r.<eos>: (.)libya: (in)to: (.)visit: (in)from: (ments)ments: (develop@@)develop@@: (positive)positive: (positive)expects: (.)fm: (egyptian)egyptian: (positive)0.00.20.40.60.81.0positivedevelop@@mentsreg@@ar@@dingpalestinianslivinginlibyaareexpected,saidegyptianforeignministeram@@r.<null>: (minister)<null>: (am@@)<null>: (living)<null>: (r)<null>: (,)<null>: (are)<null>: (said)<null>: (ding)<null>: (in)<eos>: (.)libya: (libya)to: (.)visit: (libya)from: (reg@@)ments: (ments)develop@@: (develop@@)positive: (positive)expects: (expected)fm: (foreign)egyptian: (egyptian)0.00.20.40.60.81.0Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Liang Huang, Kai Zhao, and Mingbo Ma. 2017. When
to Finish? Optimal Beam Search for Neural Text
Generation (modulo beam size). In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2017), pages
2124–2129.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
Method for Stochastic Optimization.
ings of the 3rd International Conference on Learn-
ing Representations (ICLR 2015).

Philipp Koehn and Rebecca Knowles. 2017. Six Chal-
lenges for Neural Machine Translation. In Proceed-
ings of the First Workshop on Neural Machine Trans-
lation, pages 28–39.

Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
In Proceedings of the 2006
ment by Agreement.
Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL
2006), pages 104–111.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-
ral Responding Machine for Short-Text Conversa-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (ACL & IJCNLP 2015), pages
1577–1586.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in Neural Information Process-
ing Systems 27 (NIPS 2014), pages 3104–3112.

Jun Suzuki and Masaaki Nagata. 2017. Cutting-off Re-
dundant Repeating Generations for Neural Abstrac-
tive Summarization. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2017), pages
291–297.

Kristina Toutanova, Chris Brockett, Ke M. Tran, and
Saleema Amershi. 2016. A Dataset and Evaluation
Metrics for Abstractive Compression of Sentences
and Short Paragraphs. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2016), pages 340–350.

Lemao Liu, Masao Utiyama, Andrew Finch, and Ei-
ichiro Sumita. 2016. Neural Machine Translation
with Supervised Attention. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics, pages 3093–3102.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling Coverage for Neural
Machine Translation. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2016), pages 76–85.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective Encoding for Abstractive Sentence
Summarization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2017), pages 1095–1104.

Thang Luong, Hieu Pham, and Christopher D. Man-
Effective Approaches to Attention-
ning. 2015.
In Proceed-
based Neural Machine Translation.
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2015),
pages 1412–1421.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016.
Ab-
stractive Text Summarization Using Sequence-to-
Sequence RNNs and Beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natural
Language Learning, pages 280–290.

Courtney Napoles, Matthew Gormley, and Benjamin
In Pro-
Van Durme. 2012. Annotated Gigaword.
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, AKBC-WEKEX ’12, pages 95–100.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A Neural Attention Model for Abstractive
In Proceedings of the
Sentence Summarization.
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2015), pages 379–
389.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2016), pages 1715–1725.

81
Learning Semantic Textual Similarity from Conversations

Yinfei Yanga, Steve Yuanc, Daniel Cera, Sheng-yi Konga, Noah Constanta, Petr Pilarc,

Heming Gea, Yun-Hsuan Sunga, Brian Stropea, Ray Kurzweila

aGoogle AI

Mountain View, CA, USA

bGoogle

Cambridge, MA, USA

cGoogle

Zurich, Switzerland

Abstract

We present a novel approach to learn rep-
resentations for sentence-level semantic
similarity using conversational data. Our
method trains an unsupervised model to
predict conversational responses. The re-
sulting sentence embeddings perform well
on the Semantic Textual Similarity (STS)
Benchmark and SemEval 2017’s Com-
munity Question Answering (CQA) ques-
tion similarity subtask. Performance is
further improved by introducing multi-
task training, combining conversational
response prediction and natural language
inference. Extensive experiments show
the proposed model achieves the best per-
formance among all neural models on the
STS Benchmark and is competitive with
the state-of-the-art feature engineered and
mixed systems for both tasks.

Introduction

1
We propose a novel approach to sentence-level se-
mantic similarity based on unsupervised learning
from conversational data. We observe that seman-
tically similar sentences have a similar distribution
of potential conversational responses, and that a
model trained to predict conversational responses
should implicitly learn useful semantic represen-
tations. As illustrated in Figure 1, “How old are
you?” and “What is your age?” are both questions
about age, which can be answered by similar re-
sponses such as “I am 20 years old”. In contrast,
“How are you?” and “How old are you?” use sim-
ilar words but have different meanings and lead to
different responses.

Deep learning models have been shown to pre-
dict conversational responses with increasingly
good accuracy (Henderson et al., 2017; Kannan

Figure 1: Sentences have similar meanings if they
can be answered by a similar distribution of con-
versational responses.

et al., 2016). The internal representations of such
models resolve the semantics necessary to pre-
dict the correct response across a broad selec-
tion of input messages. Meaning similarity be-
tween sentences then can be obtained by compar-
ing the sentence-level representations learned by
such models. We follow this approach, and assess
the quality of the resulting similarity scores on
the Semantic Textual Similarity (STS) Benchmark
(Cer et al., 2017) and a question similarity sub-
task from SemEval 2017’s Community Question
Answering (CQA) evaluation. The STS bench-
mark scores sentence pairs based on their degree
of meaning similarity. The Community Question
Answering (CQA) subtask B (Nakov et al., 2017)
ranks questions based on their similarity with a tar-
get question.

We ﬁrst assess representations learned from
unsupervised conversational input-response pairs.
We then explore augmenting our model with
multi-task training over a combination of unsuper-
vised conversational response prediction and su-
pervised training on Natural Language Inference
(NLI) data, as training to NLI has been shown to
independently yield useful general purpose repre-
sentations (Conneau et al., 2017). Unsupervised
training over conversational data yields represen-

Proceedingsofthe3rdWorkshoponRepresentationLearningforNLP,pages164–174Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics164Figure 2: The conversational response selection
problem attempts to identify the correct response
from a collection of candidate responses. We
train using batch negatives with each candidate re-
sponse serving as a positive example for one input
and a negative sample for the remaining inputs.

tations that perform well on STS and CQA ques-
tion similarity. The addition of supervised SNLI
data leads to further improvements and reaches
state-of-the-art performance for neural STS mod-
els, surpassing training on NLI data alone.

2 Approach

This section describes the conversational learning
task and our architecture for predicting conversa-
tional responses. We detail two encoding meth-
ods for converting sentences into sentence embed-
dings and describe multitask learning over conver-
sational and NLI data.

2.1 Conversational Response Prediction
We formulate the conversational learning task as
response prediction given an input (Kannan et al.,
2016; Henderson et al., 2017). Following prior
work, the prediction task is cast as a response se-
lection problem. As shown in Figure 2, the model
P (y|x) attempts to identify the correct response y
from K − 1 randomly sampled alternatives.
2.2 Model Architecture
Our model architecture encodes input and re-
sponse sentences into ﬁxed-length vectors u and v,
respectively. The preference of an input described
by u for a response described by v is scored by
the dot product of the two vectors. The dot prod-
uct scores are converted into probabilities using a
softmax over the scores from all other candidate
responses. Model parameters are trained to maxi-
mize the log-likelihood of the correct responses.

Figure 3 illustrates the input-response scoring
model architecture. Tied parameters are used for
the input and response encoders. In order to model
the mapping between inputs and their expected

Figure 3: Conversational response prediction
model. The sentence encoders are in red and use
shared parameters. Fully connected DNN layers
perform the mapping between the semantics of the
input sentence and the candidate response.

responses, the response embeddings are passed
through an additional feed-forward network to get
the ﬁnal response vector v(cid:48) before computing the
dot product with the input sentence embedding.1
Training is performed using batches of K ran-
domly shufﬂed input-response pairs. Within a
batch, each response serves as the correct answer
to its corresponding input and the incorrect re-
sponse to the remaining K − 1 inputs in the batch.
In the remaining sections, this architecture is re-
ferred to as the input-response model.

networks

averaging

2.3 Encoders
Figure 4 illustrates the encoders we explore for ob-
taining sentence embeddings: DANs (Iyyer et al.,
2015) and Transformer (Vaswani et al., 2017).2
2.3.1 DAN
Deep
compute
sentence-level embeddings by ﬁrst averaging
word-level embeddings and then feeding the
averaged representation to a deep neural network
(DNN) (Iyyer et al., 2015). We provide our
encoder with input embeddings for both words
and bigrams in the sentence being encoded. This
simple architecture has been found to outperform
LSTMs on email response prediction (Henderson
et al., 2017). The embeddings for words and

(DAN)

1While feed-forward layers could have been added to the
input encoder as well, early experiments suggested it was suf-
ﬁcient to add additional layers to only one of the encoders.

2We tried other encoder architectures, notably LSTM
(Hochreiter and Schmidhuber, 1997) and Bi-LSTM (Graves
and Schmidhuber, 2005), but found they performed worse
than transformer in preliminary experiments.

165EncoderInput Responseuvv’(uT • v’) EncoderFully-connected layersbigrams are learned during training of the input-
response model. Our implementation sums the
input embeddings and then divides by sqrt(n),
where n is the sentence length.3 The resulting
vector is passed as input to the DNN.

(a) DAN encoder

(b) Transformer encoder

Figure 4: Model architectures for the DAN and
Transformer sentence encoders.

DANs perform well in practice on sentence-
level prediction and encoding tasks (Iyyer et al.,
2015; Henderson et al., 2017). However, they lack
any explicit network structure for encoding long
range relationships between words.

2.3.2 Transformer
Transformer (Vaswani et al., 2017) is a recent
network architecture that makes use of attention
mechanisms to explicitly capture relationships be-
tween words appearing at any position in a sen-
tence. The architecture is able to achieve state-
of-the-art performance on translation tasks and is
available as open-source.4

While the original transformer architecture con-
tains an encoder and decoder, we only need the en-
coder component in our training procedure. The
encoder is constructed as a series of attention lay-
ers consisting of a multi-headed self-attention op-
eration over all input positions followed by a feed-
forward layer that processes each position inde-
pendently (see ﬁgure 4b). Positional information
is captured by injecting a “timing signal” into the

3sqrtn is one of TensorFlow’s built-in embedding com-
biners. The intuition behind dividing by sqrt(n) is as fol-
lows: We want our input embeddings to be sensitive to length.
However, we also want to ensure that for short sequences the
relative differences in the representations are not dominated
by sentence length effects.

4https://github.com/tensorﬂow/tensor2tensor

input embeddings based on sine/cosine functions
at different frequencies.

The transformer encoder output is a variable-
length sequence. We reduce it to ﬁxed length
by averaging across all sequence positions. Intu-
itively, this is similar to building a bag-of-words
representation, except that the words have had
a chance to interact with their contexts through
the attention layers. In practice, we see that the
learned attention masks focus largely on nearby
words in the ﬁrst layer, and attend to progressively
more distant context in the higher layers.

2.4 Multitask Encoder
We anticipate that learning good semantic repre-
sentations may beneﬁt from the inclusion of mul-
tiple distinct tasks during training. Multiple tasks
should improve the coverage of semantic phe-
nomenon that are critical to one task but less es-
sential to another. We explore multitask mod-
els that use a shared encoder for learning con-
versational response prediction and natural lan-
guage inference (NLI). The NLI data are from
the Stanford Natural Language Inference (SNLI)
(Bowman et al., 2015) corpus. The sentences are
mostly non-conversational, providing a comple-
mentary learning signal.

Figure 5 illustrates the multitask model with
SNLI. We keep the input-response model the
same, and build another two encoders for SNLI
pairs, sharing parameters with the input-response
encoders. Following Conneau et al. (2017), we en-
code a sentence pair into vectors u1, u2 and con-
struct a feature vector (u1, u2,|u1 − u2|, u1 ∗ u2).
The feature vector is fed into a 3-way classiﬁer
consisting of a feedforward network culminating
in a softmax layer. Following prior work, we use a
single 512 unit hidden layer for our experiments.

3 Conversational Data

Our unsupervised model relies on structured con-
versational data. The data for our experiments are
drawn from Reddit conversations spanning 2007
to 2016, extracted by Al-Rfou et al. (2016). This
corpus contains 133 million posts and a total of 2.4
billion comments. The comments are mostly con-
versational and well structured, making it a good
resource for training conversational models.

Figure 6 provides an example of a Reddit com-
ment chain. Comment B is a child of comment A
if comment B is a reply to comment A. We extract

166Input Sentence x0x1xn∑Fully Connected  Layer0Fully Connected Layer1Fully Connected LayernSentence EmbeddingWord EmbeddingsAttn. LayernAttn. LayernInput Sentence x0x1xnSentence Embedding∑Attn. LayernWord Embeddings
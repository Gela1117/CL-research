Learning to Summarize Radiology Findings

Yuhao Zhang, Daisy Yi Ding, Tianpei Qian,
Christopher D. Manning, Curtis P. Langlotz

Stanford University
Stanford, CA 94305

{yuhaozhang, dingd, tianpei}@stanford.edu

{manning, langlotz}@stanford.edu

Abstract

The Impression section of a radiology report
summarizes crucial radiology ﬁndings in nat-
ural language and plays a central role in com-
municating these ﬁndings to physicians. How-
ever, the process of generating impressions by
summarizing ﬁndings is time-consuming for
radiologists and prone to errors. We propose to
automate the generation of radiology impres-
sions with neural sequence-to-sequence learn-
ing. We further propose a customized neural
model for this task which learns to encode the
study background information and use this in-
formation to guide the decoding process. On
a large dataset of radiology reports collected
from actual hospital studies, our model outper-
forms existing non-neural and neural baselines
under the ROUGE metrics. In a blind exper-
iment, a board-certiﬁed radiologist indicated
that 67% of sampled system summaries are
at least as good as the corresponding human-
written summaries, suggesting signiﬁcant clin-
ical validity. To our knowledge our work rep-
resents the ﬁrst attempt in this direction.
Introduction

1
The radiology report documents and communi-
cates crucial ﬁndings in a radiology study. As
shown in Figure 1, a standard radiology report
usually consists of a Background section that de-
scribes the exam and patient information, a Find-
ings section, and an Impression section (Kahn Jr
et al., 2009). In a typical workﬂow, a radiologist
ﬁrst dictates the detailed ﬁndings into the report,
and then summarizes the salient ﬁndings into the
more concise Impression section based also on the
condition of the patient.

The impressions are the most signiﬁcant part
of a radiology report that communicate the ﬁnd-
ings. Previous studies have shown that over 50%
of referring physicians read only the impression
statements in a report (Lafortune et al., 1988;

Background: history: swelling; pain.
technique: 3
views of the left ankle were acquired. comparison: no
prior study available.
Findings: there is normal mineralization and alignment.
no fracture or osseous lesion is identiﬁed. the ankle mor-
tise and hindfoot joint spaces are maintained. there is no
joint effusion. the soft tissues are normal.

Human Impression:
normal left ankle radiographs.
Extractive Baseline:
there is no joint effusion.
Pointer-Generator:
normal right ankle.
Our model:
normal radiographs of the left ankle.

Figure 1: An example radiology report with study
background information organized into a Background
Section, and radiology ﬁndings in a Findings Sec-
tion. The human-written summary (or impression) and
predicted summaries from different models are also
shown. The extractive baseline does not summarize
well, the baseline pointer-generator model generates
spurious sequence, while our model gives correct sum-
mary by incorporating the background information.

Bosmans et al., 2011). Despite its importance, the
generation of the impression statements is error-
prone. For example, crucial ﬁndings may be for-
gotten, which would cause signiﬁcant miscommu-
nications (Gershanik et al., 2011). Additionally,
the process of writing the impression statements is
time-consuming and highly repetitive with the dic-
tation of the ﬁndings. This suggests a crucial need
to automate the radiology impression generation
process.

In this work, we propose to automate the
generation of radiology impressions with neural
sequence-to-sequence learning. In particular, we
argue that this task could be viewed as a text sum-
marization problem, where the source sequence is
the radiology ﬁndings and the target sequence the

Proceedingsofthe9thInternationalWorkshoponHealthTextMiningandInformationAnalysis(LOUHI2018),pages204–213Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics204impression statements. We collect a dataset of ra-
diology reports from actual hospital radiographic
studies, and ﬁnd that this task involves both ex-
tractive summarization where descriptions of ra-
diology observations can be taken directly from
the ﬁndings, and abstractive summarization where
new words and phrases, such as conclusions of
the study, need to be generated from scratch. We
empirically evaluate existing popular summariza-
tion systems on this task and ﬁnd that, while ex-
isting neural models such as the pointer-generater
network can generate plausible summaries, they
sometimes fail to model the study background in-
formation and thus generate spurious results. To
solve this problem, we propose a customized sum-
marization model that properly encodes the study
background information and uses the encoded in-
formation to guide the decoding process.

We show that our model outperforms existing
non-neural and neural baselines on our dataset
measured by the standard ROUGE metrics. More-
over, in a blind experiment, a board-certiﬁed radi-
ologist indicated that 67% of sampled system sum-
maries are at least as good as the reference sum-
maries written by well-trained radiologists, sug-
gesting signiﬁcant clinical validity of the resulting
system. We further show through detailed analy-
sis that our model could be reliably transferred to
radiology reports from another organization, and
that the model can sometimes summarize radiol-
ogy studies for body parts unseen during training.
To review, our main contributions in this paper
include: (i) we propose to summarize radiology
ﬁndings into impression statements with neural
sequence-to-sequence learning, and to our knowl-
edge our work represents the ﬁrst attempt in this
direction; (ii) we propose a new customized sum-
marization model to this task that improves over
existing methods by better leveraging study back-
ground information; (iii) we further show via a ra-
diologist evaluation that the summaries generated
by our model have signiﬁcant clinical validity.

2 Related Work

Early Summarization Systems. Early work on
summarization systems mainly focused on extrac-
tive approaches, where the summaries are gener-
ated by scoring and selecting sentences from the
input. Luhn (1958) proposed to represent the in-
put by topic words and score each sentence by the
amount of topic words it contains. Kupiec et al.

(1995) scored sentences with a feature-based sta-
tistical classiﬁer. Steinberger and Jezek (2004) ap-
plied latent semantic analysis to cluster the top-
ics and then select sentences that cover the most
topics. Meanwhile, various graph-based methods,
such as the LexRank (Mihalcea and Tarau, 2004)
and the TextRank algorithm (Erkan and Radev,
2004), were applied to model sentence depen-
dency by representing sentences as vertices and
similarities as edges. Sentences are then scored
and selected via modeling of the graph properties.

Neural Summarization Systems. Summariza-
tion systems based on neural network models en-
able abstractive summarization, where new words
and phrases are generated to form the summaries.
Rush et al. (2015) ﬁrst applied an attention-based
neural encoder and a neural language model de-
coder to this task. Nallapati et al. (2016) used re-
current neural networks for both the encoder and
the decoder. To address the limitation that neural
models with a ﬁxed vocabulary cannot handle out-
of-vocabulary words, a pointer-generator model
was proposed which uses an attention mechanism
that copies elements directly from the input (Nal-
lapati et al., 2016; Merity et al., 2017; See et al.,
2017). See et al. (2017) further proposed a cover-
age mechanism to address the repetition problem
in the generated summaries. Paulus et al. (2018)
applied reinforcement learning to summarization
and more recently, Chen and Bansal (2018) ob-
tained improved result with a model that ﬁrst se-
lects sentences and then rewrites them.

Summarization of Radiology Reports. Most
prior work that attempts to “summarize” radiol-
ogy reports focused on classifying and extracting
information from the report text (Friedman et al.,
1995; Hripcsak et al., 1998; Elkins et al., 2000;
Hripcsak et al., 2002). More recently, Hassanpour
and Langlotz (2016) studied extracting named en-
tities from multi-institutional radiology reports us-
ing traditional feature-based classiﬁers. Goff and
Loehfelm (2018) built an NLP pipeline to identify
asserted and negated disease entities in the impres-
sion section of radiology reports as a step towards
report summarization. Cornegruta et al. (2016)
proposed to use a recurrent neural network archi-
tecture to model radiological language in solving
the medical named entity recognition and nega-
tion detection tasks on radiology reports. To our
knowledge, our work represents the ﬁrst attempt

205at automatic summarization of radiology ﬁndings
into natural language impression statements.

3 Task Deﬁnition
We now give a formal deﬁnition of the task of
summarizing radiology ﬁndings. Given a passage
of ﬁndings represented as a sequence of tokens
x = {x1, x2, . . . , xN}, with N being the length
of the ﬁndings, our goal is to ﬁnd a sequence of
tokens y = {y1, y2, . . . , yL} that best summarizes
the salient and clinically signiﬁcant ﬁndings in x,
with L being an arbitrary length of the summary.1
Note that the mapping between x and y can either
be modeled in an unsupervised way (as done in un-
supervised summarization systems), or be learned
from a dataset of ﬁndings-summary pairs.

4 Models
In this section we introduce our model for the
task of summarizing radiology ﬁndings. As our
model builds on top of existing work on neu-
ral sequence-to-sequence learning and the pointer-
generator model, we start by introducing them.

4.1 Neural Sequence-to-Sequence Model
At a high-level, our model implements the sum-
marization task with an encoder-decoder architec-
ture, where the encoder learns hidden state repre-
sentations of the input, and the decoder decodes
the input representations into an output sequence.
For the encoder, we use a Bi-directional Long
Short-Term Memory (Bi-LSTM) network. Given
the ﬁndings sequence x = {x1, x2, . . . , xN}, we
encode x into hidden state vectors with:

h = Bi-LSTM(x),

(1)
where h = {h1, h2, . . . , hN}. Here hN combines
the last hidden states from both directions in the
encoder.

After the entire input sequence is encoded, we
generate the output sequence step by step with a
separate LSTM decoder. Formally, at the t-th step,
given the previously generated token yt−1 and the
previous decoder state st−1, the decoder calculates
the current state st with:

st = LSTM(st−1, yt−1).

(2)

We then use st to predict the output word. For the
initial decoder state we set s0 = hN .

1While the name “impression” is often used in clinical set-

tings, we use “summary” and “impression” interchangably.

(cid:88)

h∗
t =

The vanilla sequence-to-sequence model that
uses only st to predict the output word has a major
limitation: it generates the entire output sequence
based solely on a vector representation of the in-
put (i.e., hN ), which may result in signiﬁcant in-
formation loss. For better decoding we therefore
employ the attention mechanism (Bahdanau et al.,
2015; Luong et al., 2015), which uses a weighted
sum of all input states at every decoding step.

Given the decoder state st and an input hidden

state hi, we calculate an input distribution at as:

i = v(cid:62) tanh(Whhi + Wsst),
et
at = softmax(et),

(3)
(4)
where Wh, Ws and v are learnable parameters.2
We then calculate a weighted input vector as:

at
ihi.

(5)

i

t ])), (6)

h∗
t encodes the salient input information that is
useful at decoding step t. Lastly, we obtain the
output vocabulary distribution at step t as:
P (yt|x, y<t) = softmax(V (cid:48) tanh(V [st; h∗
where V (cid:48) and V are learnable parameters.
4.2 Pointer-Generator Network
While the encoder-decoder framework described
above can generate impressions from a ﬁxed vo-
cabulary, the model can clearly beneﬁt from being
able to “copy” salient observations directly from
the input ﬁndings. To add such “copying” capacity
into the model, we use a pointer-generator network
similar to the one described in See et al. (2017).

The main idea is that at each decoding step t,
we allow the model to either generate a word from
the vocabulary with a generation probability pgen,
or copy a word directly from the input sequence
with probability 1 − pgen. We model pgen as:
s st + wyyt−1),

pgen = σ(w(cid:62)

t + w(cid:62)

h∗h∗

(7)

where yt−1 denotes the previous decoder output,
wh∗, ws and wy learnable parameters and σ a
sigmoid function. For the copy distribution, we
reuse the attention distribution at calculated in (4).
Therefore, the overall output distribution in the
pointer-generator network is:
P (yt|x, y<t) = pgenPvocab(yt)+(1−pgen)

(cid:88)

at
i,

(8)
2For clarity we leave out the bias terms in all linear layers.

i:xi=yt

206
Creating a Dataset for Multilingual Fine-grained Emotion-detection

Using Gamiﬁcation-based Annotation

Emily ¨Ohman

Kaisla Kajava

J¨org Tiedemann

Timo Honkela

University of Helsinki

ﬁrstname.lastname@helsinki.ﬁ

Abstract

This paper introduces a gamiﬁed framework
for ﬁne-grained sentiment analysis and emo-
tion detection. We present a ﬂexible tool, Sen-
timentator, that can be used for efﬁcient an-
notation based on crowd sourcing and a self-
perpetuating gold standard. We also present
a novel dataset with multi-dimensional anno-
tations of emotions and sentiments in movie
subtitles that enables research on sentiment
preservation across languages and the creation
of robust multilingual emotion detection tools.
The tools and datasets are public and open-
source and can easily be extended and applied
for various purposes.
Introduction

1
Sentiment analysis and emotion detection is a cru-
cial component in many practical applications but
also deﬁnes a great challenge in natural language
processing and artiﬁcial intelligence. Detecting
emotions is crucial in human-computer interac-
tion, and human behavior in communication is to
a large degree affected by the emotional states that
are created in a message. These states are typi-
cally ﬁne-grained and fuzzy covering various di-
mensions of human feelings and attitudes. Nev-
ertheless, it is often the practice to consider sen-
timents and emotions as very coarse and discrete
features that can be detected with simple classi-
ﬁers on a small scale of a few classes.

In our work, we focus on a high-dimensional
model of emotions that allows a more natural and
ﬁne-grained classiﬁcation and, furthermore, we
tackle emotion detection in a multilingual setting.
One of the biggest issues that stand in the way of
creating reliable emotion detection algorithms is
the lack of properly annotated datasets for training
and testing purposes, especially in the case of the
dimensionality that we consider and the multilin-
gual support that we envision. This is the reason

why we created Sentimentator, a new annotation
tool that facilitates the efﬁcient creation of appro-
priate datasets ( ¨Ohman and Kajava, 2018).

The main contribution of the paper is the frame-
work based on a gamiﬁed environment that we
develop to efﬁciently build large-scale resources.
Our setup results in a self-perpetuating gold stan-
dard, which is initialized by seed sentences that are
annotated by experts and augmented by crowd an-
notators. A combination of correlation-based scor-
ing and ranking makes it possible to build datasets
with weighted judgments based on the annotator
conﬁdence that we measure. Initial rankings are
based on the comparison to seed annotation only
but they will be adjusted dynamically once the cor-
relation between crowd annotators allows to esti-
mate further reliability scores. The main idea is
that we can trust annotators that provide identical
or at least similar judgments as other reliable an-
notators. With this scheme, we can move away
from the use of limited seed sentences for con-
ﬁdence estimation to a more dynamic and self-
perpetuating gold standard.

Another fundamental decision in our setup is
the use of multilingual material on which to base
our annotations. We are interested in the cross-
lingual use of emotions and the development of
multilingual classiﬁers (see ¨Ohman et al., 2016).
Therefore, we start with sentences extracted from
movie subtitles (English originals in our case) for
which we also have plenty of translations into a
large number of languages. Movies contain a lot
of emotional content and, as a side effect, it is in-
teresting to see how that is reﬂected in subtitles
and their translations.

Before presenting Sentimentator itself, we will
ﬁrst discuss related work and the theoretical
framework we work with. The presentation of the
seed/pilot dataset and its application for emotion
detection follows the description of the tool and

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages24–30Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P1724ends with a concluding discussion.

2 Related Work

Sentiment analysis is a widely studied task in nat-
ural language processing. Most of the existing
datasets applied in sentiment analysis use binary
or ternary annotation schemes (positive-negative,
or positive, negative, and neutral) (Andreevskaia
and Bergler, 2007), or some kind of combination
of these (i.e.
the addition of e.g. ”mixed” (Saif
et al., 2013)). This is not enough if the aim is to
detect emotions rather than overarching sentiment
(de Albornoz et al., 2012; Li and Hovy, 2017;
Cambria et al., 2013). Furthermore, many of the
existing datasets or tools (Munezero et al., 2015;
Eryigit et al., 2013; Musat et al., 2012; Kakko-
nen and Kakkonen, 2011; Calefato et al., 2017;
Saif et al., 2013; Abdul-Mageed and Ungar, 2017)
are domain-dependent (often Twitter data) and/or
document-level. Very few of these are also open
data or open source.

An important questions is whether to show
wider context or not. Boland et al. (2013) show
that context can lead to the effect of double
weighting for ﬁne-grained annotations. For that
reason, we also opted for the annotation of isolated
sentences even though our tools would easily sup-
port other setups.

2.1 Crowd-sourcing Annotations
The annotation of datasets can be very costly and
time consuming (Andreevskaia and Bergler, 2007;
Devitt and Ahmad, 2008) if done by expert anno-
tators. Crowd-sourcing can often be a cheaper al-
ternative to hiring expert annotators, and has been
used successfully by several researchers to create
different types of datasets (Turney, 2002; Green-
hill et al., 2014; Mohammad and Turney, 2013).

However, one issue with using non-experts to
solicit annotations is that there is a risk of the qual-
ity suffering. Our solution to annotation-reliability
related issues is gamiﬁcation, which will be dis-
cussed in detail in section 3.

2.2 Theory of Emotion
The underlying theory of emotion for Sentimen-
tator is Plutchik’s theory of emotion (Plutchik,
1980). The eight core emotions he proposes are
anger, anticipation, disgust, fear, joy, sadness,
surprise, and trust. He uses a wheel, or ﬂower,
to illustrate these emotions. For a more intuitive

interface, we have inverted the wheel (see ﬁgure
1).

Figure 1: Inverted wheel of emotions

Although Sentimentator takes into account the
intensity of the emotions, the complexity of the
annotation task does not increase linearly with the
number of classes this produces. It is possible to
annotate for all 24+1 emotions, but only use the
eight core emotions for classiﬁcation, which was
done very successfully by Abdul-Mageed and Un-
gar (2017).

Using Euclidean distance on the inverted wheel
to calculate the similarity of annotations, we can
see that annotations for neutral and low intensity
emotions are in fact quite similar. This means we
can avoid unnecessarily dismissing an annotation
as noise, as might be the case if a more traditional
interface was used where neutral was a separate
category from low-intensity emotions.

2.3 Classiﬁcation
Table 1 shows the accuracies achieved by a
few other multidimensional approaches (generally
those of Ekman (1971)) using various classiﬁ-
cation methods such as SVMs, neural networks,
maximum entropy, Na¨ıve Bayes, and k-Nearest
Neighbor to name a few. When more classes are
included in a model, accuracies achieved are typi-
cally lower than what binary or ternary models of
sentiment analysis achieve (Purver and Battersby,
2012; Tokuhisa et al., 2008), with the exception
of Abdul-Mageed and Ungar (2017) who apply a
gated recurrent neural network model.

1Quoted in Purver and Battersby (2012)

25Study
Go et al. (2009)
Danisman and Alpkocak (2008)
Chuang and Wu (2004)
Chuang and Wu (2004)
Ansari 20101
Purver and Battersby (2012)
Seol et al. (2008)
Abdul-Mageed and Ungar (2017)
Tokuhisa et al. (2008)
Abdul-Mageed and Ungar (2017)

classes
2
6
6+1
6+1, audio
6
6
8
8
10
24

accuracy
82.2-83%
32%
56-74%
81.5%
81%
varies
45-65%
95.68%
up to 80%
87.58%

Table 1: Accuracies achieved by previous studies

Although the data and methods are different,
it seems reasonable to expect accuracies between
30-70% depending on the category for an initial
multiclass classiﬁcation. Depending on the avail-
ability, we will try to apply our model to the same
datasets that have been used in the studies listed in
table 1 in order to directly compare results.

3 Gamifying the Annotation Platform

Our goal is to implement efﬁcient crowd-sourcing
through gamiﬁcation. Gamiﬁcation refers the use
of game elements in environments that are not typ-
ically games (Deterding et al., 2011; Hamari and
Koivisto, 2013). Previous research shows that one
can achieve a high number of quality annotations
by non-experts by using carefully considered gam-
iﬁed aspects such as (1) Relatedness (connected
to other players), (2) Competence (mastering the
game problems), and (3) Autonomy (control of
own life) (Musat et al., 2012).

Robson et al. (2015) posits that gamiﬁcation
can change behavior by tapping into motivational
drivers of human behavior:
reinforcements and
emotions. The emotions we want to elicit are
of course enjoyment, but negative emotions such
as disappointment can also increase commitment
and a desire to increase one’s competence. Sim-
ilarly, both positive and negative reinforcements
increase repetitive behavior in players (Robson
et al., 2015).

Our platform offers players leaderboards and
statistics about both their immediate and longterm
progress (relatedness). Progress, rank and prestige
are important measures that help players feel com-
pensated for the work they are doing within the
game (competence). Rank has an additional func-
tion in our platform; as we lack a gold standard
against which to compare the annotations we re-
ceive, we use rank to determine noisy annotations

and noisy annotators. Furthermore, the player can
see how each of their choice affects their standing
in the ranks.

The dataset that the experiments in this paper
rely on is our validated seed sentences. These
sentences will be used as a type of seeded gold
standard. What this means is that annotators will
annotate both non-seed sentences and seed sen-
tences. They receive a score from their annota-
tion based on similarity with gold annotations that
determines their rank. In practice, rank is equiv-
alent with conﬁdence level. With enough partic-
ipants, players will also be ranked according to
how closely their annotations match those of high-
ranked annotators. We, thus, have the option to
include only the least noisy, highest quality anno-
tations in our dataset.

In order to compare annotations, we map them
on the inverted Plutchik’s wheel we propose in
Figure 1 projected on a standard two-dimensional
space with coordinates [x, y], where the least in-
tense emotions are at the center and the most in-
tense at the tips of the petals. The origin of our
emotion space is located in the center of the wheel
and represents the case of a neutral expression.

We can then calculate the distance D(Gx, Gy)
between any pair of points corresponding to emo-
tion labels GX and Gy by computing the Eu-
clidean distance normalized by the maximum dis-
tance that can be observed between opposite emo-
tions with maximum intensity. Assuming that ge-
ometric location expresses the relatedness of emo-
tions, this distance metric takes into account all
different types of similarities/dissimilarities be-
tween annotations, including labels that combine
neighboring emotions.

The distance metric is the basis for the compu-
tation of annotation conﬁdence Cx for new anno-
tation Gx that we obtain. We deﬁne annotation
conﬁdence as
Cx = Rannotator ∗ 1
N

(1 − Cn ∗ D(Gx, Gn))

n(cid:88)

n=1

where Gn ∈ {G1, .., GN} are annotations of
the same instance (sentence) from the current gold
standard with corresponding conﬁdence scores
{C1, .., CN}.2
In other words, we add an aver-
aged penalty for annotations that differ from exist-
ing gold annotations weighted by their conﬁdence
scores. Note that our seed annotations Gs obtain

2Note that we set Cx = Rannotator if N = 0.

26a perfect conﬁdence Cs = 1. Another component
of the conﬁdence score is the rank of the annota-
tor based on the score Rannotator. This score is
initialized with one and will be updated by each
submitted annotation. Currently, we use a simple
average over annotation conﬁdence scores of that
particular annotator:

Rannotator =

1
M

M(cid:88)

m=1

Cm

Our

self-perpetuating gold standard with
ranking-based conﬁdence ratings will reduce the
need for manual screening and will ensure that we
can receive consistent emotion annotations with a
measurable conﬁdence attached to them.

4 Creating the Dataset

For the seed data, we used the following proce-
dure: On completed expert annotation, another ex-
pert annotates the same sentences with the data or-
der randomized. Ambiguous sentences were re-
viewed and the correct class was agreed upon. In
some cases where no agreement could be reached
the sentence was excluded from the seed dataset.
Our data collection will be unique in that it will
provide a ﬁne-grained multi-dimensional open
source dataset for sentiment analysis and emo-
tion detection in various languages. Annotation is
on-going and the ﬁrst real dataset will be avail-
able later in 2018. For now we have a set of
sentences with validated annotations that we will
use as our seed data to get the gamiﬁed annota-
tion started. This dataset has already been used
to investigate sentiment preservation in Finnish,
French, and Italian (Kajava, 2018).

We wanted to make the dataset as useful as pos-
sible to as many researchers as possible from the
beginning. This is why we selected an open par-
allel corpus, namely the OPUS movie subtitles
corpus (Tiedemann, 2012; Lison and Tiedemann,
2016). From this collection, approximately 9,000
English sentences were annotated into the follow-
ing emotion classes: anger, anticipation, disgust,
joy, fear, sadness, surprise, and trust. This yielded
a preliminary dataset of between 649 to 908 sen-
tences per class (see table 4). For the classiﬁcation
experiments, we did not take into account the mea-
sure of emotion intensity that we introduce later in
our annotation framework, which is also part of
the seed sentence annotation.

ang
816
92

ant
739
84

dis
775
87

fea
615
70

joy
876
99

sad
633
72

sur
583
66

tru
737
83

Total
5,774
653

train
test

Table 2: Emotion distribution in seed data

We also keep the metadata and therefore all
sentences can be paired with a particular movie,
genre, time period, as well as its counterpart in an-
other language. This is valuable information for
future research avenues.

We expect to have a full dataset by the end of the
year as we will be collecting at least 100 000 anno-
tations in September-October of 2018 from crowd
annotators (students). Snow et al. (2008) suggest
using four non-experts to match the quality of one
expert annotator, however, gamiﬁcation, seed sen-
tences, and rank-validation means that fewer an-
notations per sentence might be sufﬁcient using
our platform. Inter-annotator agreement is on av-
erage around 70-90% depending on the type of
annotation and who is doing the annotation work
(expert vs. non-expert) (Nowak and R¨uger, 2010),
and this is where annotator agreement was for our
data as well, varying between classes.

Based on initial timed annotations, a typical an-
notator can be expected to annotate up to 10 sen-
tences per minute. This means that it only takes
just over one hour to annotate around 600 sen-
tences.
If every annotator is asked to annotate
1000 sentences, this should not take more than
a few hours each on average taking the learning
curve into account. The students are encouraged to
annotate in languages other than English as well,
resulting in at least two or three separate datasets
with an expected minimum of 40 000 annotations
each. We currently have preliminary datasets for
English, Italian, French, and Finnish.

5 Validation of the Data Quality

In order to test the quality of the data for the pur-
pose of developing an automatic emotion detector
we ran some initial experiments using our anno-
tated seed data for training, and evaluating stan-
dard multi-class classiﬁers. The data was tok-
enized and lowercased as a preprocessing step. We
selected Multinomial Na¨ıve Bayes (NB) and Mul-
tilayer Perceptron (MLP) classiﬁers for our exper-
iments. For the classiﬁers we use the scikit-learn
(Pedregosa et al., 2011) machine learning toolkit.
In both classiﬁcation scenarios, the data was split
into class-stratiﬁed training and test sets of 90%

27and 10%, respectively.

The MLP network used in this work is a three-
layer network. The model creates a lexicon from
the dataset using a bag-of-words approach, em-
ploying it for extracting a set of features for each
class. We use Adam for training the network
and apply Rectiﬁed Linear Unit (ReLu) activiation
functions in the hidden layers of feed-forward net-
work.

ang
62
9
25
11
2
7
5
5

ant dis
3
10
43
6
29
6
6
8
5
4
2
8
10
4
3
5

fea
3
1
5
20
1
5
4
3

joy
5
5
9
2
77
12
13
16

sad sur
1
1
3
6
2
30
1
3

3
5
3
5
3
2
25
2

tru <- classiﬁed as
5
14
7
12
5
6
4
45

anger
anticipation
disgust
fear
joy
sadness
surprise
trust

Table 4: Confusion matrix for NB-based classiﬁcation
of the test set.

Classiﬁer Accuracy
NB
MLP

0.5069
0.5023

Table 3: Overall classiﬁcation accuracy

As can be seen in table 3, the baseline classi-
ﬁers perform reasonably well for such a small data
set and such a ﬁne-grained task. Note that we did
spend any time on optimizing features and hyper-
parameters to obtain a better performance. The
purpose of this study is entirely to test the feasi-
bility of ﬁne-grained classiﬁcation and validity of
our seed data.

In the confusion matrix for the best perform-
ing classiﬁer (see Table 4), we can see that there
is some signiﬁcant confusion between anger and
fear, between disgust and sadness and also surpris-
ingly between trust and fear. These are the same
classes that others have struggled to distinguish
(e.g. (Purver and Battersby, 2012)), which is re-
assuring that our data is in good shape. With this
promising performance on our pilot data set (de-
spite its limited size) we are encouraged to pro-
ceed with future experiments and the more ﬁne-
grained distinctions we propose that take intensity
into account.

These experiments also demonstrate that the
seed data is sufﬁcient for initial classiﬁcations and
that we can go ahead in developing our gami-
ﬁed strategy of getting more annotations based on
correlations between annotators and their level of
trust, which will initially be based on the compar-
ison to the validated seed sentences.

6 Conclusions

In this paper, we present an open annotation tool
for ﬁne-grained emotion detection and a dataset
of seed sentences that can be used to gamify the
annotation efforts. The classiﬁcation results show
that the dataset is reliable enough to be used as
seed sentences, indicating that gamiﬁcation based

on the seed data is a viable option for compil-
ing sentiment datasets, and that multidimensional
classiﬁcation yields acceptable results even with
a small dataset. However, already with our lim-
ited validation experiments we can see that the
choice of model and learning algorithm inﬂuences
the quality of the resulting classiﬁer. This is an im-
portant outcome that needs to be considered when
designing tools with scarce resources. We will
continue to monitor performance to measure the
impact of gamiﬁcation and cross-lingual transfer
on classiﬁcation performance.

7 Discussion and Future Work

As our system collects both coarse (ternary) senti-
ment annotations, and ﬁne-grained emotion anno-
tations, a future option could be to apply the suc-
cessful approach demonstrated by Tokuhisa et al.
(2008), and utilize the coarser sentiment polarity
to pre-classify data before emotion classiﬁcation,
and then implement a k-nearest neighbors algo-
rithm on the larger dataset. Our platform does not
show context by default, however, it is easy to add
the context of the target segment to be annotated if
required for a different type of project.

It might be valuable to re-annotate parts of
the data showing additional context to check the
impact on annotation and annotator conﬁdence.
Tokuhisa et al. (2008) found that in their data,
context-dependent samples were useful for train-
ing their classiﬁer and yielded slightly higher ac-
curacies than their non-context-dependent data.
Finally, we will also consider models that make
use of sentence-internal relations to improve the
classiﬁcation results. In particular, we will inves-
tigate the use of sequence models and gated recur-
rent networks as proposed by Abdul-Mageed and
Ungar (2017).

28References
Muhammad Abdul-Mageed and Lyle Ungar. 2017.
Emonet: Fine-grained emotion detection with gated
In Proceedings of the
recurrent neural networks.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 718–728.

Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerv´as. 2012.
Sentisense: An easily scalable
concept-based affective lexicon for sentiment anal-
ysis.

Alina Andreevskaia and Sabine Bergler. 2007. Clac
and clac-nb: Knowledge-based and corpus-based
In Proceedings
approaches to sentiment tagging.
of the 4th international workshop on semantic eval-
uations, pages 117–120. Association for Computa-
tional Linguistics.

Katarina Boland, Andias Wira-Alam, and Reinhard
Messerschmidt. 2013. Creating an annotated corpus
for sentiment analysis of german product reviews.

Fabio Calefato, Filippo Lanubile, and Nicole Novielli.
2017. Emotxt: a toolkit for emotion recognition
from text. arXiv preprint arXiv:1708.03892.

Erik Cambria, Bj¨orn Schuller, Yunqing Xia, and
Catherine Havasi. 2013. New avenues in opinion
IEEE Intelligent
mining and sentiment analysis.
Systems, 28(2):15–21.

Ze-Jing Chuang and Chung-Hsien Wu. 2004. Multi-
modal emotion recognition from speech and text.
International Journal of Computational Linguistics
& Chinese Language Processing, Volume 9, Num-
ber 2, August 2004: Special Issue on New Trends of
Speech and Language Processing, 9(2):45–62.

Taner Danisman and Adil Alpkocak. 2008. Feeler:
Emotion classiﬁcation of text using vector space
In AISB 2008 Convention Communica-
model.
tion, Interaction and Social Intelligence, volume 1,
page 53.

Sebastian Deterding, Miguel Sicart, Lennart Nacke,
Kenton O’Hara, and Dan Dixon. 2011. Gamiﬁ-
cation. using game-design elements in non-gaming
In CHI’11 extended abstracts on human
contexts.
factors in computing systems, pages 2425–2428.
ACM.

Ann Devitt and Khurshid Ahmad. 2008. Sentiment
analysis and the use of extrinsic datasets in evalu-
ation. In LREC.

Paul Ekman. 1971. Universals and cultural differences
in facial expressions of emotion. In Nebraska sym-
posium on motivation. University of Nebraska Press.

G¨ulsen Eryigit, Fatih Samet Cetin, Meltem Yanik,
Tanel Temel, and Ilyas C¸ ic¸ekli. 2013. Turksent:
A sentiment annotation tool for social media.
In
LAW@ ACL, pages 131–134.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N Project Report, Stanford, 1(12).

Anita Greenhill, Kate Holmes, Chris Lintott, Brooke
Simmons, Karen Masters, Joe Cox, and Gary Gra-
ham. 2014. Playing with science: Gamised aspects
of gamiﬁcation found on the online citizen science
project-zooniverse. In GAMEON’2014. EUROSIS.

Juho Hamari and Jonna Koivisto. 2013. Social moti-
vations to use gamiﬁcation: An empirical study of
gamifying exercise. In ECIS, page 105.

Kaisla Kajava. 2018. Cross-lingual sentiment preser-
vation in binary and multi-dimensional classiﬁca-
tion.

Tuomo Kakkonen and Gordana Gali´c Kakkonen. 2011.
Sentiproﬁler: creating comparable visual proﬁles of
sentimental content in texts. Language Technolo-
gies for Digital Humanities and Cultural Heritage,
62:189–204.

Jiwei Li and Eduard Hovy. 2017. Reﬂections on senti-
ment/opinion analysis. In A Practical Guide to Sen-
timent Analysis, pages 41–59. Springer.

Pierre Lison and J¨org Tiedemann. 2016. Opensub-
titles2016: Extracting large parallel corpora from
movie and tv subtitles.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Myriam Munezero, Calkin Suero Montero, Maxim
Mozgovoy, and Erkki Sutinen. 2015. Emotwitter–a
ﬁne-grained visualization system for identifying en-
during sentiments in tweets. In International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics, pages 78–91. Springer.

Claudiu-Cristian Musat, Alireza Ghasemi, and Boi
Faltings. 2012. Sentiment analysis using a novel hu-
man computation game. In Proceedings of the 3rd
Workshop on the People’s Web Meets NLP: Collabo-
ratively Constructed Semantic Resources and Their
Applications to NLP, pages 1–9, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Stefanie Nowak and Stefan R¨uger. 2010. How reliable
are annotations via crowdsourcing: a study about
inter-annotator agreement for multi-label image an-
In Proceedings of the international con-
notation.
ference on Multimedia information retrieval, pages
557–566. ACM.

Emily ¨Ohman, Timo Honkela, and J¨org Tiedemann.
2016. The challenges of multi-dimensional senti-
ment analysis across languages. PEOPLES 2016,
page 138.

Emily ¨Ohman and Kaisla Kajava. 2018. Sentimenta-
tor: Gamifying ﬁne-grained sentiment annotation.
In Digital Humanities in the Nordic Countries 2018.
CEUR Workshop Proceedings.

29F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Robert Plutchik. 1980. A general psychoevolutionary

theory of emotion. Theories of emotion, 1:3–31.

Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
ﬁcation. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 482–491. Association
for Computational Linguistics.

Karen Robson, Kirk Plangger, Jan H. Kietzmann, Ian
McCarthy, and Leyland Pitt. 2015. Is it all a game?
understanding the principles of gamiﬁcation. Busi-
ness Horizons, 58(4):411 – 420.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold.

Yong-Soo Seol, Dong-Joo Kim, and Han-Woo Kim.
Emotion recognition from text using
2008.
knowledge-based ann. In ITC-CSCC: International
Technical Conference on Circuits Systems, Comput-
ers and Communications, pages 1569–1572.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.

Jorg Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey. European Lan-
guage Resources Association (ELRA).

Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classiﬁcation using massive exam-
ples extracted from the web. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 881–888. Association
for Computational Linguistics.

Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised clas-
In Proceedings of the 40th
siﬁcation of reviews.
Annual Meeting on Association for Computational
Linguistics, pages 417–424, Stroudsburg, PA, USA.
Association for Computational Linguistics.

30
Leveraging Writing Systems Change for Deep Learning Based Chinese

Emotion Analysis

Rong Xiang1, Yunfei Long1, Qin Lu1, Dan Xiong1, I-Hsuan Chen2
1Department of Computing, The Hong Kong Polytechnic University

csrxiang,csylong,csluqin,csdxiong@comp.polyu.edu.hk

2Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University

ihcucb@gmail.com

Abstract

Social media text written in Chinese commu-
nities contains mixed scripts including ma-
jor text written in Chinese, an ideograph-
based writing system, and some minor text
using Latin letters, an alphabet-based writ-
ing system. This phenomenon is called writ-
ing systems changes (WSCs). Past studies
have shown that WSCs can be used to ex-
press emotions, particularly where the social
and political environment is more conserva-
tive. However, because WSCs can break the
syntax of the major text, it poses more chal-
lenges in Natural Language Processing (NLP)
tasks like emotion classiﬁcation. In this work,
we present a novel deep learning based method
to include WSCs as an effective feature for
emotion analysis. The method ﬁrst identiﬁes
all WSCs points. Then representation of the
major text is learned through an LSTM model
whereas the minor text is learned by a sepa-
rate CNN model. Emotions in the minor text
are further highlighted through an attention
mechanism before emotion classiﬁcation. Per-
formance evaluation shows that incorporating
WSCs features using deep learning models can
improve performance measured by F1-scores
compared to the state-of-the-art model.

1

Introduction

Emotion analysis has been studied using differ-
ent NLP methods from a variety of linguistic per-
spectives such as semantic, syntactic, and cog-
nitive properties (Barbosa and Feng, 2010; Bal-
amurali et al., 2011; Liu and Zhang, 2012; Wil-
son et al., 2013; Joshi and Itkat, 2014; Long et
al., 2017).
In many areas, such as Hong Kong
and the Chinese Mainland, social media text is of-
ten written in mixed text with major text written
in Chinese characters, an ideograph-based writ-
ing system. The minor text can be written in En-

glish, emoji, Pinyin1 (phonetic denotation for Chi-
nese), or other new Internet shorthand notations
using Roman characters of some Latin-based writ-
ing systems. Using mixed characters in different
writing systems is known as WSCs.

Generally speaking, WSCs refers to the use of
mixed text which switches between two or more
writing systems (Clyne, 2000; Lee and Liu, 2012).
A narrower deﬁnition, often referred to as code-
switching, is the use of more than one linguistic
variety in a manner consistent with the syntax and
phonology of each variety2. The use of alteration
of different systems or languages of symbols is
rooted in pragmatic and socio-linguistic motiva-
tions (Cromdal, 2001; Musk, 2012). The use of
WSCs is a case of Economy principle in language
(Vicentini, 2003) which is pursued by human be-
ing in various activities due to the innate indo-
lence. It aims at the maximum effect with the least
input. For instance, ’Good luck’ becomes more
popular than the Chinese version of ’祝你好运’
(Good luck) because inputting the English version
takes shorter time in expressing the same emotion.
Studies in social psychology (Bond and Lai,
1986; Heredia and Altarriba, 2001) also show that
WSCs is an effective and commonly used strat-
egy to express emotion or mark emotion change
especially in some society where social and politi-
cal environment is more conservative (Wei, 2003).
For instance, a new-born swear word ’zz’ is of-
ten used in place of the Chinese version of ’mo-
ron’. This is because ’zz’, which is the acronym
of the Pinyin ’zhi zhang (moron)’, looks less dis-
respectful lexically and more acceptable in social
networks. With the rapid growth of internation-
alization, Chinese youngsters like to use English
acronyms such as ’wtf’ (what the fuck) ’stfu’ (shut

1https://en.wikipedia.org/wiki/Pinyin
2https://en.wikipedia.org/wiki/Code-switching

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages91–96Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P1791nism before emotion classiﬁcation. The atten-
tion mechanism is achieved by projecting the ma-
jor text representation into attention vectors while
aggregating the representation of the informative
words from WSCs context.

2 The Hybrid Attention Network Model

Let D be the dataset as a collection of documents
for emotion classiﬁcation. Each document di is an
instance in D. The goal of an emotion analysis
is to predict the emotion label for each di. The
set of emotion labels includes {Happiness, Sad-
ness, Anger, Fear, Surprise}. Let us use the term
WSC segments to refer to the minor WSC text
pieces. WSC segments can be easily marked in a
pre-processing step using code ranges of Chinese
characters and Romanized Pinyin or English text.
To make better use of WSCs scripts, a deep
learning based HAN model is proposed to explic-
itly assemble WSCs information in an attention
mechanism. Figure 1 shows the framework of
HAN. The LSTM model on the left side is used
to learn the representation of a document includ-
ing the WSC segments. This is because docu-
ments with WSCs are generally coherent and in-
tact despite few WSC segments that may break
the syntax. The CNN model on the right is used
to learn the representation of WSCs segments ex-
tracted from the sentence because they often occur
discontinuously without syntactic structure. The
outputs of both models are integrated into a hy-
brid attention layer before classiﬁcation is carried
out.

the fuck up). People also use WSCs to express id-
iosyncrasies written in English or other languages
because text in other writing systems is much more
difﬁcult to be censored. For example, the sensitive
term of democracy in Chinese (民 主’min zhu’)
is often written as an intended misspelled Pinyin
minzu or the English word democracy. This pa-
per studies WSCs related textual features from the
orthography perspective to explore their effective-
ness as emotion indicators.

Previous studies in emotion analysis mostly rely
on emotion lexicon, context information, or se-
mantic knowledge to improve sentence level clas-
siﬁcation tasks. This linguistic knowledge is of-
ten used to transform raw data into feature vec-
tor, called feature engineering (Kanter and Veera-
machaneni, 2015). However, WSCs can break the
syntax of the major text and the switched minor
text also lacks linguistic cues in this type of so-
cial media data (Dos Santos and Gatti, 2014). This
makes feature engineering-based methods difﬁcult
to work. Neologism in the Internet forums in-
creases the difﬁculty for both syntactic and seman-
tic analysis.
In particular, newly coined phrases
tend to contain different types of symbols. Despite
the challenges, this type of datasets is rich in shifts
of writing systems orthographically. This charac-
teristic offers reliable clues for emotion classiﬁca-
tion. Since WSCs is relatively common in real-
time on-line platforms like microblog in China
3. This work adopts a broader scope of WSCs
to include switching between two languages, and
change of writing systems in the same language
such as Chinese characters to Pinyin notations.
Notably, the accessibility of different character
sets and symbols, as well as the frequent expo-
sures to other languages and cultures characterize
the nature of such short and informal text.

This paper presents our work in progress which
uses a novel deep learning based method to in-
corporate textual features associated with WSCs
via an attention mechanism. More speciﬁcally,
the proposed Hybrid Attention Network (HAN)
method ﬁrst identiﬁes all WSCs points. The rep-
resentation of the major text is learned through
a Long-Short Term Memory (LSTM) model
whereas the representation of the minority text is
learned by a separate Convolution Neural Network
(CNN). Emotions expressed in the minor text are
further highlighted through an attention mecha-

3https://en.wikipedia.org/wiki/Microblogging

Figure 1: Hybrid attention network framework

92Using deep learning methods, the word repre-
sentation in di = (cid:126)w1, ..., (cid:126)wm, di, is learned us-
ing two networks. To distinguish the WSC units,
j ⊂di,
they are given designed switch labels (cid:126)ws
j = 1...k) and are extracted to be fed into the CNN
as an extra feature. di is fed into LSTM to gener-
ate the hidden vector (cid:126)h1, (cid:126)h2...(cid:126)hm from di. In Chi-
nese social media, WSCs segments are generally
dispersed sporadically. So, for di with k WSC seg-
ments, the convolution is calculated using a sliding
window of size 2n + 1:

j( (cid:126)ws

k+n(cid:88)
−−−→convp =
(cid:80)k

p=k−n

(cid:126)Rwsc =

p=1
k

(cid:126)ws
p,

−−−→convp

(1)

(2)

.

and

The WSC feature vector Rwsc is generated by

average pooling.

Attention model was introduced by Yang et
al.(2016) to show different contribution of differ-
ent words semantically. To include both the infor-
mation learned from LSTM and CNN, the consol-
idated representation, (cid:126)up, includes the representa-
tions of both (cid:126)hp, and the WSC representation vec-
tor (cid:126)Rwsc into a perceptron deﬁned below:

(cid:126)up = tanh(W(cid:126)hp + Wwsc (cid:126)Rwsc + b).

(3)

In order to re-evaluate the signiﬁcance of word (cid:126)wp,
a coefﬁcient vector (cid:126)U is introduced as an infor-
mative representation of the words in a network
memory. The representation of a word (cid:126)up and the
corresponding word-level context vector (cid:126)U is inte-
grated to obtain a normalized attention weight:

(cid:80)
exp( (cid:126)up · (cid:126)U )
p exp( (cid:126)up · (cid:126)U )

.

αp =

(4)

The updated document representation (cid:126)v can be
generated as a weighted sum of the word vectors
given below:

(cid:126)v =

(αp(cid:126)hp),

(5)

p

where (cid:126)v contains both document information and
WSCs representation with attention shall be used
in the ﬁnal SoftMax method, producing the out-
put vector. Lastly, an argmax classiﬁer is used to
predict the class label.

(cid:88)

3 Performance Evaluation
A Chinese microblog dataset is used for perfor-
mance evaluation (Lee and Wang, 2015). We ﬁrst
present the dataset with some analysis ﬁrst and
then proceed to make performance comparison to
baseline systems on emotion classiﬁcation.

3.1 Dataset and Statistics
The dataset for WSCs is collected from Chinese
microblog by Lee’s group (2015).
It contains
8,728 instances with an average length of 48.8.
Every instance contains at least one WSCs script.
In previous studies, half is used as the training set
and the rest serves as the testing set.

The major text is written in Chinese characters.
The WSC segments contain English words and
Pinyin scripts, acronyms of Pinyin or other scripts.
The annotation of emotions in each instance al-
lows more than one class label. Each instance is
labeled independently by the ﬁve emotion classes,
happiness, sadness, anger, fear and surprise, based
on the Ekman model (1992) except for Disgust.
The emotion label can be contributed by Chinese
text (E1), WSCs (E2) or Both (E3). Some of the
instances have NULL emotion labels (E4). Out of
the 6 labels, 25% of all instances has the happiness
label, which is the most signiﬁcant emotion. 16%
has the sadness label, the percentages of anger,
fear, surprise and NULL labels are 9%, 9%, 11%
and 30%, respectively. Below shows four example
instances in the three WSC types:

E1 Emotion: Happiness

这个年每天都吃好饱！初三来点小朋友的
最爱 麦当劳和pizza！！(We are so full for
every meal during Spring Festival! Will take
kids to their favorite, MacDonald and pizza!)

E2 Emotion: Anger

我们会因为金希澈开了微博而看到许多
无下限的nc发言。(We will see a lot of mo-
rons (”nc”) comments once Jin Xicheu opens
her Weibo.)(”nc” is short for Pinyin ”nao-
can”, which means moron.)

E3 Emotion: Happiness

真 是 速 度 啊 ， 收 到 快 递 了 。 。happy!
(What a fast delivery, I got the parcel already,
happy!)

E4 Emotion: None

我在bean bar (I am in the bean bar.)

93Hap
SVM 0.693
CNN
0.675
LSTM 0.717
BAN
0.724
0.729
HAN

Sad Anger
0.640
0.671
0.704
0.712
0.729

0.560
0.618
0.642
0.649
0.658

Fear Surprise Avg. F1 Wgt. F1
0.549
0.623
0.641
0.596
0.671
0.606
0.627
0.678
0.688
0.625

0.593
0.603
0.628
0.628
0.641

0.607
0.633
0.659
0.668
0.676

Table 1: Performance evaluation (best result: marked bold; 2nd best: underlined.)

3.2 Analysis of WSCs Linked to Emotions
According to the work of Lee and Wang (2015),
emotion words often serve as the cues of bilingual
context. However, many WSCs segments which
are not emotional words can also express emotion.
To gain more insight on different types of WSC
segments, we examine three types of WSCs: (1)
English emotion words found from the NRC emo-
tion lexicon (Mohammad and Turney, 2013), (2)
Pinyin/acronym segments, and (3) others which
include English words (no emotion), symbolic ex-
pressions, and emoji symbols, etc..

Figure 2 depicts their distribution in training
dataset in different emotion labels. Note that En-
glish emotion words only serve about 1/3 of all
WSCs for emotion. The largest group of WSC
is in the category of ’Others’. This means non-
emotion linked English words and symbols of
other orthographic forms place an important part
in emotion analysis of text with WSCs.

Figure 2: Distribution of different types of WSCs

3.3 Emotion Analysis
A group of experiments are implemented to exam-
ine the performance of different emotion classiﬁ-
cation methods evaluated using average F1-score

and weighted F1-score4. The baseline algorithms
include BAN (Wang et al., 2016), the current state-
of-the-art emotion classiﬁcation algorithm. Oth-
ers used in the comparison include SVM (Mullen
and Collier, 2004), CNN and LSTM (Rosenthal
et al., 2017). For all these baseline methods, WSC
segments are included in the text. The difference
compared to our HAN model is that we also sepa-
rately extract WSCs segments and feed them into
a separate CNN model.

Table 1 shows the performance evaluation re-
sult. From Table 1 we can draw a number of
observations. Firstly, the performance of SVM
is the worst since it lacks phrase level analytical
capability because each word is considered inde-
pendently in SVM. In other words, insufﬁcient
amount of information is learned in such a simple
method. Secondly, the average weighted F1-score
of CNN is lower than that of LSTM, indicating
that the memory mechanism is effective in learn-
ing semantic information sequentially. The 3.0%
gap of weighted F1-score shows that the order of
words is valuable in emotion analysis. Thirdly, in
addition to the improvement by BAN compared
to CNN and LSTM, including WSCs in BAN can
give performance gains 0.7% increase in weighted
F1 measures. For the largest class Happy, the im-
provement is over 0.7% increase. Finally, com-
pared to BAN, our proposed HAN which makes
additional use of WSCs in a separate CNN gives
another 1.0% performance gain.

3.4 Effects of Different Types of Text
In this set of experiments, we investigate the effect
of three types of text, CN only (stands for Chi-
nese), WSC segments, and CN+WSCs which are
complete instances with both Chinese and WSC
segments. We take LSTM and BAN to be com-
pared to our proposed HAN.

Table 2 shows the performance evaluation of

4https//en.wikipedia.org/wiki/F1 score

94LSTM(WSCs)
LSTM(CN)
LSTM(CN+WSCs)
BAN(WSCs)
BAN(CN)
BAN(CN+WSCs)
HAN(CN+WSCs; WSCs)

Hap
0.631
0.695
0.717
0.631
0.698
0.724
0.729

Sad
0.546
0.632
0.642
0.551
0.626
0.649
0.658

Anger Fear
0.682
0.589
0.612
0.671
0.606
0.704
0.681
0.589
0.613
0.669
0.627
0.712
0.729
0.625

Surprise Avg. F1 Wgt. F1
0.529
0.615
0.628
0.529
0.631
0.628
0.641

0.595
0.645
0.659
0.596
0.647
0.668
0.676

0.598
0.656
0.671
0.599
0.658
0.678
0.688

Table 2: Performance using single writing system;best result is marked bold; second best is underlined.

HAN(CN; WSCs)
HAN(CN+WSCs; CN)
HAN(CN+WSCs; WSCs)

Hap
0.629
0.720
0.729

Sad
0.613
0.646
0.658

Anger Fear
0.682
0.588
0.624
0.698
0.729
0.625

Surprise Avg. F1 Wgt. F1
0.531
0.616
0.641

0.606
0.661
0.676

0.613
0.673
0.688

Table 3: Performance by multiple writing systems;best result is marked bold; second best is underlined.

the three systems using different text types. Obvi-
ously, Chinese text carries more emotional infor-
mation than WSCs text. The use of both Chinese
and WCSs text gives better performance which
shows that both WSCs and Chinese text contribute
to the prediction task. However, by extracting
WSCs into a separate CNN and re-merging into
the attention layer, HAN still gives the best perfor-
mance. This is because we are able to extract more
emotion related information for the WSCs with a
separate CNN. The attention model also gives dual
consideration to WSCs. Nevertheless, note that
BAN(CN+WSCs) can be competitive when both
Chinese text and WSCs are learned.

To further analyze the effect of the two sub-
models of LSTM and CNN in HAN, we examine
the performance of HAN with different types of
text to be taken by the two sub-models. In Table 3,
the ﬁrst type of text in the parenthesis denotes the
text for the LSTM sub-model. The second text is
used by the CNN sub-model. Note that in the ﬁrst
combination, only Chinese text without WSCs is
used by the LSTM. Because this will break the
syntax of the Chinese text, the result is the worst.
In the second evaluation, the CNN sub-model is
fed with Chinese text only. Still its performance is
about 6% better than the ﬁrst combination. Com-
paring HAN(CN+WSCs; CN) with the state-of-
the-art method BAN(CN+WSCs), we can see that
applying CNN to Chinese text will only introduce
more noise and will not help to make performance
improvement. Obviously, this gives more justi-
ﬁcation of using HAN(CN+WSCs; WSCs) as it
gives the best performance gain.

4 Conclusion and Future Work

This paper presents a work in progress of an HAN
model based on an LSTM model for emotion anal-
ysis in the context of WSCs in social media. We
argue that WSCs text is potentially informative in
emotion classiﬁcation tasks such that they should
be used as additional information contributing to
deep learning based emotion classiﬁcation mod-
els. Our proposed method offers a novel way to
integrate multiple types of writing systems into an
attention-based LSTM model. Along with WSCs
text, the descriptive text of the major writing sys-
tem is informatively valuable and semantic and af-
fective information can be captured by LSTM ef-
fectively. Furthermore, WSCs texts indeed contain
addition semantic and affective information which
can be captured by a CNN model. After combin-
ing the representation of both the complete text
and the WSCs, the two vectors are incorporated
as the ﬁnal feature.

Future works include two directions. One is to
further evaluate the performance of HAN using
larger corpora as currently only one public acces-
sible corpus for writing systems for the Chinese
communities can be studied. Another direction is
to give more detailed study on how people use dif-
ferent types of WSCs to express emotions in cen-
sorship detection studies.

Acknowledgments

The work is partially funded by UGC under GRF
projects(PolyU 152111/14E & U152006/16E).

95Yunfei Long, Lu Qin, Rong Xiang, Minglei Li, and
Chu-Ren Huang. 2017. A cognition based attention
model for sentiment analysis. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 473–482.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In EMNLP, volume 4, pages 412–
418.

Nigel Musk. 2012. Performing bilingualism in wales.
Pragmatics. Quarterly Publication of the Interna-
tional Pragmatics Association (IPrA), 22(4):651–
669.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. Semeval-2017 task 4: Sentiment analysis in
In Proceedings of the 11th International
twitter.
Workshop on Semantic Evaluation (SemEval-2017),
pages 502–518.

Alessandra Vicentini. 2003. The economy principle
in language. Notes and Observations from early
modern english grammars. Mots, Palabras, Words,
3:37–57.

Zhongqing Wang, Yue Zhang, Sophia Lee, Shoushan
Li, and Guodong Zhou. 2016. A bilingual atten-
tion network for code-switched emotion prediction.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 1624–1634.

Jennifer MY Wei. 2003. Codeswitching in campaign-
ing discourse: The case of taiwanese president chen
shui-bian. Language and Linguistics, 4(1):139–165.

Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Sentiment analysis in twitter. In Proceedings
of the International Workshop on Semantic.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

References
AR Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2011. Harnessing wordnet senses for su-
In Proceedings
pervised sentiment classiﬁcation.
of the conference on empirical methods in natural
language processing, pages 1081–1091. Association
for Computational Linguistics.

Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
In Proceedings of the 23rd International
data.
Conference on Computational Linguistics: Posters,
pages 36–44. Association for Computational Lin-
guistics.

Michael H Bond and Tat-Ming Lai. 1986. Embarrass-
ment and code-switching into a second language.
Journal of Social Psychology, 126(2):179–186.

Michael Clyne. 2000. Constraints on code-switching:
How universal are they. The bilingualism reader,
pages 257–280.

Jakob Cromdal. 2001. Overlap in bilingual play: Some
implications of code-switching for overlap resolu-
tion. Research on Language and Social Interaction,
34(4):421–451.

C´ıcero Nogueira Dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In COLING, pages 69–78.

Paul Ekman. 1992. An argument for basic emotions.

Cognition & emotion, 6(3-4):169–200.

Roberto R Heredia and Jeanette Altarriba. 2001. Bilin-
gual language mixing: Why do bilinguals code-
switch? Current Directions in Psychological Sci-
ence, 10(5):164–168.

Neha S Joshi and Suhasini A Itkat. 2014. A survey on
feature level sentiment analysis. International Jour-
nal of Computer Science and Information Technolo-
gies, 5:5422–5425.

James Max Kanter and Kalyan Veeramachaneni. 2015.
Deep feature synthesis: Towards automating data
science endeavors. In Data Science and Advanced
Analytics (DSAA), 2015. 36678 2015. IEEE Inter-
national Conference on, pages 1–10. IEEE.

Jyh-An Lee and Ching-U Liu. 2012. Forbidden city
enclosed by the great ﬁrewall: The law and power
of internet ﬁltering in china. Minn. JL Sci. & Tech.,
13:125.

Sophia Lee and Zhongqing Wang. 2015. Emotion in
code-switching texts: Corpus construction and anal-
ysis. In Proceedings of the Eighth SIGHAN Work-
shop on Chinese Language Processing, pages 91–
99.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining text data,
pages 415–463. Springer.

96
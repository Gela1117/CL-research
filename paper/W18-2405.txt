Named Entity Recognition for Hindi-English Code-Mixed Social Media

Text

Vinay Singh, Deepanshu Vijay, Syed S. Akhtar, Manish Shrivastava

Language Technologies Research Centre (LTRC)

International Institute of Information Technology Hyderabad, Telangana, India

{vinay.singh, deepanshu.vijay, syed.akhtar}@research.iiit.ac.in

m.shrivastava@iiit.ac.in

Abstract

Named Entity Recognition (NER) is a
major task in the ﬁeld of Natural Lan-
guage Processing (NLP), and also is a sub-
task of Information Extraction. The chal-
lenge of NER for tweets lies in the in-
sufﬁcient information available in a tweet.
There has been a signiﬁcant amount of
work done related to entity extraction, but
only for resource-rich languages and do-
mains such as the newswire. Entity ex-
traction is, in general, a challenging task
for such an informal text, and code-mixed
text further complicates the process with
it’s unstructured and incomplete informa-
tion. We propose experiments with dif-
ferent machine learning classiﬁcation al-
gorithms with word, character and lexical
features. The algorithms we experimented
with are Decision tree, Long Short-Term
Memory (LSTM), and Conditional Ran-
dom Field (CRF). In this paper, we present
a corpus for NER in Hindi-English Code-
Mixed along with extensive experiments
on our machine learning models which
achieved the best f1-score of 0.95 with
both CRF and LSTM.

Introduction

1
Multilingual speakers often switch back and forth
between languages when speaking or writing,
mostly in informal settings. This language inter-
change involves complex grammar, and the terms
“code-switching” and “code-mixing” are used to
describe it Lipski. Code-mixing refers to the
use of linguistic units from different languages
in a single utterance or sentence, whereas code-
switching refers to the co-occurrence of speech ex-
tracts belonging to two different grammatical sys-

tems Gumperz. As both phenomena are frequently
observed on social media platforms in similar con-
texts, we use only the code-mixing scenario in this
work.

Following are some instances from a Twitter
corpus of Hindi-English code-mixed texts also
transliterated in English.

T1 : “Finally India away series jeetne mein
successful ho hi gayi :D”

Translation:
winning the away series :D”

“Finally India got success in

T2 : “This is a big surprise that Rahul Gandhi
congress ke naye president hain.”

Translation:
“This is a big surprise that
Rahul Gandhi is the new president of Congress.”

However, before delving further into code-
mixed data, it is important to ﬁrst address the
complications in social media data itself. First,
the shortness of micro-blogs makes them hard
to interpret. Consequently, ambiguity is a ma-
jor problem since semantic annotation methods
cannot easily make use of co-reference informa-
tion. Second, micro-texts exhibit much more lan-
guage variation, tend to be less grammatical than
longer posts, contain unorthodox capitalization,
and make frequent use of emoticons, abbreviations
and hashtags, which can form an important part of
the meaning. Most of the research has, however
been focused on resource rich languages, such
as English Sarkar, GermanTjong Kim Sang and
De Meulder, French Azpeitia et al. and Spanish
Zea et al.. However entity extraction and recog-
nition from social media text for Indian languages
Saha et al.; Ekbal and Bandyopadhyay; Malarkodi
et al. and Code-Mixed text Gupta et al. have been

ProceedingsoftheSeventhNamedEntitiesWorkshop,pages27–35Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics27introduced a bit late. Chieu and Ng A shared task
in FIRE-15 workshop1 and explicitly NER task on
Code-Mixed in FIRE 20162.

The structure of the paper is as follows.

In
Section 2, we review related research in the area
of Named Entity Extraction on code-mixed social
media texts. In Section 3, we describe the corpus
creation and annotation scheme. In Section 4, we
discuss the data statistics. In Section 5, we sum-
marize our classiﬁcation systems which includes
the pre-processing steps and construction of fea-
ture vector.
In Section 6, we present the results
of experiments conducted using various character,
word level and lexical features using different ma-
chine learning models. In the last section, we con-
clude our paper, followed by future work and the
references.

2 Background and Related work

Bali et al. performed analysis of data from Face-
book posts generated by English-Hindi bilingual
users. Analysis depicted that signiﬁcant amount
of code-mixing was present in the posts. Vyas
et al. formalized the problem, created a POS tag
annotated Hindi-English code-mixed corpus and
reported the challenges and problems in the Hindi-
English code-mixed text. They also performed
experiments on language identiﬁcation, translit-
eration, normalization and POS tagging of the
Dataset. Sharma et al. addressed the problem of
shallow parsing of Hindi-English code-mixed so-
cial media text and developed a system for Hindi-
English code-mixed text that can identify the lan-
guage of the words, normalize them to their stan-
dard forms, assign them their POS tag and seg-
ment into chunks. Barman et al. addressed the
problem of language identiﬁcation on Bengali-
Hindi-English Facebook comments.

In Named Entity Recognition there has been
signiﬁcant research done so far in English and
other resource rich languages Morwal et al.; Sri-
hari et al., but same cannot be said for code-mixed
text due to lack of structured resources in this do-
main. Bhargava et al. proposed a hybrid model for
NER on Hindi-English and Tamil-English code-
mixed Dataset. Bhat et al. proposed a neural
network architecture for NER on Hindi-English
code-mixed Dataset. Code-mixing got attention in
FIRE-2016 with the introduction of tasks on Code-

1http://ﬁre.irsi.res.in/ﬁre/2015/home
2http://www.au-kbc.org/nlp/CMEE-FIRE2016/

Mixed resources. Now code-mixing has found its
application in different areas such as Query Label-
ing Bhargava et al., Sentiment Analysis Bhargava
et al., Question Classiﬁcation etc.

3 Corpus and Annotation

The corpus that we created for Hindi-English
code-mixed tweets contains tweets from last 8
years on topics like politics, social events, sports,
etc. from the Indian subcontinent perspective. The
tweets were scrapped from Twitter using the Twit-
ter Python API3 which uses the advanced search
option of twitter. The mining of the tweets are
done using some speciﬁc hash-tags and are mined
in a json format which consist all the informa-
tion regarding the tweets like time-stamps, URL,
text, user, replies, etc. Extensive pre-processing
(Section 5.4) was carried out to remove the noisy
and non-useful tweets. Noisy tweets are the ones
which comprise only of hashtags or urls. Also,
tweets in which languages other than Hindi or En-
glish are used were also considered as noisy and
hence removed from the corpus . Furthermore, all
the tweets which were either in only English or
used Devanagari script text are removed too, keep-
ing only the code-mixed tweets. Further cleaning
of data is done in the annotation phase.

3.1 Annotation: Named Entity Tagging
We label the tags with the present three Named
Entity tags ‘Person’, ‘Organization’, ‘Location’,
which using the BIO standard become six NE tags
(B-Tag referring to beginning of a named entity
and I-Tag refers to the intermediate of the entity)
along with the ‘Other’ tag to all those which don’t
lie in any of the six NE tags.

‘Per’ tag refers to the ‘Person’ entity which
is the name of a Person,
twitter handles and
common nick names of people. The ‘B-Per’ states
the beginning and ‘I-Per’ for the name of the
Person, if the Person name or reference is split
into multiple continuous. In the example T3 we
show the instance of ‘Per’ tag in a tweet chosen
from our corpus.
T3: “modi/B-Per ji/I-Per na/Other kya/Other
de/Other
rakha/Other hai/Other media/B-Org
ko/Other ?/Other”
Translation: “What has modi ji given to media?”

3https://pypi.python.org/pypi/twitterscraper/0.2.7

28Tag
B-Loc
B-Org
B-Per
I-Loc
I-Org
I-Per

Total NE tokens

Count of Tokens

762
1,432
2,138

31
90
554
5,007

Table 1: Tags and their Count in Corpus

‘Loc’ tag refers to the location named entity
which is assigned to the names of places for eg.
‘Kashmir’, ‘#Delhi’, ‘Hindustan’, etc. The ‘B-
Loc’ states the beginning and ‘I-Loc’ intermediate
of name of the location, if the location name is
split into multiple tokens. Example T4 shows the
instance of ‘Loc’ tag.

T4 : “jis/Other ki/Other asar/Other saudi/B-Loc
arab/I-Loc mein/Other bhi/Other dikhai/Other
de/Other raha/Other hai/Other corruption/Other
ke/Other khilaf/Other”
Translation: “The effect of which is visible in
saudi arab against corruption”

tag refers

‘Org’
to social, political groups
like Dalit, Bhartiya, Bhartiya Jnata Party (BJP),
social media organizations
Hindus, Muslims,
like facebook, twitter, whatsapp, etc.
and also
govt.
institutions like Reserve bank of India
(RBI), banks, Swiss banks, etc. ‘B-Org’ states the
beginning and ‘I-Org’ intermediate of name of the
organization, if the organizations’ name is split
into multiple tokens. Example T5 shows instance
of ‘Org’ tag in the tweet.

“saare/Other black/Other money/Other
swiss/B-Org bank/I-Org mein/Other

T5:
to/Other
the/Other”
Translation: “all of the black money was in the
swiss bank”

tagging 68,506 tokens.

With these six NE tags and the seventh 7th
tag as “Other” we annotated 3,638 tweets which
meant
The annotated
Dataset with the classiﬁcation system is made
available online.4 The distribution of the tags in
the Dataset is shown in Table 1.

4https://github.com/SilentFlame/Named-Entity-

Recognition

Cohen Kappa
0.98
0.96
0.94
0.98
0.91
0.93

B-Loc
B-Org
B-Per
I-Loc
I-Org
I-Per

Table 2: Inter Annotator Agreement.

Inter Annotator Agreement

3.2
Annotation of the Dataset for NE tags in the tweets
was carried out by two human annotators hav-
ing linguistic background and proﬁciency in both
Hindi and English. In order to validate the qual-
ity of annotation, we calculated the inter annotator
agreement (IAA) between the two annotation sets
of 3,638 code-mixed tweets having 68,506 tokens
using Cohen’s Kappa coefﬁcient Hallgren. Table
2 shows the results of agreement analysis. We ﬁnd
that the agreement is signiﬁcantly high. Further-
more, the agreement of ‘I-Per’ and ‘I-Org’ anno-
tation is relatively lower than that of ‘I-Loc’, this
is because, the presence of uncommon/confusing
names of Organization as well as Person with un-
clear context.

4 Data statistics

Using the twitter API we retrieved 1,10,231
tweets. After manually ﬁltering as described in
Section 3, we are left with 3,638 code-mixed
tweets. This number is close to the size of Dataset
provided by FIRE 2016 which introduced the NER
task for code-mixed text in 2015 with it’s one
shared task on Entity recognition on code-mixed
data. Table 1 shows the distribution of different
tags in the corpus. We use the standard CONLL
tags (Loc, Org, Per, Other) for tagging in the an-
notation stage. The Named Entity (NE) Tag Per-
son (“Per”), Organization (“Org”) and Location
(“Loc”) are the ones we used to tag our corpus
tokens. The ‘Person’ tag comprises names of fa-
mous people, politicians, actresses, sports person-
alities, news reporters and social media celebri-
ties and their twitter handles and nick names if
used frequently as known to the annotator (like
“Pappu for Mr. Rahul Gandhi”). ‘Organizations’
comprises names of social or political organiza-
tions as well as major groups present in India, eg.
Bhartiya Janta Party (BJP), Hindu, Muslim, twit-

29Tag
B-Loc
B-Org
B-Per
Other
I-Loc
I-Org
I-Per

avg / total

Table 3:
depth=32’

Precision Recall F1-score

0.47
0.54
0.65
0.97
0.27
0.23
0.43
0.94

0.50
0.59
0.60
0.97
0.30
0.22
0.38
0.94

0.49
0.56
0.63
0.97
0.29
0.22
0.40
0.94

Decision Tree Model with ‘max-

ter, etc. The Tag ‘Location’ comprises names of
cities, towns, states and countries of the world.
Major of the location entities are present for In-
dian subcontinent places in the corpus. The ones
which does not lie in any of the mentioned tags are
assigned ‘Other’ tag.
5 System Architecture
In this section we’ll explain working of different
machine learning algorithms we used for experi-
ments on our annotated dataset.

5.1 Decision Tree
Decision Tree algorithm belongs to the family of
supervised learning algorithms. Unlike other su-
pervised learning algorithms, decision tree algo-
rithm can be used for solving regression and clas-
siﬁcation problems too. Szarvas et al.
takes a
multilingual named entity recognition system us-
ing boosting and C4.5 decision tree learning al-
gorithm. The decision tree algorithm tries to solve
the problem, by using tree representation. Each in-
ternal node of the tree corresponds to an attribute,
and each leaf node corresponds to a class label.
In decision trees, for predicting a class label for a
record we start from the root of the tree. We com-
pare the values of the root attribute with record’s
attribute. On the basis of comparison, we follow
the branch corresponding to that value and jump
to the next node. The primary challenge in the
decision tree implementation is to identify which
attributes do we need to consider as the root node
and each level. Handling this is know the attributes
selection. We have different attributes selection
measure to identify the attribute which can be con-
sidered as the root note at each level. The popular
attribute selection measures:

• Information gain

• Gini index
Information gain: Using information gain as a
criterion, we try to estimate the information con-
tained by each attribute. By calculating entropy
measure of each attribute we can calculate their
information gain. Information Gain calculates the
expected reduction in entropy due to sorting on the
attribute. Information gain can be calculated as:

H(X) = EX [I(X)] = −(cid:88)

p(x)log(p(x))

x∈X

Where p(x) is the probability of a class for the
feature we are calculating information gain. The
node/feature with lowest entropy is chosen as root
and process is repeated for other level feature se-
lection.

Gini index: It refers to a metric to measure how
often a randomly chosen element would be incor-
rectly identiﬁed. It means an attribute with lower
gini index should be preferred. It is calculated as:

Gini − index = 1 −(cid:88)

p2
j

j

Where pj is the probability of a class for a given

feature we are calculating gini index for.

5.2 Conditional Random Field (CRF)
For sequence labeling (or general structured pre-
diction) tasks, it is beneﬁcial to consider the cor-
relations between labels in neighborhoods and
jointly decode the best chain of labels for a given
input sentence. For example, in POS tagging an
adjective is more likely to be followed by a noun
than a verb, and in NER with standard BIO2 an-
notation (Tjong Kim Sang and Veenstra, 1999) I-
ORG cannot follow I-PER. Therefore, we model
label sequence jointly using a conditional random
ﬁeld (CRF) (Lafferty et al., 2001), instead of de-
coding each label independently. Since here we
are focusing on sentence level and not individual
positions hence it is generally known that CRF can
produce higher tagging accuracy.

Say we are given a sequence of inputs we de-
note by X where X = (x1, x2, x3, . . . , xm) which
are nothing but the words of the sentence and
S = (s1, s2, s3, . . . , sm) as the sequence of out-
put states, i.e the named entity tags. In conditional
random ﬁeld we model the conditional probability
as

p(s1, s2, . . . , sm|x1, x2, . . . , xm)

30Tag
B-Loc
B-Org
B-Per
I-Loc
I-Org
I-Per
Other

avg / total

Precision Recall F1-score

0.76
0.67
0.82
0.70
0.68
0.75
0.96
0.95

0.57
0.33
0.56
0.23
0.27
0.43
0.99
0.95

0.65
0.44
0.67
0.34
0.39
0.55
0.98
0.95

Table 4: CRF Model with ‘c1=0.1’ and ‘c2=0.1’
and ‘L-BFGS’ algorithm

We do this by deﬁning a feature map

Φ(x1, x2, . . . , xm, s1, s2, . . . , sm) ∈ (cid:60)d

that maps the entire input sequence X paired with
an entire state sequence S to some d-dimensional
feature vector. Then we can model the probability
as a log-linear model with parameter vector w ∈
(cid:60)d

p(s|x; w) =

exp(w.Φ(x, s))
(cid:48)
(cid:48) exp(w.Φ(x, s

))

(cid:80)

s

(cid:48)

where s

ranges over all possible input sequences.
For the estimation of w, we assume that we have
i=1. Now we

a set of n labelled examples (xi, si)n
deﬁne regularized log likelihood function L as

L(w) =

log(p(si|xi; w))− λ2
2

||w||2

2−λ1||w||1

2 ||w||2

The terms λ2

2 and λ1||w||1 force the pa-
rameter vector to be small in the respective norm.
This penalizes the model complexity and is known
as regularization. The parameters λ2 and λ1 al-
lows to enforce more or less regularization. The
parameter vector w∗ is then estimated as

w∗ = argmaxw∈(cid:60)dL(w)

If we estimated the vector w∗, we can ﬁnd the most
likely tag for a sentence s∗ for a given sentence
sequence x by

s∗ = argmaxsp(s|x; w∗)

n(cid:88)

i=1

5.3 LSTMs
Recurrent neural networks (RNN) are a fam-
ily of neural networks that operate on sequen-
tial data. They take an input sequence of vec-
tors (x1, x2, . . . , xn) ad return another sequence
(h1, h2, . . . , hn) that represents some information
about the sequence at every step of the input.
In theory RNNs can learn long dependencies but
in practice they fail to do so and tend to be bi-
ased towards the most recent input in the se-
quence.Bengio et al. Long Short Term Memory
networks usually just called ”LSTMs” are a spe-
cial kind of RNN, capable of learning long-term
dependencies. Here with our data where tweets
are not very long in the size LSTMs can provide
us a better result as keeping previous contexts is
one of the specialty of LSTM networks. LSTM
networks were ﬁrst introduced by Hochreiter and
Schmidhuber and then were reﬁned and popular-
ized by many other authors. They work well with
large variety of problems specially the one consist-
ing of sequence and are now widely used. They
do so using several gates that control the propor-
tion of the input to give to the memory cell, and
the proportion from the previous state to forget.

5.4 Pre-processing
This step is done to make the data uniform which
will be beneﬁcial for our system. The preprocess-
ing step consist of

• Removing noisy tweets
• Removing links from tweets
• Tokenization
• Separating words which appear continuous

(i.e Modi.ji.Ke.Liye as ’Modi ji Ke Liye’ )

• Converting to lowercase
• Token encoding (mapping of tokens to their

Tags)

5.5 Features
The feature set consists of word, character and lex-
ical level information like char N-Grams of Gram
size 2 and 3 for sufﬁxes, patterns for punctuation,
emoticons, numbers, numbers inside strings, so-
cial media speciﬁc characters like ‘#’, ‘@’ and
also previous tag information, and the same all

31Tag
B-Loc
B-Org
B-Per
I-Loc
I-Org
I-Per
Other

avg / total

Precision Recall F1-score

0.71
0.62
0.78
0.57
0.60
0.70
0.97
0.95

0.59
0.37
0.57
0.26
0.26
0.42
0.99
0.95

0.64
0.47
0.66
0.36
0.36
0.52
0.98
0.95

Table 5: CRF Model with ‘Avg. Perceptron’ Al-
gorithm

features of the previous and next tokens are used
as context features.

in this paper we have used the following feature

vectors for training of our supervised model.

1. Character N-Grams: Character N-Grams
are language independent Majumder et al.
and have proven to be very efﬁcient for clas-
sifying text. These are also useful in situa-
tions when the text suffers from errors such as
misspellings Cavnar et al.; Huffman; Lodhi
et al.. Group of characters can help in captur-
ing semantic meaning, especially in the code-
mixed language where there is an informal
use of words, which vary signiﬁcantly from
the standard Hindi and English words. We
use character N-Grams as one of the features,
where n is 2 and 3.

2. Word N-Grams: Bag of word features have
been widely used for NER tasks in languages
other than English Jahangir et al.. Thus we
use word N-Grams, where we used the previ-
ous and the next word as a feature vector to
train our model. These are also called con-
textual features.

3. Capitalization:

It is a very general trend
of writing any language in Roman script that
people write the names of person, place or a
things starting with capital letter von D¨aniken
and Cieliebak or for aggression on some-
one/something use the capitalization of the
entire entity name. This will make for two bi-
nary feature vectors one for starting with cap-
ital and other for the entire word capitalized.

4. Mentions and Hashtags:

It is observed
that in twitter users generally tend to address
other people or organization with their user

names which starts with ‘@’ and to empha-
size on something or to make something no-
table they use ‘#’ before that word. Hence
presence of these two gives a good probabil-
ity for the word being a named entity.

5. Numbers in String:

In social media con-
tent, users often express legitimate vocabu-
lary words in alphanumeric form for saving
typing effort, to shorten message length, or
to express their style. Examples include ab-
breviated words like gr8’ (‘great’), ‘b4’ (‘be-
fore’), etc. We observed by analyzing the
corpus that alphanumeric words generally are
not NEs. Therefore, this feature serves as a
good indicator to recognize negative exam-
ples.

6. Previous Word Tag: As mentioned in word
N-Gram feature the context helps in deciding
the tag for the current word, hence the previ-
ous tag will help in learning the tag of current
word and all the I-Tags always come after the
B-Tags.

7. Common Symbols:

It is observed that cur-
rency symbols as well as brackets like ‘(’,
‘[’, etc. symbols in general are followed by
numbers or some mention not of importance.
Hence are a good indicator for the words fol-
lowing or before to not being an NE.

6 Experiments

This section present
the experiments we per-
formed with different combinations of features
and systems.

6.1 Feature and parameter experiments
In order to determine the effect of each feature and
parameter of different models we performed sev-
eral experiments with some set of feature vectors
at a time and all at a time simultaneously chang-
ing the values of the parameters of our models
like criterion (‘Information gain’, ‘gini’), maxi-
mum depth of the tree for Decision tree model,
optimization algorithms, loss functions in LSTM,
regularization parameters and algorithms of opti-
mization for CRF like ‘L-BFGS’ 5, ‘L2 regulariza-
tion’ 6, ‘Avg. Perceptron’, etc. In all the models

5https://en.wikipedia.org/wiki/Limited-memory BFGS
6https://towardsdatascience.com/l1-and-l2-

regularization-methods-ce25e7fc831c

32we mentioned above we validated our classiﬁca-
tion models with 5-fold cross-validation.

Tables 3 shows the experiment result on Deci-
sion tree model with maximum depth = 32 which
we arrived at after ﬁne empirical tuning. Tables
4 and 5 provides the experiments on CRF model.
The c1 and c2 parameters for CRF model refers
to L1 regularization and L2 regularization. These
two regularization parameters are used to restrict
our estimation of w∗ as mentioned in Section 5.1.
When experimented with algorithm of optimiza-
tion as ‘L2 regularization’ or ‘Average Perceptron’
there is not any signiﬁcant change in the results of
our observation both in the per class statistics as
well as the overall. We arrived at these values of
c1 and c2 after ﬁne empirical tuning. Table 4 and
5 refers to this observation.

Next we move to our experiments with LSTM
model. Here we experimented with the optimizer,
activation function along with the number of units
as well as number of epochs. The best result that
we came through was with using ‘softmax’ as ac-
tivation function, ‘adam’ as optimizer and ‘sparse
categorical cross-entropy’ for our loss function.
Table 7 shows the statistics of running LSTM on
our Dataset with 5-fold cross-validation having
validation-split of 0.2 with our char, word and lex-
ical feature set of our tokens. Table 6 shows one
prediction instance of our LSTM model.

6.2 Results and Discussion
From the above results we can say that our system
learns from the structure of the text the speciﬁc NE
types like from Table 6 we can see that our system
is understanding well as it tagged most tokens cor-
rectly.

We also observe that our system is getting con-
fused in the ‘Org’ names that resemble to name of
locations like ‘America’ is tagged as ‘B-Org’ this
is because our system has seen many ‘American’
tokens tagged as ’B-Org’ hence this confusion.

From the example in the Table 11 we can see
that our system learns to tag tokens that starts with
‘#’ as beginning of a NE but majority of the time
tags it as ‘B-Per’ which is a problem. Our model
needs to learn more generic details about these
speciﬁc characters.

For ‘Loc’ and ‘Other’ tags our system works
good, giving accurate predictions. The presence
of confusing names of ‘Location’, ‘Organization’
and that of ‘Person’ in our corpus makes it difﬁ-

Word

Truth Predicted

gi

ke

kya

shaath

#Modi

mil
kar

#America

main
#Trump

#ModiMeetTrump Other
Other
B-Per
I-Per
B-Loc
Other
B-Per
Other
Other
Other
Other
B-Loc
Other
Other
Other
Other
Other
Other
Other
B-Loc
B-Loc

ka
koi
rasta

#EidMubarak

#India
#India

kya
hoga

nikalenge

#Pakistan

Other
Other
B-Per
Other
B-Per
Other
B-Per
Other
Other
Other
Other
B-Org
Other
Other
Other
Other
Other
Other
Other
B-Per
B-Org

Table 6: An Example Prediction of our LSTM
Model

Tag
B-Loc
B-Org
B-Per
I-Loc
I-Org
I-Per
Other

avg / total

Precision Recall F1-score

0.91
0.80
0.88
0.93
0.91
0.82
0.87
0.96

0.89
0.57
0.82
0.47
0.89
0.76
0.83
0.93

0.86
0.63
0.87
0.60
0.89
0.78
0.84
0.95

Table 7: LSTM model with “optimizer=’adam’”

cult for our machine learning models to learn the
proper tags of these names. For eg. ‘Hindustan’
is labeled as ‘B-Loc’ in our annotation and ‘Hin-
dustani’ is as ‘B-Org’ as the former is one of the 5
names of the country India and the later represent
the citizens which makes it a group representation
which we used for Organization during our anno-
tation. Hence lexically similar words with differ-
ent tags makes the learning phase of our model
difﬁcult and hence some incorrect tagging of the
tokens as we can see in Table 6.

337 Conclusion and future work

In this paper, we present a freely available corpus
of Hindi-English code-mixed text, consisting of
tweet ids and the corresponding annotations. We
also present NER systems on this Dataset with ex-
perimental analysis and results. This paper ﬁrst
explains about the reason of selection of some fea-
tures speciﬁc to this task at the same time exper-
imenting our results on different machine learn-
ing classiﬁcation models. Decsion Tree, CRF and
LSTM models worked with a best individual f1-
score of 0.94, 0.95 and 0.95 which is good looking
at the fact that there haven’t been much research in
this domain.

To make the predictions and models’ result
more signiﬁcant the size of the corpus needed to
be expanded. Our corpus has just 3,638 tweets,
but due to unavailability of Hindi-English Code-
Mixed Dataset it is difﬁcult to get large corpus for
our system.

Our contribution in this paper includes the fol-

lowing points:

1. Annotated corpus for Hindi-English Code-
Mixed, kind of which are not available any-
where on the internet.

2. Introduction an addressing of Hindi-English

Code-Mixed data as a research problem.

3. Proposal of suitable features targeted towards

this task.

4. Different models which deals with sequential

tagging and multi-class classiﬁcation.

5. Developing machine learning models on our

annotated corpus for the NE task.

As a part of future work, the corpus can be
annotated with part-of-speech tags at word level
which may yield better results. Moreover, the
Dataset contains very limited tweets having NE
tokens. Thus it can be extended to include more
tweets more of these speciﬁc NE tokens as well
as introducing a more number of tags on the exist-
ing corpus. The annotations and experiments de-
scribed in this paper can also be carried out for
code-mixed texts containing more than two lan-
guages from multilingual societies in future.

References
Andoni Azpeitia, Montse Cuadros, Se´an Gaines, and
German Rigau. 2014. Nerc-fr: supervised named
entity recognition for french. In International Con-
ference on Text, Speech, and Dialogue, pages 158–
165. Springer.

Kalika Bali, Jatin Sharma, Monojit Choudhury, and
Yogarshi Vyas. 2014. ” i am borrowing ya mix-
ing?” an analysis of english-hindi code mixing in
In Proceedings of the First Workshop
facebook.
on Computational Approaches to Code Switching,
pages 116–126.

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code mixing: A challenge
for language identiﬁcation in the language of social
media. In Proceedings of the ﬁrst workshop on com-
putational approaches to code switching, pages 13–
23.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difﬁcult. IEEE transactions on neural
networks, 5(2):157–166.

Rupal Bhargava, Yashvardhan Sharma, and Shubham
Sharma. 2016a. Sentiment analysis for mixed script
indic sentences. In Advances in Computing, Com-
munications and Informatics (ICACCI), 2016 Inter-
national Conference on, pages 524–529. IEEE.

Rupal Bhargava, Yashvardhan Sharma, Shubham
Sharma, and Abhinav Baid. 2015. Query labelling
for indic languages using a hybrid approach.
In
FIRE Workshops, pages 40–42.

Rupal Bhargava, Bapiraju Vamsi, and Yashvardhan
Sharma. 2016b. Named entity recognition for code
mixing in indian languages using hybrid approach.
Facilities, 23:10.

Irshad Ahmad Bhat, Manish Shrivastava,

and
Riyaz Ahmad Bhat. 2016. Code mixed entity ex-
traction in indian languages using neural networks.
In FIRE (Working Notes), pages 296–297.

William B Cavnar, John M Trenkle, et al. 1994. N-
Ann arbor mi,

gram-based text categorization.
48113(2):161–175.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th inter-
national conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.

Pius von D¨aniken and Mark Cieliebak. 2017. Trans-
fer learning and sentence level features for named
entity recognition on tweets. In Proceedings of the
3rd Workshop on Noisy User-generated Text, pages
166–171.

34Kamal Sarkar. 2015.

A hidden markov model
based system for entity extraction from social me-
arXiv preprint
dia english text at ﬁre 2015.
arXiv:1512.03950.

Arnav Sharma, Sakshi Gupta, Raveesh Motlani, Piyush
Bansal, Manish Srivastava, Radhika Mamidi, and
Dipti M Sharma. 2016. Shallow parsing pipeline for
hindi-english code-mixed social media text. arXiv
preprint arXiv:1604.03136.

Rohini Srihari, Cheng Niu, and Wei Li. 2000. A hybrid
approach for named entity and sub-type tagging. In
Proceedings of the sixth conference on Applied nat-
ural language processing, pages 247–254. Associa-
tion for Computational Linguistics.

Gy¨orgy Szarvas, Rich´ard Farkas, and Andr´as Kocsor.
2006. A multilingual named entity recognition sys-
tem using boosting and c4. 5 decision tree learning
algorithms. In International Conference on Discov-
ery Science, pages 267–278. Springer.

Erik F Tjong Kim Sang and Fien De Meulder.
Introduction to the conll-2003 shared task:
2003.
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.

Yogarshi Vyas, Spandana Gella, Jatin Sharma, Ka-
lika Bali, and Monojit Choudhury. 2014. Pos tag-
ging of english-hindi code-mixed social media con-
In Proceedings of the 2014 Conference on
tent.
Empirical Methods in Natural Language Processing
(EMNLP), pages 974–979.

Jenny Linet Copara Zea, Jose Eduardo Ochoa Luna,
Camilo Thorne, and Goran Glavaˇs. 2016. Spanish
ner with word representations and conditional ran-
dom ﬁelds. In Proceedings of the Sixth Named En-
tity Workshop, pages 34–40.

Asif Ekbal and Sivaji Bandyopadhyay. 2008. Bengali
named entity recognition using support vector ma-
chine. In Proceedings of the IJCNLP-08 Workshop
on Named Entity Recognition for South and South
East Asian Languages.

John J Gumperz. 1982. Discourse strategies, volume 1.

Cambridge University Press.

Deepak Gupta, Shubham Tripathi, Asif Ekbal, and
Pushpak Bhattacharyya. 2016. A hybrid approach
for entity extraction in code-mixed social media
data. MONEY, 25:66.

Kevin A Hallgren. 2012. Computing inter-rater relia-
bility for observational data: an overview and tuto-
rial. Tutorials in quantitative methods for psychol-
ogy, 8(1):23.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Stephen Huffman. 1995. Acquaintance: Language-
independent document categorization by n-grams.
Technical report, DEPARTMENT OF DEFENSE
FORT GEORGE G MEADE MD.

Faryal Jahangir, Waqas Anwar, Usama Ijaz Bajwa, and
Xuan Wang. 2012. N-gram and gazetteer list based
named entity recognition for urdu: A scarce re-
sourced language. In 24th International Conference
on Computational Linguistics, page 95.

John Lipski. 1978. Code-switching and the problem
of bilingual competence. Aspects of bilingualism,
250:264.

Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classiﬁcation using string kernels. Journal of Ma-
chine Learning Research, 2(Feb):419–444.

P Majumder, M Mitra, and BB Chaudhuri. 2002. N-
gram: a language independent approach to ir and
nlp. In International conference on universal knowl-
edge and language.

CS Malarkodi, RK Pattabhi, and Lalitha Devi Sobha.
2012. Tamil ner–coping with real time challenges.
In 24th International Conference on Computational
Linguistics, page 23.

Sudha Morwal, Nusrat Jahan, and Deepti Chopra.
Named entity recognition using hidden
2012.
International Journal on
markov model (hmm).
Natural Language Computing (IJNLC), 1(4):15–23.

Sujan Kumar Saha, Sanjay Chatterji, Sandipan Danda-
pat, Sudeshna Sarkar, and Pabitra Mitra. 2008. A
hybrid approach for named entity recognition in in-
In Proceedings of the IJCNLP-08
dian languages.
Workshop on NER for South and South East Asian
languages, pages 17–24.

35
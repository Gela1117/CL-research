Sentence Classiﬁcation for Investment Rules Detection

Youness Mansar and Sira Ferradans

Fortia Financial Solutions

17 av. George V, Paris

France

name.surname@fortia.fr

Abstract

In the last years, compliance requirements
for the banking sector have greatly aug-
mented, making the current compliance
processes difﬁcult to maintain. Any pro-
cess that allows to accelerate the identiﬁ-
cation and implementation of compliance
requirements can help address this issues.
The contributions of the paper are twofold:
we propose a new NLP task that is the
investment rule detection, and a group of
methods identify them. We show that the
proposed methods are highly performing
and fast, thus can be deployed in produc-
tion.

Introduction

1
Compliance requirements have augmented dra-
matically in the last years, specially in the ﬁnan-
cial sector. Investment funds are obliged by law to
publish their investment strategy at a very detailed
level. If the fund does not follow precisely these
rules, it will be ﬁned by the corresponding regu-
latory institution. According to Thomson Reuters
there were regulatory changes every 12 minutes,
on average per day in 2015 (Thomson Reuters,
2015). But, it takes months to implement every
regulatory change, thus, any process that allows
to spot regulatory changes can help accelerate this
updating process. This is important since if an
investment fund does not follow precisely these
rules, it will be ﬁned by the corresponding reg-
ulatory institution.
In fact, in the last years, the
income dedicated to ﬁnes and settlements has in-
creased by almost 45x for the biggest EU and US
banks (Kaminski and Robu, 2016).

The compliance department of Depositary
banks are in charge of controlling that these rules
are actually followed. In order to avoid sanctions,

they deﬁne a 4-eye protocol for rule identiﬁca-
tion. This protocol consists in having two or more
people read and highlight the investment rules of
the prospectus of each investment fund they con-
trol. Once two people have highlighted the same
prospectus, a third person introduces all the rules
in the system. Identifying the rules is time con-
suming and tedious. This process takes days for
human actors, we propose a method that takes sec-
onds thanks to the use of machine learning. Al-
though other methods have acknowledged the im-
portance of having the rules isolated (Cashman
et al., 2002; Beale, 2004), the current systems as-
sume that the rules have already been identiﬁed
and translated into executable code.

In this paper, we propose to detect investment
rules using binary classiﬁcation of sentences. In
section 2, we present the state of the art in sentence
classiﬁcation. In section 3.1, we give all the details
on the data and the posed problem. The proposed
solutions are given in section 3.2 and the obtained
results in section 3.3. Section 4 concludes the pa-
per and gives future work. 1

2 Related Works
Sentence Classiﬁcation. Sentence classiﬁcation
is a classic research area in natural
language
processing. Approaches previous to 2010 focus
mostly on the extraction of document meaning
through representative features that would be used
as input to classic machine learning algorithms,
such as SVM, knn, or Naive Bayes (see (Khan
et al., 2010) for a review on the topic). The rise
of Deep Learning techniques impacts also the sen-
tence classiﬁcation literature, appearing methods
based on CNNs. More speciﬁcally, a modiﬁca-
tion of (Collobert et al., 2011) was proposed by
1Note: There is a Patent Pending for the presented ap-
proach. It was submitted the 18 December 2017 at the EPO
and has the number EP17306801

ProceedingsoftheFirstWorkshoponEconomicsandNaturalLanguageProcessing,pages44–48Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics44Kim (Kim, 2014), showing how a simple model
together with pre-trained word representations can
be highly performing. But the use of word-
embeddings has been challenged for CNNs, (John-
son and Zhang, 2014, 2015) propose a semi-
supervised setting that allows to learn a small text-
region representation. Zhang et al. (Zhang et al.,
2015) propose a CNN based directly on char-
acter representations, without explicitly encoding
words. CNNs are highly dependent on the window
size, (Lai et al., 2015; Visin et al., 2015) propose
the use of Recurrent Convolutional Neural Net-
works to overcome this issue.
(Guggilla et al.,
2016) propose the use of LSTMs for classiﬁcation
of online user comments. In order to avoid prob-
lems due to lack of data, (Liu et al., 2016) propose
multitask learning using LSTMs.

Word embeddings. The lack of big databases
with tagged data is a common problem for Deep
Learning models. Collobert et al. (Collobert et al.,
2011) empirically proved the usefulness of using
unsupervised word representations for a variety of
different NLP tasks and since then, it is widely ac-
cepted that, for small and middle size databases
(< 10k samples), the use of word embeddings im-
proves the ﬁnal results. Word embeddings is the
name associated to a group of language model
methods that map words into a vector space. In-
troduced by Bengio et al. (Bengio et al., 2003),
the authors proposed a statistical language model
based on shallow neural networks. The goal was
to predict the following word, given the previous
context in the sentence, showing a major advance
with respect to n-grams. Collobert et al. (Col-
lobert et al., 2011) set the neural network archi-
tecture for many current approaches. Mikolov
et al. (Mikolov et al., 2013) proposed a simpli-
ﬁed model (word2vec) that allows to train on
larger corpora. They also show how semantic re-
lationships emerge from this training. Pennignton
et al. (Pennington et al., 2014), GloVe, maintain
the semantic capacity of word2vec while introduc-
ing the statistical information from latent semantic
analysis (LSA) showing that they can improve in
semantic and syntactic tasks.

3 Rule detection in prospectus

In this section we present the problem of rule de-
tection in investment fund prospectus, and our pro-
posal for tackling it.

3.1 The data
Investment fund prospectus are papers where the
fund informs the regulatory institution and its fu-
ture clients of its investment strategy, its risk man-
agement, the company structure, etc. Most of
these documents are publicly available in the reg-
ulation authority web page, see for instance for
French documents (AMF, 2018). The investment
rules that we want to identify are very precise rules
which can be of different kinds, and, in general,
very different from other sentences in the same
text as can be observed in Table 1.

The Gold standard database. The data used in
the supervised part of the model is around 3.5k an-
notated sentences for each language (English and
French). The sentences were split into two classes,
the label 1 is used for rules and 0 is used for non
rules, as shown in Table 1.

3.2 Proposed methods
In this subsection we detail the proposed algo-
rithms. The task required multiple pre-processing
steps that are used for data preparation before
training or inference. The ﬁrst step is to segment
the document into a list of sentence then each sen-
tence is tokenized into multiple elements based
mostly on space and punctuation characters. Each
token is then mapped to a unique id in order to
produce a list of integer from each sentence which
then will be fed to the regression model.

Word embeddings. The word vector values are
initialized using the GloVe algorithm Pennignton
et al. (Pennington et al., 2014) and then ﬁne-tuned
along with the model regression parameters during
training. We used a corpus of fund prospectuses
and wikipedia pages to train a domain-speciﬁc
word embedding. This is justiﬁed by the fact that
some words used in prospectuses are uncommon
in the general use of language and thus are not
included in available word vectors pre-trained on
Wikipedia or common crawl alone.

3.2.1 Linear network model
The Linear network model in this case is a logistic
regression applied to an un-weighted average of
dense word vectors. The advantage of this model
is that it is simple while it also takes advantage
of the unsupervised pre-training of the word em-
beddings. This also means that is very fast and
computationally cheap compared to other models

45Sentence
The Fund will invest at least 70% of its net assets in sub investment grade corporate debt se-
curities with a credit rating equivalent to BB+ or lower and denominated in USD.
The SICAV may invest in OTC markets.
The Company may not invest in gold, spot commodities, or real estate
The management fee is 0.1%
The asset manager JP Morgan assigns BNP Security Services as its depositary bank.

Tag
1

1
1
0
0

Table 1: Examples of sentences in the Data base.

presented here. In Figure 1, we can see the overall
architecture of the model.

has the same size as the number of ﬁlters in
each convolution layer.

• Concat Layer : Concatenates the output of

each Max-pooling together.

• Linear Layer : Applies a linear mapping from

the concat layer to the output.

• Sigmoid Activation : Maps the output to the

[0,1] range.

In Figure 2, we can see the overall architecture

of the model.

Figure 1: Linear Network architecture

3.2.2 Convolutional Neural Network
We used a CNN architecture similar to the one in-
troduced in (Kim, 2014). It consists of the follow-
ing layers:

• Convolutional Layer : Three 1-dimensional
convolution layers applied in parallel to the
input embedding sequence. Each convolu-
tion layer uses a different ﬁlter size {3, 4, 5}
and captures sentence information at differ-
ent scales ( 3-gram, 4-gram, 5-gram ). The
convolution ﬁlters learn translation-invariant
representations which is useful for language
because it allows for weight sharing between
neurons and thus reduces signiﬁcantly the
number of weights compared to a fully con-
nected layer. We use 100 ﬁlters for each layer
and ReLu as a non-linearity for the convolu-
tion layers.

Figure 2: CNN architecture

3.2.3 Bi-directional

Long-Short-Term-Memory
introduced
The Bi-LSTM model was ﬁrst
in (Graves and Schmidhuber, 2005).
Here,
we used a speciﬁc model that consists of the
following Layers :

• Max-pooling : Applies a max operation
across the sequence and returns an output that

• Forward LSTM : Sequential layer that is ap-
plied to the list of word embeddings from the

46ﬁrst token in the sentence to the last token and
outputs the lstm cell state of the last token of
the sentence.

• Backward LSTM : Sequential layer that is ap-
plied to the list of word embeddings from the
last token in the sentence to the ﬁrst token and
outputs the lstm cell state of the ﬁrst token of
the sentence.

• Concat Layer : Concatenates the output of

each LSTM layer.

• Linear Layer : Applies a linear mapping from

the concat layer to the output.

• Sigmoid Activation : Maps the output to the

[0,1] range.

In Figure 3, we can see the overall architecture of
the model.

Figure 3: LSTM architecture

Implementation details

3.2.4
We used Keras (Chollet et al., 2015) with Tensor-
Flow Backend throughout our experiments.
We use Adadelta (Zeiler, 2012) Optimizer with a
learning rate of 0.001 and a batch-size of 50.
A Dropout (Srivastava et al., 2014) of 0.5 is used
after the concat layer for LSTM and CNN and af-
ter the average layer for the Linear network model
for regularization.
We used Binary Cross-entropy in all the models
losses.

3.3 Results
We present a performance comparison of the ar-
chitectures described above both in terms of accu-
racy/Precision/recall but also in terms of inference
time as it is a also an important metric to consider
when deploying a model in a production environ-
ment.

Model
Linear
CNN

Acc (std)
88.2(1.5)
93.7(1.0)
Bi-LSTM 93.3(1.1)

P (std)
88.2(3.3)
90.8(2.6)
90.5(3.0)

R (std)
73.5(3.5)
89.7(3.8)
88.8(2.7)

Table 2: French 10-fold Cross-validation results

Model
Linear
CNN

Acc (std)
87.7(3.5)
94.3(1.4)
Bi-LSTM 93.7(1.1)

P (std)
83.3(4.1)
90.4(4.2)
88.8(1.9)

R (std)
60.8(1.4)
85.8(2.3)
84.7(5.3)

Table 3: English 10-fold Cross-validation results

The convolutional model seem to yield slightly
better results on average compared to the Bi-
LSTM which is in line with the results presented
in (Guggilla et al., 2016). Both Bi-LSTM and
CNN outperform the linear network model be-
cause they take into account the order of tokens in
the sentence while the linear network model does
not.

Model
Linear
CNN

Bi-LSTM

Time per sample (s)

1.2e−4
3.1e−4
1.8e−3

Table 4: Inference Time performance comparison

Because of its simplicity the linear network
model is the fastest out of the three and the Bi-
LSTM is 6 times slower than the CNN while giv-
ing worse results.

4 Conclusions and further work

We have presented a method to detect and iso-
late mandatory rules in regulatory documents. The
objective is to automate the detection of invest-
ment rule in prospectuses using a classiﬁer. This
helps compliance experts avoid the tedious work
of reading documents that are sometimes as long
as 500 pages and take days to read in order to se-
lect very few sentences.

47We described the frameworks used,
the pre-
processing steps and compared multiple classiﬁca-
tion models in terms of Accuracy/Precision/Recall
and inference time. The results show that convolu-
tional neural networks have the best trade-off be-
tween accuracy and execution time and are thus
the best model for this task.

References
AMF. 2018.

Geco database.

http://geco.
amf-france.org/Bio/rech_opcvm.aspx.
Online, accessed: 2018-04-18.

N.C.L. Beale. 2004.

System and method for gen-
trading systems.

for

erating compliance rules
https://www.google.com/patents/
EP0990215B1?cl=en. EP Patent 0,990,215.

Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research
3(Feb):1137–1155.

D. Cashman, D. Damphousse, V. Drysdale, L. Ekht-
man, C. Guerriero, K. Hebert, R. Kumar, R. Leeper,
H. Levine, B. Mandel,
Sys-
teme de gestion de fonds ﬁnanciers et de con-
formite aux directives en matiere d’investissement
de portefeuille. https://www.google.com/
patents/EP1212711A1?cl=fr.
EP Patent
App. EP20,000,921,568.

et al. 2002.

Franc¸ois Chollet et al. 2015. Keras. https://

keras.io.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research
scratch.
12(Aug):2493–2537.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional lstm
and other neural network architectures. Neural Net-
works 18(5-6):602–610.

Chinnappa Guggilla, Tristan Miller,

and Iryna
Gurevych. 2016. Cnn-and lstm-based claim classi-
In Proceedings
ﬁcation in online user comments.
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers. pages 2740–2751.

Rie Johnson and Tong Zhang. 2014.

Effective
use of word order for text categorization with
arXiv preprint
convolutional neural networks.
arXiv:1412.1058 .

Rie Johnson and Tong Zhang. 2015. Semi-supervised
convolutional neural networks for text categoriza-
In Advances in neural
tion via region embedding.
information processing systems. pages 919–927.

Piotr Kaminski

practice model

and Kate Robu. 2016.

A
compli-
http://www.mckinsey.com/business-

best
ance.
functions/risk/our-insights/a-best-practice-model-
for-bank-compliance.

bank

for

Aurangzeb Khan, Baharum Baharudin, Lam Hong Lee,
and Khairullah Khan. 2010. A review of machine
learning algorithms for text-documents classiﬁca-
tion. Journal of advances in information technology
1(1):4–20.

Yoon Kim. 2014.

works for sentence classiﬁcation.
arXiv:1408.5882 .

Convolutional neural net-
arXiv preprint

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
In AAAI. volume 333, pages 2267–
classiﬁcation.
2273.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang.
2016. Recurrent neural network for text classi-
arXiv preprint
ﬁcation with multi-task learning.
arXiv:1605.05101 .

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP 2014.
representation.
volume 14, pages 1532–43.

Nitish

Ilya

Srivastava,

Geoffrey
Sutskever,

Alex
Ruslan
Krizhevsky,
Salakhutdinov. 2014.
A simple
way to prevent neural networks from overﬁt-
J. Mach. Learn. Res. 15(1):1929–1958.
ting.
http://dl.acm.org/citation.cfm?id=2627435.2670313.

Hinton,
and

Dropout:

Thomson Reuters.

2015.
Turn regulatory compli-
https:

Financial & Risk
Risk management:
ance into a business opportunity.
//financial.thomsonreuters.
com/en/markets-industries/
risk-management-tools.html?utm_
campaign=e4&utm_medium=social&
utm_source=FRblog&utm_content=
DCraigPeakRegulatory.

Francesco Visin, Kyle Kastner, Kyunghyun Cho, Mat-
teo Matteucci, Aaron Courville, and Yoshua Bengio.
2015. Renet: A recurrent neural network based al-
ternative to convolutional networks. arXiv preprint
arXiv:1505.00393 .

Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-

ing rate method. CoRR abs/1212.5701.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems. pages 649–657.

48
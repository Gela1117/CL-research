The Linguistic Ideologies of Deep Abusive Language Classiﬁcation

Michael Castelle

Centre for Interdisciplinary Methodologies

University of Warwick

M.Castelle.1@warwick.ac.uk

Abstract

This paper brings together theories from so-
ciolinguistics and linguistic anthropology to
critically evaluate the so-called “language
ideologies”—the set of beliefs and ways of
speaking about language—in the practices of
abusive language classiﬁcation in modern ma-
chine learning-based NLP. This argument is
made at both a conceptual and empirical level,
as we review approaches to abusive language
from different ﬁelds, and use two neural net-
work methods to analyze three datasets devel-
oped for abusive language classiﬁcation tasks
(drawn from Wikipedia, Facebook, and Stack-
Overﬂow). By evaluating and comparing these
results, we argue for the importance of incor-
porating theories of pragmatics and metaprag-
matics into both the design of classiﬁcation
tasks as well as in ML architectures.

1

Introduction

Some problems lend themselves more
easily to A.I. solutions than others. So,
hate speech is one of the hardest, be-
cause determining if something is hate
speech is very linguistically nuanced.
Right?... I’m optimistic that over a ﬁve-
to 10-year period, we will have A.I.
tools that can get into some of the nu-
ances, the linguistic nuances of different
types of content to be more accurate in
ﬂagging things for our systems.
(Excerpt from testimony to the U.S.
Senate and Judiciary and Commerce,
Science and Transportation Committees
by Mark Zuckerberg, April 10th, 2018)

The term nuance, as used in the above quote
from Mark Zuckerberg’s testimony to the U.S.
Senate, appears to imply that, like the successes

of convolutional neural networks in computer vi-
sion,
the classiﬁcation (or “ﬂagging”) of (tex-
tual) hate speech should be considered a matter of
advanced pattern recognition, of recognizing the
right ‘shade’ or ‘color’ of a given sentence in iso-
lation. This view of linguistic action is in con-
trast with a tradition of speech-act theory (Austin,
1962; Searle, 1969) in which the meaning of an ut-
terance is not solely determined by lexical or syn-
tactic structure but by its social context and per-
formative effects.
In studies of hate speech fol-
lowing these latter perspectives, such as those of
Butler (1997), words are seen not just as arbitrary
lexemes but something capable of actively causing
violence to one or more addressees.

Butler, in her account of hate speech, optimisti-
cally saw a ‘gap’ between a speech act and its ef-
fects (Butler, 1997)—comparable to Austin’s dis-
tinction between the illocutionary (the addressee’s
intention) and perlocutionary (the outcome of the
utterance)—which she uses to argue against legal
regulation of hate speech and instead for the possi-
bility of ‘restaging’ and ‘resignifying’ such speech
on the part of the addressee. Schwartzman (2002),
however, contended that Butler’s account was fun-
damentally missing an awareness of existing so-
cial structures of power, and that if any successful
‘resigniﬁcation’ (as in the “taking back” of slurs)
occurs it is due to an active and political response
against oppression.

The question remains: is detecting hate speech
a matter of sufﬁciently advanced pattern recogni-
tion a matter of building the right distributed ar-
chitecture on a large enough data set of “hateful”
vs. “non-hateful” expressions? Or is it necessary
for any such automated classiﬁcation to be inte-
grated with long-term ethnographic and contex-
tual immersion in the communities, however prob-
lematic, in which said expressions emerge (Sin-
ders and Martinez, 2018)? To what extent should

ProceedingsoftheSecondWorkshoponAbusiveLanguageOnline(ALW2),pages160–170Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics160we expect researchers to attempt to integrate the
sociocultural complexities of contemporary online
communication into their “A.I. tools”, and to what
extent should we expect success or failure from
those tools?

the

This

article

assesses

current poten-
tial for, and limitations of, machine learning
methodologies—both in terms of how the ‘gold
standard’ datasets are normally constructed and
how neural networks are applied—for abusive lan-
guage detection in natural language text. We will,
through our own experiments, explore the speciﬁc
conﬂicts between the research culture of achiev-
ing ”state-of-the-art” scores (Manning, 2015)
with (relatively) black-boxed architectures and
theories of the pragmatics (and metapragmatics)
of linguistic abuse in practice.

2 The Language Ideologies of NLP

With its 1950s intellectual origins in a) na¨ıve,
word-by-word machine translation and b) Chom-
sky’s conception of language as generated by a
machinic automata, the ﬁeld of natural language
processing (NLP) inherited what we refer to as
a language ideology (Woolard and Schieffelin,
1994), a set of beliefs and ways of speaking about
‘language’.1 Fundamental to the Chomskyan lan-
guage ideology, for example, is the purported ex-
istence of an innate language faculty which is held
to explain the purported lack of training data for
humans (the so-called “poverty of the stimulus”)
(Pereira, 2000). This faculty is conceived as the
locus of a generative grammar which can in the-
ory enumerate all possible valid sentences.

Other disciplines of human communication,
however, developed alternative perspectives which
embed language in a more complex sociocultural
environment. The ﬁeld of sociolinguistics (Labov,
1972) considered quantitative measurements of
language variation (such as dialects and creoles)
not as a matter of syntactic validity but as intrin-
sically related to social differentiation and class
structure; and linguistic anthropology (Duranti,
1997) simultaneously drew from the semiotic and
structuralist traditions of Peirce and Jakobson,
emphasizing acts of reﬂexive, sociocultural en-

1The term ‘ideology’ as it used here is not intended as
a pejorative; following Woolard (1998), we see ideology as
“derived from, rooted in, reﬂective of, or responsive to the
experience or interests of a particular social position” but not
necessarily “in the service of the struggle to acquire or main-
tain power” (although it may also be that.)

textualization and contextualization (Bauman and
Briggs, 1990) inherent to any communicative situ-
ation.

From the perspective of these latter ﬁelds, the
study of ‘natural language’ by computer scientists
(so named to distinguish itself from formal logic
and ‘artiﬁcial’ programming languages devised in
early AI research) would be seen as largely fo-
cused on entextualized utterances (such as writ-
ten sentences or recorded speech) and the uncov-
ering (or ‘processing’) of their hidden symbolic
patterns and structures. The introduction of sta-
tistical approaches to NLP (Charniak, 1996) laid
the groundwork for the eventual incorporation of
a machine learning methodology which uses of a
corpus of data composed of training inputs with
labeled outputs. Today’s neural network mod-
els in NLP are strongly dependent on this ar-
guably behaviorist (i.e.
stimulus-response) ide-
ology of language, one in which syntax and se-
mantics can be durably encoded through the pre-
sentation and re-presentation of vectorized ‘stim-
uli’ (which,
through backpropagation, modiﬁes
the model’s parameters based on the distance of
the ‘response’ vector from the true values).

The study of abusive language in NLP, then,
represents a proﬁtable collision between two po-
tentially compatible language ideologies — one a
“statistical NLP” ideology which claims the po-
tential to efﬁciently and intelligently discretize
and classify the contextually dense and multi-
modal miasma of real-time communication; and
the other,
to be described in the next section,
which seeks to carve out and eliminate impurities
and danger (e.g., abusive language) in search of a
‘safe’, non-‘toxic’, and yet highly scalable envi-
ronment.

3 Abusive Language Ideologies

So, if language ideologies are a set of beliefs and
ways of speaking about ‘language’, then abusive
language ideologies are a set of beliefs and ways
of speaking about what it means for ‘language’ to
be ‘abusive’. As a review of the literature on this
topic immediately indicates, there are a variety of
theories regarding the nature of hate speech, abu-
sive language, cyberbullying, etc.; in this section
we will characterize the main positions, especially
in their potential relation to NLP methodologies.

1613.1 Politeness
The study of online antagonism was preceded
by much research in the linguistic ﬁeld of prag-
matics on politeness such as Brown and Levin-
son (1987), who isolated the concept of polite-
ness as a set of interactional strategies to preserve
‘face’, a concept from Goffman (1967) reﬂecting
the ideal self-image of the addresser or addressee
in each communicative situation. This approach
took into account the four cooperative ‘maxims’
of Grice (1975), which are implicit interactional
norms in which speakers strive to be informative,
unambiguous, brief, and orderly—norms which,
we suggest, are potentially relevant to understand-
ing online Q&A platforms like StackOverﬂow.
Culpeper (1996) showed how a focus on impolite-
ness brought the importance of interactional con-
text to the fore, in contrast to a focus on surface
form in the work of Brown and Levinson; but later
work on gender difference and politeness argued
that impoliteness itself is only something classi-
ﬁed as such by those in dominant positions of
power (Mills, 2003). The linguistic study of po-
liteness thus helpfully charts a development from
the study of lexical and syntactic structure to in-
teractional pragmatics to considerations of power
relations within and among communities of prac-
tice.

3.2 Hate Speech
The concept of hate speech (and the debates sur-
rounding its deﬁnition, legal and otherwise) is it-
self predicated on precisely a (conscious or uncon-
scious) philosophical dispute about whether lan-
guage can be segregated from action; the “ﬁghting
words” doctrine in U.S. law (Chaplinsky vs. New
Hampshire, 1942), for example, was a legal inter-
vention that (in a single instance) outlawed speech
acts capable of provoking violent action (i.e. based
on their perlocutionary force). From such an Aus-
tinian perspective, to be a free speech absolutist
is to consider speech merely as locution and not
illocution or perlocution (Hornsby and Langton,
1998); NLP methodology, which typically takes
as input a set of text-artifacts segregated from
their original communicative situation and con-
sequences, could also be said to (implicitly) take
this position of considering speech solely as lo-
cution (even if researchers commonly appreciate
that their data was drawn from a past existence in
richer contexts). For example, the work of Warner

and Hirschberg (2012) acknowledges the limits of
their decontextualized comment dataset, but ar-
gues that hate speech can still be distinguished
through the recognition of stereotyped expressions
about social groups.

The most comprehensive philosophical attempt
to give meaning to the concept of hate speech is
by Alexander Brown.
In his two recent articles
(Brown (2017a), Brown (2017b)), he explains that
the term ‘hate speech’ likely emerged from a 1988
conference paper at Hofstra (Matsuda, 1989), and
came to take on signiﬁcant value for legal schol-
ars before becoming integrated into a more popu-
lar discourse. Matsuda’s intervention represents a
performative ideology of language; if words are
action, and some words are violent action, then
hate speech can be regulated without violating
a constitutional free speech principle. However,
Brown comes to a somewhat negative conclusion
regarding the possibility of coming to a coherent
universal deﬁnition of the concept of hate speech;
while it can be summarized as “a rough but nev-
ertheless serviceable term to describe...
the ex-
pressive dimensions of identity-based envy, hostil-
ity, conﬂict, mistrust and oppression”, more ﬁne-
grained enclosures are never sufﬁcient, and he ar-
gues that the meaning of ‘hate speech’ is closer to
the ‘family resemblances’ concept of Wittgenstein
(1953), who argues that terms like ‘game’ (and—
implicitly—‘language’) can only denote an ever-
shifting family of related practices irreducable to
a precise deﬁnition.

It is thereby unsurprising that NLP’s method-
ological detachment from the speech situation
(itself embedded in other sociocultural contexts
which do not become part of the training data),
along with the fundamental indeterminacy of the
‘hate speech’ category, makes the reliability of
‘coding’ for hate speech a signiﬁcant challenge
(Ross et al., 2017).
In their survey of hate
speech detection in NLP, Schmidt and Wiegand
(2017) mention this time-consuming dependency
on hand-labeled data; but they also point out the
many strategic ways that NLP researchers have
proposed (if not always attempted) to overcome
the decontextualized limitations and problems of
deﬁnition of their data, and we will report similar
ﬁndings below.

1623.3 Abusive Language

Despite his critical conclusions, Brown argues that
the term ‘hate speech’ is still, for now, effective;
it “is used because it is useful, and it will re-
main useful so long as it can be used to do more
than merely signal disapproval. If [that was] all
it did... [it] would soon fall out of fashion or be
replaced by newer, cooler bits of language that
did the same thing but in more interesting ways”
(Brown, 2017a). One such current variant term is
abusive language, which appears in some of the
earliest literature on antagonistic Internet commu-
nication (Spertus, 1997) but has in the past years
taken on a greater prominence.

In part, the potentially milder connotations of
the ‘abusive language’ term reﬂects a shifting
from seeing online abuse as occurring on behalf
of identity-based communities to occurring to-
wards social groups in general, where those so-
cial groups might be something like “new users
of StackOverﬂow”. So for example, Nobata et al.
(2016) uses the concept of abusive language to in-
clude hate speech, derogatory language, and pro-
fanity together.
In their work on personal at-
tacks in Wikipedia talk pages, Wulczyn et al.
(2016) adopts the rhetoric of ‘toxic’ behavior, a
term which metaphorically transposes affective
concepts (such as hate) to one of environmental
contamination and taboo (Douglas, 1966; Nagle,
2009); this represents a subtle move away from an
otherwise dominant personalist ideology in which
meaning emerges from the beliefs or intentions of
the speaker (Hill, 2008).

Recognizing the overall lack of consensus on
the boundaries of abusive language, Waseem et al.
(2017) proposes a twofold typology: (1) whether
language is “directed towards a speciﬁc individ-
ual or entity” or “directed towards a generalized
group” and (2) whether the content is ‘explicit’
or ‘implicit’. The resulting four axes, then, are
each analyzed for the methodological approaches
needed. Directed abuse can be detected with atten-
tion to proper nouns and entities like usernames;
Generalized abuse may be associated with lexi-
cal patterns based on the targeted groups; Explicit
abuse also often involves speciﬁc keywords and
Implicit abuse the most difﬁcult category, where
more advanced semantic approaches such as word
embeddings can fail in a complex polysemous and
creative environment.

The view that indirectness and implicitness in

text-artifacts can be eventually ‘captured’ by ma-
chine learning models is related to the performa-
tive ideology of speech-act theory, which has been
criticized for its overemphasis on so-called explicit
performatives (such as “I now pronounce you man
and wife”) over (far more common) implicit per-
formative utterances which depend on contextual
cues (Gumperz, 1982; Lempert, 2012). As Asif
Agha puts it, “an indirect speech act is just a
name for the way in which a denotational text
diagrams an interactional text without describing
it” (Agha, 2007, p. 100). This diagramming in-
stead often happens through forms of pragmatic
and metapragmatic reﬂexivity which may be difﬁ-
cult to recognize through analyzing the utterance
detached from its interactional context, as is often
the case for NLP datasets.

That

researchers see indirect and implicit
speech as a signiﬁcant challenge, however, is in
part due to our methodological embeddedness in
a referentialist ideology which typically holds that
the meaning of words are stable (as realized, for
example, by static embedding vectors), and that
the purpose of language is to communicate infor-
mation. Jane Hill explains how the combination
of referentialist and performative ideologies typi-
ﬁes conventional approaches to racism:

Stereotypes and slurs are visible as
racist to most people because they are
made salient by referentialist and per-
formative linguistic ideologies respec-
tively. But other kinds of talk and text
that are not visible, so called covert
racist discourse, may be just as impor-
tant in reproducing the culturally shared
ideas that underpin racism. Indeed, they
may be even more important, because
they do their work while passing unno-
ticed. (Hill, 2008)

We argue that it will be essential for NLP re-
searchers to recognize how our tools and tech-
niques may, in part, be material embodiments of
these ideologies, but also how one might partially
escape those ideologies without abandoning the
use of tools and techniques entirely. One positive
example of this is from Saleem et al. (2017), which
argues that supervised labeling based on keywords
is problematic, but also that one can improve per-
formance by training on language from speciﬁc
speech communities (Gumperz, 1968).

163In the second part of this paper, we apply basic
deep NLP methods to building predictive models
for abusive language on three different datasets.
Through qualitative reﬂection on the data, train-
ing process, and results, we articulate the speciﬁc
limitations of common methods, as well as the fu-
ture directions, of deep learning methodology for
addressing concerns about abusive language.

4 Experiments

The nascent research cluster around NLP and abu-
sive language constitutes not just a ‘speech com-
munity’ but a language community, i.e., “an orga-
nization of people by their orientation to structural
(formal) norms for denotational coding (whether
explicit or implicit)” (Silverstein, 1996). The com-
bination of linguistic ideologies described above
is fully realized in the conventional experimen-
tal architecture of the shared task, in which mul-
tiple teams of researchers independently attempt
to build systems with good classiﬁcatory perfor-
mance by determining the true denotational mean-
ing of utterances which, most commonly, have
been excised from their interactional context.

For example, the tasks addressing abusive lan-
guage typically have as their goal the determi-
nation of whether stand-alone utterances should
be considered rude, offensive, or abusive. Train-
ing data is provided in the form of utterance-
label pairs, where the label may be a binary value
(i.e.
abusive or not) or multi-class (for differ-
ent categorical and/or ordinal levels of offensive-
ness). In order to explore these kinds of tasks di-
rectly, in this paper we chose to experiment with
3 datasets: the Kaggle Toxic Comment Classiﬁca-
tion Challenge2, the shared task in the 1st Work-
shop on Trolling, Aggression and Cyberbullying
(TRAC1)3, and the StackOverﬂow dataset from
the 2nd EMNLP Abusive Language Workshop.4

4.1 Data Description
The Kaggle Toxic Comment Classiﬁcation dataset
provides decontextualized Wikipedia “talk page”
comments, each paired with multi-class labels on
toxic behavior, judged by human raters; we em-
phasize that the dataset is decontextualized to in-
dicate that additional information about each dis-

2https://www.kaggle.com/c/jigsaw-toxic-comment-

classiﬁcation-challenge

3https://sites.google.com/view/trac1/home
4https://sites.google.com/view/alw2018

/resources/stackoverﬂow-dataset

cursive interaction is not provided (but for a depic-
tion of the organizational structure of their produc-
tion, one may consult Geiger (2017)’s “ethnogra-
phy of infrastructure” of Wikipedia). The labels of
toxicity include ‘toxic’, ‘severe toxic’, ‘obscene’,
‘threat’, ‘insult’ and ‘identity hate’. Because the
other datasets we examine classify differing but re-
lated categories, it was necessary to combine these
into one ‘offensive’ category to make comparisons
across datasets possible (a common methodologi-
cal decontextualization which elides available dif-
ference even at the level of the ‘clean’ dataset).
10.2% of the resulting dataset had an ‘offensive’
label. We split the data into training (150,571
observations), validation (6,000 observations) and
holdout sets (3,000 observations).

The TRAC1 shared task dataset contains 15,000
stand-alone Facebook posts and comments in
both Hindi and English unicode, each paired
with human-rated multi-class labels, distinguish-
ing “Overtly Aggressive”, “Covertly Aggressive”
and “Not Aggressive”. There are separate En-
glish and Hindi subsets, and we used the English
portion, which still contains signiﬁcant amounts
of Hindi-English code-switching (Verma, 1976).
Again for comparison, it was necessary to group
the ﬁrst two categories together; in the resulting
dataset, 58% of the comments are considered ag-
gressive. We split the dataset into training (11,999
observations) and validation (3,001 observations)
sets, and used the provided test set as our holdout
set (601 observations).

The StackOverﬂow dataset is yet another col-
lection of decontextualized comments, some of
which are ﬂagged by the users to be “Not Con-
structive or off topic”, “Obsolete”, “Other” (not
the same as unﬂagged), “Rude or offensive”, or
“Too chatty”. Notably, however, these ﬂags are
provided by the site’s users; when a comment is
ﬂagged as “Rude or offensive”, it is reportedly dy-
namically removed from the website, which makes
this dataset’s semantics different from the previous
ones which were—as far as we can tell—labeled
post hoc by independent raters. Instead, the Stack-
Overﬂow data is a textual archive of speech acts
about speech acts, or of metapragmatic utterances
(Silverstein, 1993). They are traces of in-the-
moment judgments that may have acted to spon-
taneously eliminate the judged utterance from a
discourse.

The total number of ﬂagged comments is

164525,085, of which 57,841 are “Rude or offensive”
(and thus were dynamically removed as per the
above).
In addition, there are 15 million com-
ments that are not ﬂagged to be undesirable in
any way. We joined a sample of 1 million of the
unﬂagged comments and the ﬂagged comments,
but considered only the ﬂag “Rude or offensive”
(the rest are grouped with unﬂagged). We used
this dataset, which has 3.8% comments ﬂagged as
“Rude or offensive”, for training and testing. We
split this dataset into training (1,516,085), valida-
tion (6,000) and holdout (3,000) sets.

Out of the three datasets, both StackOverﬂow
and Kaggle have a signiﬁcant class imbalance,
which is more signiﬁcant for the StackOverﬂow
set (3.8% offensive) than Kaggle (10.2% offen-
sive).

4.2 Methods and Results
While some earlier research in the classiﬁcation
of abusive language used feature-based classiﬁca-
tion techniques such as support vector machines
(Warner and Hirschberg, 2012), we were inter-
ested in evaluating deep learning methods com-
parable to work such as Founta et al. (2018).
We implemented two neural network architectures
widely used in text classiﬁcation: a convolutional
neural network (CNN) and a recurrent neural net-
work using Bidirectional Gated Recurrent Units
(RNN Bi-GRU).
CNN Model Convolutional neural networks
(CNNs), originally popularized in the context of
computer vision for recognition tasks (Le Cun
et al., 1990), can be applied to sequences of word
embeddings in a similar manner to how they are
applied to bitmap images, and have been shown
to perform well in some abusive language detec-
tion tasks (Park and Fung, 2017). Although CNNs
are unlikely to capture longer-term sequential re-
lations in the manner of the recurrent neural net-
works discussed below, they can plausibly capture
local patterns of features, and offensive speech can
often be detected by local features such as swear
words/phrases and racial slurs.

We implemented a vanilla CNN using Keras
(Chollet et al., 2015). The input is tokenized into
words, and converted into 300-dimensional word
embedding vectors using 1 million word vectors
trained on Wikipedia using the Fasttext classiﬁer
(Joulin et al., 2016).5 We set a maximum length

5https://fasttext.cc/docs/en/english-vectors.html

of 100 tokens per input, and a vocabulary size of
30,000. The input layer is then fed into 2 convolu-
tional layers (of kernel size 1*5) each followed by
a max-pooling layer. This is followed by 2 dense
layers (dimensions 128 and 64) and ﬁnally the out-
put layer. We trained the model using the Adagrad
optimizer (Duchi et al., 2011), using a batch size
of 512 and 10 maximum epochs with early stop-
ping.

RNN Model Recurrent neural networks are
widely used in NLP tasks (e.g. Pavlopoulos et al.
(2017)) because they are good at capturing longer-
term sequential patterns in text. We used the RNN
variant known as Bidirectional GRU (Chung et al.,
2014; Cho et al., 2014); GRUs are recurrent units
with both an update gate and a reset gate that
aim to solve the “vanishing gradient” problem of
vanilla RNN units.

We implemented the Bi-GRU model using
Keras. The input layer is the same word embed-
ding layer as the CNN model, which is fed into a
80-unit Bi-GRU layer, followed by a pooling layer
concatenating features from an average and a max-
pooling operation. This is then fed into the ﬁnal
output dense layer. We trained the model using
the Adam optimizer (Kingma and Ba, 2014) and a
dropout rate of 0.2, using a batch size of 512 and
a maximum of 10 epochs with early stopping.

4.3 Results

F1 (micro)

Offensive
Prec Recall

Normal
Prec Recall

.76
.76

.74
.83

.97
.97

Model
Kaggle Toxic (327 offensive, 2673 normal)
CNN
GRU
TRAC1 Trolling (354 offensive, 247 normal)
CNN
GRU
StackOverﬂow (114 offensive, 2886 normal)
CNN
GRU

.97
.98

.77
.75

73
.85

.64
.73

.70
.59

.97
.97

.99
.99

.56
.59

.19
.22

.86
.89

.71
.73

.68
.69

Table 1: Results on test sets of three data sources us-
ing two architectures. The numbers next to the data
sources shows the size of each class in the test set. Hy-
perparameters were manually tuned using the valida-
tion sets. We calculated the micro-averaged F1 score
because of the varied class imbalance in the datasets.

Our results show that the two architectures per-
formed similarly, but there were large differences
across the three datasets (see Table 1). The Kag-
gle dataset has the best results in terms of micro-
averaged F1 score, with very high precision and

165recall for the “normal” class and around 0.8 pre-
cision and recall for the “offensive” class. The
TRAC1 dataset had a lower micro-averaged F1
score, but the performance on the two classes are
more balanced than the Kaggle model. The Stack-
Overﬂow dataset has the lowest micro-averaged
F1 and the most unbalanced results between the
two classes: high precision and recall for the non-
offensive class, low precision and even lower re-
call (0.2) for the “offensive” class.

We argue that the large differences among the
three datasets using the same architectures can-
not be explained by differences in class imbalance;
both Kaggle and StackOverﬂow have heavy class
imbalance, yet the Kaggle model did much better
on the offensive class (results highlighted in bold
in Table1). Why, then, did the models perform so
poorly on detecting offensive comments on Stack-
Overﬂow?6 Looking at the model predictions,
we found that the predictions given by the GRU
and the CNN models are highly correlated (Chi-
squared = 1009.9, p < 2.2e-16).7 The mediocre
precision on the “offensive class” is mainly caused
by the fact that StackOverﬂow users don’t always
ﬂag offensive comments, i.e., most of the false
positives (where positive is a classiﬁcation of ‘of-
fensive’) should arguably be true positives. There
are 23 comments that are predicted to be offen-
sive by both models but don’t receive an ‘offen-
sive’ ﬂag in the data. Out of the 23, two comments
are indeed not (overtly or covertly) ‘offensive’:
“`close(f)`–> `f.close()`”; “fuck bro !!! how the
fuck didnt i see that , jesus !! thanksssssss !!!!!!”.
Among the rest, most are overtly offensive but not
ﬂagged, e.g. “dude can you answer the question
or not? if not stop wasting my time”; “teach him
instead of being a dick.”. A few can be considered
offensive in particular contexts or by certain users,
e.g. “jesus christ! what’re you doing?”; “don’t.
migrate. crap.”; “no shit sherlock”. This implies
that the models would have had a higher precision
if the gold standard was provided by annotators
who judge every comment in the dataset. In this

6In this section we focus on predictions of offensive
comments in the StackOverﬂow dataset, and compare it
with results of Kaggle. Because of the heavy presence of
Hindi and English code switching in the TRAC1 data, we
did not perform an error analysis for this dataset. For in-
depth discussions, please see the TRAC1 proceedings at
https://sites.google.com/view/trac1/accepted-papers.

7We looked at predictions of both the validation set and
the holdout set in order to have more samples to form a better
understanding of the models.

case, the pragmatic context of labeling matters.

The even lower recall, on the other hand, re-
veals a genuine limitation of the models and of the
dataset. Again, the two models agree highly. Out
of the 355 comments that are ﬂagged as “offen-
sive” by StackOverﬂow users, the majority (75%)
are considered not offensive by both models (i.e.
they are false negatives). 22 comments (6%) are
identiﬁed as offensive by only one model, and
only 52 comments (15%) are correctely labeled
as offensive by both models. Why is the recall
so low? To investigate, we sampled 100 of the
false negatives and asked three human raters to
determine whether these comments are offensive.
Only 11 were considered offensive by at least two
out of three raters even though they are ﬂagged as
“rude or offensive” by StackOverﬂow users. Here
are some examples of comments ﬂagged as offen-
sive but not considered offensive by a majority of
raters:

• please post *code,* not screenshots.

• did not get you? where in the query that you

have provided should i add this?

• the phrase is “want to.”

• no testing!!!! i would prefer no coding

• you sir, deserve an unlimited amount of up-

votes for that comment

While these comments’ lexical ‘surface’ con-
tent is unlikely to be considered offensive by our
classiﬁer, they can potentially be considered of-
fensive in their pragmatic implicatures (Levinson,
1983), which can only be recovered or enriched
given the context of the interaction and/or the
broader context of conventions and norms in the
StackOverﬂow forum.

Such context-dependent offensive comments
appear to account for the majority of the false neg-
atives in the StackOverﬂow results; this pattern is
much less obvious in the Kaggle results. Unlike
the StackOverﬂow dataset, the Kaggle dataset was
constructed by showing annotators stand-alone
comments. Therefore, the interactional context
of those comments was not overtly considered
during the rating, although it is likely that raters
would sometimes imagine or “accommodate” con-
text (Tian and Breheny, 2016). An analysis of the
false negatives show that while a few comments
likely require contextual enrichment (i.e., in the

166referentialist ideology, they are “implicitly” offen-
sive), the majority of the errors are due to uncon-
ventional ways of spelling, a known problem al-
ready being tackled by previous researchers who
convincingly argue for character-level as opposed
to word-level approaches (Mehdad and Tetreault,
2016).

To sum up, we saw that neural network models
with different architectures (CNN and Bi-GRU)
performed similarly and have the potential of re-
liable abusive/offensive language detection when
the offensiveness is signaled and/or classiﬁed via
expressions in the text-artifact itself (supported by
the Kaggle results). However, when the offensive-
ness is marked in a context-dependent way, cur-
rent neural network methods perform poorly; this
is not necessarily because neural networks cannot
be used to model context, but because the avail-
able datasets on abusive language detection do not
provide this context. This is manifested in the
poor performance of neural models on the Stack-
Overﬂow data: the context-dependency of offen-
siveness results in low recall, and the inconsis-
tency of user-generated ﬂagging results in low pre-
cision. Because the ﬂags are provided by users
who have seen the entire interaction, many com-
ments are considered offensive in context but not
offensive when standing alone. By contrast, Kag-
gle and TRAC1 are labeled by independent anno-
tators who did not participate or observe the full
interaction.

5 Conclusions and Future Directions

In this paper, we have attempted to provide a quan-
titative justiﬁcation for a qualitative perspective:
namely, that theories of pragmatics (such as the
primacy of context in the dynamic construction of
meaning (Levinson, 1983)) and of metapragmatics
(e.g.
the fundamental reﬂexivity of interactional
speech at various semiotic levels (Agha, 2007))
should take on a greater role in the classiﬁcation
of abusive language in NLP research.

Our experiments using common neural network
architectures on text classiﬁcation show promising
performance when the offensiveness/ abusiveness
is signalled within a single utterance, but give poor
performance when the offensiveness require con-
textual enrichment. This is a limitation of popular
abusive language detection tasks. For future work,
we would propose to investigate the modeling of
not just stand-alone utterances and their labels, but

the affective and interactional dynamics of online
communication.

In the case of StackOverﬂow, we suggest that a
serious approach to tackling the problem of abu-
sive language on the site would likely want to take
advantage of the site’s periodic data dumps, which
provide millions of user questions, answers, votes,
and favorites (Anderson et al., 2012). However,
the dynamic removal of ﬂagged material from
the site poses some serious methodological issues,
and the question of how to incorporate this vast
relational data into neural network classiﬁer archi-
tectures is another challenge, which we speculate
will involve embeddings of networks of interac-
tions as in Hamilton et al. (2018).

Finally, as a longer-term goal for the study
of abusive language in online communities,
we believe that it is quite promising that some
researchers have implicitly or explicitly moved
towards the notion of a speech community,
in
which actors in different social spaces may
possess differing norms for appropriate behavior
(Saleem et al., 2017). However, we argue that
it will ultimately be necessary to attend to those
theorists emphasizing so-called communities of
practice (Holmes and Meyerhoff, 1999), a per-
spective which brings to the fore the embodiment
of communities in practical action (of which
language is only a part); to consider the role of
conﬂict as well as consensus; to see identity as
more than just a static set of categories; and to
more seriously take into account the participants’
understanding of their own practices (Bucholtz,
1999).

6 Acknowledgments
Many thanks to Ye Tian and Ioannis Douratsos
for their assistance and suggestions; thanks also
to Ana-Maria Popescu and Gideon Mann for their
remarks. We would also like to thank the anony-
mous reviewers for their comments, critiques, and
suggestions. Any opinions, ﬁndings, conclusions,
or recommendations expressed here are those of
the authors and do not necessarily reﬂect the view
of their institutions.

References
Asif Agha. 2007. Language and Social Relations, 1
edition edition. Cambridge University Press, Cam-
bridge ; New York.

167Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Discovering Value from
Community Activity on Focused Question Answer-
ing Sites: A Case Study of Stack Overﬂow. In Pro-
ceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’12, pages 850–858, New York, NY, USA.
ACM.

J.L. Austin. 1962. How to Do Things with Words. Ox-

ford University Press.

Richard Bauman and Charles L. Briggs. 1990. Poet-
ics and Performance as Critical Perspectives on Lan-
guage and Social Life. Annual Review of Anthropol-
ogy, 19:59–88.

Alexander Brown. 2017a. What is hate speech? Part 1:
The Myth of Hate. Law and Philosophy, 36(4):419–
468.

Alexander Brown. 2017b. What is Hate Speech? Part
2: Family Resemblances. Law and Philosophy,
36(5):561–613.

Penelope Brown and Stephen C. Levinson. 1987. Po-
Some Universals in Language Usage.

liteness:
Cambridge University Press.

Mary Bucholtz. 1999. ”Why Be Normal?”: Language
and Identity Practices in a Community of Nerd Girls.
Language in Society, 28(2):203–223.

Judith Butler. 1997. Excitable Speech: A Politics of the

Performative. Routledge, New York.

Chaplinsky vs. New Hampshire. 1942. 315 U.S. 568.

Eugene Charniak. 1996. Statistical Language Learn-

ing. A Bradford Book, Cambridge, Mass.

Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259.

Franc¸ois Chollet et al. 2015. Keras. https://

keras.io.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Jonathan Culpeper. 1996. Towards an anatomy of im-
politeness. Journal of Pragmatics, 25(3):349–367.

Mary Douglas. 1966. Purity and Danger: An Analysis
of the Concepts of Pollution and Taboo. Routledge,
London.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
Journal of Machine
and stochastic optimization.
Learning Research, 12(Jul):2121–2159.

Alessandro Duranti. 1997. Linguistic Anthropology.

Cambridge University Press, New York.

Antigoni-Maria Founta, Despoina Chatzakou, Nicolas
Kourtellis, Jeremy Blackburn, Athena Vakali, and Il-
ias Leontiadis. 2018. A Uniﬁed Deep Learning Ar-
chitecture for Abuse Detection. arXiv:1802.00385
[cs]. ArXiv: 1802.00385.

R. Stuart Geiger. 2017. Beyond opening up the black
box: Investigating the role of algorithmic systems
in Wikipedian organizational culture. Big Data &
Society, 4(2):2053951717730735.

Erving Goffman. 1967. Interaction Ritual: Essays on
Face to Face Behaviour. Penguin, Harmondsworth.

Paul Grice. 1975. Logic and Conversation. In P. Cole
and N.L. Morgan, editors, Syntax and Semantics,
vol. 3: Speech Acts, pages 41–58. Academic Press,
New York.

John Gumperz. 1968. The Speech Community.

In
International Encyclopedia of the Social Sciences,
pages 381–386. Macmillan, New York.

John J. Gumperz. 1982. Discourse Strategies. Cam-

bridge University Press, Cambridge.

William L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan
Jurafsky, and Jure Leskovec. 2018. Querying Com-
plex Networks in Vector Space. arXiv:1806.01445
[cs, stat]. ArXiv: 1806.01445.

Jane H. Hill. 2008. The Everyday Language of White
Racism, 1 edition edition. Wiley-Blackwell, Chich-
ester, U.K. ; Malden, MA.

Janet Holmes and Miriam Meyerhoff. 1999. The Com-
munity of Practice: Theories and Methodologies in
Language and Gender Research. Language in Soci-
ety, 28(2):173–183.

Jennifer Hornsby and Rae Langton. 1998. Free Speech

and Illocution. Legal Theory, 4(1):21–37.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of Tricks for Efﬁcient
Text Classiﬁcation. arXiv:1607.01759 [cs]. ArXiv:
1607.01759.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

William Labov. 1972. Sociolinguistic Patterns. Uni-

versity of Pennsylvania Press, Philadelphia.

Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard,
W. Habbard, L. D. Jackel, and D. Henderson. 1990.
Advances in Neural Information Processing Systems
2. pages 396–404. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.

168Michael Lempert. 2012.

In
Christina Bratt Paulston, Scott F. Kiesling, and
Elizabeth S. Rangel, editors, The Handbook of
Intercultural Discourse and Communication. John
Wiley & Sons.

Implicitness.

Anna Schmidt and Michael Wiegand. 2017. A Survey
on Hate Speech Detection using Natural Language
Processing. Proceedings of the Fifth International
Workshop on Natural Language Processing for So-
cial Media, pages 1–10.

Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press, Cambridge Cambridgeshire ; New
York.

Lisa H. Schwartzman. 2002. Hate Speech, Illocution,
and Social Context: A Critique of Judith Butler.
Journal of Social Philosophy, 33(3):421–441.

Christopher D. Manning. 2015. Computational Lin-
guistics and Deep Learning. Computational Lin-
guistics, 41(4):701–707.

Mari J. Matsuda. 1989. Public Response to Racist
Speech: Considering the Victim’s Story. Michigan
Law Review, 87(8):2320–2381.

Yashar Mehdad and Joel Tetreault. 2016. Do Charac-
ters Abuse More Than Words? Proceedings of the
17th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 299–303.

Sara Mills. 2003. Gender and Politeness. Cambridge

University Press, Cambridge ; New York.

John Copeland Nagle. 2009. The Idea of Pollution. UC

Davis Law Review, 43(1):1–78.

Chikashi Nobata,

Joel Tetreault, Achint Thomas,
Yashar Mehdad, and Yi Chang. 2016. Abusive Lan-
In Pro-
guage Detection in Online User Content.
ceedings of the 25th International Conference on
World Wide Web, WWW ’16, pages 145–153, Re-
public and Canton of Geneva, Switzerland. Interna-
tional World Wide Web Conferences Steering Com-
mittee.

Ji Ho Park and Pascale Fung. 2017. One-step and two-
step classiﬁcation for abusive language detection on
twitter. arXiv preprint arXiv:1706.01206.

John Pavlopoulos, Prodromos Malakasiotis,

Ion Androutsopoulos. 2017.
for user comment moderation.
arXiv:1705.09993.

and
Deep learning
arXiv preprint

Fernando Pereira. 2000.

Formal grammar and in-
Philosophi-
formation theory:
cal Transactions of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences,
358(1769):1239–1253.

together again?

Bj¨orn Ross, Michael Rist, Guillermo Carbonell, Ben-
jamin Cabrera, Nils Kurowsky, and Michael Wo-
jatzki. 2017. Measuring the Reliability of Hate
Speech Annotations: The Case of the European
arXiv:1701.08118 [cs]. ArXiv:
Refugee Crisis.
1701.08118.

Haji Mohammad Saleem, Kelly P. Dillon, Susan Be-
nesch, and Derek Ruths. 2017. A Web of Hate:
Tackling Hateful Speech in Online Social Spaces.

John Searle. 1969. Speech Acts: An Essay in the Phi-
losophy of Language. Cambridge University Press.

Michael Silverstein. 1993. Metapragmatic Discourse
and Metapragmatic Function.
In John Lucy, ed-
itor, Reﬂexive Language: Reported Speech and
Metapragmatics, pages 33–58. Cambridge Univer-
sity Press, Cambridge.

Michael Silverstein. 1996. Encountering Language
and Languages of Encounter in North American
Ethnohistory. Journal of Linguistic Anthropology,
6(2):126–144.

Caroline Sinders and Freddy Martinez. 2018. Online
Monitoring of the Alt-Right. The Circle of HOPE,
New York City, 27th July 2018.

Ellen Spertus. 1997. Smokey: Automatic Recognition
In Proceedings of the Four-
of Hostile Messages.
teenth National Conference on Artiﬁcial Intelligence
and Ninth Conference on Innovative Applications
of Artiﬁcial Intelligence, AAAI’97/IAAI’97, pages
1058–1065, Providence, Rhode Island. AAAI Press.

Ye Tian and Richard Breheny. 2016. Dynamic prag-
matic view of negation processing. In Negation and
polarity: Experimental perspectives, pages 21–43.
Springer.

S. K. Verma. 1976. Code-switching: Hindi-English.

Lingua, 38(2):153–165.

William Warner and Julia Hirschberg. 2012. Detect-
ing Hate Speech on the World Wide Web. In Pro-
ceedings of the Second Workshop on Language in
Social Media, LSM ’12, pages 19–26, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Zeerak Waseem, Thomas Davidson, Dana Warmsley,
and Ingmar Weber. 2017. Understanding abuse:
A typology of abusive language detection subtasks.
CoRR, abs/1705.09899.

Ludwig Wittgenstein. 1953. Philosophical investiga-

tions. Macmillan Publishing Company.

Kathryn A. Woolard. 1998.

Introduction: Language
Ideology as a Field of Inquiry.
In Bambi B. Schi-
effelin, Kathryn A. Woolard, and Paul V. Kroskrity,
editors, Language Ideologies: Practice and Theory,
pages 3–32. Oxford University Press, U.S.A., New
York.

169Kathryn A. Woolard and Bambi B. Schieffelin. 1994.
Language Ideology. Annual Review of Anthropol-
ogy, 23(1):55–82.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2016. Ex Machina: Personal Attacks Seen at Scale.
arXiv:1610.08914 [cs]. ArXiv: 1610.08914.

170
The University of Texas System Submission for the Code-Switching

Workshop Shared Task 2018

Florian Janke, Tongrui Li, Gualberto Guzm´an,

Eric Rinc´on, Barbara Bullock, Almeida Jacqueline Toribio

Bilingual Annotation Task (BATs) Research Group

{florian,tli1998,eric.rincon,gualbertoguzman}@utexas.edu

{bbullock,toribio}@austin.utexas.edu

University of Texas at Austin

Abstract

This paper describes the system for the
Named Entity Recognition Shared Task
of the Third Workshop on Computational
Approaches to Linguistic Code-Switching
(CALCS) submitted by the Bilingual An-
notations Tasks (BATs) research group of
the University of Texas. Our system uses
several features to train a Conditional Ran-
dom Field (CRF) model for classifying in-
put words as Named Entities (NEs) us-
ing the Inside-Outside-Beginning (IOB)
tagging scheme. We participated in the
Modern Standard Arabic-Egyptian Arabic
(MSA-EGY) and English-Spanish (ENG-
SPA) tasks, achieving weighted average F-
scores of 65.62 and 54.16 respectively. We
also describe the performance of a deep
neural network (NN) trained on a subset
of the CRF features, which did not surpass
CRF performance.

Introduction & Prior Approaches

1
Named entity recognition (NER) and classiﬁca-
tion are essential
tasks in information extrac-
tion (Nadeau and Sekine, 2007). However, NER
in texts in which multiple languages are repre-
sented is not straightforward because NEs can be
language-speciﬁc (e.g., Estados Unidos in Span-
ish vs. United States) or language-neutral but re-
gionally speciﬁc (e.g., Los Angeles) or even mixed
(e.g., Nueva York in Spanish) (C¸ etinoglu, 2016;
Guzman et al., 2016). The task is further com-
plicated by the fact that names of companies, in-
stitutions and brands in one language can be com-
mon nouns in another (e.g, Toro is a brand name
for a U.S. company but toro in Spanish means
bull’). These challenges confound the already
difﬁcult task of working with multilingual texts,

which can be considered resource scarce’ with re-
spect to the availability of NLP tools (Riaz, 2010;
Zirikly and Diab, 2015; Sitaram and Black, 2016;
Guzm´an et al., 2017). But NER in multilingual
communication is essential given that multilin-
gualism is common throughout the world, and, for
many speakers, language mixing is a shared prac-
tice and one that can be prevalent in social media
like Twitter (Jurgens et al., 2014; Jamatia et al.,
2015, 2016; Vilares et al., 2015).

2 Data Description

Over 62k Tweets were collected and manually
annotated for NEs to be used in this shared
task (Aguilar et al., 2018). The annotators la-
beled each NE using one of ten tags: PERSON,
LOCATION, ORGANIZATION, PRODUCT, GROUP,
EVENT, TIME, TITLE, OTHER, or NOT-NE. All to-
kens are tagged using the IOB scheme while ignor-
ing hashtags and @-mentions, i.e. Louis Vuitton is
tagged with B-ORG and I-ORG but @RideAlong
is tagged as O. NEs can occur in all languages
and, since this is Twitter data, can frequently be
misspelled or missing orthographic features that
would ease identiﬁcation. The Tweets were di-
vided into training, development, and test sets and
released to the participants of the shared task along
with tools for preprocessing of the Tweets.

3 Approach & Methodology
3.1 Conditional Random Field
One approach we used to perform NE recogni-
tion in this shared task was the usage of condi-
tional random ﬁelds (Lafferty et al., 2001), a tech-
nique used for sequence labeling. More speciﬁ-
cally, python-crfsuite (Peng and Korobov, 2014)
was used, a Python wrapper around CRFsuite
(Okazaki, 2007), an implementation of CRFs in
C/C++. CRFs work by looking at several words

ProceedingsofTheThirdWorkshoponComputationalApproachestoCode-Switching,pages120–125Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics120and their features and expected classiﬁcation (in
this case the NE classiﬁcation) as examples and
using the information gained to predict classiﬁca-
tions on future data that has not been seen before.
For our use of CRFsuite, the values of 1.0 for L1
and 0.001 for L2 regularization (from the NER ex-
ample provided by the package) were used with a
total of 150 training iterations. All other parame-
ters were left at their default values.
3.1.1 Features Used
Several different features of the tweets as whole
and individual tokens were used as input, some of
which rely on external resources to generate. Ini-
tially we developed our features on the ENG-SPA
dataset. Interestingly many of the features used for
ENG-SPA performed well on the MSA-EGY data.
Inspiration for the features used was drawn from
various papers from the First Workshop on CALCS
(Chittaranjan et al., 2014; Lin et al., 2014). The
features used can be grouped into ﬁve categories:

1. Word features: lowercase copy of the word,
its two last characters, length, whether it is
the ﬁrst word or not, whether this word is all
alphanumeric characters (only for the MSA-
EGY dataset), if this word is made up of dig-
its or not, and if the word contains emoji.

2. Capitalization: is the word all uppercase or

title case?

3. Language tags: off-the-shelf taggers from the
Natural Language Toolkit (NLTK) (Bird and
Loper, 2004) were used to perform NE and
part of speech (POS) tagging on one tweet at
a time and the tags were applied to individual
tokens.

4. Language detection:

in the ENG-SPA
dataset only,
language detection on entire
tweets was done using langdetect, a Python
port (Danil´ak, 2017) of language-detection
(Nakatani, 2010) originally written in Java.
Probabilities of the tweet being English or
Spanish rounded to 2 digits after the decimal
point were used. If the tweet was classiﬁed
as neither English or Spanish, the probabil-
ity was set to be 0. For example, “Quiero un
roadtrip asap” was falsely classiﬁed as Ro-
manian.

5. Twitter functionality: does the overall tweet
contain an @mention or #hashtag? Is this

word itself one of the two? Is this a URL?

A subset of the features mentioned above were
applied to the next and previous words and used
as features to classify the current word: the word
in lowercase form, its last two characters, if it is
the ﬁrst word, title case, uppercase, a URL, @-
mention, or #hashtag, if it contains an emoji, its
NE and POS tag classiﬁcation by NTLK.

Additional features have been experimented
with and their results are included in section 4.
These features include the last three characters of
the word, whether it contains a digit (not if it is
a digit itself), or if it is made up of exclusively
ASCII characters.

3.2 Deep / Wide Model
The deep and wide architectures have had recent
success for the use of recommendation engines
(Cheng et al., 2016), but here we adapt it for the
use of NER. Deep and wide architectures have
the beneﬁt of embedding categorical variables in
a vector space allowing for unseen feature combi-
nations and the use of cross-product feature trans-
formations for effective and interpretable features.
This combination of cross-product feature combi-
nations and dense embeddings allows for deep and
wide models to memorize and generalize to the
input data while reducing feature engineering ef-
forts.

Figure 1:
models

layers in wide (left) and deep (right)

3.2.1 Training process
The model was trained using Tensorﬂow, an open-
source machine learning framework designed by
Google (Abadi et al., 2016). The classiﬁer pro-
vides a general purpose wide and deep learning
model for users to train. The wide model is a
pre-built linear classiﬁer which attempts to clas-
sify each word in a particular tweet based on val-
ues from their linear combinations.

The deep model used a pre-built neural net-
work to classify the data by letting its features

121tempt does not perform as well as expected. For
ENG-SPA the submitted conﬁguration excluding
POS and NE seems to work best while the submit-
ted conﬁguration with a combination of changes
(shown in table 1) works best for MSA-EGY go-
ing by F1-score.

Table 1 shows the features that were modiﬁed
for use. An asterisk (*) indicates that this is a
change compared to the submitted conﬁguration
a. Rows not included are features that remained
unchanged throughout.

4.2 NN Performance
As shown in table 3, the F1-score was subopti-
mal due to a low recall score. Two different mod-
els, one implementing only the wide portion and
the other implementing the deep and wide models
were trained with features extracted from the data
set. Three different variants of the features and the
results are displayed in table 2. Surprisingly, the
wide model showed an overall better performance
than the wide and deep model. This may be due
to a lack of the features extracted from the dataset
for the deep learning to build on. The lack of recall
may occur due to the same reason, which eventu-
ally leads to the rejection of this model.

5 Conclusion
In this paper, we described the University of Texas
BATs research group’s submission for the CALCS
2018 Shared Task for NER. We found that some
features improved results of the CRF model on
one language combination, but not on the other. In
both cases, our CRF model outperformed the base-
line NER performance. However, training an NN
using the same features as the CRF did not signif-
icantly improve F1-scores, but further feature en-
gineering on or combination of both models could
improve the performance.

propagate through the network. Using Python,
Tweets from the tsv ﬁle were ﬁrst parsed into a
internal data model where the features are com-
puted as properties of the individual words. The
model outputs a csv ﬁle with each feature listed
as a column that can be conveniently passed to
the DNNLinearCombinedClassifier. We
used a subset of the CRF features including the
word itself, capitalization of the word, word type,
and the adjacent words.

The wide portion of the model enables NER
tagging through linear properties. Features were
inputted as the base column to provide informa-
tion to the activation layer of the neural network.
Some features such as the word, word’s capital-
ization, word’s type were cross validated as a set
and hence would make the model recognize that
these grouped features would have dependencies
among themselves.
Implementing a neural net-
work, the deep model greatly increased the train-
ing time with a ratio of roughly 1:20 per iteration.
The models did not perform well against the CRF
possibly due to a lack of features, hence the CRF
was used in the ﬁnal submission of the project.

4 Results & Analysis
4.1 CRF Performance
Our submission for the shared task was evaluated
using both the harmonic mean F1 and the sur-
face forms F1 metrics (Derczynski et al., 2017)
on each dataset. In line with the baseline perfor-
mance, our system performs better on the MSA-
EGY data than the ENG-SPA data despite the
difference in data size. The scores on the two
challenges were 65.62 for MSA-EGY and 54.16
for ENG-SPA. After the shared task submission
closed, we continued experimenting with different
features. The F1-scores (computed using scikit-
learn (Pedregosa et al., 2011)) of the CRF trained
on the training data set and evaluated on the test-
ing set using various conﬁgurations of features are
shown in table 2. These results are different from
those submitted to the competition as they were
evaluated on a different data set.

Inclusion or omission of certain features af-
fected the two sets of data differently: for exam-
ple including the ASCII feature improves scores
for ENG-SPA but decreases that for MSA-EGY.
The last row (special) shows an attempt to max-
imize the score by combining successful individ-
ual features and while scores do increase, this at-

122Features
en prob
es prob
ar prob
last 3 chars
has emoji
ascii
NE
POS
two words

ENG-SPA
d

e

c

b

g
a
(cid:88) (cid:88) * (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) * (cid:88) (cid:88) (cid:88) (cid:88)

f

(cid:88)*

(cid:88)*

(cid:88) (cid:88) (cid:88) (cid:88) * (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) * (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) * (cid:88)
(cid:88)*

(cid:88)*
*
(cid:88)*
*
*

h

a

b

MSA-EGY
e

d

c

f

g

h

(cid:88)*

(cid:88)*

(cid:88)*
(cid:88)*
(cid:88) (cid:88) (cid:88) (cid:88) * (cid:88) (cid:88) *
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) * (cid:88) *
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) * (cid:88) *
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) * (cid:88)

(cid:88)*

Table 1: Feature conﬁgurations

ENG-SPA

MSA-EGY

Conﬁguration
a (submission)
b (include last 3 characters)
c (toggle language probabilities)
d (check for ascii)
e (no emoji)
f (exclude POS and NE tags)
g (toggle surrounding two words)
h (special)

precision

0.69
0.67
0.49
0.73
0.71
0.69
0.50
0.63

recall F1-score
0.25
0.26
0.24
0.25
0.25
0.27
0.23
0.27

0.32
0.33
0.31
0.34
0.33
0.34
0.30
0.33

precision

0.86
0.84
0.86
0.86
0.87
0.86
0.84
0.84

recall F1-score
0.68
0.7
0.68
0.67
0.68
0.68
0.65
0.71

0.76
0.76
0.76
0.75
0.76
0.76
0.73
0.77

Table 2: Performance of CRF on various conﬁgurations

ENG-SPA (Wide model only)
F1-score
precision

ENG-SPA (Deep + Wide model)
F1-score
precision

Conﬁguration
original
excluding next word
excluding next word and length

recall
0.03
0.04
0.04

0.23
0.28
0.31

0.05
0.07
0.07

0.22
0.13
0.13

recall
0.0373
0.02
0.03

0.06
0.03
0.05

Table 3: Performance of NN on various conﬁgurations

123References
Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona
Diab, Julia Hirschberg, and Thamar Solorio. 2018.
Overview of the CALCS 2018 Shared Task: Named
Entity Recognition on Code-switched Data.
In
Proceedings of
the Third Workshop on Compu-
tational Approaches to Linguistic Code-Switching,
Melbourne, Australia. Association for Computa-
tional Linguistics.

Steven Bird and Edward Loper. 2004. Nltk: the nat-
In Proceedings of the ACL
ural language toolkit.
2004 on Interactive poster and demonstration ses-
sions, page 31. Association for Computational Lin-
guistics.

¨Ozlem C¸ etinoglu. 2016. A turkish-german code-

switching corpus. In LREC.

Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal
Shaked, Tushar Chandra, Hrishi Aradhye, Glen An-
derson, Greg Corrado, Wei Chai, Mustafa Ispir, Ro-
han Anil, Zakaria Haque, Lichan Hong, Vihan Jain,
Xiaobing Liu, and Hemal Shah. 2016. Wide & deep
In Proceed-
learning for recommender systems.
ings of the 1st Workshop on Deep Learning for Rec-
ommender Systems, DLRS 2016, pages 7–10, New
York, NY, USA. ACM.

Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. Word-level language
identiﬁcation using crf: Code-switching shared task
report of msr india system. In Proceedings of The
First Workshop on Computational Approaches to
Code Switching, pages 73–79.

Michal Danil´ak. 2017. Python port of google language

detection library.

Leon Derczynski, Eric Nichols, Marieke van Erp, and
Nut Limsopatham. 2017. Results of the wnut2017
shared task on novel and emerging entity recogni-
tion. In Proceedings of the 3rd Workshop on Noisy
User-generated Text, pages 140–147. Association
for Computational Linguistics.

Gualberto Guzm´an, Joseph Ricard, Jacqueline Serigos,
Barbara E Bullock, and Almeida Jacqueline Toribio.
2017. Metrics for modeling code-switching across
corpora. Proc. Interspeech 2017, pages 67–71.

Anupam Jamatia, Bj¨orn Gamb¨ack, and Amitava Das.
2015.
Part-of-speech tagging for code-mixed
english-hindi twitter and facebook chat messages.
In Proceedings of the International Conference Re-
cent Advances in Natural Language Processing,
pages 239–248.

Anupam Jamatia, Bj¨orn Gamb¨ack, and Amitava Das.
2016. Collecting and annotating indian social me-
In the 17th International
dia code-mixed corpora.
Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 3–9.

David Jurgens, Stefan Dimitrov, and Derek Ruths.
2014.
Twitter users# codeswitch hashtags!#
moltoimportante# wow. In Proceedings of the First
Workshop on Computational Approaches to Code
Switching, pages 51–61.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data.

Chu-Cheng Lin, Waleed Ammar, Lori Levin, and Chris
Dyer. 2014. The cmu submission for the shared task
on language identiﬁcation in code-switched data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code Switching, pages 80–86.

David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classiﬁcation.
Lingvisticae Investigationes, 30(1):3–26.

Shuyo Nakatani. 2010. Language detection library for

java.

Naoaki Okazaki. 2007. Crfsuite: a fast implementation

of conditional random ﬁelds (crfs).

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Terry Peng and Mikhail Korobov. 2014.

python-
https://python-crfsuite.

crfsuite.
readthedocs.org/.

Kashif Riaz. 2010. Rule-based named entity recog-
In Proceedings of the 2010 named
nition in urdu.
entities workshop, pages 126–135. Association for
Computational Linguistics.

Sunayana Sitaram and Alan W Black. 2016. Speech

synthesis of code-mixed text. In LREC.

Gualberto A Guzman, Jacqueline Serigos, Barbara E
Bullock, and Almeida Jacqueline Toribio. 2016.
Simple tools for exploring variation in code-
switching for linguists. In Proceedings of the Sec-
ond Workshop on Computational Approaches to
Code Switching, pages 12–20.

David Vilares, Miguel A Alonso, and Carlos G´omez-
Rodr´ıguez. 2015. Sentiment analysis on monolin-
gual, multilingual and code-switching twitter cor-
pora. In Proceedings of the 6th Workshop on Com-
putational Approaches to Subjectivity, Sentiment
and Social Media Analysis, pages 2–8.

124Ayah Zirikly and Mona Diab. 2015. Named entity
recognition for arabic social media. In Proceedings
of the 1st Workshop on Vector Space Modeling for
Natural Language Processing, pages 176–185.

125
AttentionMeSH: Simple, Effective and Interpretable Automatic MeSH

Indexer

Qiao Jin*

University of Pittsburgh
qiao.jin@pitt.edu

William W. Cohen†
wcohen@google.com

Google, Inc.

Abstract

There are millions of articles in PubMed
database. To facilitate information retrieval,
curators in the National Library of Medicine
(NLM) assign a set of Medical Subject Head-
ings (MeSH) to each article. MeSH is a
hierarchically-organized vocabulary, contain-
ing about 28K different concepts, covering
the ﬁelds from clinical medicine to informa-
tion sciences. Several automatic MeSH index-
ing models have been developed to improve
the time-consuming and ﬁnancially expensive
manual annotation, including the NLM ofﬁcial
tool – Medical Text Indexer, and the winner
of BioASQ Task5a challenge – DeepMeSH.
However, these models are complex and not
interpretable. We propose a novel end-to-end
model, AttentionMeSH, which utilizes deep
learning and attention mechanism to index
MeSH terms to biomedical text. The attention
mechanism enables the model to associate tex-
tual evidence with annotations, thus providing
interpretability at the word level. The model
also uses a novel masking mechanism to en-
hance accuracy and speed. In the ﬁnal week
of BioASQ Chanllenge Task6a, we ranked
2nd by average MiF using an on-construction
model. After the contest, we achieve close to
state-of-the-art MiF performance of ∼ 0.684
using our ﬁnal model. Human evaluations
show AttentionMeSH also provides high level
of interpretability, retrieving about 90% of all
expert-labeled relevant words given an MeSH-
article pair at 20 output.

Introduction

1
MEDLINE is a database containing more than
24 million biomedical journal citations by 20181.

*These authors contribute equally to the paper.
†This work was done while the author was at CMU.
1https://www.nlm.nih.gov/pubs/

Bhuwan Dhingra*

Carnegie Mellon University
bdhingra@cs.cmu.edu

Xinghua Lu

University of Pittsburgh
xinghua@pitt.edu

PubMed provides free access to MEDLINE for
worldwide researchers. To facilitate information
storage and retrieval, curators at the National Li-
brary of Medicine (NLM) assign a set of Med-
ical Subject Headings (MeSH) to each article.
MeSH2 is a hierarchically-organized terminology
developed by NLM for indexing and cataloging
biomedical texts like MEDLINE articles. MeSH
has about 28 thousand terms by 20183, covering
the ﬁelds from clinical medicine to information
sciences.
Indexers examine the full article and
annotate it with MeSH terms according to rules
set by NLM4. Its estimated that indexing an ar-
ticle costs $9.4 on average (Mork et al., 2013),
and there are more than 813,500 citations added to
MEDLINE in 20175. Indexing all citations manu-
ally would cost several million dollars in one year.
Thus, several automatic annotation models have
been developed to improve the time-consuming
and ﬁnancially expensive manual annotation. We
will discuss these models in section 2.1.

Automatical annotating PubMed abstracts with
MeSH terms is hard in several aspects: There
are 28 thousand possible classes and even more
of their combinations. The frequencies of dif-
ferent MeSH terms also vary a lot: The most
frequent MeSH term is ‘Humans’ and it is an-
notated to more than 8 million articles in the
MEDLINE database; while the 20,000th frequent
MeSH ‘Hypnosis, Anesthetic’ is indexed to only
about 200 articles (Peng et al., 2016). It causes se-
vere class imbalance problems. Above difﬁculties
are further complicated by the fact that indexers
at the NLM usually inspect the whole articles to

2https://www.nlm.nih.gov/mesh
3https://www.nlm.nih.gov/pubs/

factsheets/mesh.html

4https://www.nlm.nih.gov/bsd/indexing/

training/TIP_010.html

5https://www.nlm.nih.gov/bsd/bsd_key.

factsheets/medline.html

html

Proceedingsofthe2018EMNLPWorkshopBioASQ:Large-scaleBiomedicalSemanticIndexingandQuestionAnswering,pages47–56Brussels,Belgium,November1st,2018.c(cid:13)2018AssociationforComputationalLinguistics47do the annotation, but the challenge only provides
PubMed abstracts and titles, which might not be
enough to ﬁnd all MeSH terms. We will discuss it
more detailedly in section 5.

Deep learning is a subtype of machine learning
that arranges the computational models in multiple
processing layers to learn the representations of
data with multiple levels of abstractions as well as
the mapping from these features to the output (Le-
Cun et al., 2015). Attention is a strategy for deep
learning models to learn both the mapping from
input to output and the relevance between input
parts and output parts (Bahdanau et al., 2014). The
learnt relevance helps improve the mapping per-
formance as well as provide interpretability. We
will discuss relevant works of deep learning in au-
tomatic annotations in section 2.2.

Here we propose a novel model, Attention-
MeSH, which utilizes deep learning and attention
mechanism to index MeSH terms to biomedical
texts and provides interpretation at the word level.
Each abstract, together with title and journal name,
is tokenized to words, then the model feeds word
vectors to a bidirectional gated recurrent unit (Bi-
GRU) to derive word representations with con-
textual information (Schuster and Paliwal, 1997;
Cho et al., 2014). We narrow down the MeSH
term vocabulary for each abstract using a mask-
ing mechanism. Then for each candidate MeSH
term, the model calculates the attention attribution
over words. Next, each MeSH term gets a spe-
ciﬁc document representations by MeSH-speciﬁc
attention-weighted sum of the word vectors. Fi-
nally, the model uses nonlinear layers to classify
each MeSH term using the learnt MeSH-speciﬁc
document representation.

We participated in BioASQ Challenge Task6A
We achieve
while developing the model.
close to state-of-the-art performance with an on-
construction model in the ﬁnal week of the contest
and with our ﬁnal model after the contest. The
model also achieves high level of interpretability
evaluated by human experts.

The main contributions of this work are summa-

rized as follows:

1. To the best of our knowledge, Attention-
MeSH is the ﬁrst end-to-end deep learning
model with soft-attention mechanism to in-
dex MeSH terms in such a large scale (mil-
lions of training data). With this relatively
simple model, we achieved close to state-

of-the-art performance without any sophisti-
cated feature engineering or preprocessing.

2. We develop a novel masking mechanism,
which is aimed to handle multi-class clas-
siﬁcation problems with a large number of
classes, like indexing MeSH. We also con-
duct extensive experiments on how the mask-
ing layer settings inﬂuence classiﬁcation per-
formance.

3. We believe AttentionMeSH is the ﬁrst MeSH
annotation model that is capable of providing
textual evidence and interpretations of its pre-
dictions. We argue that interpretability mat-
ters because humans are needed to complete
the annotation task.

2 Related Work
2.1 Automatic MeSH Indexing
NLM developed Medical Text Indexer (MTI), a
software for providing human indexers with au-
tomatic MeSH recommendations (Aronson et al.,
2004). MTI takes as input a title and correspond-
ing abstract to generate a set of recommended
MeSH terms. MTI has two steps: the ﬁrst is to
generate MeSH candidates for recommendation,
and the second is to ﬁlter and rank the candi-
dates to give a ﬁnal output. MTI uses MetaMap
and nearest neighbor methods. MetaMap is an-
other NLM-developed tool, which maps mentions
in biomedical texts to Uniﬁed Medical Language
System concepts (Aronson, 2001).

BioASQ is an European Union-funded project
that organizes tasks on biomedical semantic in-
dexing and question answering (Tsatsaronis et al.,
2015).
In the task A of BioASQ, participants
are asked to annotate un-indexed PubMed arti-
cles with MeSH terms using their models, be-
fore they are annotated by the human indexers.
The manual annotations are taken as ground truth
to evaluate the participating models. DeepMeSH
(Peng et al., 2016) is the winner of the latest chal-
lenge, BioASQ task 5a, held in 2017. DeepMeSH
also uses a two-step strategy: the ﬁrst step is to
generate MeSH candidates and predict the num-
ber of output MeSH terms, and the second step
is to rank the candidates and take the highest-
ranked predicted number of MeSH terms as out-
put. DeepMeSH uses Term Frequency Inverse
Document Frequency (TFIDF) and document to
vector (D2V) schemes to represent each abstract

48and generate MeSH candidates using binary clas-
siﬁers and k-nearest neighbor (KNN) methods
over using these features. TFIDF is a traditional
weighted bag of word sparse representation of the
text and D2V learns a deep semantic representa-
tion of the text.

Because state-of-the-art models have less than
0.7 Micro-F, automatic MeSH indexing systems
can just serve to assist human indexers. Since hu-
man indexers usually add or delete MeSH terms
based on the recommendations, interpretability of
the automatic annotations is very important for
them. In this paper we adopt a local explanation
view of model interpretability (Lipton, 2016), and
argue that a good system, in addition to being ac-
curate, should also be able to tell which part of
the input supports the indexed MeSH term. This
would allow human indexers to be more effective
at annotating the article.

2.2 Deep Learning for Text Classiﬁcation

Automatic indexing of MeSH terms to PubMed
articles is a multi-label text classiﬁcation prob-
lem. FastText (Joulin et al., 2016) is a simple
and effective method for classifying texts based
on n-gram embeddings.
(Kim, 2014) used Con-
volutional Neural Networks (CNNs) for sentence-
level classiﬁcation tasks with state-of-the-art per-
formance on 4 out of 7 tasks they tried. Very deep
architectures such as that of (Conneau et al., 2017)
have also been proposed for text classiﬁcation.
Motivated by these works we use an RNN-based
model for classifying each MeSH term as being a
positive label for a given article. We further use at-
tention mechanism to boost performance and pro-
vide word-level interpretability.

Recently, there has been work on automatic an-
notation of International Classiﬁcation of Diseases
codes from clinical texts. (Shi et al., 2017) used
character-level and word-level Long Short-Term
Memory netowrks to get the document represen-
tations and (Mullenbach et al., 2018) used word-
level 1-D CNN to get the document representa-
tions. Both these works utilized a soft attention
strategy where each class gets a speciﬁc document
represetation by weighted sum of the attention
over words or phrases. Mullenbach et al. (2018)
also highlighted the need for interpretability when
annotating medical texts – in this work we apply
similar ideas to the domain of MeSH indexing.

3 Methods
The model architecture is visualized in Figure 1.
Starting from an input abstract, title and journal
name, words in the document are embedded and
fed to BiGRU to derive context-aware represen-
tations; KNN-derived articles from training cor-
pus are identiﬁed and frequent MeSH terms in
them are included as candidate annotations for the
document. MeSH terms are embedded, and only
those candidates are further considered in atten-
tion mechanism. We call it a masking mecha-
nism. We apply an attention mechanism to as-
sign attention weights to each word with respect
to each candidate MeSH term, which leads to a
MeSH-speciﬁc document representation. Finally,
we use MeSH-speciﬁc document representations
as input to perform classiﬁcations. For each candi-
date MeSH term of a document, the model outputs
a probability. Binary cross-entropy loss is used for
a gradient-based method to optimize the parame-
ters. At inference time, the sigmoid outputs are
converted to binary variables by thresholding.

3.1 Document Representation
For each article to be indexed, we ﬁrst tokenize the
journal name, title and abstract to words. In order
to use the pre-trained word embeddings6 provided
by BioASQ organizer, we use the same tokenizer
as they did. The pre-trained word embeddings are

denoted as E ∈ R|V|×de1, where |V| is the vocab-
ulary size and de1 is the embedding size.

We can represent each article by a sequence of
word embeddings corresponding to the tokenized
text. The word embeddings are initialized by the
BioASQ pre-trained word embeddings.

D =(cid:2)w1

... wL(cid:3)T

∈ RL×de1,

where L is the number of words in the journal
name, title and abstract, and wi is a vector for
word at position i.

For each document representation D, we feed
this sequence of word vectors to an BiGRU to de-
rive a context-aware sequence of word vectors:

(cid:101)D = BiGRU (D) =(cid:2)(cid:102)w1
∈ RL×2dh,
where (cid:102)wi is the corresponding concatenated for-

ward and backward hidden states of each word,
and dh is the hidden size of BiGRU.

... (cid:102)wL(cid:3)T

6http://participants-area.bioasq.org/

tools/BioASQword2vec/

49Figure 1: Model Architecture. BiGRU: Bi-directional recurrent gated unit. The example abstract is from (Karaa
and Goldstein, 2015).

3.2 MeSH Representation and Masking
We learn the MeSH embedding matrix H ∈
RN×de2, where N is the number of all MeSH
terms (28,340), and de2 is the embedding size. For
each article, we consider only a subset of all 28k
MeSH terms for two reasons: 1. For each MeSH
term, there are far more negative samples than the
positive ones. We achieve down-sampling of the
negative samples by considering only a subset of
all MeSH terms as candidate for each article, so
that the classiﬁer only concentrate on choosing a
most suitable MeSH among a set of plausible an-
notations; 2. It’s more time efﬁcient than training
all the MeSH terms or training the MeSH classi-
ﬁers one by one. We call it a masking layer.

We use KNN strategy to choose a speciﬁc sub-

set of MeSH terms to train for each article:

Each abstract can be represented by IDF-

weighted sum of word vectors:

d = (cid:80)n
(cid:80)n

i=1 IDF i × wi

i=1 IDF i

∈ Rde1,

where wi is the corresponding word vector, and
IDF i is the inverse document frequency of this
word.

tations between the abstracts:

Similarity(i, j) =

dT

i dj

||di|| × ||dj||

For each article, we ﬁnd its K nearest neighbors
based on cosine similarity. And then we count
the MeSH term frequency in these neighbors. The
most frequent M MeSH terms are trained for each
article. We denote the masked MeSH embedding
as H(cid:48),

(cid:48)

H

=(cid:2)m1 m2

... mM(cid:3) ∈ RM×de2,

where we make de2 = dh so that we could directly
get the dot product of each MeSH representation
and word vector.

3.3 Attention Mechanism
After getting the document representation and
masked MeSH representations, we calculate the
dot products between each context-aware word
vector and each MeSH embedding, which repre-
sents the similarity within each pair:

S = H

(cid:48) (cid:101)DT =(cid:104)(cid:101)D m1

... (cid:101)D mM(cid:105)T

We then uses SoftMax function to normalize over
the word axis to get attention weights attribution
for each MeSH term:

∈ RM×L,

We then calculate cosine similarity of represen-

SoftMax (Sim)

50m1m2...mNmi1...miM............XXLinear ˆyi12[0,1]ˆyiM2[0,1]......Mask.Tokenize&EmbedAttentionMatrixyi1,...,yiM2{0,1}BinaryEncodingEmbedOptimizationBiGRUArticlewi1wi2gwi1gwi2gwiLiwiLiPediatricDiabetesJournalName:i-thGradientBasedBinaryCross-EntropyLossTrainingLabelPredictionInferenceThreshold-ingAbstract:___________________________MeSH:___,____,___,…Abstract:___________________________MeSH:___,____,___,…Abstract:___________________________MeSH:___,____,___,…......KNNFrequentMeSH=(cid:2)α1

... αM(cid:3)T

... SoftMax((cid:101)D mM)(cid:105)T
∈ [0, 1]M×L,

=(cid:104)SoftMax((cid:101)D m1)
where αj ∈ [0, 1]L is the attention weights over
words for MeSH term j, and(cid:80)L

3.4 Classiﬁcation
For each MeSH term, we can have a MeSH-
speciﬁc representation of document by sum of
word vectors weighted by attention weights:

k=1 αjk = 1.

Rj = αj(cid:101)D ∈ R2dh,

where Rj is MeSH term j speciﬁc document rep-
resentation. We apply a linear projection layer and
sigmoid activatin function to each MeSH term, ﬁ-
nally getting the output probability:

ˆyj = σ(RT

(cid:48)
j + bj) ∈ [0, 1],
j m

where m(cid:48)
parameters for MeSH term j. We model

j and bj are learnable linear projection

P (MeSH j indexed| Journal, Title, Abstract) = ˆyj.
3.5 Training
After get the conditioned probability we model,
we can calculate the binary cross-entropy loss for
each MeSH term:

Lj = −(yjlog( ˆyj) + (1 − yj)log(1 − ˆyj)),

where yj ∈ {0, 1} is the ground-truth label of
MeSH j. yj = 0 means MeSH j is not annotated
to the article by human indexers, while yj = 1
means MeSH j is annotated. We can get the total
loss by summing them up:

L =

1
M

M(cid:88)j=1

Lj

The model is trained end-to-end from word and
MeSH embedding to the ﬁnal projection layer by
a gradient-based optimization algorithm to mini-
mize L.
3.6
At inference time, we will predict the MeSH terms
whose predicted probability is larger than a tuned
threshold:

Inference

(predict MeSH j) = 1( ˆyj > pj),

where pj is the tuned threshold for MeSH term j.
The thresholds are tuned to maximize MiF:

p1, ..., pN = argmax
p1,...,pN

MiF(Model, p1, ..., pN )

We tune p by the the micro-F optimization algo-
rithm described in (Pillai et al., 2013), which they
proved to be able to achieve the global maximum.

4 Experiments
4.1 Dataset
We use the dataset provided by BioASQ7, which
contains about 13.5 million manually annotated
PubMed articles.
The dataset covers 28,340
MeSH terms in total, and each article is annotated
12.69 MeSH terms on average. We selected 3 mil-
lion articles from 2012 to 2017 for training.

The results reported in this paper are derived
from two test sets: BioASQ Test Sets: During the
challenge, BioASQ provides a test set of several
thousands articles each week. Ours: we use 100
thousand latest articles to test our model, and all
other results are calculated by this dataset. Since
our test set is very large, the results will be precise.

4.2 Conﬁguration
The model is implemented using PyTorch (Paszke
et al., 2017). The parameter settings are shown
in Table 1. We use Adam optimizer and batch
size of 32. We train 2 epochs of each model on
the 3M article training set, and apply hyperbolic
learning rate decay and early stopping strategies
(Yao et al., 2007). The training takes 4 days on
2 GPUs (GeForce GTX TITAN X). Before tuning
the thresholds for all individual MeSH term, we
use a global threshold of 0.35 due to the highly
imbalanced dataset.

4.3 Evaluation Metric
The major metric for performance evaluation is
Micro-F, which is a harmonic mean of micro-
precision (MiP) and micro-recall (MiR) , and is
calculated as follows:

Micro-F =

2 · MiP · MiR
MiP + MiR

,

where

MiP = (cid:80)Na
i=1(cid:80)N
(cid:80)Na
i=1(cid:80)N

j=1 yij · ˆyij

j=1 ˆyij

7http://participants-area.bioasq.org/

general_information/Task6a/

51Parameter
|V|
de1
de2
dh
N
L
Na BioASQ
Na Ours
K
M
Learning Rate
BiGRU Layer(s)

Value(s)

1.7M
256
512
256

28,340

≤512 (truncated if longer)

5,833∼10,488

100,000

0.1k, 0.5k, 1k, 3M
128, 256, 512, 1,024
0.002, 0.001, 0.0005

1, 2, 3, 4

Table 1: Parameter Values. For hyperparameters, we
highlight the optimal ones among all tried values.

Figure 2: The micro-recall of MeSH terms versus dif-
ferent mask sizes for different numbers of neighbor ar-
ticles.

i=1(cid:80)N
MiR = (cid:80)Na
i=1(cid:80)N
(cid:80)Na

j=1 yij · ˆyij

j=1 yij

In these equations, i is indexed for articles and j is
indexed for MeSH terms, so Na is the number of
articles in the test set, and N is the number of all
MeSH terms. yij and ˆyij are both binary encoded
variables to denote whether MeSH term j is in ar-
ticle i in ground-truth and prediction, respectively.

4.4 Evaluation of Masking Layer
Selecting relevant MeSH terms from neighbor ar-
ticles can be regarded as a weak classiﬁer itself,
and high-recall setting is favored in this step. We
measure the micro-recall for different masking
layer settings, and the results are shown in Fig-
ure 2. Basically, there are two hyperparameters for
it: the number of neighbor articles K and the num-
ber of highest ranking MeSH terms selected M. A
non-trivial baseline for K is 3M, i.e. the number
of all training articles. Under this circumstance,
the ranked MeSH list is determined by global fre-
quency, thus is non-speciﬁc to any article.

We choose the number of nearest articles K =

Mask Setting MiP
1,024 rd.
0.5891
0.6863
1,024 freq.
0.6354
128 n.n.
0.6690
256 n.n.
512 n.n.
0.6663
1,024 n.n.
0.6698

MiR
0.0173
0.4257
0.5880
0.5975
0.6116
0.6262

MiF
0.0337
0.5262
0.6108
0.6312
0.6378
0.6472

Table 2: Model Performance with Different Mask Set-
tings. n.n.: MeSH mask selected from nearest neighbor
articles (K = 1000); freq.: MeSH mask selected from
globally frequent MeSH terms; rd.: MeSH mask ran-
domly selected. All results are averaged over models
trained by 3 random seeds.

1000 for it gives the highest recalls with the in-
crease of mask size. In fact, micro-recall at M =
1024 and K = 1000 is about 0.97, which almost
guarantees that all true annotations are included as
candidate for a document. Before ﬁne-tuning on
other hyperparameters and the thresholds of mak-
ing predictions, we ﬁrst train the model with dif-
ferent M, and report the results in Table 2.

4.5 Evaluation of Performance
While we were developing the model, we partic-
ipated in the BioASQ Task6a challenge. During
the challenge, there is a test set available each
week. Each test set contains several thousands of
un-indexed PubMed citations. Each citation has
journal name, title, abstract information. Partici-
pants will run their models on the test set and up-
load their predictions of MeSH annotations within
a given time. The organizers will then evaluate ev-
ery participants’ predictions and make the results
available. The results of the whole Challenges are
showed in Figure 3. Furthermore, the results of the
last week of the Challenge are showed in Table 3.

Model
Access Inn MAIstro
MeSHmallow
UMass Amherst T2T
iria
MTI First Line Index
DeepMeSH
Default MTI
AttentionMeSH
xgx

Average MiF Maximum MiF

0.2788
0.3161
0.4988
0.4992
0.6332
0.6451
0.6474
0.6635
0.6862

0.2788
0.3161
0.4988
0.5161
0.6332
0.6637
0.6474
0.6635
0.6880

Table 3: Model Performance of the Final BioASQ Test
Set. The models are ranked top-down from the lowest
average MiF to the highest one. Our on-construction
AttentionMeSH ranked second by average MiF.

It should be noted that the models we used in

5202004006008001000Mask Size0.20.40.60.81.0Micro-Recall0.1k nn0.5k nn1k nn3M nn
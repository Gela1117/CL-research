Semi-Supervised Learning with Auxiliary Evaluation Component for

Large Scale e-Commerce Text Classiﬁcation

Mingkuan Liu, Musen Wen, Selcuk Kopru, Xianjing Liu, Alan Lu

2145 Hamilton Avenue, San Jose, CA, 95032, USA

{mingkliu,mwen,skopru,xianjliu,alalu}@ebay.com

eBay Inc.,

Abstract

The lack of high-quality labeled training
data has been one of the critical challenges
facing many industrial machine learning
tasks. To tackle this challenge, in this pa-
per, we propose a semi-supervised learn-
ing method to utilize unlabeled data and
user feedback signals to improve the per-
formance of ML models. The method
employs a primary model M ain and an
auxiliary evaluation model Eval, where
M ain and Eval models are trained it-
eratively by automatically generating la-
beled data from unlabeled data and/or
users feedback signals. The proposed ap-
proach is applied to different text classi-
ﬁcation tasks. We report results on both
the publicly available Yahoo! Answers
dataset and our e-commerce product clas-
siﬁcation dataset. The experimental re-
sults show that the proposed method re-
duces the classiﬁcation error rate by 4%
and up to 15% across various experimen-
tal setups and datasets. A detailed compar-
ison with other semi-supervised learning
approaches is also presented later in the
paper. The results from various text classi-
ﬁcation tasks demonstrate that our method
outperforms those developed in previous
related studies.

Introduction

1
There are many ways to improve the performance
of a machine learning model. Improving the train-
ing data is one such method. Obtaining high-
quality training data, such as human labeled data,
is usually expensive and time-consuming. Many
machine learning systems use unlabeled data or a
mixture of labeled and unlabeled data for train-

ing because it is cheaper and easier to collect
enormous amounts of unlabeled data.
Industry-
deployed machine learning systems that serve mil-
lions of users generate vast amounts of unlabeled
data and noisy user feedback signals every day.
Those data and signals are very important and can
be utilized in the training of real-world machine
learning systems.

In this paper, we propose a new semi-supervised
learning method with a feedback loop to leverage
vast amounts of unlabeled data and feedback sig-
nals.
In particular, we train two machine learn-
ing models iteratively. The main model, which
is represented as M ain, performs the main task
at runtime. The auxiliary model, which is repre-
sented as Eval, works ofﬂine and it estimates the
correctness of the M ain models output. The in-
formation available to the auxiliary model Eval is
much richer than the run-time model M ain. Ex-
tra data, such as user feed-back data and session
context information, can be used when training the
auxiliary model. The idea is to control the false
positive rate of Eval to produce high-quality, au-
tomatically labeled data from unlabeled data. The
entire process runs iteratively and the performance
of both models is improved in an iterative man-
ner. The assumption of run-time M ain model
has much fewer available information is due to the
business logic ﬂow and/or UX design constraints
which limit the run-time M ain model to collect
richer features in some industry setups.

In this paper, we use text classiﬁcation experi-
ments to illustrate the proposed approach. How-
ever, this semi-supervised learning approach can
also be applied to other machine learning tasks,
such as machine translation and search relevance.
Different semi-supervised learning approaches
have been previously proposed to leverage unla-
beled data, including Blum and Mitchell (1998),
Weiss et al. (2016), Goodfellow et al. (2014) and

ProceedingsoftheWorkshoponDeepLearningApproachesforLow-ResourceNLP,pages68–76Melbourne,AustraliaJuly19,2018.c(cid:13)2018AssociationforComputationalLinguistics68Cohn et al. (1996). Experimental results on the
public Yahoo! Answers dataset and on a new
public e-commerce dataset for product classiﬁca-
tion demonstrate the advantages and potential of
the proposed framework compared with previous
work.

The contributions of this work are as follows:
• A new semi-supervised learning method with
the introducing of an auxiliary evaluation
component, and

• A scalable, cost-effective and efﬁcient way to
convert vast amounts of unlabeled data into
high quality labeled data for supervised train-
ing purposes.

In section 2, we present the details of the pro-
posed semi-supervised learning approach in the
context of text classiﬁcation tasks. In section 3,
the theoretical analysis of the proposed method is
provided. Next, we give an overview of related
works and highlight their differences compared to
our approach. Section 5 deﬁnes various experi-
mental setups and presents the results for two dif-
ferent datasets. Finally, we present the papers con-
clusions in section 6.

2 Proposed Method
2.1 Modeling Approach
The suggested method is based on a few observa-
tions from real-world machine learning systems.
• The main model M ain that serves online
users may have limited information avail-
able at prediction time due to the business
logic ﬂow or UX design constraints in indus-
try setups. Therefore, the predictions from
the M ain component may contain errors and
cannot be used directly as labeled data for
model training purposes.

• The evaluation model Eval runs ofﬂine.
Hence, it can access much richer information
than the main task component without those
constraints. User feedback data, user behav-
ioral data, prior and post task session history
data, and the knowledge base about the users
main task constitute richer ofﬂine informa-
tion. All those data helps the Eval model
to reliably estimate whether Mains output is
acceptable or not.

• Large-scale supervised machine learning
methods typically need a larger amount of la-
beled training data for a good performance.
However, in reality, it is very expensive to
manually label millions of training data. On
the other hand, large scale machine learning
systems serving millions of users are generat-
ing millions of unlabeled data and user feed-
back signals every day that is not effectively
utilized.

The main idea of the proposed approach is to
train and deploy two parallel machine learning
models. The ﬁrst model M ain is used to serve
live user requests for the main task. The second
machine learning model Eval is used as an of-
ﬂine model to estimate the accuracy of M ains
prediction. The Eval model utilizes additional
signals, such as user feedback signals, system logs
that are related to user sessions, the output conﬁ-
dence scores from Main, etc. An EM-style iter-
ative process is applied to train M ain and Eval
in a repeated manner. High-quality, automatically
labeled data are extracted from unlabeled data by
controlling the false positive rate and the false neg-
ative rate in Eval. Mathematical analysis (sec-
tion 3) and experiments (section 5) on different
datasets show that after multiple iterations, the ac-
curacy of Main can be improved substantially.

Algorithm 1 shows the details of the proposed
semi-supervised learning algorithm. The input
to the algorithm is an initial small set of labeled
data L, a large set of unlabeled data U and an
optional set of user feedback data F . The al-
gorithm leverages the auxiliary evaluation model
Eval and the optional user feedback data F to pro-
duce high-quality, automatically labeled data AL
from a large amount of unlabeled data U. Once
the labeled data AL is extracted and added to the
training corpus C, we use a shallow neural net-
work1 to train model M ain for the text classiﬁca-
tion tasks.

2.2 Auxiliary Evaluation Model
The auxiliary evaluation model Eval is a binary
classiﬁer that predicts whether the automatic la-
bel is correct or not. It is trained using gradient
boosting2.
If the false positive rate of the eval-

1We use the FastText library Joulin et al. (2016) for the

shallow neural network implementation.

2We use the XGboost library Chen and Guestrin (2016)

for gradient boosting.

69Algorithm 1 Proposed Algorithm

Given:
Labeled dataset L : {li}
Unlabeled dataset U : {ui}
Optional feedback data F : {fi}
M ain = trainMainModel(L)
f pRate = false positive rate threshold
for k = 1 to M axIter do

data F . For each example in L, based on lis la-
beled class, we create one positive training sam-
ple with the correct class and one negative train-
ing sample with a wrong class that is chosen ran-
domly. The features can be generated using the
aforementioned multiple signal sources, such as
Ms prediction conﬁdence scores, the SLM rank-
ing scores and the optional user feedback.

Eval = trainEvalModel(M ain, L, F )
AL = emptyList()
for ui in U do

P Ci = predClass(M, ui)
Score(P Ci) = evalScore(E, ui, fi, P Ci)
if Score(P Ci) > (1 − f pRate) then

AL.append(ui, P Ci)

end if
end for
C = L + AL
M = trainMainModel(C)

end for

uation model Eval is controlled at a low thresh-
old, vast amounts of high-quality, automatically
labeled data can be extracted from the unlabeled
data.

For text classiﬁcation tasks,

the evaluation
model Eval leverages a variety of features. The
conﬁdence score of the main model M ains pre-
diction, the n-gram language model related rank-
ing, the input sentence probability scores evalu-
ated by the language model from M ains predicted
class and the optional noisy user feedback sig-
nal about M ains output are the features used by
Eval. The language model related ranking and
input sentence probability scores are based on the
assumption that sentences belonging to the same
class are more similar than sentences belonging
to different classes. Thus, we train a language
model SLMi for each CLASSi using sentences
belonging to that class. If a sentence belongs to
CLASSi, its sentence probability that is evaluated
with SLMi should be higher than the sentence
probability evaluated with CLASSj, where i (cid:54)= j.
We use the trigram statistical language model3 to
train SLMi for each CLASSi.

Details on how to train the auxiliary evaluation
model Eval are described in Algorithm 2. The
input to the algorithm is the M ain model, the la-
beled data L and the optional set of user feedback

3We use KenLM library Heaﬁeld (2011) to build and

query SLMs.

Algorithm 2 Train Auxiliary Evaluation Model

Given:
M ain model
Labeled data L : {li}
Optional feedback data F : {fi}
for lj in L do

CLASSi = class label of lj
Append lj to SLM Corpusi

end for
for i in AllClasses do

SLMi = trainSLM(SLM Corpusi)

end for
Corpusxgb = emptyList()
for lj in L do

CLASSi = class label of lj
XgbSample+ = getXgbFeatures(lj,fj, M,
SLM s, CLASSi)
XgbSample− = getXgbFeatures(lj,fj, M,
SLM s, CLASSk where k (cid:54)= i)
Corpusxgb.append(+, XgbSample+)
Corpusxgb.append(−, XgbSample−)

end for
Eval = trainXgbModel(Corpusxgb)

3 Theoretical Analysis

The EM-like semi-supervised learning approach
with an auxiliary evaluation component is de-
signed to tackle large scale ML problems. In sec-
tion 5, we will demonstrate that our framework has
a superior and consistently better performance in
various real-world machine learning tasks based
on the empirical results. In this section, we will
ﬁrst analyze and highlight some mathematical as-
pects of this dual-player, semi-supervised learning
approach, and illustrate its deep connection to the
Expectation-Maximization algorithm.
Suppose we are given an initial set of N manu-
ally labeled text S (0), and our main task is to clas-
sify unseen text to a label. As described before,
we use a shallow neural network similar to Joulin
et al. (2016) to build the M ain model. For this

70purpose, according to Joulin et al. (2016), we want
to minimize the negative log-likelihood

as

N(cid:88)

1

− 1
N

ynlog (f (BAxn))

(1)

where xn is the normalized bag of features of the
nth text, yn is the category labels, and A and B are
the weight matrices. As part of the auxiliary evalu-
ation component Eval, we established a machine
learning system with richer context compared to
M ain. The task of Eval is to estimate the prob-
ability that the given input text belongs to the cat-
egory predicted by M ain. This probability is de-
ﬁned as ptexti,cj .

ptexti,cj = P (Cj|texti)

(2)

Notice that the entire purpose of the evaluation
system is to select newly labeled data to enrich
the training set of the main machine learning sys-
tem. Thus, the Eval model estimates the con-
ﬁdence score of this prediction for each sample.
The whole learning process of M ain → Eval it-
erates as described in the previous sections. The
dual system runs iteratively. We stress that it has
a close connection to the popular Expectation-
Maximization algorithm Dempster et al. (1977)
via the following result.

Theory 1. Given a two-player machine learning
system comprised of Main and Eval, the Main
model converges to the local minima of the neg-
ative log-likelihood with the controlled false posi-
tive rate given enough capacity.
Proof. Given a set of training data S (0) = (xi, yi),
i = 1,··· , N, which are the observed features and
labels, let us denote a hidden variable zi ∈ {0, 1}
that is a variable indicating the quality of the ob-
servation. zi takes a value of 1 if the label for the
corresponding instance is correct or relevant, and
0 otherwise. Without the loss of generality, sup-
pose that the M ain model is trained to maximize
the log-likelihood function:

(cid:96) (Θ|X, Y)

(3)

Using equation (1), equation (3) can be rewritten

(cid:96) (Θ|X, Y) = log p (X, Y|Θ)

(cid:88)
(cid:88)

z

= log

= log

≥(cid:88)

z

p (x, y, z|Θ)

p(z)

z

p(z) log

p (x, y, z|Θ)

p(z)

p (x, y, z|Θ)

p(z)

(4)

= Ep(z) log p (p(x, y, z|Θ)
+ Entropy[p(z)]

= L (p, Θ; X, Y)

where the inequality is obtained by Jensen’s in-
equality. The equality holds if and only if

p(z) = p(z|X, y, Θ)

The term Ep(z) log p (p(x, y, z|Θ) is the expected
complete log-likelihood (or, Q-function). The
two machine learning systems then iterate through
the following two steps. From a set of noisy data,
Eval performs similarly in the E-step of the EM
algorithm. For nthstep, (n = 1, 2,··· ):

p(z)(n) = arg max

L

p, Θ(n−1); X, Y

(cid:16)

(cid:17)

(5)

p

= p(z|X, y, Θ(n−1))

Notice that the conditional distribution of the hid-
den variable z is not necessarily fully predictable
by the machine learning model even if the ob-
served data and the models parameters are given.
The evaluation system mainly provides a conﬁ-
dence score of the correctness or conﬁdence of the
prediction, which is deﬁned by equation (5). By
properly controlling the false positive rate, we se-
lect only those new training examples with a good
estimate of p(z)(n) by the Eval model. This re-
sults in a set of ﬁltered samples S (n) to be added
to our M ain system for the next iteration. The
main system then performs the maximization step
role in the EM algorithm framework, which is the
M-step that follows:

(cid:17)

(cid:16)
L
Ep(z)(n) log p (x, y, z|Θ)

p(n), Θ; X, Y

(6)

Θ(n+1) = arg max

Θ

= arg max

Θ

over S (0) ∪ S (1) ∪ ··· ∪ S (n) which is read-
ily solvable from the main machine learning sys-
tem. Notice that in the M-step, it is not neces-
sary to ﬁnd the optimal values over the whole pa-
rameter space. Using the monotonic convergence

71property of the generalized EM algorithm, given
enough capacity, the M ain system would even-
tually converge to its local optimum after enough
iterations.

In the e-commerce scenario, we have more in-
formative features in the ofﬂine Eval system, and
thus the evaluation system can have a very high ac-
curacy. According to the proof, the main machine
learning system eventually reaches a stable state.

4 Related Work

Various semi-supervised learning approaches have
been proposed to leverage unsupervised data to
improve the performance of machine learning sys-
tems Triguero et al. (2015).

Active learning Cohn et al. (1996); Nigam et al.
(1998); Beygelzimer et al. (2009), which is a spe-
cial kind of semi-supervised learning, provides
ways to actively select the most informative data
samples from a vast amount of unlabeled data. The
selected samples are then labeled by humans. In
this way, the total amount of data needed for man-
ual labeling is reduced to save resources. How to
handle the problem of label quality is one of the
active areas of active learning research. Zhang and
Chaudhuri (2015) studied the problem of active
learning where labels were obtained from strong
and weak labelers. In addition to the standard ac-
tive learning setting, they consider the problem
where they have extra weaker labelers that may
provide incorrect labels. Yan et al. (2016) stud-
ies the adaptive active learning problem where the
labeler can return incorrect labels and also abstain
from labeling.

Although active learning can signiﬁcantly re-
duce the amount of manual labeling, it still re-
quires extra human labeling, which is costly and
time consuming. Compared with active learning,
our approach does not require any additional man-
ual labeling effort.

The self-labeled technique is another type of
semi-supervised approach to boost the models per-
formance by iteratively labeling parts of the un-
labeled data. This approach aims to obtain an
enlarged labeled set, which is based on its most
conﬁdent predictions, to classify unlabeled data.
Zhu and Goldberg (2009) divides the self-labeled
methods into self-training and co-training.

In the self-training process Triguero et al.
(2014); Yarowsky (1995), a model is trained with

an initially small number of human labeled exam-
ples that aim to predict unlabeled data. Then, it is
retrained with its most conﬁdent predictions, thus
enlarging its labeled training set. The process iter-
ates in the same manner.

In the co-training process Blum and Mitchell
(1998); Chen et al. (2011), two learning models
are trained separately to provide distinct views of
the data set by using different feature sets of the
data. These two models are initially trained with a
small amount of human labeled data, and then the
most conﬁdent predictions of one model on the un-
labeled data are used to construct the training data
for the other model. This process is repeated itera-
tively. Similar to our proposed approach, the self-
labeled method uses the EM-based iterative pro-
cess to boost the models accuracy and also does
not need any further manual labeling efforts.

The major difference between the self-labeled
approach and our approach is as follows. In the
self-labeled method, with either self-training or
co-training, all the models are main task machine
learning models. In our proposed approach, there
exists only one main-task model and another aux-
iliary evaluation model that runs ofﬂine. Using an
ofﬂine auxiliary evaluation model has the beneﬁt
of utilizing ofﬂine information that is not avail-
able at prediction time. Thus, the auxiliary evalu-
ation model has a better estimation capability than
the main model regarding whether Mains output is
correct or not.

The Generative Adversarial Network Goodfel-
low et al. (2014) is another semi-supervised ap-
proach that tries to generate unlimited synthetic
fake samples that can mimic real data. The GAN
also builds two models, namely, the generative
model and the discriminative model, and puts
them against each other. The generative model
takes random inputs and tries to generate out-
put data that looks similar to real data. The dis-
criminative model takes input data from both the
generative model and real data and tries to cor-
rectly distinguish between them. The GAN has
been successfully applied to image and audio ar-
eas where the synthetic data is real-valued.
It’s
quite challenging for the GAN to generate se-
quence of discrete tokens in the NLP domain. Yu
et al. (2017) has proposed the SeqGAN method
to address this challenge by directly performing
gradient policy update with reinforcement learn-
ing. Kusner and Hern´andez-Lobato (2016) pro-

72posed an alternative method to address this chal-
lenge using the Gumbel-softmax distribution.

One of the differences between our approach
and GAN is that our approach relies on real un-
labeled data while the GAN generates plausible
data with random inputs. Another major differ-
ence is that the evaluation component in our ap-
proach tries to evaluate whether the main model
results are correct or not. Meanwhile, in the GAN
approach, the adversarial component learns to tell
whether the current data sample is real or fake.

5 Experiments

To illustrate the effectiveness of the proposed
semi-supervised learning method, we evaluate it
with different text classiﬁcation tasks. We com-
pare the new method with a few other bench-
mark semi-supervised approaches using the pub-
lic Yahoo! Answers topic classiﬁcation dataset
Zhang et al. (2015); Joulin et al. (2016) and our
e-commerce product categorization dataset.

5.1 Yahoo! Answers Dataset Experiments
The Yahoo! Answers topic classiﬁcation dataset
contains 10 classes. Each class contains 140K
training samples and 6K testing samples. In this
dataset, the total number of training instances is
1.4M and total number of test instances is 60K
Zhang et al. (2015). We shufﬂe and split the orig-
inal 1.4 M labeled training data into two sets. The
ﬁrst set contains 100K instances with labels and
is used as the initial labeled dataset L. The sec-
ond set contains 1.3 M instances and the labels are
deleted to form the unlabeled dataset U. The 60K
test samples are untouched as the blind test set T .

5.1.1 Results
Using the initial 100K labeled dataset L and 1.3
M unlabeled dataset U, we compare our approach
to the three benchmark approaches: supervised
learning, co-training and active-learning. In all ex-
periments, once the labeled training corpus for the
main model is derived, we use the shallow neural
network classiﬁer described in Joulin et al. (2016)
to train M ain.

1. 1.4M Supervised Learning: Use the entire
Yahoo! dataset and build a model similar to
Joulin et al. (2016). The accuracy is reported as
72.3%. This is the theoretical upper bound for
a semi-supervised learning training. Any pro-

posed method with less labeled data tries ap-
proach this accuracy.

2. 100K Supervised Learning: Use L dataset
and build a model similar to Joulin et al. (2016).
The accuracy for this model is 65.9%. This is
the lower bound result. Any proposed method
to leverage unlabeled data should outperform
this number as much as possible.

3. Co-Training: Use L to build two initial models
and use U data in a co-training setup to enhance
the initial models. The system converged to an
accuracy of 69.03% after 40 iterations.

4. Self-Training: Use L to train an initial model
and use this initial model to predict the labels
of U. In the next step, mix L and U with its
predictions to train a new model. Keep iter-
ating the predicting labels for U, mixing cor-
pus and training the classiﬁer until the system
converges. In our experiments, the self-labeled
baseline converged after 30 iterations to an ac-
curacy of 67.5%.

5. Active Learning: Train the initial classiﬁer L,
use it to evaluate and select the most informa-
tive samples from U and reveal their ground
truth labels. Then, update the classiﬁer with the
mixed corpus of L and reveal the label samples.
As more samples get selected, the performance
of the active-learning algorithm improves. Fig
1 shows the improvements in the accuracy with
the increasing amount of manual labeling data.

Figure 1: active learning accuracy w.r.t the number
of samples selected for labeling

73020000040000060000080000010000001200000Number of Sample Data Added656667686970717273Accuracy (%)Active Learning6. EMAEC without Enriched Data: Use L and
U and apply Algorithm 1 and Algorithm 2 to
iteratively train M ain and Eval. After ap-
proximately 20 iterations, M ain can achieve
an accuracy of 70.42%. The Eval model yields
92.8% precision with 83.5% recall. At the
convergence stage, the system generated labels
for 1.12M instances in U with a false positive
rate of <8%. This approach automatically la-
bels the majority (>86%) of the U dataset with
high-quality (error rate <8%).

7. EMAEC with Enriched Data: The Yahoo!
dataset does not contain any additional user
session feedback data. To simulate the sce-
nario where user-provided feedback data is un-
reliable to produce 100% correct automatic la-
bels, we assume that user feedback data can be
simulated by randomly introducing noises to
the original ground truth labels in the dataset.
Thus, we ﬁrst reveal all the ground truth labels
in U, and then randomly select x% of U. Next,
we randomly ﬂip their correct labels into wrong
labels and then mix them with the remaining
instances in U. We call the mixed and blurred
dataset as B which is the U dataset with noisy
labels. We use the B dataset to simulate user
noisy feedback signals. In this experiment, we
use L, B, Algorithm 1 and Algorithm 2 to it-
eratively train M ain and Eval. As expected,
the higher that the level of blurring is, the worse
that the system performs. The theoretical upper
bound for this experiment would be a classiﬁer
trained with 100K + (1 − x%) * 1.3M ground
truth labeled data. Figure 2 demonstrates the
varying system performance varying with dif-
ferent noise level x%. We can see that user
feedback loop data can further improve the sys-
tem’s performance even if we introduce 50%
noise to B.

5.1.2 Discussion
The experimental results on the Yahoo! Answers
topic classiﬁcation dataset are summarized in Ta-
ble 1. The results demonstrate that by starting with
only 100K labeled data and 1.3 M unlabeled data,
and by using an auxiliary evaluation component,
the systems accuracy can be increased from 65.9%
to 70.4%. Active learning can reach the same per-
formance only after adding another 400K manu-
ally labeled data.

Figure 2: EMAEC with enriched data at different
noise levels and the comparison with other base-
lines.

Accuracy Error Reduction

Approach

100K Supervised Learning
Self-Training
Active Learning (extra 100K labeled)
Co-Training
Active Learning (extra 400K labeled)
EMAEC w/o Enriched Data
EMAEC w/ Enriched Data (20% noise)
Supervised Learning with 20% noise
1.4M Supervised Learning

[%]
65.90
67.57
68.23
69.04
70.48
70.49
71.33
57.8
72.40

[%]
0.000
4.888
6.85
9.208
13.436
13.456
15.913
-12.29
19.062

Table 1: Test accuracy and error reduction rate [%]
on the Yahoo! Answers dataset. The method pro-
posed in this study is printed in bold.

Moreover, the proposed approach can automat-
ically generate high-quality labels for over 86% of
the unlabeled data with an error rate less than 8%.
The results also show that by adding simu-
lated user feedback loop signals into the evalu-
ation component, the ﬁnal system accuracy can
be further improved. Even with 50% label noise,
the system achieves 71.33% accuracy. The ac-
tive learning system can achieve the same accu-
racy only after adding extra 800K manually la-
beled data.
It’s also worth noting that with the
same noisy blurred label dataset, the supervised
learning approach has much worse performance.
Its classiﬁer accuracy signiﬁcantly drops to 57.8%
with 100K golden label and 1.3M blurred labels at
20% noise level.

5.2 E-commerce Product Categorization

Dataset Experiments

The proposed method is derived to tackle large
scale text classiﬁcation problems that occur in the
e-Commerce industry, where the challenge is that

74010203040506070Noise Added (%)646566676869707172Accuracy (%)1.4M_SLEMAECwEnrich_UBoundEMAECwEnrichEMAECwoEnrichCoTrainSelfLabel100K_SLwe signiﬁcantly lacked high-quality labeled data
for these problems. For example, the e-commerce
product categorization dataset contains product ti-
tles and 600 different categories for the product
titles. This dataset contains four different parts:
the product category description data for 600 cate-
gories, a 6K observation manually labeled initial
training dataset L, a 28K observation manually
labeled blind test set T , and a 3.5 million obser-
vation unlabeled dataset U that included rich user
feedback session data F . The main task here is to
predict the product category as soon as the online
user enters the product title. For example, the user
might enter a product title, such as green coach
bag to describe a product. The system should clas-
sify this input title into the most relevant product
category, such as ”women’s purse & bag”. The 3.5
million unlabeled user behavior dataset contains a
seller chosen category and a category suggested
by a machine learning model. We consider these
data to be unlabeled since the seller chosen cate-
gory has a greater than 30% error rate according
to our evaluations. The reason for this high error
rate is due to the fact that the users are not famil-
iar with the category tree or they just intentionally
select the wrong category to increase the chance
of selling their product. Note that for the main-
task system that runs online, only the product title
information is available to main-task model.
5.2.1 Results
With the initial 6K labeled dataset L, the 3.5M un-
labeled dataset U and the 3.5M feedback session
dataset F , we compare our proposed EMAEC ap-
proach with the auxiliary evaluation component
to a weak supervised learning baseline and co-
training baseline as described below. Similar to
the previous set of experiments, once the labeled
training corpus for the main model is derived, we
use the shallow neural network classiﬁer described
in Joulin et al. (2016) to train M ain.
1. Supervised Learning with Small Labeled
Data: We train a supervised baseline model
with the 6K labeled dataset L. This is the weak
baseline model. Any proposed method that
leverages unlabeled data U and session data F
should outperform this number.

2. Supervised Learning with Noisy Data: We
treat the 3.5M seller-chosen category as the cor-
rect labeled data from F , and then mix it with
the 6K labeled dataset L to train a supervised

Model
Supervised Learning with 6K Label Data
Supervised Learning with Noisy Data
Co-Training
EMAEC

Error Reduction Rate [%]

0.00
12.30
15.20
19.23

Table 2: EMAEC gain in error reduction rate [%]
compared to the co-training baseline on the e-
commerce dataset

model. Total error reduction rate by adding
seller-chosen labels is 12.3%

3. Co-Training: Use L to build two initial mod-
els and use U data in a co-training setup to en-
hance the initial models. Different feature sets
from F are used to train two different models.
After approximately 30 iterations, the system
will converge to best performance. Total error
reduction rate for co-training is 15.2%.

4. EMAEC with Enriched Data: Build the ini-
tial classiﬁer M ain using L and F . Apply Al-
gorithm 1 and Algorithm 2 to iteratively train
the M ain and Eval models. After approx-
imately 20 iterations, the main task classiﬁer
M ain converges to its best performance. To-
tal error reduction for our approach is 19.23%.

5.2.2 Discussions
The experiment results on the e-commerce prod-
uct categorization dataset are summarized in ta-
ble 2. The results demonstrate that our proposed
approach with the auxiliary evaluation compo-
nent outperforms the co-training approach sub-
stantially. The classiﬁcation error rate is reduced
by 5%. This improvement is well aligned with the
results on the public Yahoo! Answers dataset.

6 Conclusions

In this paper, we proposed a semi-supervised
learning approach to tackle the challenge of lack-
ing high-quality labeled data. The experimental
results in text classiﬁcation tasks with both open
source Yahoo! Answer data and our e-commerce
data show the effectiveness of the proposed ap-
proach. This general dual player machine learning
framework can also be applied to other machine
learning tasks, such as search ranking, speech
recognition, machine translation, etc.

The proposed method comes with advantages
and disadvantages over existing semi-supervised
learning approaches. The advantages have been

75demonstrated in text classiﬁcation tasks in that it
can automatically extract fairly high-quality pre-
dicted labeled data from massive unlabeled data.
Thus, it can further improve prediction accuracy
by adding those automatically enriched labeled
data into the original training corpus.

On the other side, a potential drawback could
be that its effectiveness may be limited by the
prediction performance of the auxiliary evaluation
model.
If the auxiliary evaluation model is not
able to generate many labeled samples with low
false positive rate, the automatically enriched la-
beled data might not be well distributed to reﬂect
the real problems underlying data distribution. To
overcome this, we must rely on vast amounts of
real-world unlabeled data.

At last, we know that the GAN based approach
Goodfellow et al. (2014); Yu et al. (2017); and
Kusner and Hern´andez-Lobato (2016) can auto-
matically generate inﬁnite amounts of fake data.
Combining the advantages of the GAN framework
and the proposed approach is a very interesting re-
search direction for us in the future.

References
Alina Beygelzimer, Sanjoy Dasgupta, and John Lang-
ford. 2009. Importance weighted active learning. In
Proceedings of the 26th annual international confer-
ence on machine learning, pages 49–56. ACM.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92–100. ACM.

Minmin Chen, Kilian Q Weinberger, and John Blitzer.
In Ad-
2011. Co-training for domain adaptation.
vances in neural information processing systems,
pages 2456–2464.

Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
In Proceedings of
scalable tree boosting system.
the 22Nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
785–794. ACM.

David A Cohn, Zoubin Ghahramani, and Michael I Jor-
dan. 1996. Active learning with statistical models.
NIPS.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1–38.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. 2014. Generative adversarial nets. Advances in

neural information processing systems, pages 2672–
2680.

Kenneth Heaﬁeld. 2011. KenLM: faster and smaller
In Proceedings of the
language model queries.
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom.

A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov.
2016. Bag of tricks for efﬁcient text classiﬁcation.
arXiv:1607.01759.

Matt J Kusner and Jos´e Miguel Hern´andez-Lobato.
2016. Gans for sequences of discrete elements with
arXiv preprint
the gumbel-softmax distribution.
arXiv:1611.04051.

Kamal Nigam, Andrew McCallum, Sebastian Thrun,
Tom Mitchell, et al. 1998. Learning to classify text
from labeled and unlabeled documents. AAAI/IAAI,
792.

Isaac Triguero, Salvador Garca, and Francisco Herrera.
2015. Self-labeled techniques for semi-supervised
learning: Taxonomy, software and empirical study.
Knowledge and Information Systems.

Isaac Triguero, Jos´e A S´aez, Juli´an Luengo, Salvador
Garc´ıa, and Francisco Herrera. 2014. On the char-
acterization of noise ﬁlters for self-training semi-
supervised in nearest neighbor classiﬁcation. Neu-
rocomputing, 132:30–41.

Karl Weiss, Taghi M Khoshgoftaar, and DingDing
Wang. 2016. A survey of transfer learning. Jour-
nal of Big Data, 3(1):1–40.

Songbai Yan, Kamalika Chaudhuri, and Tara Javidi.
2016. Active learning from imperfect labelers. In
Advances in Neural Information Processing Sys-
tems, pages 2128–2136.

David Yarowsky. 1995. Unsupervised word sense dis-
In Pro-
ambiguation rivaling supervised methods.
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, pages 189–196. As-
sociation for Computational Linguistics.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI, pages 2852–2858.

Chicheng Zhang and Kamalika Chaudhuri. 2015. Ac-
tive learning from weak and strong labelers. In Ad-
vances in Neural Information Processing Systems,
pages 703–711.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. NIPS.

Xiaojin Zhu and Andrew B Goldberg. 2009.

Intro-
duction to semi-supervised learning. Synthesis lec-
tures on artiﬁcial intelligence and machine learning,
3(1):1–130.

76
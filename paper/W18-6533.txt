Automatically Generating Questions about Novel Metaphors in Literature

Natalie Parde

Computer Science

University of Illinois at Chicago

Rodney D. Nielsen

Computer Science and Engineering

University of North Texas

parde@uic.edu

rodney.nielsen@unt.edu

Abstract

The automatic generation of stimulating
questions is crucial to the development
of intelligent cognitive exercise applica-
tions. We developed an approach that
generates appropriate Questioning the Au-
thor queries based on novel metaphors
in diverse syntactic relations in litera-
ture. We show that the generated ques-
tions are comparable to human-generated
questions in terms of naturalness, sensibil-
ity, and depth, and score slightly higher
than human-generated questions in terms
of clarity. We also show that ques-
tions generated about novel metaphors are
rated as cognitively deeper than ques-
tions generated about non- or conven-
tional metaphors, providing evidence that
metaphor novelty can be leveraged to pro-
mote cognitive exercise.

Introduction

1
Automatic question generation is useful for a wide
range of applications, including those providing
educational and cognitive exercise support. Most
question generation work to date has focused on
generating factoid questions—that is, questions
regarding factual content that is readily available
in the source text. Factoid questions are well-
suited to some contexts, such as quizzing for sim-
ple comprehension. However, answering them
typically requires only shallow reasoning skills,
rendering them unsuitable for situations in which
deeper cognitive engagement is desired.

Less work has been conducted with the goal
of generating deeper questions. We work to-
ward ﬁlling that void by presenting an approach
for automatically generating questions about novel
metaphors from popular classic ﬁction using the

Questioning the Author strategy. Metaphor nov-
elty is deﬁned here as the degree of likelihood
with which one can expect to encounter a given
metaphor on a regular basis. Consider the sen-
tence:

I spent an hour on my homework.

The word pair, {spent, hour}, is a highly con-
ventional metaphor—although one cannot literally
spend time, phrases such as this are highly com-
mon in the English language. Alternately, con-
sider:

The Queen was frowning like a thunder-
storm.

The word pair, {frowning, thunderstorm}, is a
highly novel metaphor. Novel metaphors reside
at the opposite end of the continuum from conven-
tional metaphors, and should strike one as being
particularly interesting or creative. These are the
metaphors of particular interest to us in this work.
The targeted focus on novel metaphors stems
from prior work showing that novel metaphors are
more difﬁcult to process, both in young adults
(Lai et al., 2009) and in older adults with and
without dementia (Amanzio et al., 2008; Mashal
et al., 2011). The latter are a key demographic
for our use case, an elder-focused human-robot
book discussion system (Parde, 2018). Here, we
(1) introduce a method for generating deep ques-
tions about diverse novel metaphors following the
Questioning the Author strategy. We (2) show that
the resulting questions are comparable to or score
slightly higher than questions generated by every-
day users about the same topics in terms of natu-
ralness, clarity, sensibility, and depth. Moreover,
we (3) provide empirical evidence that questions
automatically generated about novel metaphors
are rated as having greater depth than questions

ProceedingsofThe11thInternationalNaturalLanguageGenerationConference,pages264–273,Tilburg,TheNetherlands,November5-8,2018.c(cid:13)2018AssociationforComputationalLinguistics264automatically generated about non- or conven-
tional metaphors. Finally, we (4) publicly release
our source code and a corpus of question ratings
and responses for both human- and automatically-
generated questions to the research community to
foster additional work in this area.

2 Related Work

Automatic question generation and its potential
to facilitate learning has been of interest to re-
searchers since at least the 1970s, when John
Wolfe introduced the pattern-matching AUTO-
QUEST question generation algorithm (Wolfe,
1977). Today, many question generation systems
exist for educational applications, with most gen-
erating factoid questions about content found in
expository text (Araki et al., 2016; Du et al., 2017;
Gates, 2008; Heilman and Smith, 2010; Mazidi
and Nielsen, 2014, 2015; Rus et al., 2007; Serban
et al., 2016; Wyse and Piwek, 2009).

These systems achieve their goals in a number
of ways, including templates (Araki et al., 2016;
Mazidi and Nielsen, 2014, 2015; Rus et al., 2007;
Wyse and Piwek, 2009), sentence transformations
(Gates, 2008; Heilman and Smith, 2010), and re-
cently, neural networks (Du et al., 2017; Serban
et al., 2016). Template-based systems select tem-
plates based on syntactic structure, semantic role
labels, dependency parses, and/or discourse cues
to produce generally shallower questions (e.g.,
“Inﬂation is deﬁned as an increase in the price
level.” → “How is inﬂation deﬁned?” (Mazidi and
Nielsen, 2015)). The template-based system de-
veloped by Araki et al. (2016) generated questions
over multiple sentences to produce questions that
required more inference steps than those generated
from a single sentence, using event coreference,
entity coreference, and paraphrases. However, the
answers to these questions were still readily avail-
able in the original text passage (in fact, ensur-
ing that this was the case was a goal of the sys-
tem). Although shallower questions are suitable
for quizzing comprehension of expository text (the
most common scenario to which they are applied),
they are inadequate for more involved discussions,
such as those analyzing ﬁction narrative.

Deeper questions

(or more aptly, writing
prompts) were generated by Liu et al.’s (2012) sys-
tem, designed to help students write better litera-
ture reviews. Sentences containing citations were
classiﬁed as describing opinions, methods, results,

or one of several other categories, and templates
were selected based on those classiﬁcations to
construct questions using content from the origi-
nal sentence (e.g., “Cannon (1927) challenged this
view mentioning that physiological changes were
not sufﬁcient to discriminate emotions.” → “Why
did Cannon challenge this view mentioning that
physiological changes were not sufﬁcient to dis-
criminate emotions? (What evidence is provided
by Cannon to prove the opinion?) Does any other
scholar agree or disagree with Cannon?”). Lind-
berg et al.’s (2013) system prompted students for
summaries, causal effects, and descriptions not ex-
pected to be answerable from the immediate sen-
tence from which they were generated, using ques-
tion templates selected based on semantic role pat-
terns. The questions were then classiﬁed as hav-
ing/not having learning value, allowing the sys-
tem to automatically discard poor-quality ques-
tions. The learning value classiﬁer was trained us-
ing length, language model, semantic role label,
named entity, glossary, and syntax features.

The system developed by Becker et al. (2012)
automatically identiﬁed question topics (i.e., the
part of a sentence about which a question should
be asked), and accordingly generated cloze (ﬁll-
in-the-blank) questions. However, cloze ques-
tions are shallow and have limited potential to
stimulate deep reasoning. Olney et al. (2012)
developed a system that automatically extracted
concept maps from expository text, and gener-
ated questions based on those concept maps us-
ing templates associated with different types of
start nodes, end nodes, and edge relations from the
maps. Finally, Mostow and Chen (2009) devel-
oped a method for automatically generating self-
questioning instruction for students reading chil-
dren’s stories. Although some stages of this in-
struction were scripted, others involved automat-
ically generating example questions for students.
The template-based generation approach resulted
in “why” questions about mental states expressed
in the stories (e.g., “And when the country mouse
saw the cheese, cake, honey, jam and other good-
ies at the house, he was pleasantly surprised.” →
“Why was the country mouse surprised?”). Al-
though these approaches for generating deeper
questions are promising, none have speciﬁcally
sought to implement the Questioning the Author
paradigm, which revolves around building mean-
ing from text rather than quizzing a reader’s com-

265prehension. Moreover, none focus on generat-
ing questions based on identiﬁed occurrences of
highly novel metaphor in the source text.

2.1 Questioning the Author
The questions generated for the work described
herein employ a questioning strategy commonly
used in K-12 education known as Questioning
the Author (QtA). QtA seeks to encourage read-
ers to consider the author’s underlying intentions
when crafting literary prose (Beck and McKeown,
2006). The strategy can be implemented with ei-
ther expository text or ﬁction narrative. One of
the key goals of QtA is to coax readers toward
building meaning from and understanding the re-
lationships between different elements or events
present in the text, as opposed to focusing on iso-
lated components as factoid questions are likely to
do. Example QtA queries (Beck and McKeown,
2006) may include prompts such as:

• What is the author trying to say here?
• What do you think the author wants us to

know?

• What is the author talking about?
• So what does the author mean right here?
• That’s what the author said, but what did the

author mean?

Such questions are open-ended, and typically
elicit more detailed, free-form responses than fac-
toid questions. They also typically encourage
deeper analysis of the source text.

3 Template Development and Selection

We built templates based on sample questions
from the book on QtA (Beck and McKeown,
2006), with slots to be ﬁlled using predicted novel
metaphors that were automatically identiﬁed in lit-
erature. We describe the metaphor novelty scor-
ing methodology in greater detail in Section 4.1.
The identiﬁed metaphors were all syntactically-
related pairs of words; we constructed different
sets of templates for different syntactic relation
types. The syntactic relation types for which ques-
tions could be generated included the universal de-
pendency relations (McDonald et al., 2013) in Ta-
ble 1. Some of the resulting templates are shown

Relation
Type
nsubj

nsubjpass

dobj
iobj

csubj

csubjpass

xcomp

nmod

acl

appos

amod

advcl

dep

advmod

compound

Description

Example

Nominal subject.
Nominal subject
of a passive verb.
Direct object.
Indirect object.

Clausal subject.

Clausal subject
of a passive verb.
Open clausal
component.
Nominal
modiﬁer.
Clause that
modiﬁes a noun.
Appositional
modiﬁer.
Adjectival
modiﬁer.
Adverbial clause
modiﬁer.
Dependency for
which the parser
cannot determine
a ﬁner-grained
relation.
Adverbial
modiﬁer.
Multiword
expression.

The apple is red.
Newton was hit by
an apple.
I gave him an apple.
I gave him an apple.
What he wanted
was an apple.
What he wanted as
taken to be an apple.
He asked to eat an
apple.
The stem of the
apple.
I need a way to get
an apple.
The fruit, an apple,
was red.

It was a red apple.

He got is idea as the
apple was falling.

N/A

He ate the apple
quickly.
The apple had polka
dots.

Table 1: Dependency relation types for which
questions were generated.

in Table 2; the full list of 130+ templates can be
found online.1

Templates were chosen randomly from among
the pool of all relevant templates for a given de-
pendency type. For example, if a question was
to be generated about a metaphor formed by two
words syntactically related to one another using
an nsubj dependency, a random selection from all
possible templates corresponding to the nsubj type
would be made. Surface realizations were then
constructed by ﬁtting predicted novel metaphors
into the selected templates. That process is de-
scribed in the following section.

4 Surface Realization
Realization was performed based on the linguis-
tic characteristics and syntactic parse details cor-
responding to the novel metaphors about which
questions were generated. Our procedure for iden-

1http://natalieparde.com/papers/inlg_

question_templates.pdf

266Dependency

nsubj

nsubjpass

dobj/iobj

csubj

csubjpass

xcomp/nmod/acl

appos

amod

advcl/dep

advmod

compound

Template
What is the author trying to say
with the expression ‘<DEP
(N/J/P)> <GOV (V)>’?
What do you think the author
wants us to know by ﬁguratively
saying ‘<DEP (N/J/P)>’
<WAS/WERE> ‘<GOV (V)>’?
What is the author talking about
when <HE/SHE> writes ‘<GOV
(V)>’ <? THE> ‘<DEP
(N/J/P)>’?
What’s the important message in
the expression ‘<CLAUSE>’?
So what does the author mean
when <HE/SHE> writes
‘<CLAUSE PASS>’?
The author said ‘<STRING>,’ but
what did <HE/SHE> mean?
Does the expression ‘<DEP>’
with ‘<GOV>’ make sense?
How does the expression ‘<DEP>
<GOV>’ ﬁt in with what the
author told us?
Does the author tell us why
<HE/SHE> wrote
‘<PHRASE>’?
Why do you think the author tells
us ‘<DEP (V/J/R)> <GOV
(V/J/R)>’?
What is the author telling us with
the expression ‘<W1> <W2>’?

Table 2: Example question templates.

tifying particularly novel metaphors (Parde and
Nielsen, 2018b) and our methods for incorporating
the approach in this work are described in Subsec-
tion 4.1. Subsection 4.2 describes how template
slots were subsequently ﬁlled using the identiﬁed
novel metaphors.

4.1 Metaphor Novelty Scoring
Our metaphor novelty scoring approach predicts
continuous scores for syntactically-related pairs of
content words (nouns, verbs, adjectives, and ad-
verbs), with higher scores reﬂecting greater nov-
elty than lower scores (Parde and Nielsen, 2018b).
It consists of a four-layer feedforward neural net-
work trained using features based on psycholin-
guistic characteristics (concreteness, imageability,
sentiment, and ambiguity), word co-occurrence,
syntactic structure, semantic characteristics, and
information from WordNet (Miller, 1995) regard-
ing the words in the pair. For the work here,
we trained our neural network model on a cor-
pus of word pairs originally extracted from the VU
Amsterdam Metaphor Corpus (Steen et al., 2010)

and labeled along a continuous scale for metaphor
novelty (Parde and Nielsen, 2018a); the VU Am-
seterdam Metaphor Corpus is comprised of ﬁction,
news articles, academic articles, and transcribed
conversations. We then applied the learned model
to all word pairs (52,279 total) extracted from a
subset of sentences from 58 books that are pub-
licly available on Project Gutenberg.2 Finally, we
randomly selected a small subset (457) of the word
pairs having predicted scores greater than 1.03 as
the identiﬁed novel metaphors about which to gen-
erate questions.

4.2 Slot Filling
As shown in Table 2,
each template con-
tains one or more slots: <GOV>, <DEP>,
<WAS/WERE>, <HE/SHE>, <?
THE>,
<CLAUSE>, <CLAUSE PASS>, <STRING>,
<W1>, and <W2>. Filling the <GOV> and
<DEP> slots is straightforward; the governor and
modiﬁer of the syntactic relation forming the pre-
dicted metaphor are merely substituted into the
appropriate slots in the question template. The
<HE/SHE> slot is the only slot requiring meta-
data about the source text being discussed (a gen-
der was manually assigned to each book).

The token “was” or “were” is selected to ﬁll the
<WAS/WERE> slot based on the part-of-speech
tags associated with the two words forming the
predicted metaphor. Metaphors including plural
nouns are given the verb “were,” and all other
metaphors are given the verb “was.” The word
“the” is optionally included in realizations of tem-
plates including the <? THE> slot based on the
distance between the two words forming the pre-
dicted metaphor; if they are immediately next to
one another, it is omitted, and otherwise it is in-
cluded.

Filling the <CLAUSE> slot

is more com-
plex. A full dependency parse of the predicted
metaphor’s source sentence is ﬁrst acquired us-
ing Stanford CoreNLP (Manning et al., 2014).
A clause is then constructed using only tokens
that are syntactically related to words forming the
metaphor, in the order in which they occur in the
source sentence. Consider the example sentence:

2https://www.gutenberg.org;

the books se-
lected included all books written in or translated to English
and classiﬁed as ﬁction in the “Top 100 Books Over The Last
30 Days” list as of May 18, 2017.

3Across all word pairs, novelty predictions ranged from

0.24-1.41.

267What he tasted on this dark and stormy
night was a dream.

dream} into the

To ﬁt {tasted,
template,
“What’s the important message in the expression
‘<CLAUSE>’?”, the following words would be
identiﬁed as syntactically related to tasted and
dream: {What, he, tasted, was, a, dream}. The
realized question, retaining the words in their orig-
inal order, would thus be: “What’s the important
message in the expression ‘What he tasted was a
dream’?” <CLAUSE PASS> is constructed sim-
ilarly, but requires only words syntactically related
to the modiﬁer.

The <STRING> slot was ﬁlled by tokenizing
the source sentence, and extracting the full span of
text from one of the words in the metaphor up to
and including the other. Consider the sentence:

She smelled a melody of appetizers and
knew she had reached the networking
event.

The word pair {melody, appetizers} would
into the nmod template “The author said
ﬁt
‘<STRING>,’ but what did <HE/SHE> mean?”
as “The author said ‘melody of appetizers,’ but
what did she mean?” The <PHRASE> slot was
ﬁlled using a slightly broader window of text: the
span reaching from the ﬁrst word syntactically
related to either of the words in the metaphor,
to the last word syntactically related to either
of those words, inclusive of the words forming
the metaphor. Finally, <W1> and <W2> were
ﬁlled simply by substituting the word from the
metaphor that occurred ﬁrst in the source sentence
for <W1>, and the word that occurred second in
the source sentence for <W2>.

5 Evaluation

The quality of the automatically-generated ques-
tions was evaluated relative to that of questions
written by humans. The human-generated ques-
tions were comprised of two subsets: (1) those
generated based on sentences containing predicted
novel metaphors, and (2) those generated based on
pairs of words predicted to be novel metaphors.

Source Sentence
An icy horror of loneliness
seized him; he saw himself
standing apart and watching all
the world fade away from him –
a world of shadows, of ﬁckle
dreams.
Perhaps, from the casement,
standing hand-in-hand, they
were watching the calm
moonlight on the river, while
from the distant halls the
boisterous revelry ﬂoated in
broken bursts of faint-heard din
and tumult.
I had crossed a marshy tract full
of willows, bulrushes, and odd,
outlandish, swampy trees; and I
had now come out upon the
skirts of an open piece of
undulating, sandy country, about
a mile long, dotted with a few
pines and a great number of
contorted trees, not unlike the
oak in growth, but pale in the
foliage, like willows.
I quickly destroyed part of my
sledge to construct oars, and by
these means was enabled, with
inﬁnite fatigue, to move my ice
raft in the direction of your ship.

Question Received

What did he feel as
he stood alone?

Do you think the
people holding
hands are supposed
to be happy or sad?

Do you think the
landscape reﬂects
his inner feelings?

If you were to make
oars that way, how
long do you think it
would take?

Table 3: Sample questions generated by humans
based on sentences.

457 predicted novel metaphors about which ques-
tions were automatically generated, using Ama-
zon Mechanical Turk (AMT).4 Workers were sim-
ply instructed to create “good” questions, such as
what they might ask in a book discussion group
if they came across the sentence (or the bolded
word pair within that sentence) when reading.
These instructions were purposely open-ended to
foster diversity in the collected data. To that
end, we also continued to collect questions until
the human-generated question dataset included 35
unique question authors (180 questions; 90 of each
type). Sample questions collected based on sen-
tences and word pairs are shown in Tables 3 and
4.

180

The

human-generated

457
automatically-generated
in-
termixed, and responses to the questions and
ratings for four criteria for each question were
also solicited using a separate pool of workers

questions were

and

5.1 Data Collection
We crowdsourced human-generated questions
based on a randomly-selected subset of the same

4https://www.mturk.com; we crowdsourced ques-
tions from everyday users to facilitate comparison with the
most likely alternative to a human-robot book discussion—a
typical human book club.

268Source Sentence
Wavewhite wedded words
shimmering on the dim tide.
All about me gathered the
invisible terrors of the Martians;
that pitiless sword of heat
seemed whirling to and fro,
ﬂourishing overhead before it
descended and smote me out of
life.
My father saw this change with
pleasure, and he turned his
thoughts towards the best
method of eradicating the
remains of my melancholy,
which every now and then would
return by ﬁts, and with a
devouring blackness overcast
the approaching sunshine.
But the overﬂowing misery I
now felt, and the excess of
agitation that I endured rendered
me incapable of any exertion.

Question Received
what does dim tide
mean?

Why would the
martians kill him?

How is the image of
mortality described
with the weather?

How did things get
so bad that they
essentially felt
overﬂowing misery?

Table 4: Sample questions generated generated by
humans based on word pairs.

from AMT. The criteria considered were as
follows:

• Naturalness: The degree to which the ques-
tion seems natural, or sounds “normal” to the
reader.

• Clarity: The degree to which it is clear to the
reader how he or she is supposed to respond
to the question, regardless of whether he or
she is sure of the answer.

• Sensibility: The degree to which the reader
feels it makes sense to ask the question, given
the source sentence upon which it is based.
• Depth: The degree to which the reader feels
challenged in coming up with an answer to
the question.

Two workers rated each criterion using a ﬁve-
point scale. Small disagreements were adjudicated
by averaging, and disagreements greater than a
difference of 2.0 (e.g., a 1 and a 4) were for-
warded to a third-party, native English speaking
adjudicator (211 questions required adjudication
for at least one of the four criteria). Overall, the
crowd workers exhibited moderate agreement with
one another, with Krippendorff’s α=0.50, α=0.52,
α=0.51, and α=0.52 for ratings of naturalness,
clarity, sensibility, and depth, respectively. The
collected question answers are not used in this

work, but they serve the plural purpose of making
the dataset more broadly useful, lending insight re-
garding the types of answers expected to inform
future work on question generation and response
scaffolding, and providing a coarse-grained quan-
titative (time-based) measure of question depth.

5.2 Average Question Ratings
Average ratings for the question criteria, both
overall and when only considering questions for
a given criterion that had received above-midpoint
(> 3.0) ratings for the previous criteria, are pre-
sented in Tables 5 and 6. The latter scenario was
included to reduce the potential for confusion in
interpreting the results (for instance, unclear ques-
tions that were also unnatural may have only been
rated as such because they were unnatural; these
questions are included in the average score re-
ported in Table 5 but not in the average score re-
ported in Table 6). To elaborate further, the con-
straints considered in the latter scenario (as well as
for the results reported in Tables 7 and 8) were:
• Naturalness: All ratings were considered.
• Clarity: Only questions having a Naturalness

score > 3.0 were considered.

• Sensibility: Only questions having Natural-
ness and Clarity scores > 3.0 were consid-
ered.

• Depth: Only questions having Naturalness,
Clarity, and Sensibility scores > 3.0 were
considered.

Signiﬁcance values for both scenarios were de-
termined via one-way ANOVA between the two
groups. Not surprisingly, given the instructions to
ask good questions, automatically-generated ques-
tions did not quite match the high bar set for
depth by humans’ questions, but this difference
was not statistically signiﬁcant. The only signif-
icant (p < 0.05) difference reported between the
two groups in Table 6 was for ratings of Clarity
(automatically-generated questions scored slightly
higher). This ﬁnding was echoed when consider-
ing the overall averages (Table 5); again, the only
statistically signiﬁcant difference between groups
was that ratings of Clarity were slightly higher
for the automatically-generated questions than the
human-generated questions.

269Human-
Generated
3.89
3.78
3.83
3.78

Automatically-
Generated
4.02
4.04
4.00
3.67

p

0.13
0.00
0.06
0.24

Naturalness
Clarity
Sensibility
Depth

Table 5: Average ratings across all question crite-
ria, with signiﬁcance values.

Human-
Generated
3.89
4.18
4.45
3.92

Automatically-
Generated
4.02
4.34
4.48
3.76

p

0.13
0.03
0.61
0.17

Naturalness
Clarity
Sensibility
Depth

Table 6: Average ratings, considering only ques-
tions with above-average ratings for the preceding
criteria.

5.3 Average Ratings for Question Subgroups
In addition to these broad comparisons of human-
and automatically-generated questions, we ex-
amined the differences between different sub-
groups. Table 7 presents the average ratings for (1)
human-generated questions based on sentences,
(2) human-generated questions based on speciﬁed
word pairs, and (3) automatically-generated ques-
tions. Statistical signiﬁcance was computed using
one-way ANOVAs beween each pair of groups:
human-generated (sentence) and human-generated
(word pair); human-generated (sentence) and
automatically-generated;
and human-generated
(word pair) and automatically-generated. Only
two statistically signiﬁcant differences existed be-
tween the subgroups: the average ratings for Clar-
ity and Sensibility were higher for automatically-
generated questions than for human-generated
questions based on sentences. These differences
were not statistically signiﬁcant when comparing
human-generated questions based on word pairs
and automatically-generated questions.

Table 8 presents average ratings for two subsets
of automatically-generated questions:
true posi-
tives (TP) for which the word pair about which
the question was generated was both predicted
to be a novel metaphor and actually was a novel
metaphor, and false positives (FP) for which the
word pair about which the question was gener-
ated was predicted to be a novel metaphor but
was not actually a novel metaphor. We collected
gold standard metaphor novelty scores for these

Human-
Generated
(Sentence)
3.92
4.08
4.30
3.91

Human-
Generated
(Word Pair)
3.85
4.29
4.42
4.03

Auto.-
Generated

4.02
4.34
4.48
3.76

Nat.
Clar.
Sens.
Depth

Table 7: Average ratings for human-generated
question subgroups and automatically-generated
questions.

Naturalness
Clarity
Sensibility
Depth

TP
3.99
4.23
4.31
3.92

FP
4.06
4.46
4.44
3.64

p
0.42
0.00
0.09
0.02

Table 8: Average ratings for true and false posi-
tives among automatically-generated questions.

word pairs in the same manner by which we
built our previous VUAMC-based metaphor nov-
elty dataset (Parde and Nielsen, 2018a), used to
train the metaphor novelty prediction model in this
work. Speciﬁcally, we crowdsourced ﬁve anno-
tations for each word pair, and automatically ag-
gregated them to continuous scores using a la-
bel aggregation model learned from features based
on annotation distribution and presumed worker
trustworthiness (Parde and Nielsen, 2017).

There were two statistically signiﬁcant differ-
ences between the two groups: questions about
false positives were rated as clearer than ques-
tions about true positives, and questions about
true positives were rated as having more depth
than questions about false positives. One hy-
pothesis regarding the former ﬁnding is simply
that non-metaphoric language is more clearly in-
terpretable than metaphoric language. The ﬁnd-
ing that question depth is higher for questions
about true positives (novel metaphors) than ques-
tions about other instances provides empirical
support for the underlying motivations guiding
this work—namely, that questions regarding novel
metaphors are more cognitively challenging than
similar questions about non-metaphors or conven-
tional metaphors.

5.4 Correlations between Question Criteria
In addition to evaluating question quality on the
basis of average ratings for each question crite-
rion, we computed Pearson’s correlation scores

270Nat. Clar.

-

0.55

-

Nat.
Clar.
Sens.
Depth
Compl.
Time

Sens. Depth Compl.
Time
0.48
-0.04
0.65
-0.01
-0.04
0.07

0.03
0.05
0.05

-

-

-

Table 9: Correlations between categories of rat-
ings for both automatically- and human-generated
questions.

between the four criteria, as well as between those
criteria and completion time (the amount of time
workers took to complete each HIT, including rat-
ing the four criteria and writing a response to the
question, on AMT) to examine which of these fac-
tors were correlated with one another. Questions
rated as being natural, clear, and sensible (scores
> 3.0) were included in this evaluation. Since
a small number of HITs had outlier completion
times far exceeding the average (indicating that
the rater most likely left their browser open while
taking a break, rather than actually spending that
much time completing the HIT itself), we removed
HITs with completion times +/- two standard de-
viations from the mean completion time from con-
sideration.

Table 9 presents a matrix of overall correla-
tion scores when considering ratings for both
automatically-generated and human-generated
questions. Overall, moderately strong positive
correlations were found between naturalness
and clarity (r=0.55), naturalness and sensibility
(r=0.48), and clarity and sensibility (r=0.65).
No strong correlations were found between these
criteria and depth or completion time. When
computing correlations only between ratings col-
lected for human-generated questions or for only
automatically-generated questions,
these trends
persisted. In addition to the correlations reported
in Table 9, we computed the correlation between
completion time and metaphor novelty for each
set, ﬁnding correlations of r=0.09 across all
questions, r=0.19 for human-generated questions,
and r=0.03 for automatically-generated questions.

5.5 Discussion and Future Recommendations
The ﬁndings regarding the average ratings overall
and for different subgroups, as well as regarding
the correlations between types of ratings, provide
interesting and in a few cases (such as the higher

average Clarity score for automatically-generated
questions rather than human-generated questions)
slightly surprising observations. It is clear that the
automatically-generated questions are very com-
parable with human-generated questions in terms
of all criteria considered. Across all compar-
isons, there were no cases in which an average
rating associated with human-generated ques-
tions or a subset thereof statistically signiﬁ-
cantly outperformed an average rating for the
same criterion with automatically-generated
questions. As such, it is reasonable to assume that
the approach is capable of generating sufﬁciently
natural, clear, sensible, and challenging questions
relative to what the average person might generate.
That said, there are still some areas that could be
improved upon. For example, the average depth
rating for automatically-generated questions that
were also rated as natural, clear, and sensible (as
measured by having ratings greater than 3.0) was
3.76. Although this is above mid-range, it could
certainly be higher (the maximum score allowed
was a 5.0). Thus, additional work could be done to
improve upon question depth in future work. This
could perhaps be accomplished by introducing
complementary strategies to QtA. Considerations
could also be taken to identify optimal questioning
sequences—that is, algorithmically deciding upon
groups of questions most likely to challenge read-
ers when asked in sequence, as opposed to simply
selecting questions at the individual level.

Future work toward improved metaphor nov-
elty scoring algorithms will result in a higher
likelihood that the subjects of the automatically-
generated questions are indeed novel metaphors.
The evaluation indicates that improved identiﬁca-
tion of novel metaphors should lead to higher av-
erage ratings of question depth. Speciﬁcally, Ta-
ble 8 shows that in a comparison of automatically-
generated questions for true positives (instances
predicted to be novel metaphors that were actually
novel metaphors) versus false positives (instances
predicted to be novel metaphors that were actually
not), questions generated for novel metaphors
were rated as having more depth than similar
questions generated for conventional or non-
metaphors, and this difference was found to be
statistically signiﬁcant.

Finally, many of the correlation scores observed
between rating criteria were expected (it is difﬁcult
to think of questions that are, for instance, highly

271natural-sounding while also unclear). We had an-
ticipated a slightly higher positive correlation than
was observed between question depth and com-
pletion time, as a natural assumption is that if a
question makes a reader think quite a bit before
answering it, it will take longer to formulate an
answer than if the question doesn’t make a reader
think at all. However, the measurement of com-
pletion time was coarse-grained; it only consid-
ered the overall amount of time that it took the
worker to complete the full HIT including read-
ing the instructions and a source sentence, rating
four criteria, and ﬁnally constructing a written re-
sponse to the question. Many variables outside of
the question depth itself could therefore impact the
overall completion time. In the future, work can
be conducted to examine the correlation between
completion time and question depth in a more con-
trolled environment.

6 Conclusion

In this work, we introduced and evaluated a
question generation approach to automatically
construct QtA queries about predicted novel
metaphors. We designed and validated question
templates based on sample questions drawn di-
rectly from the book on QtA (Beck and McK-
eown, 2006), and demonstrated methods capa-
ble of producing high-quality question realiza-
tions. We evaluated the automatically-generated
questions relative to human-generated questions
based on the same source material, and discovered
that the only statistically signiﬁcant difference be-
tween the two groups with respect to four distinct
criteria (naturalness, clarity, sensibility, and depth)
was that the automatically-generated questions re-
ceived slightly higher clarity scores. We analyzed
the correlations among the four question criteria as
well as between the question criteria and comple-
tion time, and found strong positive correlations
between naturalness, clarity, and sensibility, but
only weak correlations between each of those cri-
teria and question depth.

All data and source code are publicly available.
Ultimately, our evaluation proved that questions
about novel metaphors in literature can be auto-
matically generated at a quality level comparable
to what the average human might generate. It also
provided empirical support for an underlying mo-
tivation guiding this work:
that questions about
novel metaphors can be leveraged as a means for

motivating cognitive exercise.

Acknowledgements
We thank the anonymous reviewers for their com-
ments and suggestions. This material was based
upon work supported by the National Science
Foundation Graduate Research Fellowship Pro-
gram under Grant 1144248, and the National Sci-
ence Foundation under Grant 1262860. Any opin-
ions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thor(s) and do not necessarily reﬂect the views of
the National Science Foundation.

References
Martina Amanzio, Giuliano Geminiani, Daniela
Leotta, and Stefano Cappa. 2008. Metaphor com-
prehension in alzheimers disease: Novelty matters.
Brain and Language, 107(1):1 – 10.

Jun Araki, Dheeraj Rajagopal, Sreecharan Sankara-
narayanan, Susan Holm, Yukari Yamakawa, and
Teruko Mitamura. 2016. Generating questions and
multiple-choice answers using semantic analysis of
texts. In Proceedings of COLING 2016, the 26th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1125–1136, Osaka,
Japan. The COLING 2016 Organizing Committee.

Isabel L. Beck and Margaret G. McKeown. 2006. Im-
proving Comprehension with Questioning the Au-
thor: A Fresh and Expanded View of a Powerful Ap-
proach. Theory and Practice. Scholastic.

Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the gap: Learning to choose gaps for
In Proceedings of the 2012
question generation.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 742–751, Montr´eal,
Canada. Association for Computational Linguistics.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics.

Donna M. Gates. 2008. Generating look-back strat-
egy questions from expository texts. In Proceedings
of the Workshop on the Question Generation Shared
Task and Evaluation Challenge.

Michael Heilman and Noah A. Smith. 2010. Good
question!
statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617, Los Angeles, California. Associa-
tion for Computational Linguistics.

272Association for the Advancement of Artiﬁcial Intel-
ligence.

Natalie Parde and Rodney D. Nielsen. 2017. Finding
patterns in noisy crowds: Regression-based annota-
tion aggregation for crowdsourced data. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 1908–1913,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Natalie Parde and Rodney D. Nielsen. 2018a. A corpus
of metaphor novelty scores for syntactically-related
In Proceedings of the 11th Interna-
word pairs.
tional Conference on Language Resources and Eval-
uation, Miyazaki, Japan. European Language Re-
sources Association.

Natalie Parde and Rodney D. Nielsen. 2018b. Ex-
ploring the terrain of metaphor novelty: A
regression-based approach for automatically scor-
ing metaphors. In Proceedings of the Thirty-Second
AAAI Conference on Artiﬁcial Intelligence (AAAI-
18), New Orleans, Louisiana. Association for the
Advancement of Artiﬁcial Intelligence.

Vasile Rus, Zhiqiang Cai, and Arthur C. Graesser.
2007. Experiments on generating questions about
facts. In Proceedings of the 8th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing, CICLing ’07, pages 444–455,
Berlin, Heidelberg. Springer-Verlag.

Iulian Vlad Serban, Alberto Garc´ıa-Dur´an, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
In Pro-
The 30m factoid question-answer corpus.
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 588–598, Berlin, Germany. Associa-
tion for Computational Linguistics.

Gerard J. Steen, Aletta G. Dorst, J. Berenike Herrmann,
Anna Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identiﬁca-
tion: From MIP to MIPVU, volume 14. John Ben-
jamins Publishing.

John H. Wolfe. 1977. Reading retention as a func-
tion of method for generating interspersed questions.
Technical report, San Diego: Navy Personnel Re-
search and Development Center.

Brendan Wyse and Paul Piwek. 2009. Generating
questions from openlearn study units. In Proceed-
ings of the AIED 2nd Workshop on Question Gener-
ation, pages 66–73.

Vicky Tzuyin Lai, Tim Curran, and Lise Menn. 2009.
Comprehending conventional and novel metaphors:
An ERP study. Brain Research, 1284:145 – 155.

David Lindberg, Fred Popowich, John Nesbit, and Phil
Winne. 2013. Generating natural language ques-
tions to support learning on-line. In Proceedings of
the 14th European Workshop on Natural Language
Generation, pages 105–114, Soﬁa, Bulgaria. Asso-
ciation for Computational Linguistics.

Ming Liu, Rafael A Calvo, and Vasile Rus. 2012. G-
asks: An intelligent automatic question generation
system for academic writing support. Dialogue &
Discourse, 3(2):101–124.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Nira Mashal, Ronit Gavrieli, and Gitit Kav. 2011.
Age-related changes in the appreciation of novel
metaphoric semantic relations. Aging, Neuropsy-
chology, and Cognition, 18(5):527–543.
PMID:
21819177.

Karen Mazidi and Rodney D. Nielsen. 2014. Pedagog-
ical Evaluation of Automatically Generated Ques-
tions. Springer International Publishing.

Karen Mazidi and Rodney D. Nielsen. 2015. Leverag-
ing Multiple Views of Text for Automatic Question
Generation. Springer International Publishing.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
T¨ackstr¨om, et al. 2013. Universal dependency an-
notation for multilingual parsing. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, volume 2, pages 92–97.

George A. Miller. 1995. Wordnet: A lexical database

for english. Commun. ACM, 38(11):39–41.

Jack Mostow and Wei Chen. 2009. Generating instruc-
tion automatically for the reading strategy of self-
questioning. In Proceedings of the 2009 Conference
on Artiﬁcial Intelligence in Education: Building
Learning Systems That Care: From Knowledge Rep-
resentation to Affective Modelling, pages 465–472,
Amsterdam, The Netherlands, The Netherlands. IOS
Press.

Andrew M. Olney, Arthur C. Graesser, and Natalie K.
Person. 2012. Question generation from concept
maps. Dialogue & Discourse, 3(2):75–99.

Natalie Parde. 2018.

Reading with robots: To-
wards a human-robot book discussion system for el-
In Proceedings of the Thirty-Second
derly adults.
AAAI Conference on Artiﬁcial Intelligence (AAAI-
18) Doctoral Consortium, New Orleans, Louisiana.

273
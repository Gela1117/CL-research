Does Syntactic Knowledge in Multilingual Language Models

Transfer Across Languages?

Prajit Dhar

Arianna Bisazza

Leiden Institute of Advanced Computer Science

Leiden University, The Netherlands

{p.dhar,a.bisazza}@liacs.leidenuniv.nl

Abstract

Recent work has shown that neural models can
be successfully trained on multiple languages
simultaneously. We investigate whether such
models learn to share and exploit common
syntactic knowledge among the languages on
which they are trained. This extended abstract
presents our preliminary results.

1

Introduction

Recent work has shown that state-of-the-art neu-
ral models of language and translation can be suc-
cessfully trained on multiple languages simulta-
neously without changing the model architecture
( ¨Ostling and Tiedemann, 2017; Johnson et al.,
2017). In some cases this leads to improved per-
formance compared to models only trained on
a speciﬁc language, suggesting that multilingual
models learn to share useful knowledge cross-
lingually through their learned representations.
While a large body of research exists on the mul-
tilingual mind, the mechanisms explaining knowl-
edge sharing in computational multilingual mod-
els remain largely unknown: What kind of knowl-
edge is shared among languages? Do multilingual
models mostly beneﬁt from a better modeling of
lexical entries or do they also learn to share more
abstract linguistic categories?

We focus on the case of language models (LM)
trained on two languages, one of which (L1)
is over-resourced with respect to the other (L2),
and investigate whether the syntactic knowledge
learned for L1 is transferred to L2. To this end
we use the long-distance agreement benchmark re-
cently introduced by Gulordava et al. (2018).

2 Background

The recent advances in neural networks have
opened the way to the design of architecturally

simple multilingual models for various NLP tasks,
such as language modeling or next word predic-
tion (Tsvetkov et al., 2016; ¨Ostling and Tiede-
mann, 2017; Malaviya et al., 2017; Tiedemann,
2018), translation (Dong et al., 2015; Zoph et al.,
2016; Firat et al., 2016; Johnson et al., 2017),
morphological reinﬂection (Kann et al., 2017)
and more (Bjerva, 2017). A practical beneﬁt
of training models multilingually is to transfer
knowledge from high-resource languages to low-
resource ones and improve task performance in
the latter. Here we aim at understanding how lin-
guistic knowledge is transferred among languages,
speciﬁcally at the syntactic level, which to our
knowledge has not been studied so far.

Assessing the syntactic abilities of monolin-
gual neural LMs trained without explicit super-
vision has been the focus of several recent stud-
ies: Linzen et al. (2016) analyzed the performance
of LSTM LMs at an English subject-verb agree-
ment task, while Gulordava et al. (2018) extended
the analysis to various long-range agreement pat-
terns in different languages. The latter study found
that state-of-the-art LMs trained on a standard log-
likelihood objective capture non-trivial patterns of
syntactic agreement and can approach the perfor-
mance levels of humans, even when tested on syn-
tactically well-formed but meaningless (nonce)
sentences.

Cross-language interaction during language
production and comprehension by human subjects
has been widely studied in the ﬁelds of bilin-
gualism and second language acquisition (Keller-
man and Sharwood Smith; Odlin, 1989; Jarvis
and Pavlenko, 2008) under the terms of language
transfer or cross-linguistic inﬂuence. Numerous
studies have shown that both the lexicons and the
grammars of different languages are not stored in-
dependently but together in the mind of bilinguals
and second-language learners, leading to observ-

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages374–377Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics374able lexical and syntactic transfer effects (Koot-
stra et al., 2012). For instance, through a cross-
lingual syntactic priming experiment, Hartsuiker
et al. (2004) showed that bilinguals recently ex-
posed to a given syntactic construction (passive
voice) in their L1 tend to reuse the same construc-
tion in their L2.

While the neural networks in this study are not
designed to be plausible models of the human
mind learning and processing multiple languages,
we believe there is interesting potential at the in-
tersection of these research ﬁelds.

3 Experiment

We consider the scenario where L1 is over-
resourced compared to L2 and train our bilingual
models by joint training on a mixed L1/L2 corpus
so that supervision is provided simultaneously in
the two languages ( ¨Ostling and Tiedemann, 2017;
Johnson et al., 2017). We leave the evaluation of
pre-training (or transfer learning) methods (Zoph
et al., 2016; Nguyen and Chiang, 2017) to future
work.

The monolingual LM is trained on a small L2
corpus (LML2). The bilingual LM is trained on
a shufﬂed mix of the same small L2 corpus and
a large L1 corpus, where L2 is oversampled to
approximately match the amount of L1 sentences
(LML1+L2). See Table 1 for the actual training
sizes. For our preliminary experiments we have
chosen French as the helper language (L1) and
Italian as the target language (L2). Since French
and Italian share many morphosyntactic patterns,
accuracy on the Italian agreement tasks is ex-
pected to beneﬁt from adding French sentences to
the training data if syntactic transfer occurs.

Data and training details: We train our LMs
on French and Italian Wikipedia articles extracted
using the WikiExtractor tool.1 For each language,
we maintain a vocabulary of the 50k most fre-
quent tokens, and replace the remaining tokens
by <unk>. For the bilingual LM, all words are
prepended with a language tag so that vocabular-
ies are completely disjoint. Their union (100K
types) is used to train the model. This is the least
optimistic scenario for linguistic transfer but also
the most controlled one. In future experiments we
plan to study how transfer is affected by varying
degrees of vocabulary overlap.

1https://github.com/attardi/

wikiextractor

Following the setup of Gulordava et al. (2018),
we train 2-layer LSTM models with embedding
and hidden layers of 650 dimensions for 40
epochs. The trained models are evaluated on the
Italian section of the syntactic benchmark pro-
vided by Gulordava et al. (2018), which includes
various non-trivial number agreement construc-
tions.2 Note that all models are trained on a regular
corpus likelihood objective and do not receive any
speciﬁc supervision for the syntactic tasks.

4 Results and Conclusions

Table 1 shows the results of our preliminary ex-
periments. The unigram baseline simply picks,
for each sentence, the most frequent word form
between singular or plural. As an upper-bound
we report the agreement accuracy obtained by a
monolingual model trained on a large L2 corpus.

Table 1: Accuracy on the Italian agreement set by the
unigram baseline, monolingual and bilingual LMs.

Model
Unigram
LSTMIT
LSTMF R+IT
LSTMIT (large)

Training (#tok)

—

10MIT

80MF R + 8×10MIT

80MIT

AgreementIT
Orig. Nonce
54.5
54.9
79.9
80.7
82.4
77.5
82.6
88.2

The effect of mixing the small Italian corpus
with the large French one does not appear to be
major. Agreement accuracy increases slightly in
the original sentences, where the model is free to
rely on collocational cues, but decreases slightly in
the nonce sentences, where the model must rely on
pure grammatical knowledge. Thus there is cur-
rently no evidence that syntactic transfer occurs
in our setup. A possible explanation is that the
bilingual model has to ﬁt the knowledge from two
language systems into the same number of hidden
layer parameters and this may cancel out the ben-
eﬁts of being exposed to a more diverse set of sen-
tences. In fact, the bilingual model achieves a con-
siderably worse perplexity than the monolingual
one (69.9 vs 55.62) on an Italian-only held-out set.
¨Ostling and Tiedemann (2017)
For comparison,
observed slightly better perplexities when mix-
ing a small number of related languages, however

2For more details on the benchmark and LM
https://github.com/

conﬁgurations
facebookresearch/colorlessgreenRNNs

refer

to

375their setup was considerably different (character-
level LSTM with highly overlapping vocabulary).

This is work in progress. We are currently look-
ing for a bilingual LM conﬁguration that will re-
sult in better target language perplexity and, pos-
sibly, better agreement accuracy. We also plan
to extend the evaluation to other,
less related,
language pairs and different multilingual training
techniques. Finally, we plan to examine whether
lexical syntactic categories (POS) are represented
in a shared space among the two languages.

Acknowledgments

This research was partly funded by the Nether-
lands Organization for Scientiﬁc Research (NWO)
under project number 639.021.646. The experi-
ments were conducted on the DAS computing sys-
tem (Bal et al., 2016).

References
Henri Bal, Dick Epema, Cees de Laat, Rob van Nieuw-
poort, John Romein, Frank Seinstra, Cees Snoek,
and Harry Wijshoff. 2016. A medium-scale dis-
tributed system for computer science research: In-
frastructure for the long term. Computer, 49(5):54–
63.

Johannes Bjerva. 2017. One model to rule them
all: Multitask and multilingual modelling for lexi-
cal analysis. CoRR, abs/1711.01100.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
In Proceedings of the
tiple language translation.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1723–1732, Beijing,
China. Association for Computational Linguistics.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism.
In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 866–875, San Diego, California. Association
for Computational Linguistics.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically.
In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1195–1205. Associ-
ation for Computational Linguistics.

Robert J. Hartsuiker, Martin J. Pickering, and Eline
Veltkamp. 2004.
Is syntax separate or shared be-
tween languages?: Cross-linguistic syntactic prim-
ing in spanish-english bilinguals. Psychological Sci-
ence, 15(6):409–414.

Scott Jarvis and Anna Pavlenko. 2008. Crosslinguistic

inﬂuence in language and cognition. Routledge.

Melvin Johnson, Mike Schuster, Quoc Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernand a Vigas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Katharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.
2017. One-shot neural cross-lingual transfer for
In Proceedings of the 55th
paradigm completion.
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1993–2003, Vancouver, Canada. Association for
Computational Linguistics.

Eric Kellerman and ed. Sharwood Smith, Michael.
Crosslinguistic inﬂuence in second language acqui-
sition. Pergamon.

Gerrit Jan Kootstra, Janet G. Van Hell, and Ton Dijk-
stra. 2012. Priming of code-switches in sentences:
The role of lexical repetition, cognates, and lan-
guage proﬁciency. Bilingualism: Language and
Cognition, 15(4):797819.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of lstms to learn syntax-
sensitive dependencies. Transactions of the Associ-
ation for Computational Linguistics, 4:521–535.

Chaitanya Malaviya, Graham Neubig, and Patrick Lit-
tell. 2017. Learning language representations for ty-
pology prediction. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2529–2535. Association for Com-
putational Linguistics.

Toan Q. Nguyen and David Chiang. 2017. Trans-
fer learning across low-resource, related languages
In Proceedings of
for neural machine translation.
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 296–301, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Terence Odlin. 1989.

Language Transfer: Cross-
Linguistic Inﬂuence in Language Learning. Cam-
bridge Applied Linguistics. Cambridge University
Press.

Robert ¨Ostling and J¨org Tiedemann. 2017. Continuous
multilinguality with language vectors. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, pages 644–649, Valencia,
Spain. Association for Computational Linguistics.

376J¨org Tiedemann. 2018. Emerging language spaces
learned from massively multilingual corpora. CoRR,
abs/1802.00273.

Polyglot neural

Patrick Littell,

Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,
David
Guillaume Lample,
and
Mortensen, Alan W Black, Lori Levin,
Chris Dyer. 2016.
language
models: A case study in cross-lingual phonetic
representation learning. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1357–1366, San
Diego, California. Association for Computational
Linguistics.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
In Proceedings of the
neural machine translation.
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1568–1575, Austin,
Texas. Association for Computational Linguistics.

377
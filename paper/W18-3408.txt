Investigating Effective Parameters for Fine-tuning of Word Embeddings

Using Only a Small Corpus

Kanako Komiya
Ibaraki University

4-12-1 Nakanarusawa

Hitachi Ibaraki 316-8511 Japan
kanako.komiya.nlp@vc.

Hiroyuki Shinnou
Ibaraki University

4-12-1 Nakanarusawa

Hitachi Ibaraki 316-8511 Japan

hiroyuki.shinnou.0828@vc.

ibaraki.ac.jp

ibaraki.ac.jp

Abstract

Fine-tuning is a popular method to achieve
better performance when only a small
target corpus is available. However,
it
requires tuning of a number of meta-
parameters and thus it might carry risk of
adverse effect when inappropriate meta-
parameters are used. Therefore, we inves-
tigate effective parameters for ﬁne-tuning
when only a small target corpus is avail-
able. In the current study, we target at im-
proving Japanese word embeddings cre-
ated from a huge corpus. First, we demon-
strate that even the word embeddings cre-
ated from the huge corpus are affected by
domain shift. After that, we investigate
effective parameters for ﬁne-tuning of the
word embeddings using a small target cor-
pus. We used perplexity of a language
model obtained from a Long Short-Term
Memory network to assess the word em-
beddings input into the network. The ex-
periments revealed that ﬁne-tuning some-
times give adverse effect when only a
small target corpus is used and batch size
is the most important parameter for ﬁne-
tuning. In addition, we conﬁrmed that ef-
fect of ﬁne-tuning is higher when size of a
target corpus was larger.

1

Introduction

We investigate effective parameters for ﬁne-
tuning using nwjc2vec. Nwjc2vec is Japanese
word2vec (the word embeddings proposed by
(Mikolov et al., 2013)) created from NINJAL
Web Japanese Corpus (NWJC) (Asahara et al.,
2014) (Asahara and Teruaki, 2017).
It contains
it
25.8 billion words as a whole. Therefore,
is believed that nwjc2vec is high-quality.
In

fact, some models used it showed better re-
sults (Yamaki et al., 2017) (Shinnou et al., 2017b)
(Shinnou et al., 2017a). In addition, it is also be-
lieved that nwjc2vec is useful for various docu-
ments because it contains a number of documents
described about various topics.

However, we show that a problem posed by do-
main shift occurs when nwjc2vec is used in the
current study. (See Section 4)

The simplest and most effective approach to ad-
dress the problem caused from domain shift of
word embeddings is ﬁne-tuning using a large tar-
get corpus. However, in practice, we often face
the situation where only a small corpus of the
target domain is available.
It is possible to use
other resources than a corpus, but they are not al-
ways available. Therefore, in the current study,
we investigate effective parameters for word2vec,
which is a program to create word embeddings,
when we ﬁne-tune nwjc2vec using only a small
target corpus. (See Section 5)

We evaluate the word embeddings via language
models obtained from a LSTM (Long Short-Term
Memory)
(Hochreiter and Schmidhuber, 1997)
(Gers et al., 2000) (Greff et al., 2016) (See Sec-
tion 3). First, we develop a language model using
a LSTM. Usually, word embeddings are learned
from the same corpus as a training corpus for a lan-
guage model. In other words, when we have only a
small target corpus, we use the word embeddings
learned from the target corpus for the inputs for
the LSTM that develops a language model. How-
ever, we input nwjc2vec ﬁne-tuned using the small
corpus into the LSTM instead of the word embed-
dings directly learned from the corpus. We eval-
uate the language model to assess the ﬁne-tuned
word embeddings assuming that the quality of the
output language model is higher when the qual-
ity of the word embeddings used in the LSTM is
higher.

ProceedingsoftheWorkshoponDeepLearningApproachesforLow-ResourceNLP,pages60–67Melbourne,AustraliaJuly19,2018.c(cid:13)2018AssociationforComputationalLinguistics60The experiments revealed that the batch size
is the most important parameter for word2vec to
ﬁne-tune nwjc2vec using a small corpus. In addi-
tion, they also showed that ﬁne-tuning using in-
appropriate parameters sometimes make perfor-
mance worse. Moreover, we conﬁrmed that size
of the corpus is crucial for ﬁne-tuning. (See Sec-
tions 6 and 7)

2 Related Work

Generally, effectiveness of word embeddings de-
pends on tasks and target domains of the tasks.
Therefore, (Schnabel et al., 2015) proposed tuning
of word embeddings according to tasks and their
target domains.

The simplest tuning is ﬁne-tuning, which is
an approach where learned word embeddings are
used for the initial values and tuned using an addi-
tional corpus. Its effectiveness has been shown for
object recognition (Agrawal et al., 2014), named
entity recognition (Lee et al., 2017), and many
other tasks. Usually, a large target corpus is re-
quired for ﬁne-tuning. Some works improved the
word embeddings using external knowledges such
as dictionaries.
(Yu and Dredze, 2014) changed
the loss function to use pre-knowledges and im-
proved the word embeddings.
(Faruqui et al.,
2015) proposed to use retroﬁtting, which is an ap-
proach where the word embeddings obtained from
a huge corpus are re-learned using external knowl-
edges.

Fine-tuning is one of the methods for transfer
learning (Pan and Yang, 2009). There are also
much work about multi-task learning, which is
another approach often used for transfer learn-
ing for neural networks (Aguilar et al., 2017)
(von D¨aniken and Cieliebak, 2017).

3 Evaluation Method of Word
Embeddings Using a LSTM

In the current study, we used a LSTM, which
is an extended version of an a RNN to evalu-
ate the word embeddings for a certain domain as
(Shinnou et al., 2017a). We developed a language
model using a LSTM from a training corpus and
calculated the perplexity of the language model for
a test corpus. Perplexity is given by the following
equation.

P P = 2H

where H is entropy given by the following equa-
tion.

H =

1jDj

(cid:0)P (WijM )log2P (WijM )

jDj∑

i=1

where D denotes a size of test data, M denotes a
language model, and Wi denotes ith word in the
test data.

We evaluate the quality of the word embed-
dings depending on the perplexity assuming that
the quality of the output language model is higher
when the quality of the word embeddings used in
the LSTM is higher. Usually, word embeddings
are learned from the same corpus as the training
corpus for a language model. However, we used
the word embeddings to be evaluated instead of
the word embeddings learned together with the
language model (cf. Figure1). We believe that we
can evaluate the quality of the word embeddings
by evaluating the perplexity of the language model
when they are used in a LSTM.

4 Effect of Domain Shift for Nwjc2vec

We demonstrate that even nwjc2vec, which is a
word embeddings obtained from a huge corpus,
NWJC, has a problem posed by domain shift in
this section.

4.1 Mai2Vec
To show this problem, we ﬁrstly created word
embeddings from newspapers collected for seven
years: Mainichi Shimbun newspaper articles from
1993 to 1999. We removed headlines and tables
and extracted only sentences. The sentences were
divided into words and the words were used for
inputs into word2vec. The corpus had 6,791,403
sentences. We used MeCab-0.996 as a morpho-
logical analyzer and UniDic-2.1.2 as a dictio-
nary. These word embeddings are referred to
as mai2vec. The word2vec parameters used for
mai2vec are the same as the parameters used for
nwjc2vec. The ﬁnal number of tokens of mai2vec
we obtained was 132,509.

4.2 Language Model for Blogs and Q & A

sites

First, we compared mai2vec with nwjc2vec using
blogs and Q & A sites for test data. We extracted
7,330 sentences from blogs (Yahoo! blogs) and
Q & A sites (Yahoo! Chiebukuro) of Balanced

61Figure 1: Evaluation Method of Word Embeddings Using LSTM

Corpora of Contemporary Witten Japanese (BC-
CWJ) (Maekawa et al., 2014) and used them for
the language model. We used 7,226 sentences for
the training and 104 sentences for the test. The
language model that used nwjc2vec in the LSTM
was referred to as nwjc2vec-lm-1 and the language
model that used mai2vec in the LSTM was re-
ferred to as mai2vec-lm-1. Base-lm-1, which was
a language model that used the word embeddings
learned together with the language model in the
LSTM, was also evaluated for reference. Table 1
shows the corpora used for this experiment.

Perplexity was used for the evaluation of the
language models. We conducted learning 15
epochs, saved the models, and calculated their per-
plexities for each epoch. After that, we evaluated
the lowest perplexity for each model1.

Table 2 shows the results. According to the ta-
ble, the perplexity of nwjc2vec-lm-1 is the low-
est, which indicates that the quality of nwjc2vec is
higher than that of mai2vec.

However, the domains of the training and test
corpora for the language model, blogs and Q &A
site, were different from that of mai2vec, Mainichi
Shimbun Newspaper. Therefore, nwjc2vec per-
haps had an advantage.

1The perplexity was the lowest at the fourth or ﬁfth epoch

for all the models.

4.3 Language Model for Newspaper

Next, we evaluate the word embeddings using the
training and test corpora from newspapers, whose
domain is the same as that of mai2vec. We used
100,000 sentences extracted from Mainichi Shim-
bun Newspaper in 2007 for the training of the lan-
guage models. Ten thousand sentences extracted
from Mainichi Shimbun Newspaper in 2008 were
used for the test. Nwjc2vec-lm-2 and mai2vec-
lm-2, which were the language models that used
nwjc2vec and mai2vec respectively, were devel-
oped again. Base-lm-2, which was a language
model that uses the word embeddings learned to-
gether with the language model in a LSTM, was
also evaluated for reference. Note that the training
corpora of word2vec for base-lm-1 and base-lm-2
are different. Table 3 shows the corpora used for
this experiment.

Table 4 and Figure 2 show the results. They
show that the perplexity of mai2vec-lm is the low-
est, which indicates that the quality of mai2vec is
higher than that of nwjc2vec.

The better method was shifted from nwjc2vec-
lm into mai2vec-lm when the domain of the train-
ing and test corpora were the same as that of
mai2vec. This fact indicates that even nwjc2vec
has a problem posed by domain shift.

62Table 1: Corpora Used for Word2vec and Training and Test Data for Language Model for Blogs and Q
& A Sites

Name of model Word2Vec corpus
mai2vec-lm-1
nwjc2vec-lm-1 NWJC (Web pages)
base-lm-1

Blogs and Q & A sites

Newspaper in from 1993 to 1999

Training data Test data

Blogs
And

Q & A sites

Table 2: Evaluation of Language Models Obtained
from Each Word Embeddings 1
base-lm-1
mai2vec-lm-1
130.35

nwjc2vec-lm-1

118.68

124.72

Figure 2: Perplexities of Language Models Devel-
oped Using Each Word Embeddings

Finally, Table 5 summarizes the number of sen-
tences of each corpus. Please note that the corpora
used for word2vec for base-lm-1 and base-lm-2
are the same as the corpora used for training the
language models, respectively. The word embed-
dings were learned together with the LSTMs.

5 Fine-tuning Using a Small Corpus
Fine-tuning of nwjc2vec using a target corpus is
the simplest way to address the problem caused
from domain shift. It is preferable that the large
target corpus is used for the ﬁne-tuning but some-
times only a small target corpus is available. In
these cases, it is not clear yet if the ﬁne-tuning im-
proves nwjc2vec.

Therefore, we tested various parameters of
word2vec, which was a program to develop word
embeddings, and found out if they were effective
or not.

First, we set standard parameters of word2vec

and ﬁne-tuned nwjc2vec through them using an
additional corpus (the small target corpus). Next,
only a windows size parameter was changed from
the standard one and ﬁne-tuned nwjc2vec through
them using the same additional corpus. We
changed the batch size and epoch number param-
eters and ﬁne-tuned nwjc2vec in the same way.

Table 6 lists

the standard parameters of

word2vec for the ﬁne-tuning.

The procedures to investigate the parameters
are described as follows. First, we ﬁne-tuned
nwjc2vec using the standard parameters listed in
Table 6 and developed word embeddings. The
word embeddins developed in this setting are re-
ferred to as base emb. Next, we changed the
window size parameter into 8 and ﬁne-tuned
nwjc2vec. The word embeddins developed in this
setting are referred to as win emb. After that,
we changed only the batch size parameter into
20 remaining the other parameters as the standard
ones and ﬁne-tuned nwjc2vec. The word embed-
dins developed in this setting are referred to as
batch20 emb. We also evaluated batch100 emb,
which were word embeddings ﬁne-tuned using the
standard parameters except the batch size, which
had been changed into 100. Finally, we eval-
uated epch emb, which were word embeddings
ﬁne-tuned using the standard parameters except
the epoch number, which had been changed into
20. Table 7 lists the parameters of word2vec we
tried for the ﬁne-tuning. We tested the ﬁve set-
tings of ﬁne-tuning including the setting in Table
6. We used 100,000 sentences randomly extracted
from Mainichi Shimbun in from 1993 to 1999 as
an additional corpus for the ﬁne-tuning.

6 Experiments

We developed the language models through the
LSTMs. We used the ﬁve ﬁne-tuned word em-
beddings described above, base emb, win emb,
batch20 emb, batch100 emb, and epch emb, and
used 100,000 sentences randomly extracted from
Mainichi Shimbun Newspaper in from 1993 to

6350556065707580859095123456789101112131415mai2vec-lm64.8nwjc2vec-lm67.4epochperplexityTable 3: Corpora Used for Word2vec and Training and Test Data for Language Model for Newspapers

Name of model Word2Vec corpora
mai2vec-lm-2
nwjc2vec-lm-2 NWJC (Web pages)
base-lm-2
Newspaper in 2007

Newspaper in from 1993 to 1999 Newspaper

Training data Test data

In
2007

Newspaper
In
2008

Table 4: Evaluation of Language Models Obtained
from Each Word Embeddings ２
base-lm-2

nwjc2vec-lm-2

mai2vec-lm-2

81.52

64.81

67.47

1999 to train the LSTMs. We calculated perplex-
ities of the language models obtained from the
LSTMs at each epoch using the test data, which
was 10,000 sentences from the same corpus as the
training data. These is no overlap among the data
for the ﬁne-tuning, the training, and the testing.
Table 8 summarizes the number of sentences of
each corpus.

Table 9 and Figure 3 show the results. They
include the perplexities of the language model ob-
tained from the LSTMs when original nwjc2vec
was used without the ﬁne-tuning. The asterisks in
the table mean that the language model using the
ﬁne-tuned word embeddings was better than that
using nwjc2vec.

parameters are used in the case where small
corpora are used.

7 Discussion
We think that although we might obtain better
performance if we changed parameters other than
batch size, the best results would be around the
performance of batch100 emb because the batch
size affected much more than the window size and
the epoch number according to Table 9 and Figure
3.

In addition, we believe that the most important
factor for the effective ﬁne-tuning of nwjc2vec is
the size of the additional corpus. To conﬁrm this
point, we tried some variation of the additional
corpus size, 200,000 and 300,000 sentences in ad-
dition to the original setting, 100,000 sentences.

Table 10 and Figure 4 list the results of these
experiments. These results indicate that the effect
of ﬁne-tuning is higher when the size of the addi-
tional corpus is larger.

Figure 3: Changes of Perplexities According to
Various Settings

Figure 4: Changes of Perplexities According to
Various Sizes of Additional Corpora

results

show that

the perplexities
These
of
the language model decrease only when
batch100 emb is used. It indicates that ﬁne-tuning
is only effective when the batch size parameter is
changed into 100. Other parameter changes made
the results worse. The experiments revealed that
ﬁne-tuning has an opposite effect when unsuitable

The ﬁne-tuning approach we employed is the
simplest way to tune word embeddings. Fine tun-
ing of nwjc2vec requires large-sized additional
corpus. Instead of the additional corpus, the exter-
nal resources such as dictionaries would be useful.
We plan to improve nwjc2vec using such external
resources in the future.

64656667686970717273747512345678910batch100_embnwjc2vecbatch20_embepch_embbase_embwin_emb66.2366.47perplexityepoch6465666768697012345678910100K sentences(batch100_emb)66.2365.6264.61200K sentences300KsentencesperplexityepochType
Corpus
NWJC
Training Nwjc2vec
Mainichi Shimbun 1993-1999 Training Mai2vec
BCCWJ

Table 5: Corpus Data for Domain Shift Experiments
Genre
Web pages
Newspaper
Blogs
And

Training Word2vec of base-lm-1

Aim

Language model
For blogs and Q &A sites Q & A sites

Test
Training Word2vec of base-lm-2

BCCWJ
Mainichi Shimbun 2007

Mainichi Shimbun 2008

Test

Language model
For newspaper

Newspaper

Number of Sentences

1,463,142,939

6,791,403

7,226

104

100,000

10,000

Table 6: Standard Parameters for Word2vec

Model Name
Number of Units
Window Size
Batch Size
Epoch Number
Used Model

base emb

200
5
10
10

skip-gram

8 Conclusion
We showed the problem occurred by domain
shift when nwjc2vec was used and investigated
the effective parameters of word2vec to ﬁne-tune
nwjc2vec using a small corpus.

The experiments revealed that it is possible to
obtain better results using ﬁne-tuning of nwjc2vec
if we properly adjust parameters. We showed that
the most effective parameter of the ﬁne-tuning
is the batch size and ﬁne-tuning using improper
parameters make the results worse. Finally, we
demonstrated that the size of the additional corpus
is crucial for ﬁne-tuning of nwjc2vec. We plan
to use external resources instead of the large-sized
corpus in the future.

References
Pulkit Agrawal, Ross Girshick, and Jitendra Malik.
2014. Analyzing the Performance of Multilayer
In Pro-
Neural Networks for Object Recognition.
ceedings of ECCV-2014, page arXiv:1407.1610.

Gustavo Aguilar, Suraj Maharjan, A. Pastor Lopez-
Monroy, and Thamar Solorio. 2017. A Multi-task
Approach for Named Entity Recognition in Social
Media Data. In Proceedings of the 3rd Workshop on
Noisy User-generated Text, pages 148–153.

Masayuki Asahara, Kikuo Maekawa, Mizuho Imada,
Sachi Kato, and Hikari Konishi. 2014. Archiv-
ing and Analysing Techniques of the Ultra-large-
scale Web-based Corpus Project of NINJAL, Japan.

Alexandria: The Journal of National and Interna-
tional Library and Information Issues, 25(1-2):129–
148.

Masayuki Asahara and Oka Teruaki. 2017. nwjc2vec:
word embedding data based on NINJAL Japanese
In ANLP-2017, pages
Web Corpus (in Japanese).
94–97.

Pius von D¨aniken and Mark Cieliebak. 2017. Trans-
fer Learning and Sentence Level Features for Named
Entity Recognition on Tweets. In Proceedings of the
3rd Workshop on Noisy User-generated Text, pages
166–171.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retroﬁtting Word Vectors to Semantic Lexicons. In
Proceedings of NAACL.

Felix A Gers, J¨urgen Schmidhuber, and Fred Cummins.
2000. Learning to forget: Continual prediction with
LSTM. Neural computation, 12(10):2451–2471.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık,
Bas R Steunebrink, and J¨urgen Schmidhuber. 2016.
IEEE transac-
LSTM: A Search Space Odyssey.
tions on neural networks and learning systems.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Ji Young Lee, Franck Dernoncourt,

and Peter
Szolovits. 2017. Transfer Learning for Named-
Entity Recognition with Neural Networks. In arXiv,
page arXiv:1705.06273.

Kikuo Maekawa, Makoto Yamazaki, Toshinobu
Ogiso, Takehiko Maruyama, Hideki Ogura, Wakako
Kashino, Hanae Koiso, Masaya Yamaguchi, Makiro
Tanaka, and Yasuharu Den. 2014. Balanced corpus
of contemporary written japanese. Language Re-
sources and Evaluation, 48(2):345–371.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

65Model Names
Number of Units
Window Size
Batch Size
Epoch Number
Used Model

Table 7: Standard Parameters for Word2vec

win emb

batch20 emb

batch100 emb

8
10
10

5
20
10

200

5
100
10

skip-gram

epoch emb

5
10
20

Table 8: Corpus Data for Fine-tuning Experiments

Type

Corpus
Mainichi Shimbun Fine-tuning Word2vec
1993-
1999

Training
Test

Aim

Language Newspaper
Model

100,000
100,000
10,000

Genre

Number of Sentences

Sinno Jialin Pan and Qiang Yang. 2009. A sur-
IEEE TRANSACTIONS
vey on transfer learning.
ON KNOWLEDGE AND DATA ENGINEERING,
22(10):1345 – 1359.

Tobias Schnabel, Igor Labutov, David M Mimno, and
Thorsten Joachims. 2015. Evaluation methods for
unsupervised word embeddings. In EMNLP 2015,
pages 298–307.

Hiroyuki Shinnou, Masayuki Asahara, Kanako
Komiya, and Minoru Sasaki. 2017a.
nwjc2vec:
Word Embedding Data Constructed from NIN-
Journal of Natural
JAL Web Japanese Corpus.
Language Processing, 24(5):705–720.

Hiroyuki Shinnou, Kanako Komiya, and Minoru
Sasaki. 2017b.
Supervised Word Sense Disam-
biguation using Forward Multilayered LSTM and
In IPSJ Natural
Word Embeddings (in Japanese).
Language Processing Report, pages NL–232–4.

Shoma Yamaki, Hiroyuki Shinnou, Kanako Komiya,
and Minoru Sasaki. 2017. Construction of word em-
beddings using labeled data (in Japanese). In ANLP-
2017, pages 78–81.

Mo Yu and Mark Dredze. 2014.

Improving Lexical
Embeddings with Semantic Knowledge. In ACL (2),
pages 545–550.

66epoch

nwjc2vec

base emb win emb

batch20 emb

batch100 emb

epch emb

Table 9: Perplexities of Various Settings

1
2
3
4
5
6
7
8
9
10

91.03
73.20
68.65
67.43
67.52
68.17
69.08
70.06
71.09
72.18

93.70
75.21
70.21
68.85
68.84
69.55
70.37
71.48
72.71
73.92

95.36
75.71
70.52
69.33
69.51
70.20
71.11
72.22
73.40
74.66

91.51
73.43
68.69
67.56
67.70
68.37
69.37
70.56
71.80
73.06

89.69
72.36
67.54
66.23*
66.35*
67.13*
68.17
69.37
70.58
71.82

95.06
75.89
70.30
68.46
68.17
68.54
69.29
70.36
71.49
72.68

Table 10: Perplexities of Sizes of Additional Corpora

epoch

100 thousand sentences

200 thousand sentences

300 thousand sentences

(batch100 emb)

1
2
3
4
5
6
7
8
9
10

89.69
72.36
67.54
66.23
66.35
67.13
68.17
69.37
70.58
71.82

89.55
71.50
66.96
65.65
65.62
66.27
67.32
68.46
69.64
70.89

87.94
70.28
65.83
64.61
64.75
65.44
66.45
67.56
68.78
69.92

67
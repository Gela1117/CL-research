AX Semantics’ Submission to the Surface Realization Shared Task 2018

Andreas Madsack, Johanna Heininger, Nyamsuren Davaasambuu,

Vitaliia Voronik, Michael K¨auﬂ, and Robert Weißgraeber

AX Semantics, Stuttgart, Germany

{firstname.lastname}@ax-semantics.com

Abstract

In this paper we describe our system and
experimental results on the development
set of the Surface Realisation Shared Task
(Mille et al., 2017). Our system is an entry
for Shallow-Task, with two different mod-
els based on deep-learning implementa-
tions for building the sentences combined
with a rule-based morphology component.
We trained our systems on all 10 given lan-
guages.
Introduction

1
This paper describes our approach for the First
Multilingual Surface Realisation Shared Task
(Mille et al., 2018). For the surface task the de-
pendency parse trees were given unordered and the
words lemmatized. The objective was to order the
words in the sentences and to inﬂect the given lem-
mas. The data was provided in 10 languages: En-
glish, Spanish, French, Portuguese, Italian, Dutch,
Czech, Russian, Arabic, and Finnish.

Our aim was to build new deep learning based
ordering systems, augmented by using our already
implemented (rule-based) morphology for the in-
ﬂection part. System 1 implemented the initial
idea and system 2 followed after mediocre results
in system 1.

Final scoring for the MSR shared task was using

System 2.

2 Linearization
Here we propose two systems: both are imple-
mented using Keras (Chollet et al., 2015) and Ten-
sorﬂow (Abadi et al., 2016), are trained using each
language from the CoNLL data sets separately and
ﬁnally also trained with all languages combined.
These two systems, however, differ in their inter-
nal models (see the following two sections).

To generate training data the given training
CoNLL data sets were matched to their corre-
sponding original data using tree based match-
ing. Each node was compared based on deprel,
lemma/form, upostag, and number of chil-
dren in a recursive manner traversing the tree from
top to bottom.

2.1 System 1: Sequence-to-Sequence Model
System 1 is a new approach using sequence-to-
sequence models (Vinyals et al., 2016), encoder-
decoder, and attention as described in Bahdanau
et al. (2014) for machine translation. Instead of us-
ing LSTM cells, we used bidirectional GRU cells
(Cho et al., 2014). Some early stage evaluations
showed GRU converges better than LSTM for this
task.

The input sequence is an unordered list of words
the features for each word
and their features;
consist of: id, upostag, deprel, head-id,
head-upostag, head-deprel, and level
in the syntax-tree. All features are encoded in
embeddings. The embeddings are shared be-
tween the two matching ﬁelds (i.e. deprel and
head-deprel). Figure 1 shows a visualization
of the model.

The result of the sequence model is a sequence
of correct positions of the words for a complete
sentence. This order,
together with the given
lemma and features from the data set, is then pro-
cessed by a morphology component, which also
takes care of building the “ﬁnal readable sentence”
including e.g. capitalization.

We trained two sub-models for each language
with the sequence lengths of 25 and 400. We
chose these values based on the length of the sen-
tences in the training data set – the 75% quantile
is at length 25 which includes most of the sen-
tences. 400 is the absolute maximal length of sen-
tences (the longest sentence has 398 words and is

ProceedingsoftheFirstWorkshoponMultilingualSurfaceRealisation,pages54–57Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics54tence is calculated against every other word in
the same sentence. Features used in training for
each of the two words are upostag, deprel,
head-upostag, head-deprel and level
in the syntax-tree. Same as System 1 the embed-
dings are shared between the two matching ﬁelds.
The predicted word1-is-right-of-word2 proba-
bilities are used for each subtree to ﬁnd the or-
der. On the next level the subtree is ordered by
the probability of the head node of the subtree.

that

The

show

results

particularly
the end
upostag=PUNCT is now mostly at
of sentences even for commas and other punctu-
ations. Human inspection results in a positively
increased overall readability of the output com-
pared to the Sequence-to-Sequence Model (our
System 1). See table 2 for results on the given
dev-set.

Like the Sequence-to-Sequence model, we have
evaluated this model using the matching language
and a model trained on all languages.

lang BLEU

(language)
0.205
0.100
0.154
0.153
0.105
0.161
0.099
0.239
0.044
0.078

BLEU DIST
(ALL)
0.175
0.139
0.137
0.137
0.106
0.123
0.110
0.142
0.059
0.064

(language)
0.430
0.182
0.190
0.314
0.309
0.298
0.279
0.260
0.163
0.197

DIST
(ALL)
0.354
0.273
0.308
0.308
0.258
0.270
0.235
0.245
0.199
0.223

en
es
fr
pt
it
nl
cs
ru
ar
ﬁ

Table 2: Scores for Pairwise Classiﬁcation Model
(development data)

Figure 1: Sequence-to-Sequence Model
The inputs are:
(id, head-id),
head-upos),
(level)

(deprel,

(upos,
head-deprel),

in Arabic). We used 0 value padding for sequences
shorter than the maximum given by the model.

These two sub-models are then available for the
prediction phase, during which the model is cho-
sen by the length of input sentence being shorter
than that of the next ﬁtting model. The predicted
sequence probabilities are selected so that every
word appears only once in the ﬁnal sentence.

Automatic evaluation of the dev-set resulted in
BLEU scores and DIST scores given in table 1.
We used the evaluation code given by the shared
task organizers. This evaluation step includes the
morphology described in section 3. We used the
matching model for the language and a model
trained with all languages.

lang BLEU

(language)
0.020
0.007
0.009
0.012
0.007
0.010
0.009
0.009
0.002
0.010

BLEU DIST
(ALL)
0.019
0.007
0.009
0.013
0.008
0.013
0.011
0.007
0.003
0.012

(language)
0.124
0.071
0.094
0.106
0.095
0.099
0.082
0.073
0.071
0.083

DIST
(ALL)
0.133
0.071
0.096
0.107
0.098
0.102
0.085
0.076
0.072
0.084

en
es
fr
pt
it
nl
cs
ru
ar
ﬁ

Table 1: Scores for Sequence-to-Sequence Model
(development data)

2.2 System 2: Pairwise Classiﬁcation
The second system is a classiﬁcation model that
calculates the word ordering by estimating if
word1 is right of word2. Each word of a sen-

Figure 2: Pairwise Classiﬁcation Model
The inputs are:
w2-upos, w2-head-upos),
w1-head-upos,
w2-head-upos), (w1-level, w2-level)

(w1-upos, w1-head-upos,
(w1-deprel,
w2-deprel,

55InputLayerEmbeddingInputLayerEmbeddingInputLayerInputLayerEmbeddingInputLayerInputLayerInputLayerEmbeddingDropoutDropoutDropoutDropoutDropoutDropoutDropoutConcatenateBidirectional(GRU)RepeatVectorBidirectional(GRU)TimeDistributed(Dense)ActivationInputLayerEmbeddingInputLayerEmbeddingInputLayerInputLayerInputLayerEmbeddingInputLayerInputLayerInputLayerInputLayerInputLayerDropoutDropoutDropoutDropoutDropoutDropoutDropoutDropoutDropoutDropoutConcatenateDenseFlattenDenseDropoutDenseReference
System 1
System 2
Reference

From the AP comes this story:
Comes from story this AP the:
This story comes from the AP:
I took my Mustang here and it looked amazing after they were done, they did a great job, I’m very satisﬁed
with the results.

System 1 With results job I my took satisﬁed it amazing here did very Mustang. great I, they were a are after looked

System 2

Reference
System 1
System 2
Reference
System 1
System 2

done the, and they
I took here Mustang my looked it amazing after they done were and they did a great job I are very satisﬁed
the with results,,.
Lopulta saatiin halikuva otettua
Sadaan lopulta ottattua halikuva.
Lopulta sadaan ottattua halikuva.
Pastrana begon een politiek offensief om de Copa voor Colombia te behouden.
Voor beginde. offensief te politiek een Pastrana Colombia Copa om behouden de
Pastrana om behouden te Copa de voor Colombia beginde offensief een politiek.

Table 3: Example outputs

3 Morphologization

The morphology step employs the NLG system
from AX Semantics (Weißgraeber and Madsack,
2017). That system is rule-based and for each in-
ﬂection request it runs through a decision chain, in
which all parts of speech and corresponding gram-
matical features of the speciﬁc languages are im-
plemented.

For irregular words the AX Semantics NLG
system uses lexicon entries, which always su-
percede the rule-based inﬂection. Grammatical
features like number, case, animacy and tense are
implemented in a general way, then added to each
language alongside its individual conﬁguration.

Since the CoNLL features differ from our usual
input parameters, some preprocessing was neces-
sary to map the terms accordingly. The words
were also cleaned with regard to special charac-
ters like hash tags or diacritics before they were
processed by the NLG morphology component.

The accuracy of the morphology component
was tested separately on the dev-set for each lan-
guage. Results are summarized in table 4. Most
of the languages show a decent accuracy score
of over 90%, whereas Arabic and Finnish with
their more complicated morphology still achieve
around 80%.

The table also shows that for some languages
the accuracy scores for verbs are signiﬁcantly
lower than for nouns or adjectives. For example,
in case of Dutch this happens mainly because a
given lemma is not the inﬁnitive form as expected
from our system but a ﬁnite verb form (3rd per-
son singular) and ﬁrst has to be transformed to the
inﬁnitive. This can largely be attributed to the spe-
cialization of the system for the language of com-
merce, which results in a partial under-coverage

language
en
es
fr
pt
it
nl
cs
ru
ar
ﬁ

nouns
0.90
0.94
0.94
0.91
0.94
0.86
0.82
0.92
0.73
0.60

adjectives
0.92
0.96
0.93
0.95
0.95
0.86
0.91
0.91
0.69
0.65

verbs mean
0.94
0.93
0.86
0.94
0.94
0.81
0.95
0.79
0.93
0.77
0.90
0.50
0.88
0.91
0.90
0.60
0.81
0.40
0.62
0.79

Table 4: Accuracy of the morphology step (exam-
ples for single POS categories and mean overall
accuracy)

of certain language features for edge cases. We
expect coverage to increase as usage expands to
more ﬁelds.

Furthermore, some of the errors are due to the
data being erroneous or incomplete (e.g., only case
is given, when number and animacy would also be
needed).

4 Conclusion and Future Work

On the whole, none of the systems solve the task
satisfactorily.

System 2 shows better scores and somewhat im-
proved readability in contrast to System 1. See ta-
ble 3 for illustration.

In both linearization systems, we use neither the
lemma nor an embedding of the lemma to allow a
comparison between the language models and the
ALL-language model. This serves as a baseline
for comparison against systems where language-
speciﬁc features can be added.

Our focus for this workshop was to build a lin-
earization system that is simple and does not re-
ceive any topic-speciﬁc or language-speciﬁc in-
put data nor conﬁgurations, and without building a

56neuronal network for morphologization. For pure
morphologization tasks, especially for Finnish,
Arabic and Hungarian with a large list of very rare
cases, we will improve inﬂection by adding a NN-
based morphology component as well.

References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorﬂow: A system for large-scale machine learning.
In 12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 16), pages 265–
283.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078.

Franc¸ois Chollet et al. 2015. Keras. https://

keras.io.

Simon Mille, Anja Belz, Bernd Bohnet, Yvette Gra-
ham, Emily Pitler, and Leo Wanner. 2018. The
First Multilingual Surface Realisation Shared Task
(SR’18): Overview and Evaluation Results. In Pro-
ceedings of the 1st Workshop on Multilingual Sur-
face Realisation (MSR), 56th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1–10, Melbourne, Australia.

Simon Mille, Bernd Bohnet, Leo Wanner, and Anja
Belz. 2017. Shared task proposal: Multilingual sur-
face realization using universal dependency trees. In
Proceedings of the 10th International Conference on
Natural Language Generation, pages 120–123. As-
sociation for Computational Linguistics.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
2016. Order matters: Sequence to sequence for sets.
In International Conference on Learning Represen-
tations (ICLR).

Robert Weißgraeber and Andreas Madsack. 2017. A
working, non-trivial, topically indifferent nlg system
for 17 languages. In Proceedings of the 10th Inter-
national Conference on Natural Language Genera-
tion, pages 156–157. Association for Computational
Linguistics.

57
Text Completion using a Context-Integrating Dependency Parser

Amr Rekaby Salama ∗
Department of Informatics

Universit¨at Hamburg

¨Ozge Alac¸am ∗

Wolfgang Menzel

Department of Informatics

Department of Informatics

Universit¨at Hamburg

Universit¨at Hamburg

Germany
Germany
{salama,alacam,menzel}@informatik.uni-hamburg.de

Germany

Abstract

Incomplete linguistic input, i.e. due to
a noisy environment, is one of the chal-
lenges that a successful communication
system has to deal with.
In this paper,
we study text completion with a data set
composed of sentences with gaps where a
successful completion cannot be achieved
through a uni-modal (language-based) ap-
proach. We present a solution based on
a context-integrating dependency parser
incorporating an additional non-linguistic
modality. An incompleteness in one chan-
nel is compensated by information from
another one and the parser learns the as-
sociation between the two modalities from
a multiple level knowledge representation.
We examined several model variations by
adjusting the degree of inﬂuence of dif-
ferent modalities in the decision making
on possible ﬁller words and their exact
reference to a non-linguistic context ele-
ment. Our model is able to ﬁll the gap
with 95.4% word and 95.2% exact refer-
ence accuracy hence the successful predic-
tion can be achieved not only on the word
level (such as mug) but also with respect to
the correct identiﬁcation of its context ref-
erence (such as mug 2 among several mug
instances).

1

Introduction

Text completion/prediction is a crucial element of
communication systems, due to its role in increas-
ing the ﬂuency and the effectiveness of the com-
munication in scenarios where the environment
is noisy, or the communication partner suffers

∗*These authors contributed equally to this work

from a motor, or cognitive impairment (Garay-
Vitoria and Abascal, 2004).
In this study, we
tackle the problem of compensating the incom-
pleteness of the verbal channel by additional in-
formation from visual modality. This capability
for multi-modal integration can be a very speciﬁc
yet crucial feature in resolving references and/or
performing commands for i.e. a helper robot that
aids people in their daily activities. To the au-
thors’ knowledge, there is no multi-modal data set
for a text completion task that systematically ad-
dresses challenging linguistic structures (i.e. syn-
tactic or referential ambiguities) for environments
where helper robots, who have access to visual in-
formation, would be employed.

The completion is performed by predicting
tenable ﬁllers for
the missing, unknown, or
vague parts in the input sentences through vary-
ing techniques, using single or hybrid methods.
The prediction process utilizes the available re-
sources, usually linguistic information (morpho-
logical, syntactic, and semantic properties).
It
can also use additional information sources such
as the linguistic, or visual context (Garay-Vitoria
and Abascal, 2006). If only the linguistic level is
available, a language model can be used to pre-
dict the probability of a syntactic category in a
certain context (Asnani et al., 2015; Bickel et al.,
2005). N-grams is a popular method for this task
since they provide very robust predictions for lo-
cal dependencies. Nevertheless, they loose their
power for structures with long-range dependen-
cies. Furthermore, if there are multiple instances
of the same object class (c.f. Figure 1), a text
completion based on N-gram could not differen-
tiate between them to select the proper instance
reference. As shown in several studies (Mirowski
and Vlachos, 2015; Gubbins and Vlachos, 2013),
a language model employing the syntactic depen-
dencies of a sentence brings the relevant contexts

Proceedingsofthe3rdWorkshoponRepresentationLearningforNLP,pages41–49Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics41closer. Using the Microsoft Research Sentence
Completion Challenge (Zweig and Burges, 2012),
Gubbins and Vlachos (2013) have showed that in-
corporating syntactic information leads to gram-
matically better options for a semantic text com-
pletion task.

On the other hand, semantic clustering or clas-
siﬁcation (like in ontologies) can be used to derive
predictions on the semantic level. However, when
it comes to the description of daily activities, con-
textual information coming from another modality
would be more beneﬁcial, since linguistic distribu-
tions alone could hardly contribute enough clues
to distinguish the action of washing a pan from
washing a mug, which is a crucial difference for
helper robots.

A popular trick in natural language processing
consists in training a model on one task, and then
apply it to an entirely different one. We adopt
this method by training a multi-modal dependency
parser using noise-free sentences combined with a
description of their visual context. In the second
step, we make use of the trained parser to predict
the best ﬁllers of the gaps (guided by the context
modality).

The paper starts by introducing our multi-modal
approach for the text completion task. In section
3, we present the experimental setup including
the compiled dataset. The implementation is de-
scribed in section 4. Experimental results are pre-
sented and discussed in section 5. Conclusions are
drawn and future directions of research are pointed
out at the end of the paper.

2 A Multi-Modal Approach for a Text

Completion Task

Although closing the gaps in a sentence based only
on a language model is a simple way to tackle
the issue, in extremely ambiguous situations, gap
reconstruction is almost impossible on a purely
unimodal base. In this paper, we work on multi-
modal data that consists of linguistic and context
information. The linguistic part is provided by
natural language sentences that refer to a partic-
ular visual scene. The context information is a
meta-data description of that scene. Per input sen-
tence, the context channel contains a set of context
relations: (argument, relation type, predicate)
where relation type is one of a predeﬁned set of
accepted relations, such as agent or theme while
P redicate and Argument are tokens of the input

sentence. The complexity of the text completion
task is controlled by creating challenging scenes
along the following dimensions:

• Each scene is composed of different compo-

nents (i.e., persons and objects).

• A scene might contain multiple instances of
the same class (i.e., a blue mug (id: mug 1)
and a green mug (id: mug 2).

• The different instances are taking part in var-
ious relations (more details are given in Sec-
tion 2.2).

In a series of experiments, we assess the po-
tential of a context-integrating dependency parser
for correctly solving the text completion task. We
not only try to determine whether we can ﬁll the
gap in the sentence with the correct word but also
whether it is possible to correctly determine the
exact reference to an entity in the context descrip-
tion given the contextual information, in particular
if the linguistic input is noisy and a token of the in-
put sentence is missing. At this stage of research,
we work only on one gap per sentence.

2.1 Context-integrating Dependency Parser
Dependency parsing is an essential NLP task that
determines the syntactic structure of the input sen-
tence in form of a dependency tree. Each token of
the input is represented as a tree node. The tree
consists of the dependency relations between each
word of the sentence and its head word (Nivre,
2004).

The standard input of a parser is a natural lan-
guage sentence. To supply such a parser with ad-
ditional information required for text completion
in a multi-modal environment we have to make it
sensitive to cues from the context.

In our previous research (Salama and Menzel,
2018, 2017), we have introduced a multi-modal
dependency parser adopting the graph-based ap-
proach of Eisner (1996) and Mcdonald and Pereira
(2006). Our model, called RBG-2, extends the
RBG parser (Zhang et al., 2014) by enabling
multi-channel input providing the parsing process
with context information in addition to the nat-
ural language sentence.
Integration is achieved
by combining features from both input channels
during the normal training procedure of the RBG
parser.

422.2 The Data Set
In order to test how the model behaves for differ-
ent linguistic structures, we used the nine different
grammatical templates1 given in Table 1 featur-
ing active/passive voice, PP-attachments, relative
clause (RC) attachments, and conjunctions. They
are combined with several actions performed by
different agents. The dependency structures are
represented in the CONLL-X format. The data set
consists of 429 individual sentences for 20 differ-
ent visual scenes. We performed a 10-fold cross
validation and introduced exactly one gap for ei-
ther a noun, verb or adjective into each test sen-
tence obtaining 1457 test sentences in total.

2.2.1 Linguistic Structures
In this section, we examplify the nine grammtical
templates used in our data-set. The following ex-
amples belong to the scene in Figure 1:

• T1. RC2 Attachment Ambiguity-1

T1A. Active voice in RC. “It is a mug on a vit-
rine that the woman damages.”
Either the relative clause is low-attached (the
woman damages the vitrine) or high-attached
(the woman damages the mug).
T1B. Passive voice in RC.“It is a mug on a vit-
rine that is damaged by the woman.”
• T2. RC Attachment Ambiguity-2

T2A. Active voice. “The woman damages the
vitrine with a mug on it.”
T2B. Passive voice. “The vitrine with a mug on
it is damaged by the woman.”

• T3. RC Attachment Ambiguity with a Geni-

tive Object-3
T3A. Active voice in RC.“The woman removes
the label of the medicine that lies on the shelf ”
T3B. Passive voice in RC. “The label of the
medicine that lies on the shelf is removed by
the woman.”

• T4. Scope Ambiguity

“There are a mug, a candle and books [that
lie/lying] on the vitrine.”

• T5. Simple Imperative sentence

“bring me the mug [that lies/lying] on the vit-
rine [that the woman cleans].”

1inspired by the experimental setup of pyscholinguistic

research

2Relative Clause

• T6. Imperative sentence with modiﬁers

“bring me the blue mug [that lies/lying] on the
vitrine [that the woman cleans].”

2.2.2 Context Representations
The visual information of a picture is represented
in a knowledge base that contains the relation-
ships between objects, characters and actions in
the scene. This information has been manually
annotated as triplets composed of argument, rela-
tion type and predicate. Currently, we consider six
different context relations, namely agent, theme,
location, next-to, part-of/own, as well as property
assignments for color, material, shape etc. (e.g., a
blue mug or a ceramic vase). Figure 1 exempliﬁes
the context annotations of a visual scene with an
additional concept map representation (bottom).
In this scene, the woman is the agent, who per-
forms the cleaning action, the vitrine is the theme,
i.e. the entity undergoing a change of state, caused
by the action. The entire data set and source code
can be accessed from https://github.com/
rekaby/MD-TC.V1.0

For the current study, the pictures, as the one
given in Figure 1, serve illustrative purposes, be-
cause the computational model does only have
access to the manually annotated representations.
An automatic relation extraction is not within the
scope of this study.

Implementation

The different semantic roles are distributed in
the data set as follows; Agent (%13.6), Theme
(%13.6), Location (%33.1), Next to (%9.8), Prop-
erty (%19.5), Own (%10.3). Table 2 presents a
statistics for the amount of contextual information
per scene.
3
RBG-2 parser starts by creating a fully connected
graph representing the input tokens as nodes. The
parser decodes a minimum spanning tree out of
the graph maximizing the aggregated scores of the
arcs. The scores are calculated by combining the
weights of linguistic features and context features
between the pair of tokens as follows:

ωl.f (xi, xj, y)+ωc.(cid:98)f (ci, cj, y)

y = max
y∈T (x,c)

n(cid:88)

i=1

(1)
Where y is the best dependency tree, T (x, c) is a
set of all possible dependency trees for input sen-
tence X and context c. The linguistic feature vec-
tor between node xi and its dependency head xj is

43Types Templates
T1A
T1B
T2A
T2B
T3A
T3B
T4
T5
T6
Total

PRO1nom VP1 NP1acc NP2gen, WDT*acc PRO2nom VP2
PRO1nom VP1 NP1acc NP2gen, WDT*acc PRO2nom VP2
PROnom VP1 NP1nom,pl. NP1nom,pl., WDT acc,pl. VP2 PP1
PROnom VP1 NP1nom,pl. NP1nom,pl., WDT acc,pl. VP2 PP1
NPit−cleft VP1 NP1nom NP2dat, WDT dat PRO3rd-sing. ADV VP2
NPit−cleft VP1 NP1nom NP2dat, WDT dat PRO3rd-sing. ADV VP2
EX Vaux NP1nom (CONJ NP2) WDT*nom VP1 Prep. NP3
VP1 (Prodat NP1 (WDT) VP2 Prep. NP2
VP1 (Prodat (Adj1) NP1 (WDT) VP2 Prep. (ADJ2) NP2

Sentences Gaps Gap Types
41
40
50
50
36
29
58
63
62
429

NP(114), VP(16)
NP(115), VP(15)
NP(153), VP(24)
NP(153), VP(24)
NP(139), VP(36), ADJ(2)
NP(113), VP(30), ADJ(2)
NP(154), VP(0), ADJ(2)
NP(137), VP(1), ADJ(3)
NP(135), VP(9), ADJ(80)

130
130
177
177
177
145
156
141
224
1457 NP(1213), VP(155), ADJ(89)

Table 1: POS templates, the number of sentences and gaps for each sentence types, and the number of
gaps for each POS category

Items

Relations (Min.=28, Max.=41)

Context entities (Min.=30, Max.=41)
Unique entities 5 (Min.=21, Max.=28)

Mean
34.8
35.6
25

Table 2: Complexity of the contextual information
for the visual scenes in the data set

f (xi, xj, y) with weights ωl. The context feature

vector is (cid:98)f (ci, cj, y) with weights ωc.

We build the context features using combi-
nations of the predicates’ and arguments’ POS,
lemma, and word. So far, we only use ﬁrst-order
features for both channels. That means, only in-
formation about immediately connected nodes in
the graph (head-child relationships) is accounted
for, but more complex, indirect connections (sib-
lings, grantchilds, etc.) are ignored. We add the
context features to the graph arc only if the pair of
nodes (words) has a context relation in between.
Using no higher-order features makes the learn-
ing process faster and simpler but introduces some
limitations as discussed in the result section. Each
record in the training data set consists of a com-
plete input sentence, a set of context relations such
as in Figure 1, and an exact context reference for
each sentence token (if it exists in the context de-
scription).

In the testing (text completion) phase, the in-
put sentence is incomplete (containing exactly one
gap) while the context information is the same as
in the training phase except the mapping between
the input sentence and context references is miss-
ing as well. E.g. in the sentence “The vitrine next
to sofa is cleaned by the GAP” accompanying the
scene in Figure 1, we have multiple goals to deter-
mine

Algorithm 1 Text Completion Workﬂow using
RBG-2
TR-L ← Training data (complete sentences).
TR-C ← Training data (context).
TE-L ← Testing data(sentences with gaps).
TE-C ← Testing data (context).
model ← train RBG-2(TR-L, TR-C)
for each pair TE-Li,TE-Ci do
bestF illerScore ← −Inf.
for each component TE-Cij ∈ TE-Ci do
TE-Li ← SetGap(TE-Cij,POSt).
score ← parse(model,TE-Li)
if score > bestF illerScore then
bestF illerScore ← score.
bestF iller ← TE-Cij.
bestP OS ← POSt.

for each POSt ∈ POS tags do

TE-Li ← ﬁllGap( bestFiller,bestPOS).

• the ﬁller word (woman),
• the context ﬁller reference (woman 2),
• the context ﬁller reference for all the other non-
gap tokens in the input (if they exist). They are
{vitrine 2, sofa 1, clean 1},
• the POS tag of the ﬁller (NN).

As shown in Algorithm 1, we train our data-driven
RGB-2 parser on the multi-modal training set de-
scribed above to learn the associations between the
context knowledge representation and the the de-
pendency structures. In the testing phase, we ﬁll
the gap by all the possible context components
and parse the sentence in a multi-modal setup.
We also iterate over different POS tags for the
ﬁller to compare the resulting dependency tree
scores. The best ﬁller (word, context-reference,
and POS) means that this word/context-reference

44Figure 1: The corresponding image4 for the sentences above and the semantic representations of the
actions and relations in the image

is the best matching one that combines two per-
spectives: grammatical correctness and compati-
bility with the context information.

Although the ratio of contextual features to syn-
tactic ones (ﬁrst-order features) is 1:2.3, which is
not high, trying all the possible context elements
is rather expensive. For each sentence, we need to
build G∗ C ∗ P ∗ M dependency trees that have to
be ranked to ﬁnd the best one. Here, G is the num-
ber of gaps (1 in our experiments), C the number
of context entities (35.6 in average), P the number
i=1 Mi, where Mi is
the count of possible candidates references and N
the number of sentence tokens with probable con-
text references.

of PoS tags (3) and M =(cid:81)N

The search space could be reduced by avoiding
irregular combinations of POS and ﬁller words. In
this stage of research, however, we do not prune it
at all.

3.1 Context Data Preprocessing
In a preprocessing phase, we enrich the context in-
formation by inferring new relations from the orig-
inal ones (colored red in Figure 1 and Figure 2).
We have used two kinds of inferred relations:

Location to Agent/Theme: If we have a con-
text relation such as (X, Location, Y ), this might
appear in the linguistic modality in two different
forms having either direct or indirect syntactic de-
pendency. For example, mug and vitrine as in
Figure 2A and 2B have a direct syntactic depen-
dency and context relation respectively. In other
sentence forms as in Figure 2C and 2D, there is no
direct correspondence between the linguistic de-
pendency and the context relation. Contextually,
the two tokens are related through the Location
relation, but syntactically they are daughters of the
same action lie (no direct dependency).
In this
form, the Location relation is presented in the lin-

454 Results

In order to show the effect of contextual informa-
tion and to optimize the performance of the current
model, we carried out several experiments with
different parameters of the model by keeping the
data set constant. We used 18 scenes (386 sen-
tences in average) for training and kept the remain-
ing 2 scenes (146 sentences on average) for test
using a 10-fold cross-validation. In case the gap
can be ﬁlled with more than one reference (< 5%
of our dataset), we consider any possible one of
them as correct. We used ﬁve evaluation metrics
as listed below.

• POS-tag Accuracy
• Filler Word Accuracy
• Exact Filler Identiﬁcation (EFI) Accuracy (i.e

mug 1 in contrast to mug 2)

• Non-gap Identiﬁcation Accuracy, for all the

other tokens in the input sentence.

• Complete Sentence Identiﬁcation Accuracy
• Dependency Tree Accuracy (unlabeled attach-

ment score, UAS)

Table 3 presents the results obtained from dif-
ferent variations of the model described in the
previous section. We test a uni-modal parser
(linguistic-only) only to show that the data set in-
deed is consisting of sentences, where reference
resolution/text completion cannot be achieved on
a purely uni-modal sense. For that purpose, the
contribution of contextual information is turned
off. Because of the uniform structure of the train-
ing dataset, the POS and dependency tree accura-
cies are very high 97.6% and 95.6% respectively.
However, the model’s prediction performance is
drastically low for the gap words; 13.5% for the
ﬁller word and 7.8% for the exact ﬁller identiﬁca-
tion.

As described in Section 3.2, the ﬁrst model
is based on having equal weights (S2C-1to1) for
both syntactic (S) and contextual features (C) and
the weights of original contextual (OC) features to
the inferred features (IC) are kept equal as well
(OC2IC-1to1). Giving equal weights leads to ap-
prox. 83% accuracy in both ﬁller word and ex-
act ﬁller ID predictions, while increasing the in-
ﬂuence of the context resulted in 95% accuracy6.

6The other models with weights > 5 produced almost

similar results.

Figure 2: Location Inferred Agent/Theme Rela-
tions: A and C) dependency trees, B) original con-
text information, D) original and inferred context
relations

guistic modality using the verb to lie, which does
not appear as a predicate in the context descrip-
tion.

To enrich the context repreresentation with in-
formation corresponding directly to the linguis-
tic one, we deﬁne a set of verbs (LV) that have
a location meaning (i.e., lie, stand, hang). From
any location relation (X, Location, Y ), we in-
fer another two relations (X, Agent, LVi) and
(X, T heme, LVi), where LVi ∈ LV and LVi is
a token in the input sentence.

location relations

Location to Next To relations: Given each
pair of
(X, Location, Z)
and (Y, Location, Z) we infer new relation
(X, N ext − T o, Y ), where X, Y, Z ∈ W , W is
the set of the input tokens. The inferred relations
are added to the original list of the context input.
In the rest of this paper, we use (IC) to refer to
the Inferred Context relations and (OC) for the
Original Context relations.

3.2 Model Variations

Varying syntactic/context’s weight ratios (S2C):
In the testing phase, we experiment with different
ratios giving more inﬂuence (weight) to the con-
text relations than to the linguistic ones. We assess
different ratios (1to1,1to5, 1to10, and 1to25).

Original/Inferred

relations’ weight

ratio
(OC2IC): Similar to S2C, in the testing phase, we
give more weight to the original relations than to
the inferred ones by assigning the OC2IC ratio to
5to1.

46Furthermore, giving more weight to the original
relations over the inferred ones resulted in lower
accuracy, therefore OCtoIC-1to1 variation is cho-
sen as the standard for the analysis in this sec-
tion. It is apparent from Table 3, a higher inﬂuence
of the context is beneﬁcial for a correct reference
prediction. However, it should be noted that giv-
ing more weight to contextual features causes the
model to be less sensitive about choosing a cor-
rect dependency tree. A closer look at the differ-
ences between the predictions of the S2C-1to5 and
S2C-1to10 variations showed that 60 instances ei-
ther in the dependency tree or in the ﬁller ID were
observed in the results. While S2C-1to5 builds
51 correct dependency trees and 43 correct refer-
ences, S2C-1to10 chooses the correct dependency
tree in only 12 instances, but even if the depen-
dency tree is wrong, it ﬁlls the gap correctly in 48
out of 60 instances.

95 inaccurate EFI in 73 test sentences were ob-
served. False predictions of the model variations
can be categorized into several groups:

Inferred Relations. 60% of the inaccurate pre-
dictions occurred within this category. As ex-
plained in the Section 3.1, a phrase like “an
entity-1 that lies on an entity-2” can be resolved
due to an inferred relation. However, for sen-
tences containing structures like “an entity-1 that
lie/stand/hang(s) next to an entity-2” with a gap in
a position of entity-2, the model prefers the most
plausible ﬁller that has a location relation (either
original or inferred) with the entity-1 instead of
having a next to relation with it.

cage 1),

A Chain of Relations. This problem arises
when for example there is a chain of location re-
(bird 1,
lations among the entities (7.4%), i.e.
and (cage 1, Location,
Location,
chest 1) with a description “It is a cage on a chest
that the man cleans” with a gap in a chest posi-
tion. While the S2C-1to5 model correctly ﬁlls the
gap, S2C-1to10 chooses bird for the gap position.
Assigning more weight to the context information
leads to similar scores for the various entities of
the chain, which may cause some wrong ﬁller pre-
dictions.

Less represented PP associations. Syntacti-
cally, all prepositions (with, of, on and next to)
have the same PoS tag but semantically they differ.
While preposition of is associated with the own re-
lation, and preposition next to with next-to, there
are two prepositions which are related to the lo-

cation relation; on/in and with. The distribution of
them is as follows; with: 21.3%, and on/in/under7:
78.7%. As shown in Figure 3, the most likely
association between syntactic and contextual fea-
location relations) is head to argu-
tures (w.r.t.
ment and dependent to predicate. This association
is ﬂipped for the prepositional phrase like “entity-
2 with entity-1 on it”. Regardless of giving more
inﬂuence to the context in that case, the model
makes the prediction more strongly biased to the
canonical direction of prepositional phrases result-
ing a wrong text prediction.

A Verb in a Noun Position. This error oc-
curs irrespective of the linguistic structure if more
weight (1to10 or 1to25) is given to the context
(6.3%). As an example, a gap in the shelf posi-
tion in the sentence “There are a cat, a ﬂower and
books on the shelf” is ﬁlled with chase, caused by
the (cat 1, Theme, chase 1) relation. In that case,
a stronger contextual inﬂuence overrides the syn-
tactic form of the PP-attachment, and favors a ref-
erence with the theme relation, which has a consis-
tent syntactic representation; its argument always
points to the predicate. The goal to ﬁnd syntac-
tically correct PP-attachment is overruled by the
more powerful features of the context relations,
and so chase is selected considering that a cat is
the only entity with a theme relation among oth-
ers.

Far-Attachments. Far-attachments of the rela-
tive clauses or prepositional phrases are not that
frequent as short-attachments, yet they are gram-
matically correct and occur in a data set. The re-
sults indicate that giving more inﬂuence to con-
textual information (S2C-1to10 and 1to25) helps
to correctly ﬁll the gap, while a model with lower
weight for the contextual information (S2C-1to1
and -1to5) tends to choose the wrong reference
for the gap position. To illustrate, the sentence
“It is a blanket on a couch that is grasped by the
woman” refers to one instance of a blanket class,
and the context contains two instances: blanket 1
and blanket 2, where blanket 2 is the theme of the
grasp action. When the gap is in the couch po-
sition, S2C-1to5 chooses a dependency tree with
a short attachment of the RC. It attaches the gap
to the action grasp and thus ﬁlls it with blanket 2.
This is consistent with the theme relation in the
context, resulting a sentence “It is a blanket 1 on

7excluding the occurrences of “on/in/under” in the reﬂex-

ive phrases as in “with a mug on it”

47Model Variations

PoS

Filler Word

EFI

Non-Gap Complete DP-UAS

Sentence

Uni-model (linguistic only)

S2C Weight (1to1) + OC2IC Weight (1to1)
S2C Weight (1to5) + OC2IC Weight (1to1)
S2C Weight (1to10) + OC2IC Weight (1to1)
S2C Weight (1to25) + OC2IC Weight (1to1)
S2C Weight (1to5) + OC2IC Weight (5to1)

97.58
98.34
98.89
98.48
98.13
98.82

13.50
83.53
95.36
95.57
95.57
92.39

7.75
83.11
95.22
95.50
95.50
92.32

62.05
97.35
99.24
99.07
99.36
98.85

2.21
83.60
94.67
94.81
94.81
91.76

95.60
95.60
95.11
94.80
94.51
95.05

Table 3: The results of the different model variations

Figure 3: Syntactic/Context feature association for the prepositions on and with

a blanket 2 that is grasped by the woman”8.
If
the context had only one blanket, that instance of
the blanket had to be assigned to a non-gap blan-
ket position in the sentence, and then the model is
forced to switch to another dependency tree with
a lower score but a better alignment. On the other
hand, a 1to10 model gives more inﬂuence to the
context, resulting in a correct completion even if
the dependency structure is wrong. This may indi-
cate that in order to deal with more challenging
contexts or less represented linguistic structures
(like far-attachments) increasing the inﬂuence of
the contextual information would be beneﬁcial.

Contextually Challenging Cases. This cate-
gory covers 10.5% of the errors. To illustrate,
lets consider a context, which contains two differ-
ent roles for the same agent man 1 together with
a sentence like “The handle of the mug on the
counter is hold by the man”; namely an action
wash with a theme relation to a mug and another
action hold with a theme relation to a handle. An-
other relevant relation for this sentence is (mug 1,
Own, handle 1). If in such a case the gap is in the
verb position, the model can choose the alternative
actions associated with a mug instead of forcing a
far-attachment which is also favored contextually.

dependency parser to solve this problem. There
are number of assumptions and constraints of the
current model. First, allowed gaps are only nouns,
verbs and adjectives. Pronouns are not used as a
possible gap ﬁller. Furthermore, each individual
instance is allowed to occur in the sentence once9,
thus a context reference (i.e. mug 1) can not be
assigned to more than one token of the input sen-
tence. Morever, the set of context relations is re-
stricted to the the six relations. Further studies will
need to cover more variety to relax these limita-
tions.

The results indicate that incorporating contex-
tual information and giving a strong enough inﬂu-
ence to them helps to solve a majority of the prob-
lems concerning different sentence structures with
conjunctions, relative clauses or PP-attachments.
There are still some challenging situations origi-
nating from high degrees of linguistic or contex-
tual complexity, which need to be addressed in fu-
ture work. Furthermore, we plan to address noisier
linguistic input with multiple gaps in a sentence as
well as mismatches between the sentence and its
contextual information. We also target reference
resolution at the earliest time possible by employ-
ing incremental processing.

5 Conclusion and Future Directions

Acknowledgments

In this paper, we present a data set for sentence
completion consisting of problematic instances,
which can not be effectively handled using linguis-
tic features alone. We apply a context-integrating

This research was partially funded by the Ger-
man Research Foundation - DFG Transregio SFB
169: Cross-Modal Learning, and German Aca-
demic Exchange Service (DAAD).

8Our assumption is not using same context reference

twice

9This constraint is not based on linguistic phenomena, it

is just the design decision for the current solution

48Yuan Zhang, Tao Lei, Regina Barzilay, Tommi
Steps
Jaakkola, and Amir Globerson. 2014.
Simple inference with reﬁned
to excellence:
In Proceedings
scoring of dependency trees.
of
the Associa-
tion for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, Baltimore, Maryland, pages 197–207.
http://www.aclweb.org/anthology/P14-1019.

the 52nd Annual Meeting of

Geoffrey Zweig and Chris JC Burges. 2012. A chal-
lenge set for advancing language modeling. In Pro-
ceedings of the NAACL-HLT 2012 Workshop: Will
We Ever Really Replace the N-gram Model? On the
Future of Language Modeling for HLT. Association
for Computational Linguistics, pages 29–36.

References
Kavita Asnani, Douglas Vaz, Tanay PrabhuDesai,
Surabhi Borgikar, Megha Bisht, Sharvari Bhosale,
and Nikhil Balaji. 2015. Sentence completion us-
In Proceedings of the
ing text prediction systems.
3rd International Conference on Frontiers of Intelli-
gent Computing: Theory and Applications (FICTA)
2014. Springer, pages 397–404.

Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Learning to complete sentences. In European
Conference on Machine Learning. Springer, pages
497–504.

Jason M. Eisner. 1996.

Three new probabilis-
tic models for dependency parsing: An explo-
In Proceedings of the 16th Conference
ration.
on Computational Linguistics - Volume 1. As-
sociation for Computational Linguistics, Strouds-
burg, PA, USA, COLING ’96, pages 340–345.
https://doi.org/10.3115/992628.992688.

Nestor Garay-Vitoria and Julio Abascal. 2004. A com-
parison of prediction techniques to enhance the com-
munication rate. In ERCIM Workshop on User In-
terfaces for All. Springer, pages 400–417.

Nestor Garay-Vitoria and Julio Abascal. 2006. Text
prediction systems: a survey. Universal Access in
the Information Society 4(3):188–203.

Joseph Gubbins and Andreas Vlachos. 2013. De-
pendency language models for sentence completion.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1405–1410.

Ryan Mcdonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In In Proc. of EACL. pages 81–88.

Piotr Mirowski and Andreas Vlachos. 2015. Depen-
dency recurrent neural language models for sentence
completion. arXiv preprint arXiv:1507.01193 .

Joakim Nivre. 2004.

Incrementality in determin-
In Proceedings of
istic dependency parsing.
the Workshop on Incremental Parsing: Bringing
Engineering and Cognition Together. Associa-
tion for Computational Linguistics, Stroudsburg,
PA, USA,
IncrementParsing ’04, pages 50–57.
http://dl.acm.org/citation.cfm?id=1613148.1613156.

Amr Rekaby Salama and Wolfgang Menzel. 2017.
Multimodal graph-based dependency parsing of nat-
ural language. In Proceedings of the International
Conference on Advanced Intelligent Systems and In-
formatics 2016. Springer International Publishing,
pages 22–31.

Amr Rekaby Salama and Wolfgang Menzel. 2018.
Learning Context-Integration in a Dependency
Parser for Natural Language, Springer International
Publishing, pages 545–569.

49
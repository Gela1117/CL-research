Results of the sixth edition of the BioASQ Challenge

Anastasios Nentidis1,2, Anastasia Krithara1, Konstantinos Bougiatiotis1,

Georgios Paliouras1,3 and Ioannis Kakadiaris3

1National Center for Scientiﬁc Research “Demokritos”, Athens, Greece

2Aristotle University of Thessaloniki, Thessaloniki, Greece

3University of Houston, Texas, USA

Abstract

This paper presents the results of the sixth edi-
tion of the BioASQ challenge. The BioASQ
challenge aims at the promotion of systems
and methodologies through the organization of
a challenge on two tasks: semantic indexing
and question answering.
In total, 26 teams
with more than 90 systems participated in this
year’s challenge. As in previous years, the best
systems were able to outperform the strong
baselines. This suggests that state-of-the-art
systems are continuously improving, pushing
the frontier of research.

1

Introduction

The aim of this paper is twofold. First, we aim
to give an overview of the data issued during the
BioASQ challenge in 2018. In addition, we aim to
present the systems that participated in the chal-
lenge and evaluate their performance. To achieve
these goals, we begin by giving a brief overview of
the tasks, which took place from February to May
2018, and the challenge’s data. Thereafter, we pro-
vide an overview of the systems that participated
in the challenge. Detailed descriptions of some
of the systems are given in workshop proceedings.
The evaluation of the systems, which was carried
out using state-of-the-art measures or manual as-
sessment, is the last focal point of this paper, with
remarks regarding the results of each task. The
conclusions sum up this year’s challenge.

2 Overview of the Tasks

The challenge comprised two tasks: (1) a large-
scale semantic indexing task (Task 6a) and (2) a
question answering task (Task 6b).

2.1 Large-scale semantic indexing - 6a
In Task 6a the goal is to classify documents from
the PubMed digital library into concepts of the

MeSH hierarchy. Here, new PubMed articles that
are not yet annotated by MEDLINE indexers are
collected and used as test sets for the evaluation of
the participating systems. In contrast to previous
years, articles from all journals were included in
the test data sets of task 6a. As soon as the an-
notations are available from the MEDLINE index-
ers, the performance of each system is calculated
using standard ﬂat information retrieval measures,
as well as, hierarchical ones. As in previous years,
an on-line and large-scale scenario was provided,
dividing the task into three independent batches of
5 weekly test sets each. Participants had 21 hours
to provide their answers for each test set. Table
1 shows the number of articles in each test set of
each batch of the challenge. 13,486,072 articles
with 12.69 labels per article, on average, were pro-
vided as training data to the participants.

2.2 Biomedical semantic QA - 6b
The goal of Task 6b was to provide a large-scale
question answering challenge where the systems
had to cope with all stages of a question answer-
ing task for four types of biomedical questions:
yes/no, factoid, list and summary questions (Ba-
likas et al., 2013). As in previous years, the task
comprised two phases: In phase A, BioASQ re-
leased 100 questions and participants were asked
to respond with relevant elements from speciﬁc
resources, including relevant MEDLINE articles,
relevant snippets extracted from the articles, rele-
vant concepts and relevant RDF triples. In phase
B, the released questions were enhanced with rel-
evant articles and snippets selected manually and
the participants had to respond with exact answers,
as well as with summaries in natural language
(dubbed ideal answers). The task was split into
ﬁve independent batches and the two phases for
each batch were run with a time gap of 24 hours.
In each phase, the participants received 100 ques-

Proceedingsofthe2018EMNLPWorkshopBioASQ:Large-scaleBiomedicalSemanticIndexingandQuestionAnswering,pages1–10Brussels,Belgium,November1st,2018.c(cid:13)2018AssociationforComputationalLinguistics1Batch Articles Annotated
Articles

7,240
7,678
10,488
6,225
6,617
38,248
6,239
7,152
7,113
5,833
7,379
33,716
6,469
6,544
6,743
8,487
7,478
35,721

1

Total

2

Total

3

Total

6,639
7,499
10,319
6,073
6,486
37,016
6,118
6,803
6,575
5,412
6,606
31,514
5,768
5,501
5,467
5,615
4,038
26,389

Labels

per

Article
11.67
12.95
13.04
12.32
12.96
12.65
12.51
12.75
12.75
13.00
12.65
12.73
12.58
12.86
12.67
12.70
12.63
12.69

Table 1: Statistics on test datasets for Task 6a.

tions and had 24 hours to submit their answers.
Table 2 presents the statistics of the training and
test data provided to the participants. The evalua-
tion included ﬁve test batches.

Batch
Train
Test 1
Test 2
Test 3
Test 4
Test 5
Total

Size Documents
2,251
100
100
100
100
100
2,751

12.01
4.06
3.77
3.97
3.39
3.94
10.52

Snippets

14.72
6.02
5.03
4.80
4.03
5.07
12.95

Table 2: Statistics on the training and test datasets of
Task 6b. All the numbers for the documents and snip-
pets refer to averages.

3 Overview of Participants
3.1 Task 6a
For this task, 11 teams participated and results
from 42 different systems were submitted. In the
following paragraphs we describe those systems
for which a description was available, stressing
their key characteristics. An overview of the sys-
tems and their approaches can be seen in Table 3.
The “SNOKE” system variants were developed

System

AttentionMeSH

AUTH

DeepMesh

Iria

SNOKE

Approach

RNN, w2v, attention

scheme

d2v, tf-idf, LLDA,
SVM, ensembles

d2v, tf-idf, MESHlabeler
bigrams, Luchene Index,

k-NN, ensembles,

UIMA ConceptMapper
search engine, UIMA

ConceptMapper

Table 3: Systems and approaches for Task 6a. Systems
for which no description was available at the time of
writing are omitted.

as an UIMA (Tanenblatt et al., 2010) text and
data mining workﬂow, combined with a hetero-
geneous database architecture, where different
search strategies were adopted to automatically se-
lect probable MeSH terms. More speciﬁcally, the
system is based on the ZB MED Knowledge Envi-
ronment (M¨uller et al., 2017), while also utilizing
the Snowball Stemmer (Agichtein and Gravano,
2000), to ﬁnd matches between MeSH terms and
words in the title and abstract of each target docu-
ment.

The “AttentionMeSH” systems utilize deep
learning and attention mechanisms which enable
the models to associate textual evidence with an-
notations,
thus providing interpretability at the
word level. Firstly, they use a bidirectional gated
recurrent unit to derive word representations with
contextual information (Cho et al., 2014), to repre-
sent each document. At the same time, all MeSH
terms are embedded using a technique that takes
into account co-occuring MeSH terms in textu-
ally similar articles and ﬁnally an attention ma-
trix (Mullenbach et al., 2018) is created based on
the MeSH and word representations, leading to
MeSH-speciﬁc article representations. This pro-
cedure allows the model to provide local interpre-
tations of the predicted MeSH terms in relation
to words of a speciﬁc article, raising the interest-
ing subject of how explanations of an automatic
MeSH indexer could further help human annota-
tors in this task.

Other participating systems,

including the
“DeepMeSH” systems (Peng et al., 2016),
the
systems of the “AUTH” team (Papagiannopoulou
et al., 2016) and the “Iria” systems (Ribadas-Pena

2et al., 2015) are based on the same techniques used
by theirs systems for the previous version of the
challenge which are summarized in Table 3 and
described in the corresponding challenge overview
(Nentidis et al., 2017). Similarly to the previous
year, two systems developed by the National Li-
brary of Medicine (NLM) to assist the indexers
in the annotation of MEDLINE articles, served
as baselines for the semantic indexing task of the
challenge. The Medical Text Indexer (MTI) (Mork
et al., 2014) with some enchantments introduced
in (Zavorin et al., 2016) and an extension of it, in-
corporating features of the winning system of the
ﬁrst BioASQ challenge (Tsoumakas et al., 2013).

3.2 Task 6b

The question answering task was tackled by 50
different systems, developed by 15 teams. In the
ﬁrst phase, which concerns the retrieval of infor-
mation required to answer a question, 9 teams
with 27 systems participated. In the second phase,
where teams are requested to submit exact and
ideal answers, 10 teams with 27 different systems
participated. Four of the teams participated in
both phases. An overview of the technologies em-
ployed by each team can be seen in Table 4.

The “AUEB” team that participated only in
Phase A, used novel extensions of deep learning
models for retrieving question-relevant documents
and snippets. Firstly, they pre-trained word em-
beddings (Mikolov et al., 2013) on a very large
collection of articles from MEDLINE/PubMed,
while also implementing some pre-processing
steps (stop-word removal, stemming (Krovetz,
1993), tokenization etc.). Then, for the document
retrieval task they focused on the PACRR model
of (Hui et al., 2017) and the DRMM model (Guo
et al., 2016), while for snippets retrieval they uti-
lized the ABCNN model (Yin et al., 2015). Along-
side the extensions made on these models, they
also deployed a clever post-processing scheme
for snippet retrieval, as well as a model for ini-
tial document-retrieval based on BM25 (Robert-
son and Jones, 1976) for efﬁciency purposes.

Another approach based on deep learning
methodologies for Phase A, focusing again on
document and snippet retrieval, was proposed by
the “MindLaB” team from the National University
of Colombia. While for the document retrieval
they use the BM25 model and ElasticSearch for
efﬁciency, they train a Convolutional Neural Net-

Systems

Phase

Approach

Olelo

A, B

AUTH

A, B

AUEB

USTB

MindLab

MQU

Oaqa

LabZhu

UNCC

L2PS

A

A

A

B

B

B

B

B

SRL toolkits (BioKIT,
BioSmile, PathLSTM)
MetaMap, LingPipe,
Lucene Index, Stanford

Parser

BM25, w2v , DL
(PACRR, DRMM,

ABCNN)

Sequential Dependence

Models, Ensembles
ElasticSearch, BM25,
POS-Tags, w2v, DL

(CNN)

DL (LSTM), w2v,
Regression models,

Reinforcement Learning

Maximum Margin

Relevance, w2v, Block

Ordering, ILP

PubTator, Standford
POS tool, ranking
Metamap, Lexical

Chaining

SQUAD, DRQA (RNN,

LSTM), GloVe

Table 4: Systems and approaches for Task 6b. Systems
for which no information was available at the time of
writing are omitted.

work (CNN) for snippet retrieval. As in the previ-
ous approach, they utilized a very large collection
of PubMed Articles to train the CNN with sim-
ilarity matrices of question-answer pairs. More
speciﬁcally,
they deploy similar pre-processing
steps (tokenization, lowercasing, skip-gram em-
beddings (Moen and Ananiadou, 2013)) for the
question and the document texts, however they
also apply Part of Speech tagging to extract syn-
tactical information regarding the terms. Based on
the idea that not all terms are equally informative
(Dong et al., 2015), they deploy a salience weight-
ing scheme focusing on verbs, nouns and adjec-
tives. Another interesting extension is the way ﬁ-
nal rankings of the snippets are generated based on
a pseudo-relevance-feedback re-ranking step (Rie-
zler et al., 2007).

In Phase B, the Macquarie University (“MQU”)
team focused on ideal answers and explored ideas
of reinforcement learning on deep learning mod-

3els. Extending their previous work (Molla, 2017),
they implemented different models under a regres-
sion setting for ﬁnding similar sentences to a ques-
tion, based on the corresponding word2vec em-
beddings of the question-sentence pairs. They
also experimented with different ways of utiliz-
ing these embeddings, notably using a bidirec-
tional Recurrent Neural Networks with LSTM
cells (Hochreiter and Schmidhuber, 1997) to equip
the model with knowledge regarding the sentence
position. Moreover, they also run interesting ex-
periments using reinforcement learning towards
the ROUGE score of the ideal answers, based on
their previous work (Moll´a-Aliod, 2017), but the
results did not advocate for the use of such mod-
els.

on

also

ideal

focused

The Carnegie Mellon University

team
(“OAQA”),
answer
generation, building upon previous versions
of the “OAQA” system (Chandu et al., 2017).
They experimented with ways to improve the
generated answer by extracting the most relevant
non-redundant sentences from multiple docu-
ments and then re-ordering and fusing them to
make the resulting text more human-readable
and coherent. To this end, they tried different
ordering algorithms for sentences and also made
various
stages of
the candidate sentences expansion, fusion and
ﬁltering procedure that was already used by their
model. Among the notable additions is the use of
an Integer Linear Program (ILP) module that is
capable of fusing repeated content and simplify-
ing complicated sentences, thus improving human
readability.

improvements

in different

Another system deployed by the same team
focuses on answer generation using a knowl-
edge graph and a neural
learning-to-rank ap-
proach, combined with different summarization
techniques. One of the novelties introduced is
the creation of an ontology-based retrieval module
for relevant snippets, through the relation extrac-
tion between biomedical entities found in the ab-
stracts’ texts (Abacha and Zweigenbaum, 2015).
Also, different learning-to-rank approaches were
explored (Qin et al., 2010; Cao et al., 2006, 2007)
alongside both extractive (Allahyari et al., 2017)
and abstractive (See et al., 2017) summarization
techniques for the ideal answers generation.

An interesting approach comes

from the
“L2PS” team where they use an open-domain

model (Chen et al., 2017), pre-trained on the
SQUAD (Rajpurkar et al., 2016) dataset, and ﬁne-
tuned to the biomedical domain. An interesting
difference with other deep learning approaches is
the fact that the GloVe embeddings (Pennington
et al., 2014) were the best amongst the ones tried.
Moreover, they raise interesting questions regard-
ing the effects of non-normalized answers (syn-
onyms, abbreviations, multi-word answers) in the
evaluation of different systems.

The “UNCC” team participated in Phase B, de-
ploying lexical chaining techniques (Reeve et al.,
2006) for sentence similarity and ranking to ex-
tract summaries from related snippets and efﬁ-
ciently fuse them in an ideal answer. They take ad-
vantage of the MetaMap tool (Aronson and Lang,
2010) for biomedical entity recognition and they
also present a way to extend their methodology to
factoid/list question answering in Phase A as well.
“Olelo” is one of the approaches that tackles
both phases of the question answering task. More
speciﬁcally,
in Phase A Semantic Role Label-
ing (SRL) approaches for QA systems were uti-
lized. These focus on the automatic extraction
of predicate-argument structures (PAS) from both
questions and document text, aimed at ﬁnding se-
mantically related PAS between associated pairs.
For Phase B, the system is built on top of the SAP
HANA database and uses various NLP compo-
nents, such as question processing, document and
passage retrieval, answer processing and multi-
document summarization based on previous ap-
proaches (Schulze et al., 2016) to develop a com-
prehensive system that retrieves relevant informa-
tion and provides both exact and ideal answers for
biomedical questions.

Other systems, including the “USTB” (Jin et al.,
2017) and the “LabZhu” (Peng et al., 2015) sys-
tems employed the same techniques used by their
systems for the previous version of the challenge,
as summarized in Table 4 and described in the pre-
vious challenge overview (Nentidis et al., 2017).
In this challenge too, the open source OAQA sys-
tem proposed by (Yang et al., 2016) served as
baseline for phase B. The system which achieved
among the highest performances in previous ver-
sions of the challenge remains a strong base-
line for the exact answer generation task. The
system is developed based on the UIMA frame-
work. ClearNLP is employed for question and
snippet parsing. MetaMap, TmTool (Wei et al.,

4System

Batch 1

Batch 2

Batch 3

LCA-F MiF
10

AttentionMeSH
AttentionMeSH2
AttentionMeSH3
AttentionMeSH4
AttentionMeSH5

DeepMeSH1
DeepMeSH2
DeepMeSH3
DeepMeSH4
Default MTI

iria-1
iria-2

MeSHmallow-1
MeSHmallow-2
MeSHmallow-3

MTI First Line Index
Semantic NoSQL KE 1
Semantic NoSQL KE 2
Semantic NoSQL KE 3
Semantic NoSQL KE 4
Semantic NoSQL KE 5
UMass Amherst T2T

xgx
xgx0
xgx1
xgx2
xgx3

MiF

-
-
-
-
-

3.75
1.875
2.625

1

4.875
9.75

-
-
-
-
6
-
-
-
-
-
-
8.5
-
-
-
-

LCA-F MiF
12.75
13.25
11.875
10.625
9.875

-
-
-
-
-

4.75
1.875
2.625

1
3.75
9.75

-
-
-
-
6
-
-
-
-
-
-
8.5
-
-
-
-

4
2
3
1
5
13
-
-
-
-

8.25
16.25
15.75
19.5
17.5
18.5

-

5.75
8.5
-
-
-

13
13.5
12

10.75

11
5
2
3
1
3.75
13
-
-
-
-
7.5
16
17
20
18
19
-
6
7
-
-
-

LCA-F
12.875
11.625
10.625
11.375

11

10.75
7.5
6

7.75
5.25
15.75
18.75

24
24
24
9.75

-
-
-
-
-

19.25

4
2

2.375
3.875
4.25

9.125
8.625
7.375
7.25
9.75
7.25
5.75
7.25
10.5
15.75
18.75

24
24
24

13.75

-
-
-
-
-

19.25
5.25
3.25
4.5
3.5
4.75

Table 5: Average system ranks across the batches of the Task 6a. A hyphenation symbol (-) is used whenever the
system participated in fewer than 4 tests in the batch. Systems with fewer than 4 participations in all batches are
omitted.

2016), C-Value and LingPipe (Baldwin and Car-
penter, 2003) are used for concept identiﬁcation
and UMLS Terminology Services (UTS) for con-
cept retrieval. The ﬁnal steps include identiﬁcation
of concept, document and snippet relevance, based
on classiﬁer components and scoring, ranking and
reranking techniques.

4 Results
4.1 Task 6a
Each of the three batches of Task 6a were eval-
uated independently. The classiﬁcation perfor-
mance of the systems were measured using ﬂat
and hierarchical evaluation measures (Balikas
et al., 2013). The micro F-measure (MiF) and
the Lowest Common Ancestor F-measure (LCA-
F) were used to choose the winners for each batch
(Kosmopoulos et al., 2013).

According to (Demsar, 2006) the appropriate
way to compare multiple classiﬁcation systems
over multiple datasets is based on their average
rank across all the datasets. On each dataset the
system with the best performance gets rank 1.0,
the second best rank 2.0 and so on. In case two

or more systems tie, they all receive the average
rank. Table 5 presents the average rank (according
to MiF and LCA-F) of each system over all the test
sets for the corresponding batches. Note, that the
average ranks are calculated for the 4 best results
of each system in the batch according to the rules
of the challenge.

The results in Task 6a show that in all test
batches and for both ﬂat and hierarchical mea-
sures, some systems outperform the strong base-
lines. The “DeepMeSH” systems achieve the best
performance in the ﬁrst two batches, outperformed
only by “xgx” systems in the third batch. More
detailed results can be found in the online results
page1. Comparison of these results with corre-
sponding system results from previous years re-
veals the improvement of both the baseline and
the top performing systems through the years of
the competition as shown in Figure 1.

5Figure 1: The micro f-measure achieved by systems across different years of the BioASQ challenge. For each test
set the micro F-measure is presented for the best performing system (Top) and the MTI, as well as the average
micro f-measure of all the participating systems (Avg).

System

aueb-nlp-5
MindLab QA

Reloaded
aueb-nlp-1
aueb-nlp-3
aueb-nlp-4
aueb-nlp-2
MindLab QA

System

MindLab Red

Lions++

MindLab QA
System ++

testtext

Mean

Precision
0.2551

0.1614

0.1384
0.1341
0.1325
0.1308

0.1542

0.1406

0.1325

0.1802

Mean Recall

0.3412

0.2657

0.2288
0.2263
0.2252
0.2204

0.2754

0.2346

0.2252

0.2331

Mean

F-measure

0.2744

0.1877

0.1563
0.1526
0.1519
0.1494

0.1833

0.1636

0.1559

0.1831

MAP
0.2314

0.1344

0.1331
0.1294
0.1293
0.1262

0.1156

0.1150

0.1148

0.1124

GMAP
0.0068

0.0014

0.0046
0.0038
0.0038
0.0034

0.0023

0.0013

0.0001

0.0035

Table 6: Results for snippet retrieval in batch 3 of phase A of Task 6b. Only the top-10 systems are presented.

4.2 Task 6b
Phase A: For phase A and for each of the four
types of annotations: documents, concepts, snip-
pets and RDF triples, we rank the systems accord-
ing to the Mean Average Precision (MAP) mea-
sure. The ﬁnal ranking for each batch is calculated
as the average of the individual rankings in the dif-
ferent categories. In Tables 6 and 7 some indica-
tive results from batch 3 are presented. Full results

are available in the online results page of Task 6b,
phase A2. These results are preliminary. The ﬁnal
results for Task 6b, phase A will be available after
the manual assessment of the system responses.

Phase B: In phase B of Task 6b the systems
were asked to produce exact and ideal answers.
For ideal answers,
the systems will eventually
be ranked according to manual evaluation by the
BioASQ experts (Balikas et al., 2013). Regarding

1http://participants-area.bioasq.org/

2http://participants-area.bioasq.org/

results/6a/

results/6b/phaseA/

6System

ustb prir2
ustb prir3
testtext
ustb prir4
ustb prir1
aueb-nlp-2
aueb-nlp-4
aueb-nlp-3
aueb-nlp-1
sdm/rerank

Mean

Precision
0.1660
0.2007
0.2007
0.1620
0.1700
0.1877
0.1877
0.1877
0.1877
0.1810

Mean Recall

0.5674
0.5609
0.5609
0.5601
0.5559
0.5352
0.5399
0.5429
0.5399
0.5422

Mean

F-measure

0.2186
0.2496
0.2496
0.2136
0.2203
0.2345
0.2345
0.2350
0.2345
0.2301

MAP
0.1281
0.1259
0.1254
0.1253
0.1217
0.1147
0.1137
0.1135
0.1122
0.1061

GMAP
0.0113
0.0106
0.0106
0.0105
0.0100
0.0108
0.0106
0.0109
0.0101
0.0087

Table 7: Results for document retrieval in batch 3 of phase A of Task 6b. Only the top-10 systems are presented.

System

Oaqa-5b

fa2
fa4
fa1
fa3

Lab Zhu ,FDU

MQ-1
MQ-2
MQ-3
MQ-4
MQ-5
fa5

Lab Zhu,FDU
LabZhu,FDU

F1

Yes/No

Str.
Acc.

Factoid
Len.
Acc.
Acc.
0.6667 0.6592 0.0606 0.2121
0.6296 0.3864 0.2121 0.3030
0.6296 0.3864 0.2121 0.3030
0.6296 0.3864 0.2121 0.2727
0.6296 0.3864 0.2121 0.2727
0.6296 0.3864 0.0909 0.1212
0.6296 0.3864
0.6296 0.3864
0.6296 0.3864
0.6296 0.3864
0.6296 0.3864
0.6296 0.5559 0.2121 0.3030
0.6296 0.3864 0.2121 0.2424
0.6296 0.3864 0.2424 0.2424
0.0606 0.1212

-
-
-
-
-

-
-
-
-
-

List

MRR

0.1313
0.2475
0.2434
0.2374
0.2283
0.1061

Prec.

Rec.

F1

0.0867 0.2722
0.2511 0.3889
0.2800 0.3889
0.1600 0.4333
0.1800 0.4778
0.1657 0.2833

0.1299
0.2955
0.3131
0.2290
0.2564
0.1663

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

-
-
-
-
-

0.2434
0.2273
0.2424
0.0859

0.2800 0.3889
0.2944 0.3444
0.4130 0.3389
0.1774 0.3944

0.3131
0.2934
0.3312
0.2236

BioASQ Baseline 0.4815 0.475

Table 8: Results for batch 4 for exact answers in phase B of Task 6b.

exact answers3, the systems were ranked accord-
ing to accuracy, F1 score on prediction of yes an-
swer, F1 on prediction of no and macro-averaged
F1 score for the yes/no questions, mean reciprocal
rank (MRR) for the factoids and mean F-measure
for the list questions. Table 8 shows the results for
exact answers for the fourth batch of Task 6b. The
symbol (-) is used when systems don’t provide ex-
act answers for a particular type of question. The
full results of phase B of Task 6b are available on-
line4. These results are preliminary. The ﬁnal re-
sults for Task 6b, phase B will be available after

3For summary questions, no exact answers are required
4http://participants-area.bioasq.org/

results/6b/phaseB/

the manual assessment of the system responses.

The results presented in Table 8 show that eval-
uation of system performance in the yes/no ques-
tions using the macro averaged F1 measure this
year is useful to identify systems that achieve good
performance regardless of any dataset imbalance
in the yes-no classes. In batch 4 for example, two
systems outperformed the strong baseline based
on previous versions of the OAQA system, which
is not clear considering only the accuracy. Re-
garding factoid and list questions, the performance
achieved by the systems indicates that there is
even more room for improvement in these types
of question.

75 Conclusions

In this paper, an overview of the sixth BioASQ
challenge is presented. The challenge consisted
of two tasks:
semantic indexing and question
answering. Overall, as in previous years,
the
best systems were able to outperform the strong
baselines provided by the organizers. This sug-
gests that advances over the state of the art were
achieved through the BioASQ challenge but also
that the benchmark in itself is challenging. More-
over, a clear shift towards the use of systems that
incorporate ideas based on deep learning mod-
els can be seen, with respect to previous years.
Novel ideas have been tested and state-of-the-art
deep learning methodologies have been adapted to
biomedical question answering with great results.
Consequently, we believe that the challenge is suc-
cessfully pushing the research frontier in biomed-
ical information systems. In future editions of the
challenge, we aim to provide even more bench-
mark data derived from a community-driven ac-
quisition process.

Acknowledgments

The sixth edition of BioASQ is supported by a
conference grant from the NIH/NLM (number
1R13LM012214-01) and sponsored by the Aty-
pon Systems inc. BioASQ is grateful to NLM for
providing baselines for task 6a and the CMU team
for providing the baselines for task 6b. Finally,
we would also like to thank all teams for their
participation.

References
Asma Ben Abacha and Pierre Zweigenbaum. 2015.
Means: A medical question-answering system com-
bining nlp techniques and semantic web tech-
Information processing & management,
nologies.
51(5):570–594.

Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
In Proceedings of the Fifth ACM Con-
lections.
ference on Digital Libraries, DL ’00, pages 85–94,
New York, NY, USA. ACM.

Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Asseﬁ,
Saeid Safaei, Elizabeth D Trippe, Juan B Gutier-
Text summariza-
rez, and Krys Kochut. 2017.
arXiv preprint
tion techniques: a brief survey.
arXiv:1707.02268.

Alan R. Aronson and Franois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association, 17:229–236.

Breck Baldwin and Bob Carpenter. 2003.

Ling-
pipe. Available from World Wide Web: http://alias-i.
com/lingpipe.

Georgios Balikas, Ioannis Partalas, Aris Kosmopoulos,
Sergios Petridis, Prodromos Malakasiotis, Ioannis
Pavlopoulos, Ion Androutsopoulos, Nicolas Baskio-
tis, Eric Gaussier, Thierry Artieres, and Patrick Gal-
linari. 2013. Evaluation framework speciﬁcations.
Project deliverable D4.1, UPMC.

Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting rank-
In Proceedings of
ing svm to document retrieval.
the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 186–193. ACM.

Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise ap-
proach to listwise approach. In Proceedings of the
24th international conference on Machine learning,
pages 129–136. ACM.

Khyathi Chandu, Aakanksha Naik, Aditya Chan-
drasekar, Zi Yang, Niloy Gupta, and Eric Nyberg.
2017.
Tackling biomedical text summarization:
Oaqa at bioasq 5b. BioNLP 2017, pages 58–66.

Danqi Chen, Adam Fisch, Jason Weston, and An-
Reading wikipedia to an-
arXiv preprint

toine Bordes. 2017.
swer open-domain questions.
arXiv:1704.00051.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078.

Janez Demsar. 2006. Statistical comparisons of clas-
siﬁers over multiple data sets. Journal of Machine
Learning Research, 7:1–30.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015. Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), volume 1, pages
260–269.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce
Croft. 2016. A deep relevance matching model
In Proceedings of the 25th
for ad-hoc retrieval.
ACM International on Conference on Information
and Knowledge Management, pages 55–64. ACM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

8Kai Hui, Andrew Yates, Klaus Berberich, and Ger-
ard de Melo. 2017. Pacrr: A position-aware neu-
ral ir model for relevance matching. arXiv preprint
arXiv:1704.03940.

Zan-Xia Jin, Bo-Wen Zhang, Fan Fang, Le-Le Zhang,
and Xu-Cheng Yin. 2017. A multi-strategy query
processing approach for biomedical question an-
swering: Ustb prir at bioasq 2017 task 5b. BioNLP
2017, pages 373–380.

Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier,
Georgios Paliouras, and Ion Androutsopoulos. 2013.
Evaluation Measures for Hierarchical Classiﬁca-
tion: a uniﬁed view and novel approaches. CoRR,
abs/1306.6802.

Robert Krovetz. 1993. Viewing morphology as an in-
In Proceedings of the 16th an-
ference process.
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 191–202. ACM.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

SPFGH Moen and Tapio Salakoski2 Sophia Anani-
adou. 2013. Distributional semantics resources for
In Proceedings of the
biomedical text processing.
5th International Symposium on Languages in Biol-
ogy and Medicine, Tokyo, Japan, pages 39–43.

Diego Molla. 2017. Macquarie university at bioasq 5b
query-based summarisation techniques for selecting
the ideal answers. In Proceedings BioNLP 2017.

Diego Moll´a-Aliod. 2017. Towards the use of deep re-
inforcement learning with global policy for query-
based extractive summarisation. In Proceedings of
the Australasian Language Technology Association
Workshop 2017, pages 103–107.

James G. Mork, Dina Demner-Fushman, Susan C.
Schmidt, and Alan R. Aronson. 2014. Recent en-
hancements to the nlm medical text indexer. In Pro-
ceedings of Question Answering Lab at CLEF.

James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng
Sun, and Jacob Eisenstein. 2018. Explainable pre-
diction of medical codes from clinical text. CoRR,
abs/1802.05695.

Bernd M¨uller, Christoph Poley, Jana P¨ossel, Alexandra
Hagelstein, and Thomas G¨ubitz. 2017. Livivo – the
vertical search engine for life sciences. Datenbank-
Spektrum, 17(1):29–34.

Anastasios Nentidis, Konstantinos Bougiatiotis, Anas-
tasia Krithara, Georgios Paliouras, and Ioannis
Kakadiaris. 2017. Results of the ﬁfth edition of the
bioasq challenge. BioNLP 2017, pages 48–57.

E Papagiannopoulou, Y Papanikolaou, D Dimitriadis,
S Lagopoulos, G Tsoumakas, M Laliotis, N Markan-
tonatos, and I Vlahavas. 2016. Large-scale seman-
tic indexing and question answering in biomedicine.
ACL 2016, page 50.

Shengwen Peng, Ronghui You, Hongning Wang,
Chengxiang Zhai, Hiroshi Mamitsuka, and Shan-
feng Zhu. 2016. Deepmesh: deep semantic repre-
sentation for improving large-scale mesh indexing.
Bioinformatics, 32(12):i70–i79.

Shengwen Peng, Ronghui You, Zhikai Xie, Yanchun
Zhang, and Shanfeng Zhu. 2015. The fudan par-
ticipation in the 2015 bioasq challenge: Large-scale
biomedical semantic indexing and question answer-
ing. In CEUR Workshop Proceedings, volume 1391.
CEUR Workshop Proceedings.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010.
Letor: A benchmark collection for research on learn-
Information
ing to rank for information retrieval.
Retrieval, 13(4):346–374.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Lawrence Reeve, Hyoil Han, and Ari D Brooks. 2006.
Biochain: lexical chaining methods for biomedical
In Proceedings of the 2006
text summarization.
ACM symposium on Applied computing, pages 180–
184. ACM.

Francisco J. Ribadas-Pena, Luis M. de Campos, V´ıctor
Manuel Darriba Bilbao, and Alfonso E. Romero.
2015. Cole and UTAI at bioasq 2015: Experi-
ments with similarity based descriptor assignment.
In Working Notes of CLEF 2015 - Conference and
Labs of the Evaluation forum, Toulouse, France,
September 8-11, 2015.

Stefan Riezler, Alexander Vasserman,

Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 464–471.

Stephen E Robertson and K Sparck Jones. 1976.
Journal
Relevance weighting of search terms.
of the American Society for Information science,
27(3):129–146.

Frederik Schulze, Ricarda Sch¨uler, Tim Draeger,
Daniel Dummer, Alexander Ernst, Pedro Flemming,
Cindy Perscheid, and Mariana Neves. 2016. Hpi

9question answering system in bioasq 2016. In Pro-
ceedings of the Fourth BioASQ workshop at the Con-
ference of the Association for Computational Lin-
guistics, pages 38–44.

Abigail See, Peter J Liu, and Christopher D Man-
to the point: Summarization
arXiv preprint

ning. 2017. Get
with pointer-generator networks.
arXiv:1704.04368.

Michael A Tanenblatt, Anni Coden, and Igor L Somin-
sky. 2010. The conceptmapper approach to named
entity recognition. In LREC.

Grigorios Tsoumakas, Manos Laliotis, Nikos Markon-
tanatos, and Ioannis Vlahavas. 2013. Large-Scale
Semantic Indexing of Biomedical Publications.
In
1st BioASQ Workshop: A challenge on large-scale
biomedical semantic indexing and question answer-
ing.

Chih-Hsuan Wei, Robert Leaman, and Zhiyong Lu.
2016. Beyond accuracy: creating interoperable and
scalable text-mining web services. Bioinformatics
(Oxford, England), 32(12):1907–10.

Zi Yang, Yue Zhou, and Nyberg Eric. 2016. Learning
to answer biomedical questions: Oaqa at bioasq 4b.
ACL 2016, page 23.

Wenpeng Yin, Hinrich Sch¨utze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
arXiv preprint arXiv:1512.05193.

Ilya Zavorin, James G Mork, and Dina Demner-
Fushman. 2016. Using learning-to-rank to enhance
nlm medical text indexer results. ACL 2016, page 8.

10
Automatic Opinion Question Generation

Yllias Chali

University of Lethbridge
4401 University Drive

Lethbridge, Alberta, T1K 3M4

chali@cs.uleth.ca

Tina Baghaee

University of Lethbridge
4401 University Drive

Lethbridge, Alberta, T1K 3M4
tina.baghaee@gmail.com

Abstract

We study the problem of opinion question
generation from sentences with the help
of community-based question answering
systems. For this purpose, we use a se-
quence to sequence attentional model, and
we adopt coverage mechanism to prevent
sentences from repeating themselves. Ex-
perimental results on the Amazon ques-
tion/answer dataset show an improvement
in automatic evaluation metrics as well as
human evaluations from the state-of-the-
art question generation systems.

Introduction

1
Question generation (QG) can be considered as a
task which affects many aspects of people’s lives.
One of the main signiﬁcance of the question gen-
eration is its capability to improve one’s learning
ability. Studies have shown that asking questions
can help students realize their knowledge deﬁcits
and encourages them to look for information to
compensate for those deﬁcits (Graesser and Per-
son, 1994). Additionally, QG can be used as an aid
to search engines by providing suggestions regard-
ing the users’ queries (Chali and Hasan, 2015).
This way, the users can either choose one of those
suggestions or obtain a better idea on how to mod-
ify their query to get better results. Moreover, QG
can assist the reading comprehension task and the
question answering community by providing a ro-
bust input for their systems (Serban et al., 2016;
Rajpurkar et al., 2016; Yang et al., 2017).

In this work, we propose a sequence to sequence
model that uses attention and coverage mecha-
nisms for addressing the question generation prob-
lem at the sentence level. The attention and cover-
age mechanisms prevent language generation sys-
tems from generating the same word over and over

again, and have been shown to improve a system’s
output (See et al., 2017).

We beneﬁt from the community-based question
answering systems. Speciﬁcally, we use the Ama-
zon question/answer dataset (McAuley and Yang,
2016). The sentences are mostly informal and
sometimes do not follow the correct grammatical
structure. We utilize the answers that people post
on the community question answering system as
inputs to our model; hence, proposing an opinion
question generation system which could be used
as an interface to online forums helping users in
browsing and querying them by making questions
as suggestions.

In the subsequent section, we describe the re-
lated works to QG. The next section is on the task
deﬁnition, followed by the demonstration of the
model structure. After that, we discuss the exper-
imental settings and at the end provide a thorough
discussion of our results.

2 Related Work

After the ﬁrst question generation shared task eval-
uation challenge (Rus et al., 2010),
the ques-
tion generation task has received a huge atten-
tion from the natural language generation commu-
nity. Many of the traditional approaches involve
human resources to create robust templates and
then employing them to generate questions. For
instance, Heilman and Smith (2010) approach is
to overgenerate questions by some hand-written
rules and then rank them using a logistic regres-
sion model. Labutov et al. (2015) beneﬁt from
a low-dimensional ontology for document seg-
ments. They crowdsource a set of promising ques-
tion templates that are matched with that repre-
sentation and rank the results based on their rel-
evance to the source. Lindberg et al. (2013) em-
ployed a template-based approach while taking

ProceedingsofThe11thInternationalNaturalLanguageGenerationConference,pages152–158,Tilburg,TheNetherlands,November5-8,2018.c(cid:13)2018AssociationforComputationalLinguistics152advantage of semantic information to generate nat-
ural language questions for on-line learning sup-
port. Chali and Hasan (2015) consider the auto-
matic generation of all possible questions from a
topic of interest by exploiting the named entity in-
formation and the predicate argument structures of
the sentences.

Lately, more approaches have been presented
that utilize the neural encoder-decoder architec-
ture. Serban et al. (2016) address the problem by
transducing knowledge graph facts into questions.
They created a factoid question and answer corpus
by using the Recurrent Neural Network architec-
ture.

QG can also be combined with its complemen-
tary task, Question Answering (QA) for further
improvement. Tang et al. (2017) consider QG and
QA as dual tasks and train their relative models
simultaneously. Their training framework takes
advantage of the probabilistic correlation between
the two tasks. QG has also entered other com-
munities such as computer vision. Mostafazadeh
et al. (2016) introduced the visual question gener-
ation task where the goal of the system is to create
a question given an image.

One of the latest studies on the QG task has been
conducted by Du et al. (2017). Their task is a QG
on both sentences and paragraphs for the reading
comprehension task, and they adopt an attention-
based sequence learning model. Another recent
work is by Yuan et al. (2017), they generate ques-
tions from documents using supervised and rein-
forcement learning.

In our work, we generate questions using com-
munity questions and answers and apply the
encoder-decoder structure. To boost the perfor-
mance of our system, we use attention and cover-
age mechanisms as suggested in See et al. (2017).

3 Task Formulation
Given an answer A = (a1, a2, ..., aN ), we are go-
ing to generate a natural language question Q =
(q1, q2, ..., qM ), where its answer is embedded
in A. Our goal is to ﬁnd Q such that the con-
ditional probability p(Q|A) is maximized. We
model p(Q|A) as a product of word predictions:

p(Q|A) =

p(qt|q1:t−1, A)

1

This indicates that the probability of each qt re-
lies on the previously generated words and the in-

M(cid:89)

put sentence A.

4 Model Structure
For modeling p(Q|A), we use the simple RNN
encoder-decoder architecture (Cho et al., 2014)
with the global attentional model (Luong et al.,
2015), which lets the decoder learn to focus on
a particular range of the input sequence during
the generation task. To improve upon this model,
we apply coverage mechanism (See et al., 2017),
which prevents the word repetition problem.

4.1 Encoder
An encoder network maps an input sequence into
word vectors and then converts them into hidden
states b1, ..., bN . In our case, the encoder is a two
layer bidirectional LSTM network (Hochreiter and
Schmidhuber, 1997). We concatenate the output
−→
of the forward hidden states
bj and the backward
←−
←−
bj ] for input
bj , namely, bj = [
hidden states
token j. This bj is used later by the decoder to
calculate the context vector ct, which stores the
relevant source-side information and simpliﬁes the
prediction of the next target word. ct is computed
as a weighted sum of bi:

−→
bj ;

ct =

at(i)bi

(1)

where at is an alignment vector and is calculated
according to the general attention model:

(cid:80)

at(i) =

exp(hT
j exp(hT

t Wabi)
t Wabj))

(2)

To initialize the decoder’s hidden state, we con-
catenate the hidden states of the forward and the
backward pass of the encoder.

4.2 Decoder
The decoder is a two layer unidirectional LSTM.
It keeps a coverage vector s, which is the sum of
the previous alignment vectors:

t−1(cid:88)

t(cid:48)=0

at(cid:48)

st =

It shows how much coverage each input word
has received from the attention mechanism so far
and it helps the mechanism to avoid attending to
the same words again once they have been at-
tended to initially (See et al., 2017). It should be
mentioned that s0 is a zero vector since nothing

N(cid:88)

i=1

153has been covered on the ﬁrst time step. This cov-
erage vector will be added to the source hidden
states bi:

bi = tanh(bi + wsst(i))

This bi will be substituted in equations (1) and
(2) where ws is a parameter to be learned. This
way, with the help of st, the attention mechanism
always has a memory of its past decisions.

The decoder predicts the next word qt given the
context vector ct and all the previously predicted
words {q1, ..., qt−1}. We use a softmax layer to
produce the predictive distribution:

p(qt|q1:t−1, A) = sof tmax(Ws(cid:101)ht)

(cid:101)ht is the attentional hidden state which is cal-

culated given the target hidden state ht and the
source context vector ct:

(cid:101)ht = tanh(Wc[ct; ht])

where Ws and Wc are learnable parameters.
The hidden state at time step t of the decoder is
generated by:

ht = LST M (qt−1, ht−1)

where qt−1 is the previously generated word and
ht−1 is the former hidden state.

Moreover, we use the input feeding approach
(Luong et al., 2015), which informs the decoder
which words were considered for the past align-
ments. We do this by concatenating the attentional

hidden state (cid:101)ht with the inputs at the next time

steps.

4.3 Training and Generation
The training objective is to minimize the negative
log-likelihood of the training corpus. Considering
S = {(ai, qi)}|S|
as our whole training data, we
deﬁne the objective as:

1

Jt =

− log p(qi|ai)

(3)

i=1

In addition to this primary loss function, it is re-
quired to introduce a coverage loss to penalize an
overlap between the coverage vector and the at-
tention distribution, which means attending to the
same location multiple times.

|S|(cid:88)

(cid:88)

i

covlosst =

min(at(i), st(i))

After being reweigted by some hyperparame-

ter λ, this amount is added to equation (3):

|S|(cid:88)

i=1

Jt =

− log p(qi|ai) + λcovlosst

In the generation step, we utilize the beam
search for the inference to maximize the condi-
tional probability.

Since the size of our vocabulary is limited to a
small number, many unknown words (UNK) will
be generated during the inference. We substitute
the (UNK) tokens with the words with the highest
attention weight from the source sentence.

5 Experiments

5.1 Dataset
We use the Amazon question/answer dataset
(McAuley and Yang, 2016). We set the minimum
length of the questions to 4 tokens, including the
question mark to ﬁlter out poorly structured sen-
tences. The answers must be at least 10 tokens
long. Moreover, we set the maximum length of
the questions and the answers to 20 and 35 to-
kens, respectively. As there are many URLs in the
dataset, we replace them with a URL token to re-
duce the vocabulary size. We lower-case the entire
dataset and use the NLTK toolkit 1 for sentence
tokenization. There can be many examples where
the questions are not grammatically correct. Peo-
ple may just ask: “Waterproof ?”. The same prob-
lem occurs with the answers: the answer might be
a single “Yes”. We use 80% of the dataset as the
training set, and the rest is divided between the
validation set and the test set. Table 1 shows the
total number of examples in each dataset after re-
moving very long or very short sentences from the
training and the validation datasets.

Train
233729

Validation

28969

Test
70648

# pairs

Table 1: Statistics of the dataset

1http://www.nltk.org

1545.2 Experimental Setting
Our base model is from OpenNMT system (Klein
et al., 2017), and we use the PyTorch 2 library, a
deep learning framework that provides maximum
ﬂexibility and speed. It accelerates the computa-
tion on both CPU and GPU by a great amount,
and the memory usage is extremely efﬁcient in
PyTorch compared to other options. We ﬁx the
size of the answer and the question vocabularies to
50k. Only the most frequent words are kept, and
the rest are replaced with the UNK token. We set
the word embedding dimension to 300 and we use
glove.840B.300d (Pennington et al., 2014) as the
pre-trained word embedding on both the encoder
and the decoder sides. These embeddings are up-
dated during training. The LSTM hidden unit size
is set to 600 and we set the number of layers to 2.
We employ the stochastic gradient descent (SGD)
as the optimization method with an initial learn-
ing rate of 1.0 and halve the learning rate after 10
epochs. The training continues for 20 epochs with
the batch size of 64 and dropout probability of 0.3.
The hyperparameter λ that is used for weighting
the coverage loss is set to 13. The decoding is done
using the beam search with the beam size of 5, and
the generation is stopped when we reach the EOS
token. In the end, we choose the model with the
lowest perplexity on the validation set.

5.3 Baseline
We compare our model4 to that of Du et al.
(2017). We only experiment with their sentence-
level model and run the same Amazon question
and answer dataset on the system provided by the
ﬁrst author. We keep the source and target vocab-
ulary size the same as ours, (i.e., 50k) and set the
maximum and the minimum length of the ques-
tions and answers the same as our model. Every-
thing else is left to the default values.

5.4 Automatic Evaluation Metrics
For evaluating our system automatically, we use
three different evaluation metrics. The ﬁrst one
is BLEU (Papineni et al., 2002) that uses the n-
gram similarity between a prediction and a set
of references. We calculate BLEU score for un-
igrams and bigrams. The next one is METEOR

2http://pytorch.org
3We also experimented with λ = 2 but did not ﬁnd it to

be helpful.

4https://github.com/Tina-19/Question-Generation

(Denkowski and Lavie, 2014), which scores pre-
dictions by aligning them to ground truth sen-
tences with the help of stemming, synonyms and
paraphrases. The last evaluation metric is Rouge
(Lin, 2004). It compares the generated sentences
with the references based on n-gram. For this task,
we use ROUGEL, which reports the results based
on the longest common subsequence. We use the
evaluation package by Chen et al. (2015).

6 Results and Discussion

Table 2 shows the results of our system and the
baseline. Our model improves the BLEU 1 score
by at least 1.5 points.
It also achieves a bet-
ter result regarding the BLEU 2 and the ME-
TEOR whereas the ROUGE is lower than the base-
line.
If we consider the results reported in Du
et al. (2017), we notice that the BLEU scores
are much higher compared to our work. The
reason is that they use the SQuAD dataset (Ra-
jpurkar et al., 2016), which is a human-generated
corpus. The sentences are well-structured, gram-
matically correct with fewer unnecessary punctu-
ation and colloquialism. However, when work-
ing with the community-based question answer-
ing systems, the structure of sentences do not al-
ways follow the correct path. These sentences of-
ten contain useless information and symbols.

BLEU 1
BLEU 2
METEOR
ROUGEL

Baseline Our Model
12.89
6.95
8.76
25.91

14.67
7.74
9.43
25.21

Table 2: BLEU 1-2, METEOR and ROUGEL
scores on the test set. Bold numbers demonstrate
the best performing system for each evaluation
metric.

Another problem is that multiple questions can
be generated from a single sentence. The system
may generate a question which is correct both se-
mantically and grammatically and also asks about
accurate information in the sentence. However, if
it is not the same as the ground-truth, the results
will be affected.

Figure 1 shows some examples generated by
our system and Du et al. (2017), where the cov-
erage mechanism becomes useful and prevents the
model from generating the same word ’material’
twice.

155I really don’t know, I did full size

Answer 1:
cupcakes, mini ones it would hold a ton!
GT Question: How many mini-cupcakes will
this hold?
DSC: what size is it?
Ours: how many cupcakes will it hold?
Answer 2: Nothing out of the ordinary. just a
simple screw driver. if I recall correctly, I think it
may have came with the tools needed to assem-
ble. good luck and congratulations
GT Question: What tools are required to assem-
ble unit?
DSC: What is the assembly required?
Ours: what tools do I need to assemble this?
Answer 3: You can deﬁnitely still do pushups
with the wraps on. The wraps just give extra sup-
port, they really don’t impact your range of mo-
tion at all.
GT Question: Can I do pushups while wearing
these wraps, or is the material too stiff?
DSC: Can you still use the material while wear-
ing the material?
Ours: Can I do pushups while wearing these
wraps?
Answer 4: I would go with a medium it ﬁts well
and when you adjust it with the helmet it’s tight
to the chin.
GT Question: What size to buy for 14 yr old
125lb and 5’5?
DSC: I’m a woman with a small head, what size
should I get?
Ours: What size should I get for a child who is
5’6”?
Answer 5: There’s the ability to forward the
bp measurement information via email to friends,
family and doctors so I assume that once it’s been
sent an email you can print - it however I haven’t
tested this functionality yet. At the very least
when you bring up the bp readings on your screen
you can do a screen capture and then print that
screen capture.
GT Question: Is it possible to print the BP read-
ings?
DSC: What is the difference between the BP and
the BP?
Ours: How do you print from the BP?

Figure 1: Examples of generated questions:
ground truth (GT), Du et al. (2017) (DSC) and our
model, with their answers.

7 Human Evaluations

To further assess the performance of our system,
we performed human evaluations on the results.
Three English-speaker students were asked to give
a score from 1 (very poor) to 5 (very good) to the
questions generated from both systems according
to two criteria: syntactic correctness and rele-
vance. Syntactic correctness indicates the gram-
maticality and the ﬂuency and relevance demon-
strates whether the question is meaningful and re-
lated to the sentence it is generated from. The
three assessors performed the evaluations on 100
randomly selected question and answer pairs from
the results. The comparison of human evaluations
between our system and the Du et al. (2017) model
is shown in Table 3. Bold numbers demonstrate
the best performing system for each evaluation cri-
teria, and we see that our system outperforms the
Du et al. (2017) model on both criteria.

Baseline Our Model

Syntactic correctness
Relevance

4.4
2.93

4.52
3.37

Table 3: Human evaluation results for the syntac-
tic correctness and relevance between our model
and Du et al. (2017).

8 Conclusion

In this work, we presented a sequence to sequence
learning model to address the opinion question
generation task. We showed the training process
using the global attention and applied the cover-
age mechanism to improve the model. We took
advantage of community-based question answer-
ing systems which contain informal speech and its
sentences do not always follow grammatical rules.
Experimental results show an improvement in the
automatic evaluation metrics as well as the human
evaluations compared to the baseline system.

Acknowledgements

We would like to thank the anonymous review-
ers for their useful comments. The research re-
ported in this paper was conducted at the Uni-
versity of Lethbridge and supported by the Nat-
ural Sciences and Engineering Research Council
(NSERC) of Canada discovery grant and the Uni-
versity of Lethbridge.

156References
Yllias Chali and Sadid A. Hasan. 2015. Towards topic-
to-question generation. Computational Linguistics,
41(1):1–20.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Doll´ar, and
C. Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. arXiv
preprint, arXiv:1504.00325.

Kyunghyun Cho, Bart van Merrienboer, Caglar
Gulcehre, Dzmitry nau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1724–1734. Asso-
ciation for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language speciﬁc translation evaluation
In Proceedings of the
for any target language.
Ninth Workshop on Statistical Machine Translation,
pages 376–380, Baltimore, Maryland. Association
for Computational Linguistics.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
In Proceedings of the 55th An-
comprehension.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1342–
1352, Vancouver, Canada. Association for Compu-
tational Linguistics.

Arthur C. Graesser and Natalie K. Person. 1994. Ques-
tion asking during tutoring. American Educational
Research Journal, 31(1):104–137.

Michael Heilman and Noah A. Smith. 2010. Good
question!
statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9:1735–
1780.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Igor Labutov, Sumit Basu, and Lucy Vanderwende.
2015. Deep questions without deep understanding.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, pages 889–898, Beijing, China.
Association for Computational Linguistics.

Chin-Yew Lin. 2004. Rouge: a package for automatic
In Text Summarization

evaluation of summaries.

Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain. Association
for Computational Linguistics.

David Lindberg, Fred Popowich, John C. Nesbit, and
Philip H. Winne. 2013. Generating natural language
In Proceed-
questions to support learning on-line.
ings of the 14th European Workshop on Natural Lan-
guage Generation, pages 105–114, Soﬁa, Bulgaria.
Association for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In Proceedings of the
neural machine translation.
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421. Associa-
tion for Computational Linguistics.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
In Proceedings of the 25th In-
customer reviews.
ternational Conference on World Wide Web, pages
625–635.

Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar-
garet Mitchell, Xiaodong He, and Lucy Vander-
wende. 2016. Generating natural questions about
an image. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1802–1813. Asso-
ciation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In Proceedings of the 2014 Con-
representation.
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392. Asso-
ciation for Computational Linguistics.

Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lin-
tean, Svetlana Stoyanchev, and Cristian Moldovan.
2010. The ﬁrst question generation shared task eval-
In Proceedings of the 6th Inter-
uation challenge.
national Natural Language Generation Conference,
pages 251–257. Association for Computational Lin-
guistics.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083. Association for Computational Linguistics.

157Iulian V. Serban, Alberto Garc´ıa-Dur´an, C¸ alar
G¨ulc¸ehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
In Pro-
The 30m factoid question-answer corpus.
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 588–598,
Berlin, Germany.

Duyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.
Question answering and question generation as dual
tasks. arXiv preprint, arXiv:1706.02027.

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William Cohen. 2017.
Semi-supervised qa with
generative domain-adaptive nets. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1040–1050, Vancouver, Canada. Association
for Computational Linguistics.

Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-
dro Sordoni, Philip Bachman, Saizheng Zhang,
Sandeep Subramanian, and Adam Trischler. 2017.
Machine comprehension by text-to-text neural ques-
tion generation. In Proceedings of the 2nd Workshop
on Representation Learning for NLP, pages 15–25,
Vancouver, Canada. Association for Computational
Linguistics.

158
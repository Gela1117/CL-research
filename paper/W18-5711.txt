Exploring Named Entity Recognition As an Auxiliary Task
for Slot Filling in Conversational Language Understanding

Samuel Louvan

University of Trento

Fondazione Bruno Kessler

slouvan@fbk.eu

Abstract

Slot ﬁlling is a crucial task in the Natural Lan-
guage Understanding (NLU) component of a
dialogue system. Most approaches for this task
rely solely on the domain-speciﬁc datasets for
training. We propose a joint model of slot ﬁll-
ing and Named Entity Recognition (NER) in
a multi-task learning (MTL) setup. Our ex-
periments on three slot ﬁlling datasets show
that using NER as an auxiliary task improves
slot ﬁlling performance and achieve competi-
tive performance compared with state-of-the-
art. In particular, NER is effective when su-
pervised at the lower layer of the model. For
low-resource scenarios, we found that MTL is
effective for one dataset.
Introduction

1
Most of the current dialogue systems depend on
an NLU component to extract semantic informa-
tion from an utterance. Such semantic information
is often represented as a semantic frame which
contains the domain, intent of the user, and pre-
deﬁned attributes (slots). Each word of the utter-
ance is labeled with a slot, which deﬁnes a par-
ticular attribute (an entity, time, etc) of the utter-
ance. Table 1 shows an example of a semantic
frame for the sentence ”Show me the prices of all
ﬂights from Atlanta to Washington DC” with Be-
gin/In/Out (BIO) representation.

We focus on slot ﬁlling, a task of automatically
extracting slots for a given utterance. This task
can be treated as a sequence labeling problem and
the most successful approach is to employ a con-
ditional random ﬁelds (CRF) on top of a deep re-
current neural networks (RNN). In general, there
are two ways of training a slot ﬁlling model: (i)
train a domain-speciﬁc model (Goo et al., 2018;
Wang et al., 2018) or (ii) train a model that per-
forms well across domains using domain adapta-
tion or transfer learning techniques (Hakkani-T¨ur

Bernardo Magnini

Fondazione Bruno Kessler

magnini@fbk.eu

Domain
Intent
Utterance
show
me
the
prices
of
all
ﬂights
from
Atlanta
to
Washington
DC

airline
search airfare
Slot Label
O
O
O
O
O
O
O
O
B-fromloc.city name
O
B-toloc.city name
I-toloc.city name

Table 1: An example of a semantic frame with its
coressponding domain, intent and slots.

et al., 2016; Jaech et al., 2016; Jha et al., 2018;
Kim et al., 2017). One popular transfer learning
technique is multi-task learning (MTL) (Caruana,
1997) in which a joint model is trained on a tar-
get (main) task and several auxiliary tasks simul-
taneously to learn better feature representations
across tasks. This technique has shown potential
on various NLP tasks and offer ﬂexibility as it
allows transfer learning across different domains
and tasks (Yang et al., 2017). On slot ﬁlling, Jaech
et al. (2016) train a single slot ﬁlling model on dif-
ferent domains and show that MTL is particulary
useful in low resource scenarios.

Identifying beneﬁcial auxiliary task for the tar-
get task is important when applying MTL (Bin-
gel and Søgaard, 2017). In this work, we inves-
tigate the effectiveness of Named Entity Recogni-
tion (NER) as an auxiliary task for slot ﬁlling. We
propose NER because of two main reasons. First,
the slot values are typically named entities, for ex-
ample airline name, city name, etc. Second, the
state of the art performance of models for NER
have been relatively high (Lample et al., 2016; Ma
and Hovy, 2016). Therefore, we expect that the

Proceedingsofthe2018EMNLPWorkshopSCAI:The2ndInt’lWorkshoponSearch-OrientedConversationalAI,pages74–80Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticsISBN978-1-948087-75-974learned features of NER can improve the slot ﬁll-
ing performance. Finally, NER corpus is relatively
easier to obtain compared to domain speciﬁc slot
ﬁlling datasets.

We are interested to answer the following ques-

tions:

labels.

For example,

• Does NER help the performance of slot ﬁll-
ing in the MTL setup? As NER labels are
usually more coarse-grained than slot ﬁll-
ing labels, predicted NER label might pro-
vide good signal to the more ﬁne-grained
slot
the location
LOC label in NER can be a strong indi-
cator for slots fromloc.city name or
toloc.city name and ﬁlter out other slot
labels which are not related to location.
We hope the model can learn more general
knowledge ﬁrst and transfer such knowledge
to predict more speciﬁc slot information us-
ing MTL.

• What is the effect of supervising NER on the
lower layer of the MTL model to the slot ﬁll-
ing performance? Inspired by recent work
of Søgaard and Goldberg (2016), we inves-
tigate the effect of supervising NER on dif-
ferent layers of the model. Our hypothesis is
that a more “general” feature is better learned
on the lower layer in order to support a task
which depends on a more “speciﬁc” feature.

In addition, we also experiment on cross-
domain slot ﬁlling models by jointly training slot
ﬁlling datasets from similar domains using a MTL
setup. We explore two techniques to measure
similarity between domains: domain similarity by
Ruder and Plank (2017a) and label embedding
mapping by Kim et al. (2015).

We experiment with three datasets from differ-
ent domains. Our experiments show that for all
datasets, using NER as an auxiliary task is beneﬁ-
cial for the slot ﬁlling performance. NER is con-
sistently helpful when it is supervised at the lower
layer. On the low resource scenario, we found
mixed results, in which MTL is only effective for
1 dataset.

2 Model

This section describes the slot ﬁlling model, the
multi-task learning setup, and the data selection
that we use in our experiments.

Figure 1: Multi-task Learning with different supervision
level

2.1 Slot Filling Model
For the slot ﬁlling model, we adopt a neural based
model similar to (Lample et al., 2016; Ma and
Hovy, 2016), as it achieves the state of the art
performance in sequence labeling task (NER). Re-
cent slot ﬁlling model of Jha et al. (2018) also
used a variant of this model. Given an input sen-
tence, we represent each word wi using a concate-
nation of its word embedding e(wi) and character-
level embeddings c(wi) : xi = [e(wi); c(wi)].
The character-level embeddings are computed us-
ing convolutional neural networks (CNN), similar
to the one proposed by Kim et al. (2016). We then
feed xi to a bidirectional LSTM (biLSTM) word-
level encoder to incorporate the contextual infor-
mation of wi. The output of the backward and for-
ward LSTM at each time step is then concatenated
and fed into a CRF layer. The CRF layer computes
the ﬁnal output, e.g. the tag of each input. We use
one hidden layer between biLSTM and CRF as it
has been shown by Lample et al. (2016) that it can
improve performance.

2.2 Multi-Task Learning
One simple technique to perform MTL is by train-
ing the target and auxiliary tasks simultaneously.
In this setting, the parameters of the model are
shared across tasks, pushing the model to learn
feature representations that work well across tasks.
Figure 1 depicts the MTL setting that we use
in our work. The lower parts of the network,
i.e. word embeddings, character-level embed-
dings, and bi-LSTM encoder are shared among
tasks. After the bi-LSTM layer, we use differ-
ent CRF layers for each task to predict the task-
speciﬁc tags (NER or slot ﬁlling). We also exper-
iment with MTL setup which uses different level
of supervision for the auxiliary task (Søgaard and
Goldberg, 2016), in which we use two layers of
biLSTM encoder and only share the lower layer of

75#token

#label

Label Examples

Dataset

Slot Filling
ATIS
MIT Restaurant
MIT Movie
NER
CoNLL 2003
OntoNotes 5.0

#sent
dev

500
1532
1955

train

4478
6128
7820

test

893
3385
2443

869
4166
5953

14987
34970

3466
5896

3684
2327

21010
34662

79
8
12

4
18

airport name, airline name, return date
restaurant name, dish, price, hours
actor, director, genre, title, character

person, location, organization
organization, gpe, date, money, quantity

Table 2: Statistics of the datasets. For each dataset, number of sentence in train/dev/test set, the number of unique token and
label in the training set.

the encoder and keep the outer layer for the main
slot ﬁlling task.

2.3 Data Selection
Ruder and Plank (2017b) demonstrate that select-
ing data for training the auxiliary task might im-
prove the target task performance. We investigate
two data selection techniques for our MTL exper-
iments:
Domain Similarity. We use Jensen-Shannon
divergence (JSD; Lin, 1991) to measure do-
main similarity as proposed by Ruder and Plank
2 (DKL(P||M ) + DKL(K||M )) where
(2017b): 1
2 (P + Q) . DKL(P||Q) is the Kullback-
M = 1
Leibler divergence between two distributions P
and Q. We use term distributions (Plank and
Van Noord, 2011) of each domain to compute P
and Q. We select the most similar domain to the
main task domain to be used as the auxiliary task.
Label Embedding Mapping.
In an MTL setup,
sometimes we only want to keep auxiliary la-
bels which are semantically similar to target task
labels and remove other irrelevant labels of the
auxiliary task. For example, the slot ﬁlling la-
bel airport.statename is similar to LOC
but not to TIME auxiliary NER label. We em-
ploy label embedding mapping approach by Kim
et al. (2015) using Canonical Correlation Analy-
sis (CCA). The idea is to construct matrix repre-
sentation where rows are labels and columns are
words in the vocabulary. The cell value in the ma-
trix is the pointwise mutual information (PMI) be-
tween the label and the word. After that, we per-
form rank-k SVD on the matrix and normalized
the rows of the matrix. Each row with k dimen-
sion of the matrix is the label embedding of a par-
ticular label. We use the cosine similarity between
two label embedding representations to obtain the
nearest neighbor.

Target Task Most Similar Domain

ATIS
MIT-R
MIT-M

MIT-R
MIT-M
MIT-R

Table 3: Most similar domain for each target task computed
with JSD

3 Experimental Setup

Data. We use three slot ﬁlling datasets (Table
2): Airline Travel Information System (ATIS; T¨ur
et al., 2010), MIT Restaurant (MIT-R) and MIT
Movie (MIT-M) (Liu et al., 2013; Liu and Lane,
2017b). The ATIS dataset is widely used in con-
versational language understanding and contains
queries to a ﬂight database. We use the pro-
vided slot annotations and use the same split as
in Hakkani-T¨ur et al. (2016). The MIT-R contains
utterances related to restaurant search and MIT-M
contains queries related to movie information. For
both datasets, we use the default split.1 As for
the NER dataset, we use two datasets : CoNLL
2003 (Tjong Kim Sang and De Meulder, 2003)
and Ontonotes 5.0 (Pradhan et al., 2013). For
OntoNotes, we use the Newswire section for our
experiments.

tagger

Implementation. We use the existing BiLSTM-
CRF sequence
implementation from
Reimers and Gurevych (2017) for all experi-
ments.2 We use the pre-trained word embedding
from (Komninos and Manandhar, 2016). We set
the LSTM hidden units to 100. The word and
character embeddings dimensions are set to 300
and 30 respectively. We use dropout rate of 0.25.
We train the model using the Adam optimizer
(Kingma and Ba, 2014) for 25 epochs with early
stopping on the target task. For each epoch, we

1https://groups.csail.mit.edu/sls/downloads/
2https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf

76Model

Bi-model based (Wang et al., 2018)
Slot gated model (Goo et al., 2018)
Recurrent Attention (Liu and Lane, 2016)
Adversarial(Liu and Lane, 2017a)
Single task (STL)
MTL, same supervision level
MTL, same supervision level
MTL, same supervision level
MTL, same supervision level
MTL, same supervision level
MTL, different supervision level
MTL, different supervision level
MTL, different supervision level

Aux. Task
SF
-
-
-
-
-

most similar

most similar

all

all
-

most similar

all
-

Target Task

NER ATIS MIT-R MIT-M

-
-
-
-
-
-
-
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

96.89
95.20
95.87
95.63
95.68
95.47
95.68
95.50
95.34
95.71
95.70
95.94
95.40

-
-
-

74.47
78.58
78.56
78.70
78.41
78.27
78.40
79.10
79.00
79.13

-
-
-

85.33
87.34
86.89
87.22
86.77
86.76
87.09
86.94
86.92
87.41

Table 4: F1 scores comparison between MTL, STL, and previous published results on each dataset. “Most Similar” auxiliary
task means we take the most similar slot ﬁlling domain (excluding NER ) as the auxiliary task. “All” includes all the slot ﬁlling
domains as the auxiliary tasks (excluding NER). For the “different supervision level”, NER is supervised at the lower layer and
slot ﬁlling tasks at the higher layer. Bold: best, Underline: second best.

train the model of each task in alternate fashion.
We evaluate the performance by computing
the F1-score on the test set using the standard
CoNLL-2000 evaluation3

Target Task & Auxiliary Tasks. For each MTL
experiment, there is exactly one target task and one
or more auxiliary task(s). The target task is always
a slot ﬁlling task, i.e. either ATIS, MIT-R, or MIT-
M. The auxiliary task(s) consist of a combination
of slot ﬁlling tasks from different domains of the
target task with (or without) a NER task. We select
the most similar slot ﬁlling task for the target task
using the domain similarity technique described in
(§2.3). Table 3 presents the most similar slot ﬁlling
domain for each slot ﬁlling task.

4 Results and Analysis
Overall Performance. Table 4 summarizes the
slot ﬁlling performance of our single task (STL)
versus MTL models. The performance from previ-
ous studies are directly copied from their reported
numbers. When using the same supervision level
for both target and auxiliary tasks, using the most
similar domain performs worse than using all do-
mains. In contrast, using NER together with the
most similar domain as auxiliary tasks performs
better than using all the domains.

Experiments on different supervision level
show that using NER as an auxiliary task consis-
tently improves slot ﬁlling performance. This re-

3https://www.clips.uantwerpen.be/conll2000/chunking/

output.htm

sult matches our intuition that the task with more
coarse-label, such as NER, is better to be super-
vised at the lower layer of the model. On ATIS and
MIT-R datasets, MTL achieves better performance
compared to STL. However, on MIT-M, STL out-
performs some MTL models.

In order to understand better the behavior of
the models, we analyze the results from the de-
velopment set. For the ATIS dataset, STL and
MTL have the same performance in 44 out of
67 slots in the development set. For the rest of
the slots, STL performs better mostly on slots
related to time such as arrive time.time
and depart date.month name while MTL is
better on recognizing location related slots such
as city name and toloc.state name. For
the MIT Restaurant dataset, MTL performs bet-
ter on 5 out of 8 slots. MTL performs well in
identifying slots related to time and location in
the MIT Restaurant dataset. For the MIT movie,
MTL yields better results for time related slots. As
for the person related slots such as character
, actor, and director, STL gives better re-
sults. Overall, although incorporating NER with
slot ﬁlling shows improvements, the difference is
still rather small especially for the ATIS and the
MIT Movie datasets. Further work is needed to ex-
plore better mechanism to inject NER information
to help slot ﬁlling in the MTL setup. It is also in-
teresting to compare the performance of MTL and
pipeline based system which utilizes NER predic-
tion as one of the feature for the slot ﬁlling model.

77Model
MTL
MTL+Label Emb.

ATIS MIT-R MIT-M
95.94
87.34
86.84
95.66

79.10
78.37

Table 5: The effect of the label ﬁltering on MTL perfor-
mance

Dataset
ATIS

MIT-R

MIT-M

# training sents

200
400
800
200
400
800
200
400
800

STL
83.88
85.54
90.48
54.65
61.36
67.48
68.28
74.09
79.33

MTL
81.27
85.21
90.68
54.91
61.88
68.27
69.12
75.15††
79.08

Table 6: Performance comparison between STL and MTL
for low resource scenarios. †† indicates signiﬁcant improve-
ment over STL baseline with p < 0.05 using approximate
randomization testing.

Effect of Label Embedding Mapping. We ap-
ply label ﬁltering on the auxiliary tasks using the
label embedding mapping (§2.3). On the auxil-
iary dataset(s), we keep the most similar labels and
replace irrelevant labels with O. The MTL setup
that we use is the best performing MTL for each
dataset in Table 4. As shown in Table 5, the per-
formance of MTL drops when we apply ﬁltering
to the auxiliary labels. We suspect that this is due
to the quality of the label mapping and also a high
number of “O” label after the ﬁltering process.

Low Resource Scenarios. We experiment on
low resource scenarios where we vary the num-
ber of training sentences to 200, 400, and 800 sen-
tences for each dataset. The MTL setup that we
use is the best performing MTL for each dataset
in Table 4. As shown in Table 6, MTL consis-
tently performs better than STL for the MIT-R
dataset. While for the ATIS and MIT-M datasets,
STL mostly gives better results than MTL.

5 Related Work

Recent studies on slot ﬁlling in conversational sys-
tems are mostly based on neural models. Wang
et al. (2018) introduce a bi-model (RNN) structure
to consider cross-impact between intent detection
and slot ﬁlling. Liu and Lane (2016) propose an at-
tention mechanism on the encoder-decoder model
for joint intent classiﬁcation and slot ﬁlling. (Goo
et al., 2018) extend the attention mechanism us-

ing a slot gated model to learn relationship be-
tween slot and intent attention vectors. Hakkani-
T¨ur et al. (2016) use bidirectional RNN as a sin-
gle model that handle multiple domains by adding
a ﬁnal state that contains domain identiﬁer. The
work by Jha et al. (2018); Kim et al. (2017) uses
expert based domain adaptation while Jaech et al.
(2016) propose a multi-task learning approach to
guide the training of a model for new domain. All
of these studies train their model solely on slot ﬁll-
ing datasets, while our focus is to exploit a more
“general” resource, such as NER, by training the
model jointly with slot ﬁlling through MTL with
different supervision level.
6 Conclusion
In this work, we investigate the effectiveness of
training a slot ﬁlling model jointly with NER as
an auxiliary task through MTL setup. Our experi-
ments demonstrate that NER is helpful for slot ﬁll-
ing. In particular, NER is more effective when it
is supervised at the lower layer of the MTL model.
However, further work is needed to investigate the
effectiveness of domain similarity metric or label
embedding mapping as a way to perform data se-
lection in the preprocessing step.
Acknowledgments
The authors would like to thank anonymous re-
viewers and Clara Vania for the helpful comments
and feedback. This work was supported by the
grant of Fondazione Bruno Kessler PhD scholar-
ship.

References
Joachim Bingel and Anders Søgaard. 2017. Identify-
ing beneﬁcial task relations for multi-task learning
in deep neural networks. EACL 2017, page 164.

Rich Caruana. 1997. Multitask learning. Machine

Learning, 28:41–75.

Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li
Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-
Nung Chen. 2018. Slot-gated modeling for joint slot
ﬁlling and intent prediction. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 2
(Short Papers), pages 753–757.

Dilek Z. Hakkani-T¨ur, G¨okhan T¨ur, Asli elikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame

78parsing using bi-directional rnn-lstm.
SPEECH.

In INTER-

Aaron Jaech, Larry P. Heck, and Mari Ostendorf.
2016. Domain adaptation of recurrent neural net-
In IN-
works for natural language understanding.
TERSPEECH.

Rahul Jha, Alex Marin, Suvamsh Shivaprasad, and
Imed Zitouni. 2018. Bag of experts architectures
for model reuse in conversational language under-
In Proceedings of the 2018 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 3 (Industry Papers), volume 3,
pages 153–161.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der Rush. 2016. Character-aware neural language
models. In Proceedings of the 2016 Conference on
Artiﬁcial Intelligence (AAAI).

Young-Bum Kim, Karl Stratos, and Dongchan Kim.
2017. Domain attention with an ensemble of ex-
perts. In ACL.

Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015. New transfer learning tech-
niques for disparate label sets. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 473–482.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Alexandros Komninos and Suresh Manandhar. 2016.
Dependency based embeddings for sentence classi-
ﬁcation tasks. In HLT-NAACL.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
theory, 37(1):145–151.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot ﬁlling. In Interspeech 2016.

Bing Liu and Ian Lane. 2017a. Multi-Domain Adver-
sarial Learning for Slot Filling in Spoken Language
Understanding.

Jingjing Liu, Panupong Pasupat, Yining Wang, Scott
Cyphers, and James R. Glass. 2013. Query un-
derstanding enhanced by hierarchical parsing struc-
tures. In 2013 IEEE Workshop on Automatic Speech
Recognition and Understanding, Olomouc, Czech
Republic, December 8-12, 2013, pages 72–77.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074.

Barbara Plank and Gertjan Van Noord. 2011. Effective
measures of domain similarity for parsing. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1566–1576.
Association for Computational Linguistics.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 143–152.

Nils Reimers and Iryna Gurevych. 2017. Reporting
Score Distributions Makes a Difference: Perfor-
mance Study of LSTM-networks for Sequence Tag-
In Proceedings of the 2017 Conference on
ging.
Empirical Methods in Natural Language Processing
(EMNLP), pages 338–348, Copenhagen, Denmark.

Sebastian Ruder and Barbara Plank. 2017a. Learning
to select data for transfer learning with bayesian op-
timization. In EMNLP.

Sebastian Ruder and Barbara Plank. 2017b. Learn-
ing to select data for transfer learning with bayesian
In Proceedings of the 2017 Confer-
optimization.
ence on Empirical Methods in Natural Language
Processing, pages 372–382, Copenhagen, Denmark.
Association for Computational Linguistics.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), volume 2, pages
231–235.

Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.

Bing Liu and Ian Lane. 2017b. Multi-domain adver-
sarial learning for slot ﬁlling in spoken language un-
derstanding. In NIPS Workshop on Conversational
AI.

G¨okhan T¨ur, Dilek Z. Hakkani-T¨ur, and Larry P.
Heck. 2010. What is left to be understood in atis?
2010 IEEE Spoken Language Technology Workshop,
pages 19–24.

79Yu Wang, Yilin Shen, and Hongxia Jin. 2018. A bi
model based rnn semantic frame parsing model for
intent detection and slot ﬁlling. In NAACL.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
arXiv
ging with hierarchical recurrent networks.
preprint arXiv:1703.06345.

80
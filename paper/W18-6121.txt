Classiﬁcation of Tweets about Reported Events

using Neural Networks

Kiminobu Makino, Yuka Takei, Taro Miyazaki, Jun Goto

NHK Science & Technology Research Laboratories

fmakino.k-gg, takei.y-ek, miyazaki.t-jw, goto.j-fwg@nhk.or.jp

1-10-11 Kinuta, Setagaya-ku, Tokyo, Japan

Abstract

We developed a system that automatically ex-
tracts “Event-describing Tweets” which in-
clude incidents or accidents information for
creating news reports.
Event-describing
Tweets can be classiﬁed into “Reported-
event Tweets” and “New-information Tweets.”
Reported-event Tweets cite news agencies
or user generated content sites, and New-
information Tweets are other Event-describing
Tweets. A system is needed to classify them
so that creators of factual TV programs can
use them in their productions. Proposing this
Tweet classiﬁcation task is one of the contri-
butions of this paper, because no prior papers
have used the same task even though program
creators and other events information collec-
tors have to do it to extract required informa-
tion from social networking sites. To clas-
sify Tweets in this task, this paper proposes a
method to input and concatenate character and
word sequences in Japanese Tweets by using
convolutional neural networks. This proposed
method is another contribution of this paper.
For comparison, character or word input meth-
ods and other neural networks are also used.
Results show that a system using the proposed
method and architectures can classify Tweets
with an F1 score of 88 %.

1

Introduction

Many companies including news agencies have
increasingly been extracting news information
from postings on Social Networking Sites (SNSs)
such as Twitter and Facebook and using it for
various purposes (Neubig et al., 2011; Iso et al.,
2016). However, choosing important information
for news reports from Twitter is very tough, be-
cause Twitter contains a vast amount of posts.
For this reason, many researchers have stud-
ied how to extract
important posts for each
purpose (Papadopoulos et al., 2014; Litvak et al.,

2016; Zhou et al., 2016; Vakulenko et al., 2017).
A system using Neural Networks (NNs) has been
developed by using models that are trained by ex-
tracting Tweets in factual TV program produc-
tion, and these systems extract “Event-describing
Tweets (EVENT)” which include incidents or ac-
cidents information for news reports from a large
amount of Tweets (Miyazaki et al., 2017). How-
ever, there are many Tweets, so there can be many
extracted Tweets which include EVENT for news
reports about any event. Hence, people have difﬁ-
culty monitoring all EVENT. In addition, EVENT
are used differently in different types of programs.
For these purposes, it is better to display only
Tweets suitable to the program contents.

For example, program creators who want to ob-
tain primary reports of an event posted by Twitter
users do not require Tweets put out by news agen-
cies and User Generated Content (UGC) sites or
Tweets that quote or cite them. The part of new
information of these Tweets is able to be gotten
by crawling each site, so no longer these Tweets
do not include new events information for pro-
gram creators. We call these Tweets “I: Reported-
event Tweets (REPORTED)” and others “II: New-
information Tweets (NEW).” Both types of Tweets
are requested for different reasons. Only types of
Tweets suitable to creators’ purpose need to be
displayed, but extracting EVENT and classifying
them are essentially different processes.

For these reasons, this paper uses a two-stage
processing system that separates EVENT from a
large amount of Tweets by using an existing sys-
tem and classiﬁes them into REPORTED and NEW
by using text-based machine learning methods in
real-time (Section 4). Our proposed method inputs
both character and word sequences (Section 5.2).
Both proposed and conventional methods use sev-
eral NN architectures including Recurrent NNs
(RNNs) and Convolutional NNs (CNNs) (Sec-

Proceedingsofthe2018EMNLPWorkshopW-NUT:The4thWorkshoponNoisyUser-generatedText,pages153–163Brussels,Belgium,Nov1,2018.c(cid:13)2018AssociationforComputationalLinguistics153tion 5). Evaluation results show that the proposed
method outperformed the conventional methods in
all NN architectures (Section 6).

This paper makes two contributions. One is
proposing a new task for classifying extracted
EVENT into two classes: REPORTED and NEW.
TV program creators need to do this task for pro-
gram production automatically and do it ﬁrst to
track reports about an event. However, there have
been no prior studies about this task. The other is
proposing a new method for NN architectures that
inputs entire character sequences and entire word
sequences in parallel and concatenates them in the
intermediate layer and evaluating its performance.
This method can utilize the advantages of charac-
ter sequences (i.e., there are fewer unknown char-
acters than unknown words and it does not need
morphological analyzers even when in Japanese
which is very difﬁcult to divide words especially
for noisy texts) and word sequences (i.e., words
are more effective than characters for the task).
The method can be used for any other tasks that
need to both character and word sequences.

2 Related Work

There are many related works such as related tasks
that use Twitter datasets or classify texts and re-
lated methods that have NNs architectures using
both characters and words in Natural Language
Processing (NLP).

(Ghelani et al.,

stance Tweets

classiﬁcation of news

Related tasks include topic detection on Twit-
ter task (Papadopoulos et al., 2014; Litvak et al.,
2016; Zhou et al., 2016; Vakulenko et al., 2017),
binary classiﬁcation of Tweets (Rosenthal et al.,
2017), classiﬁcation of news related or po-
litical
2017;
Johnson and Goldwasser, 2016; Volkova et al.,
2017),
related arti-
cles (Ribeiro et al., 2017), and other classiﬁca-
tions in NLP. Binary classiﬁcation of texts and
classiﬁcation of news related texts and articles
are most closely related to this task. However,
none of these studies focused on classifying
extracted EVENT into REPORTED or NEW. Since
no classiﬁcation method meets the requirements
of this paper (i.e., extraction of REPORTED to
obtain primary reports and extraction of NEW to
collect opinions about reported events or gather
follow-up Tweets), no prior research on the same
task exists.

For related methods,

in NLP using ma-

Figure 1: Overview of our Tweets classiﬁcation.

there is one NLP conﬁg-
chine learning,
in-
uration that uses a word sequence as
put and characters as supplemental
informa-
tion (Ma and Hovy, 2016; Gr¨onroos et al., 2017;
Heyman et al., 2017; Lin et al., 2017) and another
that switches between the character NN and the
word NN (Vijayaraghavan et al., 2016). However,
the character sequence is one semantic vector set
for the entire sequence. There is one NLP con-
ﬁguration that uses gated recurrent units for a
word sequence and for CNN output of a charac-
ter sequence(Liang et al., 2017). However, it does
not purely combine characters and words in paral-
lel and takes time to process because it includes re-
current architectures and is not for noisy texts. No
method combining the output of an entire purely
character sequence and an entire purely word se-
quence in parallel with a CNN for noisy texts has
been studied and evaluated to the best of the au-
thors’ knowledge.

3 Task Description

The purpose of our system is classiﬁcation of
Tweets for different types of programs.
Fig-
ure 1 shows the overview of our Tweets clas-
siﬁcation.
Extracting EVENT is not a novel
task (Miyazaki et al., 2017), so the proposed task
in this paper is classifying EVENT into REPORTED
and NEW.

3.1 Classiﬁcation of REPORTED and NEW
REPORTED are less numerous than NEW. This
is because NEW include information about events
relevant to few people and that are low priority for
many news agencies such as local events as well
as events no news agencies know about. Tweet-
classiﬁcation system needs to extract all events in-
formation, and the program creators need to judge
the priority. Conversely, when Tweets are put out
that many people want to cite and opine about,
REPORTED quoting these Tweets will increase,

154All Tweets Ⅰ: REPORTEDⅡ: NEWnon-EVENTEVENTⅠ-①: with explicit sourcesⅠ-②: without explicit sourcesExtractionClassification I- 1⃝ REPORTED with explicit sources
e.g.
“Small plane crash. Four people dead on impact. — XXX news
I- 2⃝ REPORTED without explicit sources
e.g.
“Small plane crash. Four people dead on impact.
II

NEW

Any other EVENT

EVENT that are tweeted from or quote news agencies, UGC sites,
or others and include explicit sources

I wonder if the plane broke down from the nose.”

EVENT that are tweeted from or quote news agencies, UGC sites,
or others and do not include explicit sources

I wonder if the plane broke down from the nose.”

Table 1: Types of EVENT (example is manually translated by author).

and NEW will be hard to monitor and gather in
real-time. Similarly, creators who want to collect
opinions about reported events or gather follow-up
Tweets will not need NEW, which are the major-
ity of EVENT. Therefore, depending on the cre-
ator’s intention, either NEW or REPORTED should
be displayed in real-time. For these reasons, a
classiﬁcation task is needed.

3.2 Two Types of REPORTED
There are two types of REPORTED. One is RE-
PORTED with explicit sources (I- 1⃝), which cite
news agencies, UGC sites, or other information
dissemination agencies, so Tweet-classiﬁcation
systems are expected to easily detect these Tweets
by keyword ﬁltering using source names. The
other is REPORTED without explicit sources (I-
2⃝), which do not cite explicit sources because
Twitter users can remove source names. They
have a distinctive stylistic character (in Japanese,
they often contain sentence ending with a noun
or noun phrase and often include date, time, etc.)
and can be detected manually. However Tweet-
classiﬁcation systems cannot detect them by sim-
ple methods including keyword ﬁltering using
source names.

Table 1 shows three types of training data (de-
scribed in Section 6.1) manually classiﬁed by hu-
mans. Both I- 1⃝ and I- 2⃝ are REPORTED, and this
system only classiﬁes Tweets into two classes: I:
REPORTED and II: NEW. This is because I- 1⃝ and
I- 2⃝ seem to be used the same way. However, ex-
tracting I- 2⃝ is expected to be harder than extract-
ing I- 1⃝, because sources are grounds for decid-
ing whether a Tweet quotes a source or not. For
only evaluating the characteristic difference (Sec-
tion 6), I are classiﬁed into I- 1⃝ and I- 2⃝.
4 Conﬁguration of Our System

The structure of our system for classifying EVENT
about reported events is shown in Figure 2. Inputs
of this system are 10 % of all randomly sampled
Tweets in Japanese. The extraction process ex-

Figure 2: Structure of our system for classifying
EVENT about reported events.

tracts EVENT and removes non-Event-describing
Tweets (non-EVENT). The EVENT for news re-
ports are then input in the classiﬁcation process,
which classiﬁes them into REPORTED and NEW.
The extraction process needs to separate the
0.1% of EVENT from the 99.9% of non-EVENT.
Because there are so many non-EVENT, the ex-
traction process needs extensive training. To ex-
tract EVENT and classify them into REPORTED
and NEW in one process, the system is trained for
classiﬁcation by using Tweets required for train-
ing with a large amount of non-EVENT unrelated
to classiﬁcation. Moreover, when systems are ex-
tended to classify other types of Tweets or relearn
how to classify EVENT about reported events,
they need extensive training for Tweet extraction
and classiﬁcation. However, it is not realistic to
do such retraining every time classiﬁcation is ad-
justed in accordance with a program creator’s re-
quest. For these reasons, this paper uses a two-
stage processing system that includes an extrac-
tion process and a classiﬁcation process.

4.1 Extraction Process

an

uses

process

extraction

existing
The
method (Miyazaki et al., 2017). Figure 3 shows
the structure of the extraction process. Tweets are
converted into one-hot vector for each character,
entered into a Feed-forward NN (FFNN), a
Bi-directional Long Short-Term Memory (Bi-
LSTM) with an attention mechanism, and 2-layer

155Randomly sampledTweets in Japanese(Avg. of 8M / day)non-EVENT(Avg. of 8M / day)EVENTⅠ: REPORTED(Avg. of 2K / day)Ⅱ: NEW(Avg. of 8K / day)(Avg. of 10K / day)Classification process(proposed in this paper)Extraction process1((Miyazaki et al., 2017)Figure 3: Structure of extraction process.

FFNN, and classiﬁed as important (EVENT) or
unimportant (non-EVENT). Dimensions of each
intermediate layer are set to 200.

In this paper, a model that is trained by su-
pervised training using 19,962 Tweets manually
extracted in TV program production for positive
samples and 1,524,155 randomly sampled Tweets
for negative samples is used. The model which has
a 74.4 % F1 score is used. Whether the priority is
precision or recall can be changed by varying the
threshold of the output depending on the purpose1.

4.2 Classiﬁcation Process
The classiﬁcation process classiﬁes EVENT into
REPORTED and NEW by several classiﬁcation
methods using three types of manually classiﬁed
training data as shown in Table 1. For reasons al-
ready mentioned in Section 3.2, this process clas-
siﬁed EVENT into REPORTED and NEW.

Input in the classiﬁcation process is limited to
EVENT extracted from the extraction process, so
the classiﬁcation process needs much less train-
ing data than the extraction processes. There is
a trade-off between the hardware burden caused
by the volume of training data, structures of NNs
and the improvement of classiﬁcation accuracy by
advanced processing. However, the classiﬁcation
process does not need extensive training and so
can use computationally heavy methods within the
range where the test phase is performed in real-
time. In this paper, the accuracy and leaning speed
of these methods are evaluated in experiments.

5 Classiﬁcation Methods using NNs

For methods to classify REPORTED and NEW, sev-
eral inputs including the proposed method and
In machine
several NN architectures are used.

1The extraction process is neither the purpose nor the con-
tribution of this paper. This existing method was used only
for convenience. The performance evaluation of this paper
is for the classiﬁcation process. When EVENT extracted by
any methods are input, the methods are expected to perform
approximately the same for the classiﬁcation process.

Figure 4: Overview structure using each character se-
quences or word sequences.

learning using sentences, an input sequence is gen-
erally divided into characters or words, vectorized,
serialized, and used. The contributions of the both
words and characters are evaluated by these three
methods.

5.1 Conventional Character Input NN /

Word Input NN

Figure 4 shows an overview of a structure using ei-
ther character sequences or word sequences. First,
sentences are input to a sequential process NN and
output as hNN 2 Rm, where m is the intermediate
size. Second, hNN is inputed to the intermediate
layer FFNN ( W int: 2 Rm(cid:2)m, bint: 2 Rm) and
output as hint: 2 Rm. Finally, hint: is input to the
output layer FFNN (W out 2 R2(cid:2)m, bout 2 R2)
and the Softmax function and output as binary of
classiﬁcation results. At the training phase, loss is
calculated by the cross entropy function between
output of the Softmax function and one-hot vec-
tor of a correct answer. At the test or use phase,
output is calculated by an argmax function of the
Softmax function output. A series of processes is
obtained as follows,

hint: = a(W int:hNN + bint:)

(1)

output = softmax(hout)

= softmax(W outhint: + bout); (2)
where a((cid:1)) is an activation function and we use
a Rectiﬁed Linear Unit (ReLU) (Clevert et al.,

156Int. layer (Bi-LSTM)Output layer (FFNN)𝒉"#$%&’𝒉()*Output(Softmax)Input layer (FFNN)One-hot vecrorizeInt. layer (FFNN)𝒉#+*.{𝒉.#+}{𝒙.}∈ℝ34∈ℝ4∈ℝ3∈𝑛×ℝ4∈𝑛×ℝ7𝑁: No. of input layer dimensions𝑛	: Max no. of sequences𝑚: No. of int. layer dimensions𝑡: Sequential indexCharactersequences inputInt. layer (FFNN)Output layer (FFNN)𝒉"#$.𝒉&’$Sequential process𝑚: No. of int. layer dimensionsBinary output(Softmax)𝒉))∈ℝ,Char. or Wordsequences input∈ℝ,∈ℝ-Eq. (1)Eq. (2)2015).
For converting sentences into char-
acter sequences, sentences are separated into
individual characters.
For converting sen-
tences into word sequences, sentences are sepa-
rated using the Japanese morphological analyzer
MeCab (Kudo et al., 2004) with the customized
system dictionary mecab-ipadic-NEologd (Sato,
2015).
5.1.1 FFNN for Sequential Process
Figure 5 shows the structure of character or word
sequences input only using the FFNN. A BOW
(Bag of Words / characters) vector xBOW 2 RN
is input to the input layer FFNN (W in 2 Rm(cid:2)N ,
bin 2 Rm) and output as hin 2 Rm, where N is
the number of input layer dimensions, so FFNN
architectures do not include sequential architec-
tures. A series of processes is obtained as follows,

hFFNN = hin = a(W inxBOW + bin): (3)

=

{

to the input

the structure of character
input using a LSTM
One-hot vec-

{
x0 2 RN ; x1 2 RN ;(cid:1)(cid:1)(cid:1)}
2 Rm;(cid:1)(cid:1)(cid:1)}

the intermediate layer.
fxtg
input
g =
the output sequences are input

Then, hFFNN is fed to Equation (1) as hNN.
5.1.2 LSTM for Sequential Process
Figure 6 shows
or word sequences
for
tor
layer FFNN (W in,
is
bin) one by one, and output sequences are
fhin
. After
t
that,
to the
intermediate layer LSTM using an attention
mechanism (Bahdanau et al., 2014) one by one,
and the output vector is hLSTM 2 Rm.
In
accordance with LSTM mechanisms, all series of
input are used for training. A series of processes
is obtained as follows,

2 Rm; hin

hin
0

1

(t 2 [0; n)) (4)
t(cid:0)1 + bz) (5)

t + Rzhinc:

t(cid:0)1 + bi)
t(cid:0)1 + bf )

hin
t = a(W inxt + bin)
zt = tanh(W zhin
it = (cid:27)(W ihin
t + Rihinc:
f t = (cid:27)(W f hin
t + Rf hinc:
ct = it (cid:10) zt + f t
(cid:10) ct(cid:0)1
(
)
ot = (cid:27)(W ohin
t + Rohinc:
= ot (cid:10) tanh(ct)
(
(cid:1) hinc:
(
l(cid:0)1
hinc:
(cid:1) hinc:
hinc:
(cid:6)l(cid:0)1
j=0(cid:11)jhinc:

(
exp
(cid:6)l(cid:0)1
j=t exp
ol(cid:0)1 + a

(cid:11)t =

)

t(cid:0)1 + bo)

j

j

t

t

t

hinc:

hLSTM = a

))

;

where n is the maximum number of input se-
(cid:3) 2 Rm(cid:2)m, and
quences and W

(cid:3) 2 Rm(cid:2)m, R

Figure 5: Structure of FFNN using each character se-
quence or word sequence.

(cid:3) 2 Rm in Equations (5) are LSTM parameters
b
((cid:3) is each layer name and z is the tanh layer, i
the input layer, f the forget layer, and o the out-
put layer). Then, hLSTM is fed to Equation (1) as
hNN.
5.1.3 CNN for Sequential Process
Figure 7 shows the structure of character or word
sequences input using a CNN for the intermedi-
ate layer. When using word input, this process
is same as (Zhang and Wallace, 2017). First, one-
hot vector fxtg is input to the input layer FFNN
(W in, bin) one by one, and output sequences are
fhin
g the same for the LSTM. After that, the out-
put sequences are input to the intermediate con-
2 Rm(cid:2)m,bp 2 Rm)
volutional layer (each W p
j
using l kinds of ﬁlters (ﬁlter index is p = [0; l),
each ﬁlter size is k, and the index in each ﬁl-
ter is j = [0; k)) with zero padding and input to
max-pooling in each ﬁlter. Output is l kinds of
vectors fhpool;p 2 Rmg, which are all input to
the intermediate layer FFNN (W CNN 2 Rm(cid:2)lm,
bCNN 2 Rm). In accordance with the CNN archi-
tectures, a part of a time series relies on k. A series
of processes is obtained as follows,

t

t

= a

(

))
(t 2 [0; n))

(
hin
t = a(W inxt + bin)
(
(cid:6)k(cid:0)1
hConv:;p
{
}
j hin
p 2 [0; l); hin
[
hConv:;p
hpool;0;(cid:1)(cid:1)(cid:1) ; hpool;l(cid:0)1

(6)
t+j + bp
(7)
q = Om (q ≧ n)
(8)

hCNN = a(W CNN

hpool;p = max

W p

]

)

j=0

t

t

+bCNN):

(9)

Then, hCNN is fed to Equation (2) as hint:.

5.2 Proposed Concat Input NN (iii)
This paper proposes a method to input character
and word sequences and to concatenate them at

157Int. layer (FFNN)Output layer (FFNN)𝒉"#$.𝒉&’$𝑁: No. of input layer dimensions𝑚: No. of int. layer dimensionsBinary output(Softmax)𝒉**Char. or Wordsequences inputInput layer (FFNN)𝒉"#BOW vecrorizeSequential process∈ℝ-∈ℝ-∈ℝ.∈ℝ/Eq. (1)Eq. (2)Eq. (3)Figure 6: Structure of LSTM using each character sequence or word sequence.

Figure 7: Structure of CNN using each character sequence or word sequence.

the intermediate layer.
In Section 5.1, all archi-
tectures use only a character or word sequence.
However, character sequences have the advantages
of there being fewer characters than words and of
expressing the input sentence without using high-
dimensional input layers. Moreover, in the case of
using a written language that does not have spaces
between words such as Japanese, morphological
analyzers are needed to divide words. Tweets are
noisy, so they are very difﬁcult to morphologically
analyze accurately. Thus, performance from char-
acter sequences does not depend on morpholog-
ical analyzer performance, which is another big
advantage. However, sentences are written by us-
ing word sequences, and characters are involved
in many words that cover a large number of mean-
ings. In contrast, one word has a limited number
of meanings and plays a bigger role in each sen-
tence. For these reasons, the proposed method is
expected to exploit the advantages of both charac-
ters and words.

Figure 8 shows the structure of character and
word sequences input and concatenated at the in-
termediate layer. Each sequential process NN is
described in Section 5.1.1-5.1.3 and surrounded
by broken-line boxes in Figures. 5-7 for charac-
ter sequences and word sequences independently.
Output of the each sequential process is hNN. Af-
ter that, character and word sequences are concate-
nated, and the subsequent process is the same as
that in Section 5.1. A series of processes is ob-

Figure 8: Overview of structure inputting both charac-
ter and word sequences.

]

[

(

tained as follows,

)
hint: = a
(10)
where the intermediate layer FFNN is (W int: 2
Rm(cid:2)2m, bint: 2 Rm). Then, hint: is fed to Equa-
tion (2).

char; hNN
hNN
word

+ bint:

W int:

6 Experimental Evaluation
The performances of classiﬁcation methods are
evaluated in an experimental evaluation.
For
comparison, baseline methods are used that use
keyword ﬁltering or Support Vector Machines
(SVMs) (Vapnik and Lerner, 1963), which are
well known to having high classiﬁcation perfor-
mance (Wang and Manning, 2012). In this paper,

158Int. layer (LSTM)Output layer (FFNN)𝒉"#$%𝒉&’(Binary output(Softmax)Input layer (FFNN)One-hot vecrorizeSequential processInt. layer (FFNN)𝒉)*(.{𝒉-)*}{𝒙-}∈ℝ2∈ℝ2∈ℝ3∈𝑛×ℝ2∈𝑛×ℝ6Eq. (4)Eq. (1)Eq. (2)Eq. (5)𝑁: No. of input layer dimensions𝑛	: Max no. of sequences𝑚: No. of int. layer dimensions𝑡: Sequential indexChar. or Wordsequences input𝒉"#$𝑁: No. of input layer dimensions𝑛	: Max no. of sequences𝑚: No. of int. layer dimensions𝑡: Sequential index𝑙: No. of filters𝑝: Filter indexBinary output(Softmax)Char. or Wordsequences inputInput layer (FFNN)One-hot vecrorizeSequential processInt. layer (FFNN)𝒉,--.{𝒉012}{𝒙0}Pooling layerInt. layer (Conv.)𝒉0,"25.,7{𝒉8""9,7}∈𝑙×ℝ=∈ℝ=∈ℝ>∈𝑙×𝑛×ℝ=∈𝑛×ℝ=∈𝑛×ℝ?Output layer (FFNN)Eq. (7)Eq. (9)Eq. (2)Eq. (8)Eq. (6)Intermediate layer (FFNN)Output layer (FFNN)𝒉"#$.𝒉&’$Char. seq. processBinary output(Softmax)Char. sequencesinputWord seq. processWordsequencesinput𝒉()*+,,𝑛	: Max no. of sequences𝑚: No. of int. layer dimensions∈ℝ2∈ℝ3∈ℝ2Eq. (10)Eq. (2)𝒉4&+5,,∈ℝ2I- 1⃝ I- 2⃝
5,027
4,273
1,184
844

Train.
Test

II
35,370
7,972

Total
44,670
10,000

Date
Jun., 2017
Jul., 2017

Table 2: No. of each Tweets.

594 sources of REPORTED from training data are
used for the keyword ﬁltering baseline and Lin-
earSVC modules of scikit-learn (Pedregosa et al.,
2011) are used for the SVM baseline. Thus, data
include REPORTED of various sources.
In addi-
tion, the Word2Vec vector for the SVM baseline
is the average of each 200–dimension word vector
that is made with a Wikipedia dump corpus by us-
ing Word2Vec skip-gram modules (Mikolov et al.,
2013).

6.1 Experimental Conditions
For both training data and test data, EVENT for
news reports extracted by the extraction process in
Section 4.1 are used. Training data is all 44,670
Tweets obtained in the extraction process on June
6th, 8th, 10th, and 12th, 2017. Test data is 10,000
randomly sampled Tweets obtained in the extrac-
tion process output Tweets on July 6th, 8th, 10th,
and 12th, 2017. Output data is annotated into three
categories I- 1⃝: REPORTED with explicit sources,
I- 2⃝: REPORTED without explicit sources, and II:
NEW by an annotator person. Table 2 shows the
amount of each type of annotated data. Table 3
shows the conﬁguration of experimental parame-
ters.

6.2 Experimental Results
Table 4 shows precision, recall, and F1 score for
each method with input as character, word, or
character and word. Table 5 shows recall perfor-
mance of using REPORTED to evaluate the perfor-
mance with and without explicit sources. False
negatives are judged for only REPORTED and can-
not be divided into REPORTED with and without
explicit sources, thus precision and F1 score can-
not be used in this evaluation. Table 6 shows the
time required to learn each NN. Finally, Figure 9
shows the F1 score of SVM baseline methods and
CNN architectures trained by each number of ran-
dom sampled training data.

From Table 4, the keyword baseline method has
94.1% precision and 34.0% recall. From Table 5,
its recall is 73.5 % lower when using only I- 2⃝
than when using only I- 1⃝ (3.4 % vs. 76.9 %).

All NN architectures outperform all baseline

methods. Although word input using FFNN,
which has the lowest F1 score of the NN archi-
tectures, has the same precision as the SVM word
input baseline method, which has highest F1 score
of the baseline methods, it has higher recall (76.7
% vs. 75.7 %) and F1 score (84.2 % vs. 83.6 %).
Its recall is 22.5 % lower when using only I- 2⃝
than when using only I- 1⃝ (67.3 % vs. 89.8 %).

In each NN architecture, the F1 score for char-
acter input is 0.3–2.0% higher than for word in-
put. When both characters and words are input,
LSTM has the highest F1 score, 0.5-2.2% higher
than those of other NNs. When the conventional
method is used, the highest F1 score so far is 86.7
% for LSTM architecture using character input.
For this LSTM recall is 15.8 % higher when us-
ing only I- 1⃝ than when using only I- 2⃝ (94.6 %
vs. 78.8 %).

The proposed concat input method has a higher
F1 score than character input for each NN ar-
chitecture. Especially, F1 scores for the LSTM
and CNN architectures improved from 86.7 % and
86.2 % to 88.2%. With the proposed concat input
method, the difference between recall for CNNs
with and without explicit sources is only 18.5 %
(95% vs. 76.5%), whereas it is 22.1% and 23.8%
with character and word input NNs. In addition,
the training time of the CNN architecture is almost
1/3 that of the LSTM architecture, as shown in Ta-
ble 6.

From Figure 9, baseline SVM methods have
higher F1 scores than CNN architectures when
they are trained by using fewer than 10,000 train-
ing data. However, CNN architectures have higher
F1 scores than SVM methods when they are
trained by using more than 25,000 training data.
The architecture using the proposed method has
a lower F1 score than other architectures when
trained by using fewer than 13,000 training data
but has the highest F1 score when trained by using
more than 20,000 training data.

6.3 Experimental Result Discussion

For the keyword baseline method, since Tweets
with the same source used as training data can be
detected for I- 1⃝ in test data, there are few false
detections and overall precision is relatively high.
However, the keyword baseline method can barely
detect Tweets for I- 2⃝. In contrast, all other meth-
ods can obtain recall rates of at least 67 % for I-
2⃝. This result indicates machine learning meth-

159Deep learning framework
Gradient descent algorithm
No. of iterations
Dim. of each int. layer
Mini batch size
Dropout (Srivastava et al., 2014) ratio
Each CNN ﬁlter size
Initial values of NNs
Unknown elements (character and word)
Classiﬁcation
Evaluating measures

Chainer (Tokui et al., 2015)
Adam (Kingma and Ba, 2014)
5
200
100
0.5 (except for the conv. layer and the pooling layer)
2, 2, 3, 3, 4, 4 (for CNN (Zhang and Wallace, 2017))
random
Elements that appear fewer than 10 times in training data
I: REPORTED and II: NEW (described in Section 4.2)
The macro average of 20 trials precision, recall, and F1 score

Table 3: Conﬁguration of experimental parameters.

Input

Char. input

Word input

NN arch.
FFNN
LSTM
CNN
FFNN
LSTM
CNN
FFNN
LSTM

Concat input

(Proposed) CNN

Baseline (Keyword)
Baseline (SVM Char. input)
Baseline (SVM Word input)
Baseline (SVM Word2Vec input)

Pre. Rec.
92.1
79.3
85.4
86.6
79.9
94.3
76.7
93.3
83.0
90.3
93.8
76.9
79.6
93.2
85.5
91.3
84.2
93.0
94.1
34.0
76.9
90.1
75.7
93.3
85.8
80.6

F1
85.2
86.7
86.2
84.2
86.4
84.2
85.8
88.2
88.2
50.0
83.0
83.6
83.1

Input

Char. input

Word input

NN arch.
FFNN
LSTM
CNN
FFNN
LSTM
CNN
FFNN
LSTM

Concat input

(Proposed) CNN

Baseline (Keyword)
Baseline (SVM Char. input)
Baseline (SVM Word input)
Baseline (SVM Word2Vec input)

I- 1⃝ I- 2⃝ I
79.3
92.0
85.4
94.6
79.9
92.8
76.7
89.8
83.0
93.5
90.8
76.9
79.6
92.0
85.5
95.3
84.2
95.0
76.9
34.0
76.9
89.3
75.7
87.9
88.5
80.6

70.3
78.8
70.7
67.3
75.6
67.0
70.8
78.5
76.5
3.4
68.1
67.1
74.9

Table 4: Macro average performance of proposed clas-
siﬁcation methods (%).

Table 5: Macro average recall performance for types of
each test data (%).

ods are effective for the Tweet classiﬁcation task
in this paper. Moreover, all NN architectures have
higher F1 score than SVM methods. This result
indicates NN architectures are more effective than
SVM methods for the task, especially when they
have enough training data.

The CNN or LSTM architectures have higher
F1 scores than the FFNN architecture for almost
all kinds of input. From this fact, incorporating
time series into the learning structure contributes
to classifying REPORTED and NEW. CNN archi-
tectures have a higher precision but a lower re-
call than LSTM architectures. Especially for I-
2⃝, which is expected to present a higher degree
of difﬁculty than I- 1⃝, CNN architectures using
character or word input have 67.0 –70.7% recall
whereas LSTM architectures have 75.6–78.8 %
recall. These results are due to the difference in
structure: a CNN uses time series only within the
ﬁlter size, whereas an LSTM uses the time series
of the entire sequence.

In all NN architectures, the proposed concat in-
put method has the best F1 score, followed by
character input and word input.
It is considered
that the advantage of the character sequences de-

hhhhhhhhhhhh

NN arch.

Input

Char. input
Word input
Concat input (Proposed)

FFNN LSTM CNN
86
284
440

675
615
1,200

60
40
110

Table 6: Training time of NNs (seconds).

scribed in Section 5.2 exceeds the advantage of
the word sequences, and in the proposed concat
input method, both elements are automatically se-
lected, so NN architectures are trained more ef-
fectively. The CNN architecture was particularly
improved, with its recall increasing to 84.2 % for
all reported Tweets (I) and to 76.5 % for I- 2⃝, only
slightly lower than the recall for the LSTM archi-
tecture. As a result, when the proposed concat
input method is used, though the CNN architec-
ture requires only 37 minutes at 5 epochs, it has
almost the same F1 score as the LSTM architec-
ture because the concat input method utilizes the
advantages of character sequences (i.e., there are
fewer unknown characters than unknown words),
and word sequences (i.e., words are more effective
than characters for the task).

Considering real-world use, training data can be

160ing opinions about news or reports after the pri-
mary report without citations.
In the future, on
the basis of the output of the system using the
proposed method, we will consider extending the
method to the systems collecting Tweets that men-
tion the same topic, which are used in event detec-
tion tasks. This will make it easier for TV program
creators to acquire the information they want. The
proposed task is important for our systems, so
we increase the reliability of datasets by using
more annotators. Moreover, we will consider us-
ing other tasks for evaluating proposed methods.

Figure 9: Macro average F1 score for each training data
size reduced by all samples.

gathered on the basis of feedback from the TV pro-
gram production, so training each time the kind
of classiﬁcation changes is also assumed. Under
these conditions, a short training time is highly
convenient and is a big advantage. From this re-
sult, a CNN using the proposed method has the
best balance of speed and accuracy, so it is the
most suitable for our system.

CNN architectures trained by using a few train-
ing data (less than 10,000) have lower F1 scores
than SVM methods, seems to be caused by CNN
architectures having many training parameters.
Speciﬁcally, the CNN architecture using the pro-
posed method has the lowest F1 score when it has
the most training parameters. However, it has the
highest F1 score when it is trained by using a lot of
training data. When collecting at least 10,000 and
ideally more than 30,000 training data, NN archi-
tectures are effective for the classiﬁcation task in
this paper.

7 Conclusion

We developed a system to classify extracted
“Event-describing Tweets” for news reports into
“Reported-event Tweets” to obtain reports after
a primary report or collect opinions and “New-
information Tweets” to obtain primary reports and
for tracking reports of the same event. A con-
volutional neural network could classify Tweets
with an F1 score of 88 % by using our pro-
posed method, which inputs character and word
sequences, concatenates them in the intermediate
layer, and outputs them within 37 minutes train-
ing time. However, systems using the proposed
method also incorrectly extracted Tweets includ-

161 40 50 60 70 80 900.0×1005.0×1031.0×1041.5×1042.0×1042.5×1043.0×1043.5×1044.0×1044.5×104F1 score (%)No. of training dataCNN (Char. input)CNN (Word input)CNN (Proposed: Concat input)Baseline (Keyword)Baseline (SVM Char. input)Baseline (SVM Word input)Baseline (SVM Word2Vec input)References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
In Proceedings of
learning to align and translate.
the 3rd International Conference on Learning Rep-
resentations, ICLR 2015, pages 1–15.

Djork-Arn´e Clevert, Andreas Mayr, Thomas Un-
terthiner, and Sepp Hochreiter. 2015. Rectiﬁed fac-
In Advances in Neural Information
tor networks.
Processing Systems 28, NIPS 2015, pages 1855–
1863. Curran Associates, Inc.

Nimesh Ghelani, Salman Mohammed, Shine Wang,
and Jimmy Lin. 2017. Event detection on curated
In Proceedings of the 40th Inter-
tweet streams.
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’17,
pages 1325–1328, New York, NY, USA. ACM.

Stig-Arne Gr¨onroos, Sami Virpioja, and Mikko Ku-
rimo. 2017. Extending hybrid word-character neural
machine translation with multi-task learning of mor-
In Proceedings of the Second
phological analysis.
Conference on Machine Translation, WMT, pages
296–302. Association for Computational Linguis-
tics.

Geert Heyman, Ivan Vuli´c, and Marie-Francine Moens.
2017. Bilingual lexicon induction by learning to
combine word-level and character-level representa-
tions. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, EACL
2017, pages 1085–1095. Association for Computa-
tional Linguistics.

Hayate Iso, Shoko Wakamiya, and Eiji Aramaki. 2016.
Forecasting word model: Twitter-based inﬂuenza
surveillance and prediction. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers, pages
76–86. The COLING 2016 Organizing Committee.

Kristen Johnson and Dan Goldwasser. 2016.

“All
I know about politics is what I read in Twitter”:
Weakly supervised models for extracting politicians’
In Proceedings of COLING
stances from twitter.
2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, pages 2966–
2977. The COLING 2016 Organizing Committee.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
In Proceed-
method for stochastic optimization.
ings of the 3rd International Conference on Learn-
ing Representations, ICLR 2015, pages 1–15.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
Applying conditional random ﬁelds to
2004.
japanese morphological analysis. In Proceedings of
Empirical Methods in Natural Language Process-
ing, EMNLP, pages 230–237.

Dongyun Liang, Weiran Xu, and Yinge Zhao. 2017.
Combining word-level and character-level represen-
tations for relation classiﬁcation of informal text. In
Proceedings of the 2nd Workshop on Representa-
tion Learning for NLP, pages 43–47. Association for
Computational Linguistics.

Bill Y. Lin, Frank Xu, Zhiyi Luo, and Kenny Zhu.
2017. Multi-channel bilstm-crf model for emerg-
ing named entity recognition in social media.
In
Proceedings of the 3rd Workshop on Noisy User-
generated Text, pages 160–165. Association for
Computational Linguistics.

Marina Litvak, Natalia Vanetik, Eﬁ Levi, and Michael
catch
Roistacher. 2016. What’s up on twitter?
In Proceedings of COLING 2016,
up with twist!
the 26th International Conference on Computational
Linguistics: System Demonstrations, pages 213–
217. The COLING 2016 Organizing Committee.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), ACL ’16, pages 1064–1074. Associ-
ation for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119, USA. Curran
Associates Inc.

Taro Miyazaki, Shin Toriumi, Yuka Takei, Ichiro Ya-
mada, and Jun Goto. 2017.
Extracting impor-
tant tweets for news writers using recurrent neural
network with attention mechanism and multi-task
In Proceedings of the 31st Paciﬁc Asia
learning.
Conference on Language, Information and Compu-
tation, PACLIC 31, pages 363–369. The National
University (Phillippines).

Graham Neubig, Yuichiroh Matsubayashi, Masato
Hagiwara, and Koji Murakami. 2011. Safety infor-
mation mining — what can nlp do in a disaster—
In Proceedings of 5th International Joint Con-
.
ference on Natural Language Processing, IJCNLP
2011, pages 965–973. Asian Federation of Natural
Language Processing.

Symeon Papadopoulos, David Corney, and Luca Maria
Aiello. 2014. Snow 2014 data challenge: Assess-
ing the performance of news topic detection meth-
In Proceedings of the SNOW
ods in social media.
2014 Data Challenge.

Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.

162Ye Zhang and Byron Wallace. 2017. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classiﬁcation. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), IJCNLP 2017, pages 253–263. Asian
Federation of Natural Language Processing.

Deyu Zhou, Tianmeng Gao, and Yulan He. 2016.
Jointly event extraction and visualization on twitter
In Proceedings of the
via probabilistic modelling.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), ACL
’16, pages 269–278. Association for Computational
Linguistics.

Scikit-learn: Machine learning in python. J. Mach.
Learn. Res., 12:2825–2830.

Swen Ribeiro, Olivier Ferret, and Xavier Tannier. 2017.
Unsupervised event clustering and aggregation from
In Proceedings of the
newswire and web articles.
2017 EMNLP Workshop: Natural Language Pro-
cessing meets Journalism, pages 62–67. Association
for Computational Linguistics.

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.
Semeval-2017 task 4: Sentiment analysis in twitter.
In Proceedings of the 11th International Workshop
on Semantic Evaluations (SemEval-2017), pages
502–518.

Toshinori Sato. 2015. Neologism dictionary based
on the language resources on the Web for
Mecab.
https://github.com/neologd/mecab-ipadic-
neologd. Accessed: 2018-02-01.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. J. Mach. Learn. Res., 15(1):1929–
1958.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a next-generation open
In Proceed-
source framework for deep learning.
ings of NIPS 2015 workshop on machine learning
systems (LearningSys).

Svitlana Vakulenko, Lyndon Nixon, and Mihai Lupu.
2017. Character-based neural embeddings for tweet
clustering. In Proceedings of the Fifth International
Workshop on Natural Language Processing for So-
cial Media, pages 36–44. Association for Computa-
tional Linguistics.

Vladimir Vapnik and Aleksander Lerner. 1963. Pattern
recognition using generalized portrait method. Au-
tomation and Remote Control, 24:774–780.

Prashanth Vijayaraghavan,

Ivan Sysoev, Soroush
Vosoughi, and Deb Roy. 2016. Deepstance at
semeval-2016 task 6: Detecting stance in tweets us-
ing character and word-level cnns. In the 10th Inter-
national Workshop on Semantic Evaluation, volume
abs/1606.05694 of SemEval-2016, pages 413–419.

Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and
Nathan Hodas. 2017. Separating facts from ﬁction:
Linguistic models to classify suspicious and trusted
In Proceedings of the 55th
news posts on twitter.
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), ACL
’17, pages 647–653. Association for Computational
Linguistics.

Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and
topic classiﬁcation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers - Volume 2, ACL ’12,
pages 90–94, Stroudsburg, PA, USA. Association
for Computational Linguistics.

163
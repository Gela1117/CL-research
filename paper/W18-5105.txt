Challenges for Toxic Comment Classiﬁcation:

An In-Depth Error Analysis

Betty van Aken1, Julian Risch2, Ralf Krestel2, and Alexander L¨oser1

1Beuth University of Applied Sciences, Germany
{bvanaken, aloeser}@beuth-hochschule.de

2Hasso Plattner Institute, University of Potsdam, Germany

firstname.lastname@hpi.de

Abstract

Toxic comment classiﬁcation has become an
active research ﬁeld with many recently pro-
posed approaches. However, while these ap-
proaches address some of the task’s challenges
others still remain unsolved and directions for
further research are needed. To this end, we
compare different deep learning and shallow
approaches on a new, large comment dataset
and propose an ensemble that outperforms all
individual models. Further, we validate our
ﬁndings on a second dataset. The results of
the ensemble enable us to perform an exten-
sive error analysis, which reveals open chal-
lenges for state-of-the-art methods and direc-
tions towards pending future research. These
challenges include missing paradigmatic con-
text and inconsistent dataset labels.

1

Introduction

Keeping online conversations constructive and in-
clusive is a crucial task for platform providers.
Automatic classiﬁcation of toxic comments, such
as hate speech, threats, and insults, can help in
keeping discussions fruitful. In addition, new reg-
ulations in certain European countries have been
established enforcing to delete illegal content in
less than 72 hours.1

Active research on the topic deals with com-
mon challenges of natural language processing,
such as long-range dependencies or misspelled
and idiosyncratic words. Proposed solutions in-
clude bidirectional recurrent neural networks with
attention (Pavlopoulos et al., 2017) and the use
of pretrained word embeddings (Badjatiya et al.,
2017). However, many classiﬁers suffer from in-
sufﬁcient variance in methods and training data
and therefore often tend to fail on the long tail of
real world data (Zhang and Luo, 2018). For future
research, it is essential to know which challenges

are already addressed by state-of-the-art classiﬁers
and for which challenges current solutions are still
error-prone.

We take two datasets into account to investi-
gate these errors: comments on Wikipedia talk
pages presented by Google Jigsaw during Kag-
gle’s Toxic Comment Classiﬁcation Challenge2
and a Twitter Dataset by Davidson et al. (2017).
These sets include common difﬁculties in datasets
for the task: They are labeled based on differ-
ent deﬁnitions; they include diverse language from
user comments and Tweets; and they present a
multi-class and a multi-label classiﬁcation task re-
spectively.

On these datasets we propose an ensemble of
state-of-the-art classiﬁers. By analysing false neg-
atives and false positives of the ensemble we get
insights about open challenges that all of the ap-
proaches share. Therefore, our main contributions
are:

1) We are the ﬁrst to apply and compare a
range of strong classiﬁers to a new public multi-
label dataset of more than 200,000 user comments.
Each classiﬁer, such as Logistic Regression, bidi-
rectional RNN and CNN, is meant to tackle spe-
ciﬁc challenges for text classiﬁcation. We apply
the same classiﬁers to a dataset of Tweets to vali-
date our results on a different domain.

2) We apply two different pretrained word em-
beddings for the domain of user comments and
Tweets to compensate errors such as idiosyncratic
and misspelled words.

3) We compare the classiﬁers’ predictions and
show that
they make different errors as mea-
sured by Pearson correlation coefﬁcients and F1-
measures. Based on this, we create an ensem-
ble that improves macro-averaged F1-measure es-
pecially on sparse classes and data with high
variance.

1https://www.bbc.com/news/technology-

2https://www.kaggle.com/c/jigsaw-

42510868

toxic-comment-classification-challenge

ProceedingsoftheSecondWorkshoponAbusiveLanguageOnline(ALW2),pages33–42Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics334) We perform a detailed error analysis on re-
sults of the ensemble. The analysis points to com-
mon errors of all current approaches. We propose
directions for future work based on these unsolved
challenges.

2 Related Work

Task deﬁnitions. Toxic comment classiﬁcation
is not clearly distinguishable from its related
tasks. Besides looking at toxicity of online com-
ments (Wulczyn et al., 2017; Georgakopoulos
et al., 2018), related research includes the investi-
gation of hate speech (Badjatiya et al., 2017; Bur-
nap and Williams, 2016; Davidson et al., 2017;
Gamb¨ack and Sikdar, 2017; Njagi et al., 2015;
Schmidt and Wiegand, 2017; Vigna et al., 2017;
Warner and Hirschberg, 2012), online harass-
ment (Yin and Davison, 2009; Golbeck et al.,
2017), abusive language (Mehdad and Tetreault,
2016; Park and Fung, 2017), cyberbullying (Dad-
var et al., 2013; Dinakar et al., 2012; Hee et al.,
2015; Zhong et al., 2016) and offensive lan-
guage (Chen et al., 2012; Xiang et al., 2012).
Each ﬁeld uses different deﬁnitions for their clas-
siﬁcation, still similar methods can often be ap-
plied to different tasks. In our work we focus on
toxic comment detection and show that the same
method can effectively be applied to a hate speech
detection task.

toxic

aspects of

Multi-class approaches. Besides
traditional
related work con-
binary classiﬁcation tasks,
siders different
language,
such as “racism” (Greevy and Smeaton, 2004;
Waseem, 2016; Kwok and Wang, 2013) and
“sexism” (Waseem and Hovy, 2016; Jha and
Mamidi, 2017), or the severity of toxicity (David-
son et al., 2017; Sharma et al., 2018). These
tasks are framed as multi-class problems, where
each sample is labeled with exactly one class out
of a set of multiple classes. The great majority
of related research considers only multi-class
problems. This is remarkable, considering that in
real-world scenarios toxic comment classiﬁcation
can often be seen as a multi-label problem, with
user comments fulﬁlling different predeﬁned
criteria at the same time. We therefore investigate
both a multi-label dataset containing six different
forms of toxic language and a multi-class dataset
containing three mutually exclusive classes of
toxic language.

Shallow classiﬁcation and neural networks.
Toxic comment identiﬁcation is a supervised clas-
siﬁcation task and approached by either meth-
ods including manual feature engineering (Burnap
and Williams, 2015; Mehdad and Tetreault, 2016;
Waseem, 2016; Davidson et al., 2017; Nobata
et al., 2016; Kennedy et al., 2017; Samghabadi
et al., 2017; Robinson et al., 2018) or the use of
(deep) neural networks (Ptaszynski et al., 2017;
Pavlopoulos et al., 2017; Badjatiya et al., 2017; Vi-
gna et al., 2017; Park and Fung, 2017; Gamb¨ack
and Sikdar, 2017). While in the ﬁrst case manu-
ally selected features are combined into input vec-
tors and directly used for classiﬁcation, neural net-
work approaches are supposed to automatically
learn abstract features above these input features.
Neural network approaches appear to be more ef-
fective for learning (Zhang and Luo, 2018), while
feature-based approaches preserve some sort of
explainability. We focus in this paper on base-
lines using deep neural networks (e.g. CNN and
Bi-LSTM) and shallow learners, such as Logistic
Regression approaches on word n-grams and char-
acter n-grams.

learning. Burnap

Ensemble
and Williams
(2015) studied advantages of ensembles of differ-
ent classiﬁers. They combined results from three
feature-based classiﬁers. Further the combination
of results from Logistic Regression and a Neural
Network has been studied (Gao and Huang,
2017; Risch and Krestel, 2018). Zimmerman
et al. (2018) investigated ensembling models with
different hyper-parameters. To our knowledge, the
approach presented in this paper, combining both
various model architectures and different word
embeddings for toxic comment classiﬁcation, has
not been investigated so far.

3 Datasets and Tasks

The task of toxic comment classiﬁcation lacks a
consistently labeled standard dataset for compar-
ative evaluation (Schmidt and Wiegand, 2017).
While there are a number of annotated pub-
lic datasets in adjacent ﬁelds,
such as hate
speech (Ross et al., 2016; Gao and Huang,
2017), racism/sexism (Waseem, 2016; Waseem
and Hovy, 2016) or harassment (Golbeck et al.,
2017) detection, most of them follow different def-
initions for labeling and therefore often constitute
different problems.

34Class
Clean
Toxic
Obscene
Insult
Identity Hate
Severe Toxic
Threat

# of occurrences
201,081
21,384
12,140
11,304
2,117
1,962
689

Table 1: Class distribution of Wikipedia dataset. The
distribution shows a strong class imbalance.

Class
Offensive
Clean
Hate

# of occurrences
19,190
4,163
1,430

Table 2: Class distribution of Twitter dataset. The ma-
jority class of the dataset consists of offensive Tweets.

3.1 Wikipedia Talk Pages dataset
We analyse a dataset published by Google Jigsaw
in December 2017 over the course of the ‘Toxic
Comment Classiﬁcation Challenge’ on Kaggle. It
includes 223,549 annotated user comments col-
lected from Wikipedia talk pages and is the largest
publicly available for the task. These comments
were annotated by human raters with the six labels
‘toxic’, ‘severe toxic, ‘insult’, ‘threat’, ‘obscene’
and ‘identity hate’. Comments can be associated
with multiple classes at once, which frames the
task as a multi-label classiﬁcation problem. Jig-
saw has not published ofﬁcial deﬁnitions for the
six classes. But they do state that they deﬁned a
toxic comment as “a rude, disrespectful, or unrea-
sonable comment that is likely to make you leave
a discussion”.3

The dataset features an unbalanced class distri-
bution, shown in Table 1. 201,081 samples fall un-
der the majority ‘clear’ class matching none of the
six categories, whereas 22,468 samples belong to
at least one of the other classes. While the ‘toxic’
class includes 9.6% of the samples, only 0.3% are
labeled as ‘threat’, marking the smallest class.

Comments were collected from the English
Wikipedia and are mostly written in English
with some outliers, e.g., in Arabic, Chinese or
German language. The domain covered is not

3http://www.perspectiveapi.com/

strictly locatable, due to various article topics
being discussed. Still it is possible to apply a
simple categorization of comments as follows:4
1) ‘community-related’:
Example: “If you continue to vandalize
Wikipedia, you will be blocked from editing.”
2) ‘article-related’:
Example: “Dark Jedi Miraluka from the Mid-
Rim world of Katarr, Visas Marr is the lone sur-
viving member of her species.”
3) ‘off-topic’:
Example: “== I hate how my life goes today
== Just kill me now.”

3.2 Twitter dataset
Additionally we investigate a dataset introduced
by Davidson et al. (2017).
It contains 24,783
Tweets fetched using the Twitter API and anno-
tated by CrowdFlower workers with the labels
‘hate speech’, ‘offensive but not hate speech’ and
‘neither offensive nor hate speech’. Table 2 shows
the class distribution. We observe a strong bias
towards the offensive class making up 77.4% of
the comments caused by sampling tweets by seed
keywords from Hatebase.org. We choose this
dataset to show that our method is also applicable
to multi-class problems and works with Tweets,
which usually have a different structure than other
online user comments due to character limitation.

3.3 Common Challenges
We observe three common challenges for Natural
Language Processing in both datasets:

Out-of-vocabulary words. A common problem
for the task is the occurrence of words that are
not present in the training data. These words
include slang or misspellings, but also inten-
tionally obfuscated content.

Long-Range Dependencies. The toxicity of a
comment often depends on expressions made
in early parts of the comment. This is espe-
cially problematic for longer comments (>50
words) where the inﬂuence of earlier parts on
the result can vanish.

4Disclaimer: This paper contains examples that may be
considered profane, vulgar, or offensive. These contents do
not reﬂect the views of the authors and exclusively serve to
explain linguistic research challenges.

35Multi-word phrases. We see many occurrences
of multi-word phrases in both datasets. Our
algorithms can detect their toxicity only if
they can recognize multiple words as a sin-
gle (typical) hateful phrase.

4 Methods and Ensemble

In this section we study baseline methods for the
above mentioned common challenges. Further, we
propose our ensemble learning architecture.
Its
goal is to minimize errors by detecting optimal
methods for a given comment.

4.1 Logistic Regression
The Logistic Regression (LR) algorithm is widely
used for binary classiﬁcation tasks. Unlike deep
learning models, it requires manual feature engi-
neering. Contrary to Deep Learning methods, LR
permits obtaining insights about the model, such
as observed coefﬁcients. Research from Waseem
and Hovy (2016) shows that word and character
n-grams belong to one of the most indicative fea-
tures for the task of hate speech detection. For this
reason we investigate the use of word and charac-
ter n-grams for LR models.

4.2 Recurrent Neural Networks
Recurrent Neural Networks (RNNs) interpret a
document as a sequence of words or character n-
grams. We use four different RNN approaches:
An LSTM (Long-Short-Term-Memory Network),
a bidirectional LSTM, a bidirectional GRU (Gated
Recurrent Unit) architecture and a bidirectional
GRU with an additional attention layer.

LSTM. Our LSTM model takes a sequence of
words as input. An embedding layer transforms
one-hot-encoded words to dense vector represen-
tations and a spatial dropout, which randomly
masks 10% of the input words, makes the network
more robust. To process the sequence of word em-
beddings, we use an LSTM layer with 128 units,
followed by a dropout of 10%. Finally, a dense
layer with a sigmoid activation makes the predic-
tion for the multi-label classiﬁcation and a dense
layer with softmax activation makes the prediction
for the multi-class classiﬁcation.

Bidirectional LSTM and GRU. Bidirectional
RNNs can compensate certain errors on long range
dependencies. In contrast to the standard LSTM
model, the bidirectional LSTM model uses two

LSTM layers that process the input sequence in
opposite directions. Thereby, the input sequence is
processed with correct and reverse order of words.
The outputs of these two layers are averaged. Sim-
ilarly, we use a bidirectional GRU model, which
consists of two stacked GRU layers. We use lay-
ers with 64 units. All other parts of the neural
network are inherited from our standard LSTM
model. As a result, this network can recognize
signals on longer sentences where neurons repre-
senting words further apart from each other in the
LSTM sequence will ‘ﬁre’ more likely together.

Bidirectional GRU with Attention Layer. Gao
and Huang (2017) phrase that “attention mecha-
nisms are suitable for identifying speciﬁc small
regions indicating hatefulness in long comments”.
In order to detect these small regions in our com-
ments, we add an attention layer to our bidirec-
tional GRU-based network following the work
of Yang et al. (2016).

4.3 Convolutional Neural Networks
Convolutional Neural Networks (CNNs) are re-
cently becoming more popular for text classiﬁca-
tion tasks. By intuition they can detect speciﬁc
combinations of features, while RNNs can extract
orderly information (Zhang and Luo, 2018). On
character level, CNNs can deal with obfuscation
of words. For our model we choose an architec-
ture comparable to the approach of Kim (2014).

(Sub-)Word Embeddings

4.4
Using word embeddings trained on very large cor-
pora can be helpful in order to capture informa-
tion that is missing from the training data (Zhang
and Luo, 2018). Therefore we apply Glove word
embeddings trained on a large Twitter corpus by
Pennington et al. (2014). In addition, we use sub-
word embeddings as introduced by Bojanowski
et al. (2017) within the FastText tool. The ap-
proach considers substrings of a word to infer its
embedding. This is important for learning rep-
resentations for misspelled, obfuscated or abbre-
viated words which are often present in online
comments. We train FastText embeddings on 95
million comments on Wikipedia user talk pages
and article talk pages.5 We apply the skip-gram
method with a context window size of 5 and train
for 5 epochs.

5https://figshare.com/articles/

Wikipedia_Talk_Corpus/4264973

36Model

CNN (FastText)
CNN (Glove)
LSTM (FastText)
LSTM (Glove)
Bidirectional LSTM (FastText)
Bidirectional LSTM (Glove)
Bidirectional GRU (FastText)
Bidirectional GRU (Glove)
Bidirectional GRU Attention (FastText)
Bidirectional GRU Attention (Glove)
Logistic Regression (char-ngrams)
Logistic Regression (word-ngrams)
Ensemble

P
.73
.70
.71
.74
.71
.74
.72
.73
.74
.73
.74
.70
.74

Wikipedia
R
.86
.85
.85
.84
.86
.84
.86
.85
.87
.87
.84
.83
.88

.776
.748
.752
.777
.755
.777
.765
.772
.783
.779
.776
.747
.791

F1 AUC
.981
.979
.978
.980
.979
.981
.981
.981
.983
.983
.975
.962
.983

P
.73
.72
.73
.74
.72
.73
.72
.76
.74
.77
.73
.71
.76

Twitter
R
.83
.82
.83
.82
.84
.85
.83
.81
.83
.82
.81
.80
.83

.775
.769
.778
.781
.775
.783
.773
.784
.791
.790
.764
.746
.793

F1 AUC
.948
.945
.955
.953
.954
.953
.955
.955
.958
.952
.937
.933
.953

Table 3: Comparison of precision, recall, F1-measure, and ROC AUC on two datasets. The results show that the
ensemble outperforms the individual classiﬁers in F1-measure. The strongest individual classiﬁer on both datasets
is a bidirectional GRU network with attention layer.

4.5 Ensemble Learning
Each classiﬁcation method varies in its predictive
power and may conduct speciﬁc errors. For exam-
ple, GRUs or LSTMs may miss long range depen-
dencies for very long sentences with 50 or more
words but are powerful in capturing phrases and
complex context information. Bi-LSTMs and at-
tention based networks can compensate these er-
rors to a certain extent. Subword Embeddings can
model even misspelled or obfuscated words.

Therefore, we propose an ensemble deciding
which of the single classiﬁers is most powerful on
a speciﬁc kind of comment. The ensemble ob-
serves features in comments, weights and learns
an optimal classiﬁer selection for a given feature
combination. For achieving this functionality, we
observe the set of out-of-fold predictions from the
various approaches and train an ensemble with
gradient boosting decision trees. We perform 5-
fold cross-validation and average ﬁnal predictions
on the test set across the ﬁve trained models.
5 Experimental Study
Our hypothesis is that the ensemble learns to
choose an optimal combination of classiﬁers based
on a set of comment features. Because the classi-
ﬁers have different strengths and weaknesses, we
expect the ensemble to outperform each individ-
ual classiﬁer. Based on results from previous ex-
periments mentioned in Section 2 we expect that

the state-of-the-art models have a comparable per-
formance and none outperforms the others sig-
niﬁcantly. This is important because otherwise
the ensemble learner constantly prioritizes the out-
performing classiﬁer. We expect our ensemble
to perform well on both online comments and
Tweets despite their differing language character-
istics such as comment length and use of slang
words.

5.1 Setup
To evaluate our hypotheses, we use the following
setup: We compare six methods from Section 4.
For the neural network approaches we apply two
different word embeddings each and for LR we
use character and word n-grams as features.

We need binary predictions to calculate pre-
cision, recall and the resulting F1-measure. To
translate the continuous sigmoid output for the
multi-label task (Wikipedia dataset) into binary la-
bels we estimate appropriate threshold values per
class. For this purpose we perform a parame-
ter search for the threshold to optimize the F1-
measure using the whole training set as validation.
In case of the multi-class task (Twitter dataset) the
softmax layer makes the parameter search need-
less, because we can simply take the label with the
highest value as the predicted one.

We choose the macro-average F1 measure since
it is more indicative than the micro-average F1

37for strongly unbalanced datasets (Zhang and Luo,
2018). For the multi-label classiﬁcation we mea-
sure macro-precision and -recall for each class
separately and average their results to get the F1-
measure per classiﬁer. The Area under the Re-
ceiver Operating Curve (ROC AUC) gives us a
measurement of classiﬁer performance without the
need for a speciﬁc threshold. We add it to provide
additional comparability of the results.

5.2 Correlation Analysis
Total accuracy of the ensemble can only improve
when models with comparable accuracy produce
uncorrelated predictions. We therefore measure
the correlation of the predictions of different clas-
siﬁers. We look at a set of combinations, such as
shallow learner combined with a neural net, and
inspect their potential for improving the overall
prediction. For measuring the disparity of two
models we use the Pearson correlation coefﬁcient.
The results are shown in Table 4.

5.3 Experimental Results
As shown in Table 3 our ensemble outperforms
the strongest individual method on the Wikipedia
dataset by approximately one percent F1-measure.
We see that the difference in F1-measure between
the best individual classiﬁer and the ensemble is
higher on the Wikipedia dataset as on the Twitter
dataset. This ﬁnding is accompanied by the results
in Table 4 which show that most classiﬁer com-
binations present a high correlation on the Twit-
ter dataset and are therefore less effective on the
ensemble. An explanation for this effect is that
the text sequences within the Twitter set show less
variance than the ones in the Wikipedia dataset.
This can be reasoned from 1) their sampling strat-
egy based on a list of terms, 2) the smaller size of
the dataset and 3) less disparity within the three
deﬁned classes than in the six from the Wikipedia
dataset. With less variant data one selected classi-
ﬁer for a type of text can be sufﬁcient.

As the results in Table 4 show, ensembling is
especially effective on the sparse classes “threat”
(Wikipedia) and “hate” (Twitter). The predictions
for these two classes have the weakest correlation.
This can be exploited when dealing with strongly
imbalanced datasets, as often the case in toxic
comment classiﬁcation and related tasks. Table 4
gives us indicators for useful combinations of clas-
siﬁers. Combining our shallow learner approach
with Neural Networks is highly effective. Contrary

Class

F1

Pearson

Different word embeddings
GRU+G
.78
.70
.79
.53
CNN+G
.75
.67
.77
.49

GRU+FT
.78
.69
.79
.54
CNN+FT
.78
.73
.78
.53

Different NN architectures

CNN BiGRU Att
.78
.71
.79
.49

.78
.73
.78
.50

Shallow learner and NN
CNN
.78
.73
.78
.50
BiGRU Att
.78
.71
.79
.49

LR char
.78
.74
.76
.51
LR char
.78
.74
.76
.51

.95
.92
.96
.94

.91
.82
.94
.90

.85
.65
.96
.93

.86
.78
.92
.86

.84
.67
.92
.88

Character- and word-ngrams
LR word
.75
.70
.75
.50

LR char
.78
.74
.77
.51

.83
.69
.94
.91

W avg.
W threat
T avg.
T hate

W avg.
W threat
T avg.
T hate

W avg.
W threat
T avg.
T hate

W avg.
W threat
T avg.
T hate

W avg.
W threat
T avg.
T hate

W avg.
W threat
T avg.
T hate

Table 4: F1-measures and Pearson correlations of dif-
ferent combinations of classiﬁers. When the pearson
score is low and F1 is similar, an ensemble performs
best. We see that this appears mostly on the Wikipedia
dataset and on the respective minority classes ‘threat’
and ‘hate’.
‘T’: Twitter
dataset; ‘G’: Glove embeddings; ‘FT’: FastText em-
beddings; ‘avg.’: Averaged

‘W’: Wikipedia dataset;

to that we see that the different word embeddings
used do not lead to strongly differing predictions.
Another ﬁnding is that word and character n-
grams learned by our Logistic Regression classi-
ﬁer produce strongly uncorrelated predictions that
can be combined for increasing accuracy.

386 Detailed Error Analysis

The ensemble of state-of-the-art classiﬁers still
fails to reach F1-measures higher than 0.8. To ﬁnd
out the remaining problems we perform an exten-
sive error analysis on the result of the ensemble.

We analyse common error classes of our ensem-
ble based on research from Zhang and Luo (2018);
Zhang et al. (2018); Qian et al. (2018); Davidson
et al. (2017); Schmidt and Wiegand (2017); No-
bata et al. (2016). Moreover, we add additional
error classes we encountered during our manual
analysis. To address deﬁcits in both precision and
recall we inspect false negative and false positive
classiﬁcations. We focus on error classes with the
highest frequency in the observed samples. The
occurrence of an error class within a comment is
taken to be binary (occurs in comment or not).

We present the results on class ‘toxic’ of the
Wikipedia dataset and class ‘hate’ of the Twitter
dataset. Both classes are of high signiﬁcance for
the task of user comment moderation. Our ensem-
ble results in 1794 false negatives and 1581 false
positives for the Wikipedia dataset. We choose
200 random samples out of each set as represen-
tatives. For the smaller Twitter dataset we get 55
false negatives and 58 false positives, we perform
our analysis on all of these samples.

6.1 Error Classes of False Negatives
Doubtful labels. We observe a high number of
comments for which we question the original label
when taking the respective class deﬁnition into
account. A common occurrence is actual toxic
or hateful content that is cited by the comment’s
author. Another pattern is the use of potentially
toxic words within an explanation or self reproach.
Example: “No matter how upset you may be
there is never a reason to refer to another editor
as ‘an idiot’ ”
We ﬁnd that 23% of sampled comments in the
false negatives of the Wikipedia dataset do not
fulﬁll the toxic deﬁnition in our view. Taking the
hate speech deﬁnition of the authors into account,
we question 9% of the Twitter dataset samples.
For the remaining error classes we only include
the comments with undoubtful labels.

Toxicity without swear words. Davidson et al.
(2017) phrase the problem that hate speech may
not contain hate or swear words at all.
Example: “she looks like a horse”

50% of Wikipedia dataset samples have no com-
mon hate or swear word in them. This makes it the
largest error class for the Wikipedia dataset and
shows that our classiﬁers often fail when there are
no obvious hateful words present. We observe this
in 18% of hate speech comments from the Twitter
dataset. It is important to notice that the frequency
of swear words is naturally higher within this
dataset, because of its sampling method with hate-
ful words as seeds. In many cases the problem is a
lack of paradigmatic context. Hence, an important
research topic for future work is investigating
improved semantic embeddings, which can better
distinguish different paradigmatic contexts.

Rhetorical questions.
It is common practice to
wrap toxic statements online within rhetorical or
suggestive questions as pointed out by Schmidt
and Wiegand (2017).
Example: “have you no brain?!?!”
21% of Wikipedia dataset samples and 10% of
Twitter dataset samples contain a rhetorical or
suggestive question. Again paradigmatic context
can help to identify this kind of comments. An
additional signal
is the existence of question
words and question marks.

and

and

often

comparisons

comparisons. Subtle
Metaphors
metaphors
require
understanding of implications of language or
additional world knowledge.
Zhang and Luo
(2018) and Schmidt and Wiegand (2017) report
on this common error class.
Example: “Who are you a sockpuppet for?”
We only see this problem in the Wikipedia dataset
samples with 16% of false negatives impacted.

Idiosyncratic and rare words. Errors caused
by rare or unknown words are reported by Nobata
et al. (2016); Zhang and Luo (2018); Qian et al.
(2018). From our observation they include mis-
spellings, neologisms, obfuscations, abbreviations
and slang words. Even though some of these
words appear in the embedding, their frequency
may be too low to correctly detect their meaning
on our word embeddings.
Example: “fucc nicca yu pose to be pullin up”
We ﬁnd rare or unknown words in 30% of exam-
ined false negatives from the Wikipedia dataset
and in 43% of Twitter dataset samples. This
also reﬂects the common language on Twitter

39with many slang words, abbreviations and mis-
spellings. One option to circumvent this problem
is to train word embeddings on larger corpora
with even more variant language.

Sarcasm and irony. Nobata et al. (2016) and
Qian et al. (2018) report the problem of sarcasm
for hate speech detection. As sarcasm and irony
detection is a hard task itself, it also increases
difﬁculty of toxic comment classiﬁcation, because
the texts usually state the opposite of what is
really meant.
Example: “hope you’re proud of yourself.
Another milestone in idiocy.”
Sarcasm or irony appears in 11% of Wikipedia
dataset samples, but in none of the Twitter dataset
samples.

6.2 Error Classes of False Positives
Doubtful
labels. We ﬁnd that 53% of false
positive samples from the Wikipedia dataset
actually fall under the deﬁnition of toxic in our
view, even though they are labeled as non-toxic.
Most of them contain strong hateful expressions
or spam. We identify 10% of the Twitter dataset
samples to have questionable labels.
Example: “IF YOU LOOK THIS UP UR A
DUMB RUSSIAN”
The analysis show that doubtful labels belong to
one of the main reasons for a false classiﬁcation
on the Wikipedia dataset, especially for the false
positives. The results emphasize the importance
of taking labeler agreement into account when
building up a dataset to train machine learning
models.
It also shows the need for clear deﬁni-
tions especially for classes with high variance
like toxicity. Besides that, a deﬁcient selection of
annotators can amplify such problems as Waseem
et al. (2018) point out.

Usage of swear words in false positives. Clas-
siﬁers often learn that swear words are strong
indicators for toxicity in comments. This can be
problematic when non-toxic comments contain
such terms. Zhang and Luo (2018) describe this
problem as dealing with non distinctive features.
Example: “Oh, I feel like such an asshole now.
Sorry, bud.”
60% of false positive Wikipedia dataset samples
and 77% of Twitter dataset samples contain swear
words.
In this case, the paradigmatic context is

not correctly distinguished by the embedding.
Hence, the classiﬁer considered signals for the
trigger word (a swear word) stronger, than other
signals from the context, here a ﬁrst person
statement addressing the author himself.

Quotations or references. We add this error
class because we observe many cases of ref-
erences to toxic or hateful language in actual
non-hateful comments.
Example: “I deleted the Jews are dumb
comment.”
In the Wikipedia dataset samples this appears in
17% and in the Twitter dataset in 8% of com-
ments. Again the classiﬁer could not recognize
the additional paradigmatic context
referring
to typical actions in a forum, here explicitly
expressed with words ‘I deleted the. . . ’
and ‘
. . . comment’.

Idiosyncratic and rare words. Such words (as
described in Section 6) in non-toxic or non-hateful
comments cause problems when the classiﬁer
misinterprets their meaning or when they are
slang that is often used in toxic language.
Example: “WTF man. Dan Whyte is Scottish”
8% of Wikipedia dataset samples include rare
words.
In the Twitter dataset sample the fre-
quency is higher with 17%, but also inﬂuenced by
common Twitter language.

7 Conclusion

In this work we presented multiple approaches
for toxic comment classiﬁcation. We showed that
the approaches make different errors and can be
combined into an ensemble with improved F1-
measure. The ensemble especially outperforms
when there is high variance within the data and on
classes with few examples. Some combinations
such as shallow learners with deep neural net-
works are especially effective. Our error analysis
on results of the ensemble identiﬁed difﬁcult sub-
tasks of toxic comment classiﬁcation. We ﬁnd that
a large source of errors is the lack of consistent
quality of labels. Additionally most of the un-
solved challenges occur due to missing training
data with highly idiosyncratic or rare vocabulary.
Finally, we suggest further research in represent-
ing world knowledge with embeddings to improve
distinction between paradigmatic contexts.

40Acknowledgement

Our work is funded by the European Unions Hori-
zon 2020 research and innovation programme un-
der grant agreement No. 732328 (FashionBrain)
and by the German Federal Ministry of Education
and Research (BMBF) under grant agreement No.
01UG1735BX (NOHATE).

References
Pinkesh Badjatiya, Shashank Gupta, Manish Gupta,
and Vasudeva Varma. 2017. Deep learning for hate
speech detection in tweets. In WWW.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135–146.

Pete Burnap and Matthew L. Williams. 2015. Cyber
hate speech on twitter : An application of machine
classiﬁcation and statistical modeling for policy and
decision making. volume 7, pages 223–242.

Pete Burnap and Matthew L. Williams. 2016. Us and
them: identifying cyber hate on twitter across mul-
tiple protected characteristics. EPJ Data Science,
5:1–15.

Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu.
2012. Detecting offensive language in social media
to protect adolescent online safety. SOCIALCOM-
PASSAT, pages 71–80.

Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman,
and Franciska de Jong. 2013. Improving cyberbul-
lying detection with user context. In ECIR.

Thomas Davidson, Dana Warmsley, Michael W. Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
ICWSM.

Karthik Dinakar, Birago Jones, Catherine Havasi,
Henry Lieberman, and Rosalind W. Picard. 2012.
Common sense reasoning for detection, prevention,
and mitigation of cyberbullying. TiiS, 2:18:1–18:30.

Bj¨orn Gamb¨ack and Utpal Kumar Sikdar. 2017. Us-
ing convolutional neural networks to classify hate-
speech. In ALW1@ACL.

Lei Gao and Ruihong Huang. 2017. Detecting online
hate speech using context aware models. In RANLP.

Spiros V. Georgakopoulos, Sotiris K. Tasoulis, Aris-
tidis G. Vrahatis, and Vassilis P. Plagianakos. 2018.
Convolutional neural networks for toxic comment
classiﬁcation. In SETN.

Gergory, Rajesh Kumar Gnanasekaran, Raja Ra-
jan Gunasekaran, Kelly M. Hoffman, Jenny Hot-
tle, Vichita Jienjitlert, Shivika Khare, Ryan Lau,
Marianna J. Martindale, Shalmali Naik, Heather L.
Nixon, Piyush Ramachandran, Kristine M. Rogers,
Lisa Rogers, Meghna Sardana Sarin, Gaurav Sha-
hane, Jayanee Thanki, Priyanka Vengataraman, Zi-
jian Wan, and Derek Michael Wu. 2017. A large
labeled corpus for online harassment research.
In
WebSci.

Edel Greevy and Alan F. Smeaton. 2004. Classifying
racist texts using a support vector machine. In SI-
GIR.

Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie
Mennes, Bart Desmet, Guy De Pauw, Walter Daele-
mans, and V´eronique Hoste. 2015. Detection and
ﬁne-grained classiﬁcation of cyberbullying events.
In RANLP.

Akshita Jha and Radhika Mamidi. 2017. When does
a compliment become sexist? analysis and classiﬁ-
cation of ambivalent sexism using twitter data.
In
NLP+CSS@ACL.

George W. Kennedy, Andrew W. McCollough, Ed-
ward Dixon, A. M. Parra Bastidas, J. Mark Ryan,
and Chris Loo. 2017. Hack harassment : Tech-
nology solutions to combat online harassment.
In
ALW1@ACL.

Yoon Kim. 2014. Convolutional neural networks for

sentence classiﬁcation. In EMNLP.

Irene Kwok and Yuzhou Wang. 2013. Locate the hate:

Detecting tweets against blacks. In AAAI.

Yashar Mehdad and Joel R. Tetreault. 2016. Do char-

acters abuse more than words? In SIGDIAL.

Dennis Njagi, Z Zuping, Damien Hanyurwimfura, and
Jun Long. 2015. A lexicon-based approach for hate
speech detection. In International Journal of Mul-
timedia and Ubiquitous Engineering, volume 10,
pages 215–230.

Chikashi Nobata, Joel R. Tetreault, Achint Oommen
Thomas, Yashar Mehdad, and Yi Chang. 2016. Abu-
sive language detection in online user content.
In
WWW.

Ji Ho Park and Pascale Fung. 2017. One-step and two-
step classiﬁcation for abusive language detection on
twitter. CoRR, abs/1706.01206.

John Pavlopoulos, Prodromos Malakasiotis, and Ion
Androutsopoulos. 2017. Deeper attention to abusive
user content moderation. In EMNLP.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Jennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo,
Alexandra Berlinger, Siddharth Bhagwan, Cody
Buntain, Paul Cheakalos, Alicia A. Geller, Quint

Michal Ptaszynski, Juuso Kalevi Kristian Eronen, and
Fumito Masui. 2017. Learning deep on cyberbully-
ing is always better than brute force. In IJCAI.

41Dawei Yin and Brian D. Davison. 2009. Detection of

harassment on web 2 . 0. In CAW2.0@WWW.

Ziqi Zhang and Lei Luo. 2018. Hate speech detection:
A solved problem? the challenging case of long tail
on twitter. CoRR, abs/1803.03662.

Ziqi Zhang, David Robinson, and Jonathan A. Tep-
per. 2018. Detecting hate speech on twitter using
a convolution-gru based deep neural network.
In
ESWC.

Haoti Zhong, Hao Li, Anna Cinzia Squicciarini,
Sarah Michele Rajtmajer, Christopher Grifﬁn,
David J. Miller, and Cornelia Caragea. 2016.
Content-driven detection of cyberbullying on the in-
stagram social network. In IJCAI.

Steven Zimmerman, Udo Kruschwitz, and Chris Fox.
Improving hate speech detection with deep

2018.
learning ensembles. In LREC.

Jing Qian, Mai ElSherief, Elizabeth M. Belding-Royer,
and William Yang Wang. 2018. Leveraging intra-
user and inter-user representation learning for auto-
mated hate speech detection. In NAACL-HLT.

Julian Risch and Ralf Krestel. 2018. Aggression identi-
ﬁcation using deep learning and data augmentation.
In TRAC-1@COLING, pages 150–158.

David Robinson, Ziqi Zhang, and Jonathan Tepper.
2018. Hate speech detection on twitter: Feature en-
gineering v.s. feature selection. In ESWC.

Bj¨orn Ross, Michael Rist, Guillermo Carbonell, Ben-
jamin Cabrera, Nils Kurowsky, and Michael Wo-
jatzki. 2016. Measuring the reliability of hate
speech annotations: The case of the european
refugee crisis. CoRR, abs/1701.08118.

Niloofar Saﬁ Samghabadi, Suraj Maharjan, Alan
and Thamar
Sprague, Raquel Diaz-Sprague,
Solorio. 2017. Detecting nastiness in social media.
In ALW1@ACL.

Anna Schmidt and Michael Wiegand. 2017. A survey
on hate speech detection using natural language pro-
cessing. In SocialNLP@EACL.

Sanjana Sharma, Saksham Agrawal, and Manish
Degree based classiﬁcation
CoRR,

Shrivastava. 2018.
of harmful speech using twitter data.
abs/1806.04197.

Fabio Del Vigna, Andrea Cimino, Felice Dell’Orletta,
Marinella Petrocchi, and Maurizio Tesconi. 2017.
Hate me, hate me not: Hate speech detection on
facebook. In ITASEC.

W. Lloyd Warner and Julia Hirschberg. 2012. De-
In

tecting hate speech on the world wide web.
LSM@ACL.

Zeerak Waseem. 2016. Are you a racist or am i seeing
things? annotator inﬂuence on hate speech detection
on twitter. In NLP+CSS@EMNLP.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? predictive features for hate
speech detection on twitter. In SRW@NAACL-HLT.

Zeerak Waseem, James Thorne, and Joachim Bingel.
2018. Bridging the gaps: Multi task learning for do-
main transfer of hate speech detection. Online Ha-
rassment, pages 29–55.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon.
2017. Ex machina: Personal attacks seen at scale.
In WWW.

Guang Xiang, Bin Fan, Ling Wang, Jason I. Hong,
and Carolyn Penstein Ros´e. 2012. Detecting offen-
sive tweets via topical feature discovery over a large
scale twitter corpus. In CIKM.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classiﬁcation.
In
NAACL-HLT.

42
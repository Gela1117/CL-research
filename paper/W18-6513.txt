Content Aware Source Code Change Description Generation

Pablo Loyola1*, Edison Marrese-Taylor2*, Jorge A. Balazs2

Yutaka Matsuo2 and Fumiko Satoh1
{e57095, sfumiko}@jp.ibm.com

IBM Research, Tokyo, Japan1

Graduate School of Engineering, The University of Tokyo, Japan2
{emarrese, jorge, matsuo}@weblab.t-utokyo.ac.jp

*Authors contributed equally to this work.

Abstract

We propose to study the generation of de-
scriptions from source code changes by in-
tegrating the messages included on code
commits and the intra-code documentation
inside the source in the form of docstrings.
Our hypothesis is that although both types
of descriptions are not directly aligned in
semantic terms —one explaining a change
and the other the actual functionality of
the code being modiﬁed— there could be
certain common ground that is useful for
the generation. To this end, we propose
an architecture that uses the source code-
docstring relationship to guide the descrip-
tion generation. We discuss the results of
the approach comparing against a baseline
based on a sequence-to-sequence model,
using standard automatic natural language
generation metrics as well as with a human
study, thus offering a comprehensive view
of the feasibility of the approach.

Introduction

1
Transferring the semantics from source code to
natural language and vice-versa is at the core of
several machine learning endeavors, as it could
enable a direct communication between man and
machine, improving the level of interpretability
and comprehension between each other and eas-
ing their collaboration.

In that sense, source code can be conceived
as an actual medium of communication from two
perspectives, which have been explored separately
in both computational linguistics and software en-
gineering communities (Allamanis et al., 2017).

In the ﬁrst place, from a developer-program per-
spective, source code encodes, in a set of human
readable instructions, the requirements a devel-

oper commands a program to satisfy. This view
has been operationalized as a machine transla-
tion problem, trying to learn efﬁcient transitions
between the dependencies that words and source
code tokens exhibit. With this, recent approaches
have been able to summarize source code snippets
(Allamanis et al., 2016) or even synthesize natural
language instructions into actual commands (Oda
et al., 2015; Yin and Neubig, 2017).

In the second place, from a developer-developer
perspective, the collaborative nature of software
development has transformed source code into a
common ground for human interaction.
In that
sense, every new code contribution takes into ac-
count the previous modiﬁcations, allowing devel-
opers to communicate indirectly. One of these
applications is the generation of descriptions for
source code changes (Loyola et al., 2017), which
uses the information contained in a code com-
mit – the diff representing the changed code and
the message the developer provides at submission
time – to train an encoder-decoder architecture.
This problem has the particularity of containing
certain elements of summarization, as most salient
characteristics of the code change need to be ex-
tracted, and translation, as it is required to gen-
erate a natural language description from a code
change.

In this work, we consider the generation of de-
scriptions for source changes as a testing task to
explore if the perspectives presented above can
be integrated into a single learning architecture.
That is, we want to learn to generate descrip-
tions from changes exploiting the information in
the source code commits, but incorporating the
program functionality expressed through the doc-
strings contained within the source code.

Our hypothesis is that, while both perspectives
point at different semantic directions, there should
be a certain degree of dependency, since in order

ProceedingsofThe11thInternationalNaturalLanguageGenerationConference,pages119–128,Tilburg,TheNetherlands,November5-8,2018.c(cid:13)2018AssociationforComputationalLinguistics119to perform a change on the code the developer ﬁrst
needs to understand its functionality. Moreover,
we consider that integrating these two perspectives
could contribute to alleviate the issues that cur-
rent approaches for generating descriptions from
source code change present, such as the halluci-
nation in the generation, where generated descrip-
tions are syntactically correct but that do not keep
any semantic relationship with the actual code
change, and also the inability of the model to pro-
duce descriptions with a relevant amount of detail.

We propose an approach that, given a code
change, compresses the information associated to
the docstrings within the ﬁle being modiﬁed and
uses it as an additional context when selecting the
next word from the output vocabulary. We also
reported an exploratory approach that generates a
mask to be used at decoding time that considers
the inter-perspective distances based on a bilingual
embedding.

In addition to integrating change descriptions
and source code documentation, we also explore
how to represent the code change itself. Previous
work on description generation has relied on the
output from the diff command, which provides a
distinction between the portions of the source code
that were added and removed. Such data source
has been treated just as a sequence of source code
tokens, such as in the case of Loyola et al. (2017).
In contrast, we explore an architectural variation
where we use two encoders to obtain a more ex-
pressive signal from the source code perspective,
which can lead to a better natural language gener-
ation.

We constructed a dataset by merging both
change history and docstring data from several
real world open source projects to evaluate our
approach. We reported the results on standard
translation-based metrics as well through a user
study using a crowd-sourcing, to get a more quali-
tative estimation of the performance of the model.

Our results show that, on average, incorporating
a signal from the content of the source code ﬁle
has a positive impact on the performance of the
model. We consider these results could open the
door to further research that considers the genera-
tion of descriptions from software artifacts from a
more systemic perspective. The source code and
data for this approach is available at: https:
//github.com/epochx/py-commitgen.

2 Related Work

The emergence of unifying paradigms that ex-
plicitly relate programming and natural languages
in distributional terms (Hindle et al., 2012) and
the availability of large corpus mainly from open
source software opened the door for the use of lan-
guage modeling for several tasks (Raychev et al.,
2015). Examples of this are approaches for learn-
ing program representations (Mou et al., 2016),
bug localization (Huo et al., 2016), API sugges-
tion (Gu et al., 2016) and code completion (Ray-
chev et al., 2014).

Source code summarization has received spe-
cial attention, ranging from the use of information
retrieval techniques to the addition of physiologi-
cal features such as eye tracking (Rodeghero et al.,
2014). In recent years several representation learn-
ing approaches have been proposed, such as (Al-
lamanis et al., 2016), where the authors employ a
convolutional architecture embedded inside an at-
tention mechanism to learn an efﬁcient mapping
between source code tokens and natural language
keywords. More recently, Iyer et al. (2016) pro-
posed a encoder-decoder model that learns to sum-
marize from Stackoverﬂow data, which contains
snippets of code along with descriptions.

Both approaches share the use of attention
mechanisms (Bahdanau et al., 2014) to overcome
the natural disparity between the modalities when
ﬁnding relevant token alignments. Although we
also use an attention mechanism, we differ from
them in the sense we are targeting the changes in
the code rather than the description of a ﬁle.

In terms of speciﬁcally working on code change
summarization, Cort´es-Coy et al. (2014); Linares-
V´asquez et al. (2015) propose a method based
on a set of rules that considers the type and im-
pact of the changes, and Buse and Weimer (2010)
combines summarization with symbolic execu-
tion. The use of representation learning based
models has been also explored recently, such as
the work of Loyola et al. (2017) and Jiang et al.
(2017). Both approaches make use of an encoder-
decoder architecture, which receives code change,
in the form of a diff output and the associated mes-
sage submitted by the contributor.

In terms of ad-hoc datasets, we can mention
Zhong et al. (2017) for questions, SQL queries,
Oda et al. (2015) for pseudo code in Python, and
more recently Barone and Sennrich (2017) for
code-docstrings from Python projects on GitHub.

1203 Proposed Approach
Our starting point is the code commit, understood
as a pair conformed by (i) the differences in the
source code obtained as the output of the diff 1
command and (ii) the associated message the com-
mitter provided to explain the action.

Therefore, for a given software project we can
formalize our available data as the set of its T
versions v1, . . . , vT . Commits are well-deﬁned
for every pair of consecutive project versions
t−1(v) → Committ, so we end up with a to-
∆t
tal of T commits, each associated to a project ver-
sion. With this, we model each commit as a tuple
(Ct, Nt), where Ct is a representation of the code
changes associated to v in time t, and Nt is a rep-
resentation of its corresponding natural language
(NL) accompanying message. Concretely, Ct cor-
responds to the set of code tokens associated to
the commit that was applied to a certain ﬁle FCt,
based on the atomicity assumption. In principle,
we do not assume this set of source code tokens is
ordered in a sequential fashion, allowing us to also
represent it as a bag of tokens.
Let C be the set of code changes and N be the
set of all descriptions in NL. We consider a train-
ing corpus with T code snippets and message pairs
(Ct, Nt), 1 ≤ t ≤ T , Ct ∈ C , Nt ∈ N . Then, for
a given code snippet Ck ∈ C, our goal is to train a
model to produce the most likely description N (cid:63).
Following Loyola et al. (2017), we start build-
ing our models upon a vanilla encoder-decoder
model that at training time receives (diff, message)
pairs. We use an attention-augmented architecture
(Luong et al., 2015) with a bi-directional LSTM
as encoder. Let Xt = x1, . . . xn be the embedded
input code sequence Ct = c1, . . . , cn as extracted
from the diff. After feeding these through our en-
coder, we have a set of vectors H = h1, . . . hn
that represent the input. This is later given to
the decoder, in our case also an LSTM, such that
the probability of a description is modeled as the
product of the conditional next-word probabilities,
p(ni|n1, . . . , ni−1) ∝ Wc[si; ai], where Nt =
n1, . . . , nm corresponds to the message tokens, ∝
denotes a softmax operation, si represents the de-
coder hidden state and ai is the contribution from
the attention model on the input. Wc is a train-
able combination matrix. The decoder repeats the
recurrence until a ﬁxed number of words or the
special EOS token is generated.

1http://man7.org/linux/man-pages/man1/diff.1.html

1
2

3

4
5
6

7

8

9

10
11
12
13

(cid:80)n
The attention contribution ai is deﬁned as ai =
j=1 αi,j · hj, where hj is a hidden state associ-
ated to the input and αi,j is a score obtained using
the general attention scheme (Luong et al., 2015),
αi,j = exp (h(cid:62)
, where Wa is a trainable
scoring matrix.

(cid:80)n
j=1 exp (h(cid:62)

j Wasi)

i Wasi)

During training, the decoder iterates until the
end-of-sentence token is reached. For generation,
we approximate N (cid:63) by performing a beam search
on the space of all possible summaries using the
model output, with a beam size of 10 and a maxi-
mum message length equal to the maximum length
of the inputs of the dataset.

This model considers a direct transition be-
tween diffs and messages extracted from source
code commits. However, programs usually pro-
vide an additional relationship between source
code and natural language, in the form of intra-
code documentation, commonly known as doc-
strings.

This documentation appears in multiple loca-
tions inside a source code ﬁle, usually aligned with
a speciﬁc line or block, explaining its functional-
ity. The information contained in a code, docstring
pair is intrinsically local, i.e. the docstring is used
as an additional source to support the understand-
ing of a portion of a program beyond the solely
internalization of the available source code. List-
ing 1 presents an example of a real docstring as-
sociated to a class from the Pytorch library2. In
this case we can see that the docstring provides an
overall description of the functionality of the class
and a summary of the required parameters.

c l a s s LambdaLR ( LRS ched uler ) :

t h e

””” S e t s
t i m e s a g i v e n f u n c t i o n . When l a s t e p o c h =−1,

i n i t i a l

l e a r n i n g r a t e o f each p a r a m e t e r group t o t h e

s e t s

i n i t i a l

l r

o p t i m i z e r
l r l a m b d a ( f u n c t i o n o r
computes a m u l t i p l i c a t i v e

( O p t i m i z e r ) : Wrapped o p t i m i z e r .

l i s t ) : A f u n c t i o n which

f a c t o r g i v e n an i n t e g e r p a r a m e t e r epoch , o r a

f u n c t i o n s , one f o r each group i n o p t i m i z e r .

l a s t e p o c h ( i n t ) : The i n d e x o f

l a s t

epoch . D e f a u l t :

a s

l r .

l r
Args :

l i s t o f

such

p a r a m g r o u p s .
−1.
. . .
”””
d e f
. . .

i n i t

( s e l f , o p t i m i z e r ,

l r l a m b d a ,

l a s t e p o c h =−1) :

Listing 1: Example of a docstring from a Pytorch
module.

If we take a look at the changes committed to
this speciﬁc class, we can ﬁnd that most of the

2https://github.com/pytorch/pytorch/
blob/master/torch/optim/lr_scheduler.py

121commit messages associated keep certain relation-
ship with the docstring. For example, a commit3
from August 8th, 2018 states:

Changed serialization

mechanism of LambdaLR
scheduler

Therefore, we are in the presence of two sets
of pairs that provide information about the charac-
teristics of a program from two different perspec-
tives. A (diff, message) pair set that allow us to un-
derstand why and how changes are conducted over
a given ﬁle, and a (code, docstring) that allow us
to understand what is the functionality of such ﬁle.
Our goal is then to integrate both sources, i.e., to
study how the local source code - natural language
feature representations learned from the (code,
docstring) pair can be used to support the gener-
ation of natural language descriptions from code
changes. Our hypothesis is that while the (diff,
message) and (code, docstring) pairs associated
to a ﬁle are not pointing at the same semantic di-
rection, they should share certain representational
components, as both are centered on the informa-
tion contained on the ﬁle: one trying to explain the
code itself (code, docstring) and the other trying to
explain changes on such code (diff, message).

3.1 Content-aware encoder
We noted that the comments contained within a
source ﬁle are related to the local functionality of
its adjacent source code lines or blocks. In con-
trast, the message associated to a commit is related
to the actual action carried out on the given ﬁle.
Such message, in theory, is indirectly associated
to the functionality of the code, i.e. the code was
modiﬁed in a given way because its previous func-
tional state led triggered in a developer the need to
change it.

Motivated by this idea, we propose an aug-
mented encoder that allows us to capture these
relations. Again, let H = h1, . . . hn be the re-
sult of embedding and processing the input con-
tent extracted from Ct. We extract the code and
associated docstring of the total r lines of ﬁle FCt.
With this, we model each code and docstring line
as a sequence of tokens Lc
p and
q, of length p and q respectively.
Ld
k = xd
We use BiLSTMs to encode both sequences inde-

1, . . . , xd

1, . . . , xc

k = xc

3https://bit.ly/2zx4041

pendently, as follows.

−−−−→
LSTM(xc
←−−−−
LSTM(xc
−−−−→
LSTM(xd
←−−−−
LSTM(xd

i , (cid:126)hc
i , (cid:126)hc
i , (cid:126)hd
i , (cid:126)hd

i−1)
i+1)
i+1)
i+1)

(cid:126)hc
i =
(cid:126)hc
i =
(cid:126)hd
i =
(cid:126)hd
i =

(1)
(2)
(3)
(4)

p; (cid:126)hc

p; (cid:126)hd

As Figure 1 shows, we concatenate the last hid-
den state corresponding to each code and doc-
string vector to obtain a representation for each
line hk = [(cid:126)hc

q], with k = 1, . . . , r.

q; (cid:126)hd

Finally, we use a standard LSTM to model the
dependency across the r code/docstring line vec-
tors and take the last hidden state as a means of
aggregating and representing the content of both
the code and docstring in FCt. This summariz-
ing vector is concatenated to each hi coming from
the diff -level representation. The decoding phase
works in a way analogous to the vanilla encoder-
decoder model.

3.2 Content-aware decoder
During our feasibility study, we empirically ob-
served that there was a signiﬁcant overlap between
the source code vocabularies coming from the diffs
and from the code extracted from the ﬁles, which
in some cases reaches up to 90%.

Our intuition based on such observation is that
we can consider both source code vocabularies
as a single vocabulary, which is used in two dif-
ferent contexts. In other words, a deﬁned set of
source code tokens is conforming a bridge be-
tween the messages from the code changes and the
docstrings.

To exploit such bridge we explored incorporat-
ing the information contained in the (code, doc-

Figure 1: Diagram of our proposed content-aware
encoder. It can be seen how the regular encoder
hidden states h1, . . . , hn (in green) are augmented
using the representation extracted from the content
and docstring in FCt (in blue).

122string) pairs into the code change description gen-
eration by building a mask to guide the decoder.
This mask is built upon co-occurrence patterns be-
tween message and docstring words, which are
used to re-weight the scores the decoder is gener-
ating on the output vocabulary. Concretely, during
inference our goal was to re-weight the probabili-
ties that are passed to the beam search module.

As stated before, for each project, we have a set
of code changes represented as pairs (Ct, Nt), for
Ct ∈ C the set of diff outputs, Nt ∈ N the set of
messages submitted at commit time. On the other
hand, for each commit-derived modiﬁed ﬁle asso-
ciated to FCt t = 1, . . . , T we have a set of pairs
(code, docstring). We train bilingual word embed-
dings between the set of source code content and
their associated docstring lines. The intuition be-
hind this is that the docstring basically explains —
or translates to natural language— the functional-
ity of a source code block. While this intuition is
arguably not entirely true from a machine trans-
lation perspective, at least it allows us to obtain a
shared feature space between code and docstring
tokens. We adapted the approach by Artetxe et al.
(2017), calling the output embedding C-DC.

In the second place, we construct another em-
bedding space to combine the set of messages
from the commits, and the docstrings, such that
this embedding only contains natural language to-
kens. In this case we do not expect a high overlap
between the vocabularies of each set, as they are
pointing at different semantic directions —their
intent is different. To build this embedding space,
we use a standard word2vec (Mikolov et al., 2013)
implementation, and call the resulting embedding
M-DC.

Using the above embedding spaces as map-
pings, the approach works as follows. Given an
input sequence Ct = c1, . . . , cn from a diff, each
of its tokens ci used to query the C-DC embedding
to obtain a set of k neighboring NL tokens Kt. For
each NL token in Kt, we obtain its vector repre-
sentation in M-DC and identify a medoid, medKt.
We consider this medoid not only as the represen-
tative of Kt, but also indirectly of the associated
source code token ci. We then use this medoid
vector and compute its distance to all the elements
in the output message vocabulary OV present in
M-DC, dt,i = dist(medKt, i) for i ∈ OV . We
repeat this process for all the source code tokens
from the input diff sequence obtaining a distance

matrix of n × |OV |.
Finally, we compute the column-wise average
distance, obtaining a vector ds of size 1 × |OV |,
which represents a compressed association be-
tween the input source code sequence Ct and the
output vocabulary. This resulting vector is used
during inference via a convex combination with
the softmax vector ldec output by the decoder
ldec = α ∗ ldec + (1 − α) ∗ dCt. The modiﬁed vec-
tor is passed to the beam search, which selects the
next tokens in a regular fashion. It should be noted
that as the embedding training operations can be
performed off-line, the inference overhead added
by our mask is negligible, so there is almost no
impact in terms of inference time.

3.3 Structure-aware encoder
Finally, we also explore a different take on the en-
coding phase. We note that the diff associated to
a change has an inherent structure that allows us
to distinguish the lines that were added and/or re-
moved. In Loyola et al. (2017), the authors ignored
such distinction and simply generated a single se-
quence by concatenating both added and removed
parts. While this approach appears as a simple so-
lution, we consider it limits the expressiveness of
this input and introduce issues related to i) loss of
the alignment between added and removed code
within the source code ﬁle, and ii) source code to-
ken redundancy.

In order to overcome such issues, we propose to
consider the diff explicitly as two inputs, one for
the added tokens, and one for the removed tokens.
We hypothesize that the quality and richness of
the encoded input could be improved by compar-
ing these two inputs in order to identify elements
that may play a key semantic role in understanding
the diff, both in terms of added or removed code
chunks.
Concretely, let X +
t =
x−
1 , . . . , x−
m be the embedded sequences of the
concatenated added and removed code lines, as
extracted from the diff associated to example Ct.
We use a single embedding matrix E and the same
bidirectional LSTM to encode both sequences.

n and X−

1 , . . . , x+

t = x+

−−−−→
LSTM(x+
←−−−−
LSTM(x+
−−−−→
LSTM(x−
←−−−−
LSTM(x−

i , (cid:126)h+
i , (cid:126)h+
i , (cid:126)h−
i , (cid:126)h−

i−1)
i+1)
i−1)
i+1)

(cid:126)h+
i =
(cid:126)h+
i =
(cid:126)h−
i =
(cid:126)h−
i =

(5)
(6)
(7)
(8)

123We later apply two matching strategies over the
resulting vector sequences, which are based on
the multi-perspective matching operation by Wang
et al. (2017). The operation is built upon a cosine
matching function fm, which is used to compare
two vectors, as follows.

m = fm(v1, v2; W )

(9)
where v1 and v2 are two d-dimensional vec-
tors, W ∈ (cid:60)l×d is a trainable parameter with
the shape l × d, l is the number of perspectives,
and the returned value m is a l-dimensional vector
m = [m1, ..., mk, ..., ml]. Each element mk ∈ m
is a matching value from the k-th perspective, and
it is calculated by the cosine similarity between
two weighted vectors

mk = cosine(Wk ◦ v1, Wk ◦ v2)

(10)
where ◦ is the element-wise multiplication, and
Wk is the k-th row of W , which controls the k-th
perspective and assigns different weights to differ-
ent dimensions of the d-dimensional space.

Full-Matching: In this strategy, each forward
(or backward) vector (cid:126)h+
i ) is compared with
the last time step of the forward (or backward) rep-
resentation of the other sequence (cid:126)h−

i (or (cid:126)h+

m (or (cid:126)h−
m).

(cid:126)mf ull
(cid:126)mf ull

i = fm((cid:126)h+
i = fm( (cid:126)h+

i , (cid:126)h−
i , (cid:126)h−

m; W 1)
m; W 2)

(11)

Maxpooling-Matching: In this strategy, each
forward (or backward) vector (cid:126)h+
i ) is com-
i
pared with every forward (or backward) vector of
the other sequence (cid:126)h−
j ) for j ∈ 1 . . . m,
and only the maximum value of each dimension is
retained.

(or (cid:126)h−

(or (cid:126)h+

j

(cid:126)mmax

i

(cid:126)mmax

i

i ,(cid:126)h−
i , (cid:126)h−

= max

j∈(1...m)

fm((cid:126)h+

j ; W 3)

fm( (cid:126)h+

= max

j∈(1...m)
is element-wise maximum.

j ; W 4)

(12)

where max

j∈(1...m)

These matching strategies are applied for both
added and removed sequences. After, we concate-
nate each of the sequences of matching vectors and
utilize another BiLSTM model to obtain a context-
aware version each sequence. These two vector
sequences are concatenated in the time dimension
and provided to the decoder as context for the at-
tention layer. Finally, to initialize the decoder, we

concatenate the vectors from the last time-step of
the BiLSTM models and aggregate them using an
afﬁne transformation. The decoder works analo-
gously to the vanilla encoder-decoder case.

4 Empirical Study

Data: We consider real world open source Python
projects. For our experiments using the content-
augmented encoder, we resorted to the code-
docstring-corpus (Barone and Sennrich, 2017).
This dataset is a diverse parallel corpus of a hun-
dred thousand Python functions with their doc-
strings, generated by scraping open source repos-
itories on GitHub.
In order to make our ex-
periments comparable across settings, we only
worked with projects that were also present in
this dataset. We sorted the projects in the code-
docstring-corpus according to their total number
of commits in GitHub and chose the ones that con-
tained at least 10,000 commits aiming at diver-
sity in terms of topics. Speciﬁcally, in this paper
we work with Theano, astropy, nova, scikit-learn,
mne-python, ﬂocker and matplotlib.

Following Loyola et al. (2017), we obtained all
the diff ﬁles and the metadata associated to each
commit, for a given project using the GitHub API.
We also recovered information such as the author
and message of each commit. The commit mes-
sages were processed using a modiﬁed version of
the Penn Treebank tokenizer (Marcus et al., 1993).
Besides using the rules deﬁned by the original
script, we replaced commit SHAs, commit author
names and ﬁle names with generic tokens. In order
to do so, we ﬁrst collect the set of commit SHAs,
committer names and project ﬁle names using the
downloaded metadata for each commit. Each one
of these lists is then matched against the words in
the text to produce the output. Finally, we also
removed certain repetitive patterns from the mes-
sages, such as the phrase merge pull request, keep-
ing the rest of the content of each sequence, if any.
Messages that solely contained these sequences
were discarded as they provide no useful semantic
information about the nature of the content of the
commit. On the other hand, to obtain a represen-
tation of the source code content of each commit,
we parsed the diff ﬁles and used a lexer (Brandl,
2016) to tokenize their contents in a per-line fash-
ion. We ignored docstrings and code comment to-
kens, as well as tokens contained in literal strings.
For our structure-aware encoder, when parsing the

124diff ﬁle we separately extract the added and re-
moved lines, while the rest of the pre-processing
remains the same.

We discarded commits that modify more than
a single ﬁle, thus we consider only atomic com-
mits. To combine the commit data with the code-
docstring-corpus, we found the set of ﬁles modi-
ﬁed by the commits and discarded all the ones that
modify a ﬁle not present in the examples from the
code-docstring-dataset. With this list, we extract
all the source code and docstring lines from the
corpus in a per-line fashion. In this manner, we
create a mapping that allows us to recover, for each
commit in our examples, the content and docstring
lines of the ﬁle that commit modiﬁes. Table 1
summarizes the size of our raw and pre-processed
datasets.

Project
Theano
astropy
nova

scikit-learn
mne-python

ﬂocker

matplotlib

Total
22,995
19,599
13,400
15,575
12,761
16,027
20,001

Atomic
15,814
12,195
18,110
12,885
6,762
11,702
14,284

Content
7,708
4,708
4,617
3,965
4,083
4,707
5,840

Structure

15,210
11,896
17,412
12,482
6,531
10,821
13,836

Table 1: Number of commits available on each
dataset subset. Both the Content and Structure
subsets are obtained using the Atomic subset.

Evaluation: As stated in the previous sec-
tion, the problem of generating descriptions from
source code changes does not yet have a formal
way of evaluation. As the problem has certain el-
ements from both translation and summarization,
in principle metrics such as BLEU (Papineni et al.,
2002) seem to appear as feasible alternatives for
evaluation in our case. BLEU is based on n-gram
overlap between the gold standard and the gener-
ated sequences. Smoothing techniques are also ap-
plied to deal with cases in which certain generated
n-grams are not found on the gold standard.
In
particular, for this work we use BLEU-4 and for
smoothing we simply add  = 0.01 to 0 counts.

On the other hand, we ﬁnd METEOR (Lavie
and Agarwal, 2007), a metric based on the align-
ment between hypothesis-reference pairs, which
in turn is based on n-gram matching. Speciﬁcally,
METEOR computes the alignment by comparing
exact token matches, stemmed tokens and para-
phrase matches. In addition to that, it also ﬁnds se-
mantically similar tokens between hypotheses and
references by using Word-Net synonyms. To ob-
tain the ﬁnal alignment, different overlap counts

are combined using several free parameters that
are tuned to emulate various human judgment
tasks. Although this gives METEOR some extra
ﬂexibility, it makes it context dependent, speciﬁ-
cally in terms of language. In our case, we work
with the latest version available (1.5) with the
model pre-trained for English, using the included
scripts to tokenize and normalize punctuation.

Finally, we also considered MEANT (Lo and
Wu, 2011). Our interest in this metric derives from
the fact that it considers the verb as a key element
when evaluating. More speciﬁcally, MEANT is
based on semantic role labels and uses the Kuhn-
Munkres algorithm to ﬁnd matches in a bipartite
graph built upon semantic frames. Thus, this met-
ric aims at aligning the generated and gold stan-
dard sequences by ﬁnding semantically equivalent
passages focusing on the main action in each pas-
sage. Compared to other metrics, the main draw-
back of standard MEANT is that it requires the
inputs to have been annotated with their corre-
sponding semantic role labels, while also requir-
ing a notion of semantic distance to use for match-
ing frames. To this end, we work with MEANT
2.0 (Lo, 2017), which is based on automatic SRL
and word-embedding-based similarity for match-
ing. For both requirements, we rely on SENNA
(Collobert et al., 2011).

We trained our models using Adam (Kingma
and Ba, 2014) with a learning rate of 0.001 with
decay of 0.8 when there was no improvement in
the validation loss. We used early stopping when
the learning rate dropped below 10−4. For evalu-
ation on the test set, we used the three automatic
metrics introduced before.

In addition, we conducted a crowd-sourced hu-
man evaluation. Concretely, we selected the best
validation models on each case and relied on Ama-
zon Mechanical Turk to evaluate the results on
50 randomly-chosen examples from the test sets.
Each turker was presented with the gold stan-
dard and the generated message, and was asked to
rate the level of correlation between them, from 1
(min) to 5 (max). We randomly swapped the order
in which the messages appear, to avoid the turk-
ers from easily locating each message. To ensure
the quality of the evaluation, we ﬁltered turkers us-
ing a quiz-based qualiﬁcation in which users had
to prove they had basic knowledge of Python and
GitHub. In addition, each example was shown to
3 different turkers.

1255 Results and Discussion

Table 2 summarizes our results in terms of all the
evaluation metrics for both experiments, namely,
i) the use of a content-aware encoder and ii) the
use of the structure-aware encoder.
In terms of
notation, Len means the maximum length of the
input sequence, Use refers to if content-aware and
structure-aware was used (No means the standard
baseline from Loyola et al. (2017)) . We see
that in general, the usage of a context-aware en-
coder tends to increase performance, as the mod-
els with content perform better in 4 or 5 out of
our 7 datasets, for each of the automatic evalua-
tion metrics. These gains are also reﬂected in the
average correlation scores from our human eval-
uation, where we can see that the content-aware
models outperform the baseline in 4 datasets. In
terms of sequence length, we observe that some
content-aware models are able to outperform the
baseline using shorter input-output pairs.

Regarding the usage of two linearized inputs,
we see that the tendency is for the performance
to decrease. This is evidenced in both automatic
and human-based evaluation, where the majority
of the structure-aware models perform worse than
our baseline. We think this reinforces the urge for
moving into a more ad-hoc representation in terms
of structure, in which the code lines of the in-
put diff are exploited thoroughly. Despite the fact
that our current proposal goes in that direction, be-
ing designed to compare two sequential inputs, if
these two inputs lack expressive power, still there
is little the model can learn.

Comparing across models for a given automatic
evaluation metric, we see a big difference in terms
of their absolute values.
In this sense, we note
that MEANT offers scores that are arguably more
lenient compared to BLEU and METEOR. We
think the fact that these last two metrics are heav-
ily based on n-gram overlap hinders their value.
As MEANT essentially performs an action-based
alignment between hypotheses and references, our
intuition is that this could be a good direction in
terms of evaluation, as the phenomenon to model
is basically an action performed on a document,
which is naturally articulated with a verb when
summarizing the action performed (e.g. Fix a
bug). Some empirical evidence about this was
given by Jiang et al. (2017), who found that out of
a sample of 1.6 M commit messages, roughly 47%
of them begin with a verb and its direct object.

Regarding the mask-based approach, Table
3 presents some results associated to a initial
exploratory study considering a subset of the
projects. In this case, we can see that while the
results are in the order of magnitude of the best re-
sults associated to the previous approach for con-
tent integration, there is still no clear pattern in
terms of which metrics is more reliable as indi-
cator of generative performance. In that sense, we
consider it is critical to work towards obtaining an
ad-hoc metric that is better aligned with the actual
performance of the generation.

One known limitation of the current approach
is that while the the data coming from the code
changes in intrinsically time dependent, for the
case of the code-docstring source, we are just us-
ing a static version, therefore we are not con-
sidering how docstring documentation could also
change over time. While we were aware that such
decision has direct implication on the vocabulary
matching, it was a necessary simpliﬁcation given
the available dataset.

Additionally, given that the results associated to
the use of two encoders did not produce a relevant
improvement , we believe that even a two-encoder
conﬁguration does not produce a sufﬁciently ex-
pressive signal to be used by the decoder. That
makes us conclude that treating a code change just
a set of token sequences is not enough to obtain
considerable increments and that it is necessary to
obtain such input from a more ﬂexible perspec-
tive, for example, by using an explicit dependency
graph between changes, or even more complex
constructs such as differences of execution traces
or abstract syntax trees.

6 Conclusion and Future Work

We studied how to model the generation of de-
scription from source code changes by integrating
the intra-code documentation as a guiding element
to improve the quality of the descriptions. While
the results from the empirical study are not com-
pletely conclusive, we consider that adding this
extra information on average contribute positively,
measured in terms of standard NLP metrics as well
as through a human study. For future work, we
consider necessary to focus on the expressiveness
of the feature representations learned from the en-
coder. In that sense, we will explore other ways
to treat the source code change, such as exploiting
their abstract syntax tree representation.

126Dataset

Theano

astropy

ﬂocker

matplotlib

mne-python

nova

scikit-learn

Len.
200
300
200
300
300
100
300
100
200
200
300
100
200
100

Content-aware encoder

Use MEANT METEOR
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes

0.1683
0.1600
0.1942
0.2170
0.0320
0.1100
0.1944
0.1240
0.0147
0.0200
0.2798
0.3350
0.0669
0.0590

0.0505
0.0360
0.1074
0.1100
0.0405
0.0540
0.0523
0.0830
0.0099
0.0250
0.0259
0.0410
0.1327
0.1010

BLEU
0.0081
0.0080
0.0292
0.0300
0.0131
0.0110
0.0126
0.0220
0.0052
0.0170
0.0275
0.0240
0.0276
0.0220

Human
2.2533
2.0667
2.5067
2.4867
1.9133
2.0467
2.3267
2.4067
2.2733
1.7667
2.4900
2.8066
2.0600
2.2200

Len.
100
300
200
200
300
100
100
300
200
300
200
300
300
300

Structure-aware encoder

Use MEANT METEOR
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes

0.1450
0.0080
0.2586
0.2697
0.1608
0.1186
0.1687
0.1357
0.0568
0.0587
0.3151
0.2976
0.0547
0.0586

0.0310
0.0061
0.0738
0.0555
0.0668
0.0375
0.0559
0.0542
0.0265
0.0250
0.0372
0.0477
0.0577
0.0341

BLEU
0.0077
0.0053
0.0220
0.0167
0.0143
0.0054
0.0139
0.0174
0.0171
0.0230
0.0187
0.0236
0.0170
0.0113

Human
2.1667
1.4533
2.8400
2.7133
2.2267
2.1267
2.3867
2.0000
2.4200
2.3933
2.4467
2.7000
2.0300
2.1267

Table 2: Best results using our context and structure aware architectures.

Dataset
Theano
astropy
matplotlib

Best Value
METEOR MEANT
0.2103
0.2308
0.2397

0.1953
0.1077
0.0950

BLEU
0.0112
0.0302
0.0289

Table 3: Results of our content-based masking
technique.

References
Miltiadis Allamanis, Earl T. Barr, Premkumar T. De-
vanbu, and Charles A. Sutton. 2017. A survey
of machine learning for big code and naturalness.
CoRR, abs/1709.06182.

Miltiadis Allamanis, Hao Peng, and Charles Sutton.
2016. A convolutional attention network for ex-
In Inter-
treme summarization of source code.
national Conference on Machine Learning, pages
2091–2100.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Antonio Valerio Miceli Barone and Rico Sennrich.
2017. A parallel corpus of python functions and
documentation strings for automated code docu-
arXiv preprint
mentation and code generation.
arXiv:1707.02275.

Georg Brandl. 2016. Pygments: Python syntax high-

lighter. http://pygments.org.

Raymond P.L. Buse and Westley R. Weimer. 2010.
In
Automatically documenting program changes.
Proceedings of the IEEE/ACM International Confer-
ence on Automated Software Engineering, ASE ’10,
pages 33–42, New York, NY, USA. ACM.

Ronan Collobert, Jason Weston, Lon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Journal of Machine Learning Research,
Scratch.
12:2493–2537.

Luis Fernando Cort´es-Coy, Mario Linares V´asquez,
Jairo Aponte, and Denys Poshyvanyk. 2014. On
automatically generating commit messages via sum-
marization of source code changes. In SCAM, vol-
ume 14, pages 275–284.

Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and
In Pro-
Sunghun Kim. 2016. Deep api learning.
ceedings of the 2016 24th ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engi-
neering, FSE 2016, pages 631–642, New York, NY,
USA. ACM.

Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. In Software Engineering (ICSE), 2012
34th International Conference on, pages 837–847.
IEEE.

Xuan Huo, Ming Li, and Zhi-Hua Zhou. 2016. Learn-
ing uniﬁed features from natural and programming
languages for locating buggy source code.
In
Proceedings of the Twenty-Fifth International Joint
Conference on Artiﬁcial Intelligence,
IJCAI’16,
pages 1606–1612. AAAI Press.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2073–2083, Berlin, Germany. Association for
Computational Linguistics.

Siyuan Jiang, Ameer Armaly, and Collin McMillan.
2017. Automatically generating commit messages
from diffs using neural machine translation. In Pro-
ceedings of the 32nd IEEE/ACM International Con-
ference on Automated Software Engineering, pages
135–146. IEEE Press.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A

method for stochastic optimization. CoRR.

127Satoshi Nakamura. 2015.
Learning to generate
pseudo-code from source code using statistical ma-
chine translation (t). In Automated Software Engi-
neering (ASE), 2015 30th IEEE/ACM International
Conference on, pages 574–584. IEEE.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Veselin Raychev, Martin Vechev, and Andreas Krause.
2015. Predicting program properties from big code.
In ACM SIGPLAN Notices, volume 50, pages 111–
124. ACM.

Veselin Raychev, Martin Vechev, and Eran Yahav.
2014. Code completion with statistical language
models. In Acm Sigplan Notices, volume 49, pages
419–428. ACM.

Paige Rodeghero, Collin McMillan, Paul W McBurney,
Nigel Bosch, and Sidney D’Mello. 2014. Improving
automated source code summarization via an eye-
tracking study of programmers. In Proceedings of
the 36th International Conference on Software En-
gineering, pages 390–401. ACM.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the 26th Inter-
national Joint Conference on Artiﬁcial Intelligence,
IJCAI’17, pages 4144–4150. AAAI Press.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
arXiv preprint arXiv:1704.01696.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017.
Seq2sql: Generating structured queries
from natural language using reinforcement learning.
CoRR, abs/1709.00103.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Mario Linares-V´asquez, Luis Fernando Cort´es-Coy,
Jairo Aponte,
and Denys Poshyvanyk. 2015.
Changescribe: A tool for automatically generating
In Proceedings of the 37th In-
commit messages.
ternational Conference on Software Engineering-
Volume 2, pages 709–712. IEEE Press.

Chi-kiu Lo. 2017. MEANT 2.0: Accurate semantic
MT evaluation for any output language. In Proceed-
ings of the Second Conference on Machine Transla-
tion, pages 589–597.

Chi-kiu Lo and Dekai Wu. 2011. MEANT: An
inexpensive, high-accuracy, semi-automatic metric
for evaluating translation utility based on semantic
roles. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 220–229, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Pablo Loyola, Edison Marrese-Taylor, and Yutaka Mat-
suo. 2017. A neural architecture for generating natu-
ral language descriptions from source code changes.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 287–292. Association for
Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In Proceedings of the
neural machine translation.
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics, 19(2):313–330.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality.
In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.
2016. Convolutional neural networks over tree
structures for programming language processing. In
Proc. AAAI, pages 1287–1293. AAAI Press.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and

128
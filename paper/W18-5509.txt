Automated Fact-Checking of Claims

in Argumentative Parliamentary Debates

Nona Naderi

Graeme Hirst

Department of Computer Science

Department of Computer Science

University of Toronto

Toronto, Ontario, Canada
nona@cs.toronto.edu

University of Toronto

Toronto, Ontario, Canada
gh@cs.toronto.edu

Abstract

We present an automated approach to distin-
guish true, false, stretch, and dodge state-
ments in questions and answers in the Cana-
dian Parliament. We leverage the truthfulness
annotations of a U.S. fact-checking corpus by
training a neural net model and incorporating
the prediction probabilities into our models.
We ﬁnd that in concert with other linguistic
features, these probabilities can improve the
multi-class classiﬁcation results. We further
show that dodge statements can be detected
with an F1 measure as high as 82.57% in bi-
nary classiﬁcation settings.

Introduction

1
Governments and parliaments that are selected
and chosen by citizens’ votes have ipso facto at-
tracted a certain level of trust. However, govern-
ments and parliamentarians use combinations of
true statements, false statements, and exaggera-
tions in strategic ways to question other parties’
trustworthiness and to thereby create distrust to-
wards them while gaining credibility for them-
selves. Creating distrust and alienation may be
achieved by using ad hominem arguments or by
raising questions about someone’s character and
honesty (Walton, 2005). For example, consider
the claims made within the following question that
was asked in the Canadian Parliament:
Example 1.1 [Dominic LeBlanc, 2013-10-21]
The RCMP and Mike Duffy’s lawyer have shown
us that the Prime Minister has not been honest
about this scandal. When will he come clean and
stop hiding his own role in this scandal?
These claims, including the presupposition of the
second sentence that the Prime Minister has a role
in the scandal that he is hiding, may be true, false,
or simply exaggerations.
In order to be able to
analyze how these claims serve their presenter’s

purpose or intention, we need to determine their
truth.

Here, we will examine the linguistic char-
acteristics of true statements, false statements,
dodges, and stretches in argumentative parliamen-
tary statements. We examine whether falsehoods
told by members of parliament can be identiﬁed
with previously proposed approaches and we ﬁnd
that while some of these approaches improve the
classiﬁcation, identifying falsehoods by members
of parliament remains challenging.

2 Related work

Vlachos and Riedel (2014) proposed to use data
from fact-checking websites, such as PolitiFact
for the fact-checking task and suggested that one
way to approach this task would be using the se-
mantic similarity between statements. Hassan et
al. (2015) used presidential debates and proposed
three labels — Non-Factual, Unimportant Fac-
tual, and Check-worthy Factual sentence — for
the fact-checking task. They used a traditional
feature-based method and trained their models us-
ing sentiment scores using AlchemyAPI, word
counts of a sentence, bag of words, part-of-speech
tags, and entity types to classify the debates into
these three labels. They found that the part-of-
speech tag of cardinal numbers was the most in-
formative feature and word counts was the second
most informative feature. They also found that
check-worthy actual claims were more likely to
contain numeric values and non-factual sentences
were less likely to contain numeric values.

Patwari et al. (2017) used primary debates and
presidential debates for analyzing check-worthy
statements.
They used topics extracted using
LDA, entity history and type counts, part-of-
speech tuples, counts of part-of-speech tags, uni-
grams, sentiment, and token counts for their classi-

ProceedingsoftheFirstWorkshoponFactExtractionandVERiﬁcation(FEVER),pages60–65Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics60Label True False Dodge Stretch Total
#
478

255

60

93

70

Table 1: Distribution of labels in the Toronto Star
dataset

Label
True
Mostly true
Half true
Mostly false
False
Pants-on-ﬁre false
Total

#
1,780
2,003
2,152
1,717
1,964
867
10,483

Table 2: Distribution of labels in the PolitiFact dataset

ﬁcation task. Ma et al. (2017) used a kernel-based
model to detect rumors in tweets. Wang (2017)
used the statements from PolitiFact and the 6-
point scale of truthfulness; he compared the per-
formance of multiple classiﬁers and reported some
improvement by using metadata related to the per-
son making the statements.

Rashkin et al. (2017) examined the effective-
ness of LIWC (Linguistic Inquiry and Word
Count) and stylistic lexicon features in determin-
ing the reliability of the news corpus and truthful-
ness of the PolitiFact dataset. The only reliabil-
ity measurement reported on the PolitiFact dataset
is by Wang (2017), who manually analyzed 200
statements from PolitiFact and reached an agree-
ment of 0.82 using Cohen’s kappa measurement
with the journalists’ labels. Jaradat et al. (2018)
used a set of linguistic features to rank check-
worthy claims. Throne et al. (2018) created a
dataset for claim veriﬁcation. This dataset con-
sists of 185,445 claims veriﬁed against Wikipedia
pages. Here, we do not consider any external re-
sources and we focus only on the text of claims to
determine whether we can classify claims as true,
false, dodge, or stretch.

3 Data

For our analysis, we extracted our data from a
project by the Toronto Star newspaper.1 The Star
reporters2 fact-checked and annotated questions

1http://projects.thestar.com/
All

question-period/index.html.
publicly available.

the data is

2Bruce Campion-Smith, Brendan Kennedy, Marco
Chown Oved, Alex Ballingall, Alex Boutilier, and Tonda
MacCharles.

and answers from the Oral Question Period of the
Canadian Parliament (over ﬁve days in April and
May 2018). Oral Question Period is a session of
45 minutes in which the Opposition and Govern-
ment backbenchers ask questions of ministers of
the government, and the ministers must respond.
The reporters annotated all assertions within both
the questions and the answers as either true, false,
stretch, (half true), or dodge (not actually answer-
ing the question). Further, they provided a narra-
tive justiﬁcation for the assignment of each label
(we do not use that data here). Here is an example
of the annotated data (not including the justiﬁca-
tions):
Example 3.1 Q. [Michelle Rempel] Mr. Speaker,
[social programs across Canada are under severe
strain due to tens of
thousands of unplanned
immigrants illegally crossing into Canada from
the United States.]False [Forty per cent
in
Toronto’s homeless shelters are recent asylum
claimants.]True [This,
food bank usage, and
unemployment rates show that many new asylum
claimants are not having successful integration
experiences.]False

A. [Ahmed Hussen (Minister of Immigration,
Refugees and Citizenship)] Mr. Speaker, we
commend the City of Toronto, as well as the
Province of Ontario,
the Province of Quebec,
and all Canadians, on their generosity toward
newcomers. That is something this country is
proud of, and we will always be proud of our
tradition. [In terms of asylum processing, making
sure that there are minimal impacts on provincial
social services, we have provided $74 million
to make sure that the Immigration and Refugee
Board does its work so that legitimate claimants
can move on with their lives and those who do
not have legitimate claims can be removed from
Canada.]True

Here is an example of dodge annotation:

Example 3.2 Q. [Jacques Gourde] . . . How much
money does that represent for the families that will
be affected by the sexist carbon tax over a one-
year period?

A. [Catherine McKenna (Minister of Environ-
ment and Climate Change)] [Mr. Speaker, I am
quite surprised to hear them say they are con-
cerned about sexism.
is the party that
closed 12 out of 16 Status of Women Canada
ofﬁces.]Dodge We know that we must take action

That

61Features
Majority class (True)
BOW (tf-idf)
+ POS
+ NUM
+ Superlatives (Rashkin et al., 2017)
+ PolitiFact predictions
BOW + NE

F1 Accuracy Dodge True False Stretch
–
49.20
52.92
53.40
54.24
55.10
50.66

53.35
53.14
58.15
58.58
59.42
59.63
53.33

24.80
27.40
28.80
30.00
30.80
24.40

67.00
71.00
70.80
71.60
71.60
66.40

55.20
62.40
63.80
63.80
63.60
57.40

4.60
4.80
4.80
9.20
12.80
17.20

Table 3: Five-fold cross-validation results (F1 and % accuracy) of four-way classiﬁcation of fact-checking for the
overall dataset and F1 for each class.

on climate change. Canadians know that we have
a plan, but they are not so sure if the Conservatives
do.

For our analysis, we extracted the annotated
span of the text with its associated label. The dis-
tribution of the labels in this dataset is shown in
Table 1. This is a skewed dataset with more than
half of the statements annotated as true.

We also use a publicly available dataset from
PolitiFact, a website at which statements by Amer-
ican politicians and ofﬁcials are annotated with a
6-point scale of truthfulness.3 The distribution of
labels in this data is shown in Table 2. We examine
PolitiFact data to determine whether these annota-
tions can help the classiﬁcation of the Toronto Star
annotations.

4 Method

We formulate the analysis as a multi-class classiﬁ-
cation task; given a statement, we identify whether
the statement is true, false, stretch, or a dodge.

We ﬁrst examine the effective features used for

identifying deceptive texts in the prior literature.
• Tuples of words and their part-of-speech tags
(unigrams and bigrams weighted by tf-idf,
represented by POS in the result tables).

• Number of words in the statement
et al., 2015; Patwari et al., 2017).

(Hassan

• Named entity type counts, including organi-
zations and locations (Patwari et al., 2017)
(represented by NE in the result tables).

• Total number of numbers in the text, e.g.,
six organizations heard the assistant deputy

3The dataset has been made available by Hannah
Rashkin at https://homes.cs.washington.edu/
˜hrashkin/factcheck.html.

minister (Hassan et al., 2015) (represented by
NUM in the result tables).

• LIWC (Tausczik and Pennebaker, 2010) fea-

tures (Rashkin et al., 2017).

• Five lexicons of intensifying words from
Wiktionary: superlatives, comparatives, ac-
tion adverbs, manner adverbs, modal ad-
verbs (Rashkin et al., 2017).

In addition, we leverage the American Politi-
Fact data to fact-check the Canadian Parliamen-
tary questions and answers by training a Gated
Recurrent Unit classiﬁer (GRU) (Cho et al., 2014)
on this data. We will use the truthfulness predic-
tions of this classiﬁer — the probabilities of the
6-point-scale labels — as additional features for
our SVM classiﬁer (using the scikit-learn pack-
age (Pedregosa et al., 2011)). For training the
GRU classiﬁer, we initialized the word represen-
tations using the publicly available GloVe pre-
trained 100-dimension word embeddings (Pen-
nington et al., 2014)4, and restricted the vocabu-
lary to the 5,000 most-frequent words and a se-
quence length of 300. We added a dropout of 0.6
after the embedding layer and a dropout layer of
0.8 before the ﬁnal sigmoid unit layer. The model
was trained with categorical cross-entropy with
the Adam optimizer (Kingma and Ba, 2014) for
10 epochs and batch size of 64. We used 10% of
the data for validation, with the model achieving
an average F1 measure of 31.44% on this data.

5 Results and discussion

We approach the fact-checking of the statements
as a multi-class classiﬁcation task. Our baselines

4https://nlp.stanford.edu/projects/

glove/

62Dodge

Stretch

False

characteristics of labels.

Superlative + PolitiFact

Features
True
Majority class
BOW
BOW + NE
BOW + LIWC
BOW + PolitiFact
BOW + NE + Politifact
BOW + POS + NUM +

False
Majority class
BOW
BOW + NE
BOW + LIWC
BOW + PolitiFact
BOW + NE + Politifact
BOW + POS + NUM +

Stretch
Majority class
BOW
BOW + NE
BOW + LIWC
BOW + PolitiFact
BOW + NE + Politifact
BOW + POS + NUM +

Superlative + PolitiFact

Superlative + PolitiFact

52.25
54.21
52.99
49.11
55.73
53.76

58.62
58.20
61.67
53.41
58.11
63.69

54.96

55.24

60.00
55.89
56.91
53.31
52.97
55.08

54.82

54.84
76.09
75.65
52.38
77.96
76.25

77.51

53.85
81.36
82.57
52.02
80.69
82.52

78.29

57.06
75.15
76.93
45.37
79.39
77.73

80.59

Table 4: Average F1 of different models for two-
way classiﬁcation of fact-checking (ﬁve-fold cross-
validation).

are the majority class (truths) and an SVM classi-
ﬁer trained with unigrams extracted from the an-
notated spans of texts (weighted by tf-idf ). We
performed ﬁve-fold cross-validation. Table 3 re-
ports the results on the multi-class classiﬁcation
task with these baselines and with the additional
features described in section 4, including the truth-
fulness predictions of the GRU classiﬁer trained
on PolitiFact data. The best result is achieved
using unigrams, POS tags, total number of num-
bers (NUM), superlatives, and the GRU’s truthful-
ness predictions (PolitiFact predictions). We ex-
amined all ﬁve lexicons from Wiktionary provided
by Rashkin et al. (2017); however, only superla-
tives affected the performance of the classiﬁer, so
we report only the results using superlatives.

We also report in Table 3 the average F1 mea-
sure for classiﬁcation of four labels in multi-class
classiﬁcation using ﬁve-fold cross-validation. The
truthfulness predictions did not improve the classi-
ﬁcation of the dodge and true labels in multi-class
classiﬁcation setting. Superlatives slightly im-
proved the classiﬁcation of all labels except dodge.
We further perform pairwise classiﬁcation (one-
versus-one) for all possible pairs of labels to get
better insight into the impact of the features and

Therefore, we created three rather balanced
datasets of truths and falsehoods by randomly re-
sampling the true statements without replacement
(85 true statements in each dataset). The same
method was used for comparing true labels with
dodge and stretch labels, i.e., we created three rel-
atively balanced datasets for analyzing true and
dodge labels and three datasets for analyzing true
and stretch labels. This allows us to compare the
prior work on the 6-point scale truthfulness labels
on the U.S. data with the Canadian 4-point scale.
Table 4 presents the classiﬁcation results using
ﬁve-fold cross-validation with an SVM classiﬁer.
The reported F1 measure is the average of the re-
sults on all three datasets for each pairwise setting.
Dodge statements were classiﬁed more accurately
than the other statements with an F1 measure as
high as 82.57%. This shows that the answers that
do not provide a response to the question can be
detected with relatively high conﬁdence. The most
effective features for classifying false against true
and dodge statements were named entities.

The predictions obtained from training the GRU
model on the PolitiFact annotations, on their own,
were not able to distinguish false from true and
stretch statements. However, the predictions did
help in distinguishing true against stretch and
dodge statements. None of the models were
able to improve the classiﬁcation of false against
stretch statements over the majority baseline.

Overall, stretch statements were the most difﬁ-
cult statements to identify in the binary classiﬁca-
tion setting. This could also be due to some in-
consistency in the annotation process, with stretch
and false not always clearly separated. Here is an
example of stretch in the data:
Example 5.1 [Catherine McKenna]
Carbon
pricing works and it can be done while growing
the economy.
. . . Once again, I ask the member
opposite, “What are you going to do?” [Under 10
years of the [Conservative] Harper government,
you did nothing.]Stretch

Elsewhere in the data, essentially the same

claim is labelled false:
Example 5.2 [Justin Trudeau] The Conservatives
promised that
they would also tackle environ-
mental challenges and that they would do so by
means other than carbon pricing. . . . They have no
proposals, [they did nothing for 10 years.]False

63We further performed the analysis using the two
predictions of more true and more false from the
PolitiFact dataset; however, we didn’t observe any
improvements. Using the total number of words in
the statements also did not improve the results.

While Rashkin et al. (2017), found that LIWC
features were effective for predicting the truthful-
ness of the statements in PolitiFact, we did not
observe any improvements in the performance of
the classiﬁer in our classiﬁcation task on Canadian
Parliamentary data. Furthermore, we did not ob-
serve any improvements in the classiﬁcation tasks
using sentiment and subjectivity features extracted
using OpinionFinder (Wilson et al., 2005; Riloff
et al., 2003; Riloff and Wiebe, 2003).

6 Comparison with PolitiFact dataset

In this section, we perform a direct analysis with
the PolitiFact dataset. We ﬁrst train a GRU model
(used a sequence length of 200, other hyperparam-
eters the same as those of the experiment described
above) using 3-point scale annotations of Politi-
Fact (used 10% of the data for validation). We
treat the top two truthful ratings (true and mostly
true) as true; half true and mostly false as stretch;
and the last two ratings (false and pants-on-ﬁre
false) as false. We then test the model on three
annotations of true, stretch, and false from the
Toronto Star project. The results are presented in
Table 5. As the results show, none of the false
statements are detected as false and the overall F1
score is lower than the majority baseline.

We further train a GRU model (trained with bi-
nary cross-entropy and sequence length of 200,
other hyperparameters the same as above) using
2-point scale where we treat the top three truthful
ratings as true and the last three false ratings as
false. We then test the model on two annotations
of true and false from the Toronto Star project.
The results are presented in Table 6; the F1 score
remains below baseline.

The Politifact dataset provided by Rashkin et
al. includes a subset of direct quotes by original
speakers. We further performed the 3-point scale
and 2-point scale analysis using only the direct
quotes. Using only the direct quotes, also shown
in Tables 5 and 6, did not improve the classiﬁca-
tion performance.

Majority
GRU (All)
GRU (DQ)

F1 True Stretch False
63
40
50

53
75

29
13

0
8

Table 5: 3-point scale comparison of the PolitiFact
data and Toronto Star annotations. All: GRU model
is trained with all PolitiFact data and tested on Toronto
Star annotations. DQ: GRU model is trained with only
direct quotes from the PolitiFact data and tested on
Toronto Star annotations.

Majority
GRU (All)
GRU (DQ)

F1 True False
81
73
72

84
88

29
8

Table 6: 2-point scale comparison of the PolitiFact
data and Toronto Star annotations. All: GRU model
is trained with all PolitiFact data and tested on Toronto
Star annotations. DQ: GRU model is trained with only
direct quotes from the PolitiFact data and tested on
Toronto Star annotations.

7 Conclusion

We have analyzed classiﬁcation of truths, false-
hoods, dodges, and stretches in the Canadian Par-
liament and compared it with the truthfulness clas-
siﬁcation of statements in the PolitiFact dataset.
We studied whether the effective features in the
prior research can help us characterize the truthful-
ness in Canadian Parliamentary debates and found
out that while some of these features help us iden-
tify dodge statements with an F1 measure as high
as 82.57%, they were not very effective in iden-
tifying false and stretch statements. The truthful-
ness predictions obtained from training a model on
annotations of American politicians’ statements,
when used with other features, helped slightly in
distinguishing truths from other statements. In fu-
ture work, we will take advantage of journalists’
justiﬁcations in determining the truthfulness of the
statements as relying on only linguistic features is
not enough for determining falsehoods in parlia-
ment.

Acknowledgements

This research is ﬁnancially supported by an On-
tario Graduate Scholarship, the Natural Sciences
and Engineering Research Council of Canada, and
the University of Toronto. We thank the anony-
mous reviewers for their thoughtful comments and
suggestions.

64Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’03,
pages 105–112, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Seventh Con-
ference on Natural Language Learning at HLT-
NAACL 2003 - Volume 4, CONLL ’03, pages 25–
32, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
Journal of Lan-
puterized text analysis methods.
guage and Social Psychology, 29(1):24–54.

James

Thorne,

Andreas Vlachos,

Christos
Christodoulopoulos,
and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
veriﬁcation. In NAACL-HLT.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task deﬁnition and dataset construction.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18–22, Baltimore, MD, USA. Associa-
tion for Computational Linguistics.

Douglas Walton. 2005. Fundamentals of critical argu-

mentation. Cambridge University Press.

William Yang Wang. 2017. “Liar, liar pants on ﬁre”:
A new benchmark dataset for fake news detection.
arXiv preprint arXiv:1705.00648.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Association for Computational Linguistics.

References
KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259,.

Naeemul Hassan, Chengkai Li, and Mark Tremayne.
2015. Detecting check-worthy factual claims in
In Proceedings of the 24th
presidential debates.
ACM International on Conference on Information
and Knowledge Management, pages 1835–1838.
ACM.

Israa Jaradat, Pepa Gencheva, Alberto Barr´on-Cede˜no,
Llu´ıs M`arquez, and Preslav Nakov. 2018. Claim-
rank: Detecting check-worthy claims in arabic and
english. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
26–30. Association for Computational Linguistics.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jing Ma, Wei Gao, and Kam-Fai Wong. 2017. De-
tect rumors in microblog posts using propagation
structure via kernel learning. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
708–717, Vancouver, Canada. Association for Com-
putational Linguistics.

Ayush Patwari, Dan Goldwasser, and Saurabh Bagchi.
2017. TATHYA: A multi-classiﬁer system for de-
tecting check-worthy statements in political debates.
In Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, pages
2259–2262. ACM.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the 2014
word representation.
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana
Volkova, and Yejin Choi. 2017. Truth of varying
shades: Analyzing language in fake news and po-
In Proceedings of the 2017
litical fact-checking.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2931–2937, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

65
BioAMA: Towards an End to End BioMedical Question Answering System

Vasu Sharma*, Nitish Kulkarni*, Srividya Pranavi Potharaju*,

Gabriel Bayomi*, Eric Nyberg, Teruko Mitamura

Language Technologies Institute

School Of Computer Science
Carnegie Mellon University

[vasus, nitishkk, spothara, gbk, ehn, teruko] @cs.cmu.edu

Abstract

In this paper, we present a novel Biomedi-
cal Question Answering system, BioAMA:
“Biomedical Ask Me Anything” on task
5b of the annual BioASQ challenge (Ba-
likas et al., 2015). We focus on a wide va-
riety of question types including factoid,
list based, summary and yes/no type ques-
tions that generate both exact and well-
formed ‘ideal’ answers. For summary-
type questions, we combine effective IR-
based techniques for retrieval and diver-
siﬁcation of relevant snippets for a ques-
tion to create an end-to-end system which
achieves a ROUGE-2 score of 0.72 and a
ROUGE-SU4 score of 0.71 on ideal an-
swer questions (7% improvement over the
previous best model). Additionally, we
propose a novel Natural Language Infer-
ence (NLI) based framework to answer
the yes/no questions. To train the NLI
model, we also devise a transfer-learning
technique by cross-domain projection of
word embeddings. Finally, we present a
two-stage approach to address the factoid
and list type questions by ﬁrst generating a
candidate set using NER taggers and rank-
ing them using both supervised and unsu-
pervised techniques.

Introduction

1
In the era of ever advancing medical sciences and
the age of the internet, a remarkable amount of
medical literature is constantly being posted on-
line. This has led to a need for an effective re-
trieval and indexing system which can allow us
to extract meaningful information from these vast
knowledge sources. One of the most effective and
natural ways to leverage this huge amount of data

in real life is to build a Question Answering (QA)
system which will allow us to directly query this
data and extract meaningful and structured infor-
mation in a human readable form.

Our key novel contributions are as follows:

1. We achieve state of the art results in auto-
matic evaluation measures for the ideal an-
swer questions in Task 5b of the BioASQ
dataset, yielding a 7% improvement over
the previous state of the art system (Chandu
et al., 2017).

2. We introduce a novel NLI-based approach for
answering the yes/no style questions in the
BioASQ dataset. We model this as a Textual
Entailment (TE) problem and use Hierarchi-
cal Convolutional Neural Network based In-
fersent models (Conneau et al., 2017) to an-
swer the question. To address the challenge
of inadequate training data, we also intro-
duce a novel embedding projection technique
which allows for effective transfer learning
from models trained on larger datasets with
a different vocabulary to work well on the
much smaller BioASQ dataset.

3. We present two-stage approach to answer
factoid and list type questions. By using an
ensemble of biomedical NER taggers to gen-
erate a candidate answer set, we devise unsu-
pervised and supervised ranking algorithms
to generate the ﬁnal predictions.

4. We improve upon the MMR framework for
relevant sentence selection from the chosen
snippets that was introduced in the work of
Chandu et al. (2017). We experiment with a
number of more informative similarity met-
rics to replace and improve upon the baseline
Jaccard similarity metric.

ProceedingsoftheBioNLP2018workshop,pages109–117Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics1092 Relevant Literature
Biomedical Question answering has always been
a hot topic of research among the QA commu-
nity at large due to the relative signiﬁcance of the
problem and the challenge of dealing with a non
standard vocabulary and vast knowledge sources.
The BioASQ challenge has seen large scale par-
ticipation from research groups across the world.
One of the most prominent among such works
is from Chandu et al.
(2017) who experiment
with different biomedical ontologies, agglomer-
ative clustering, Maximum Marginal Relevance
(MMR) and sentence compression. However, they
only address the ideal answer generation with their
model. Peng et al. (2015) in their BioASQ sub-
mission use a 3 step pipeline for generating the
exact answers for the various question types. The
ﬁrst step is question analysis where they subdivide
each question type into ﬁner categories and clas-
sify each question into these subcategories using a
rule based system. They then perform candidate
answer generation using POS taggers and use a
word frequency-based approach to rank the can-
didate entities. Wiese et al. (2017) propose a neu-
ral QA based approach to answer the factoid and
list type questions where they use FastQA: a ma-
chine comprehension based model (Weissenborn
et al., 2017) and pre-train it on the SquaD dataset
(Rajpurkar et al., 2016) and then ﬁnetune it on the
BioASQ dataset. They report state of the art re-
sults on the Factoid and List type questions on the
BioASQ dataset. Another prominent work is from
Sarrouti and Alaoui (2017) who handle the gener-
ation of the exact answer type questions. They use
a sentiment analysis based approach to answer the
yes/no type questions making use of SentiWord-
Net for the same. For the factoid and list type
questions they use UMLS metathesaurus and term
frequency metric for extracting the exact answers.

3 The BioASQ challenge
BioASQ challenge (Balikas et al., 2015) is a large
scale biomedical question answering and seman-
tic indexing challenge, which has been running as
an annual competition since 2013. We deal with
the Phase B of the challenge which deals with
large scale biomedical question answering. The
dataset provides a set of questions and snippets
from PubMed, which are relevant to the speciﬁc
question.
It also provides users with a question
type and urls of the relevant PubMed articles it-

self. The 5b version of this dataset consists of
1,799 questions in 3 distinct categories:

1. Factoid type: This question type has a sin-
gle entity as the ground truth answer and ex-
pects the systems to output a set of entities
ordered by relevance; systems are evaluated
using the mean reciprocal rank (Radev et al.,
2003) of the answer entities with reference to
the ground truth answer entity.

2. List type: This answer type expects the sys-
tem to return an unordered list of entities as
answer and evaluates them using a F-score
based metric against a list of reference an-
swer entities which can vary in number.

3. Yes/No type: This question type asks the sys-
tems to answer a given question with a binary
output namely yes or no. The questions typi-
cally require reasoning and inference over the
evidence snippets to be able to answer the
questions correctly.

The dataset expected the participants to gener-
ate two types of answers, namely, exact and ideal
answers.
In ideal answers, the systems are ex-
pected to generate a well formed paragraph for
each of the question types which explains the an-
swer to the question. They call these answers
‘ideal’ because it is what a human would expect
as an answer by a peer biomedical scientist. In the
exact answers the systems are expected to gener-
ate “yes” or “no” in the case of yes/no questions,
named entities in the case of factoid questions and
list of named entities in the case of list questions.

Ideal Answers

4
This section describes our efforts to address the
ideal answer category on BioASQ.

Our pipeline for ideal answers has three stages.
The ﬁrst stage involves pre-processing of answer
snippets and ranking of answer sentences by var-
ious retrieval models described in the following
sections. The retrieval model scores form the
soft positional component introduced in the MMR
algorithm. We perform sentence selection next,
where we select the top 10 best sentences for gen-
erating an ideal answer. The third and ﬁnal stage
involves tiling together the selected sentences to
generate a coherent, non redundant, ideal answer
for the given question as mentioned in (Chandu
et al., 2017). The subsequent subsections explain

110Figure 1: Pipeline for ideal answer generation

the pipeline for ideal answer type questions in de-
tail (see Figure 1).

4.1 Question-Sentence Retrieval
In this section we describe various approaches
which were adapted to improve the initial re-
trieval of candidate sentences. We used the stan-
dard BM25 algorithm with custom pre-processing
of excluding medical entities from stop word re-
moval.
4.1.1
Indri (Strohman et al., 2005) is a retrieval model
based on the use of statistical language models
and query likelihood. We employed a two-stage
smoothing that considers characteristics of both
the question and answer sentences.

Indri

The Indri score for a candidate sentence is esti-

mated in a collection (C) of snippets as follows:
p(qi|d) = (1 − λ)pmle(qi|d) + λpmle(qi|C) (1)
pmle(qi|d) =
(2)
pmle(qi|C) =

tf + µpmle(qi|C)
length(d) + C

(3)

ctf

length(C)

where, λ is the coefﬁcient for linear interpola-
tion based smoothing that accounts for question
length smoothing and also compensates for differ-
ences in the word importance (gives idf-effects).
Since the questions are of moderate length, after
tuning, the best value of λ is attained at 0.75

In equation 2, µ is parameter for Bayesian
smoothing using Dirichlet priors used for sentence
length normalization, improving the estimates of
the sentence sample.Since sentences of snippets
can be of varying lengths, after tuning, the best
value of µ is attained at 5000.

4.2 Sentence Selection
Once the top most relevant snippets have been
chosen, we want to choose sentences from these

snippets which are most relevant to a speciﬁc
question. In this section we demonstrate how this
selection is done.

4.2.1 MMR
We use the Maximum Marginal Relevance
(MMR) algorithm (Forst et al., 2009) as the base-
line for sentence selection. In contrast to the ba-
sic Jaccard similarity metric used in previous work
(Chandu et al., 2017), we experimented with other
similarity measures which consistently perform
better than the Jaccard baseline. MMR ensures the
selected set contains non-redundant yet complete
information. The sentences are selected based on
two aspects, the sentence’s relevance to the ques-
tion and how different it is to the already selected
sentences. At each step we select a sentence to ap-
pend to the ranking based on the equation below.

si = arg max
sj∈R\S

(λ · sim(q, si)

− (1 − λ) · maxs∈S(simsent(si, sj)))

(4)

We deﬁne a custom similarity metric between
sentences which uses positional values of sen-
tences from the initial ranking as follows:
simsent(si, sj) = (1 − β) · (1 − rank(di)
+ β · sim(si, sj)

(5)

n

)

Here, simsent(si, sj) is the sentence to sentence
similarity, sim(q, si) is the question - sentence
similarity, rank(di) is the rank of the snippet di,
which contains the sentence si, S are Sentences al-
ready selected for summary i.e. which are ranked
above this position.
In the above equation, we
tried various metrics to account for the sentence to
sentence similarity. In cases where β is non-zero,
equation 4 is identiﬁed as our SoftMMR which in-
cludes soft scoring based on sentence position.

111baseline

Conﬁguration

β
-
0.5 BM25, Jaccard
BM25, Dice
0.5
0.6
BM25, Dice
0.6 BM25, Jaccard
0.5
Indri, Jaccard
0.5

Indri, Dice

Rouge-2 Rouge-SU4
0.7064
0.7175
0.7193
0.7133
0.7133
0.7206
0.7113

0.6962
0.7110
0.7106
0.7053
0.7053
0.7135
0.7052

Table 1: ROUGE scores for different experiments
on similarity metrics for extractive summarization

4.2.2 Dice’s similarity Coefﬁcient (DSC)
Dice’s similarity Coefﬁcient
(Srensen,
1948) is a quotient of similarity between two sam-
ples and ranges between 0 and 1 calculated as

(DSC)

dsc = (2 ∗ nt)/(nx + ny)

where nt is the number of character bigrams found
in both strings, nx is number of bigrams in string
x and ny is the number of bigrams in string y.
We used Dice coefﬁcient as a similarity metric be-
tween two sentences in 5

4.3 Evaluation
The pipeline described above is primarily de-
signed to improve the ROUGE evaluation metric
(Lin, 2004). Although a higher ROUGE score
does not necessarily reﬂect improved human read-
ability, MMR can improve readability by reduc-
ing redundancy in generated answers. Results for
ideal answers for Task 5 phase b are shown in Ta-
ble 1. We also compare our results with other state
of the art approaches in Table 4.

5 Exact answers

Exact answers represent the subset of the BioASQ
task where the responses are not structured para-
graphs, but instead either a single entity (yes/no
types) or a combination of named entities (factoid
or list types) that compose the correct reply to the
given query. The main idea refers to evaluating if
a response is able to capture the most important
components of an answer. For factoid or list types
of questions, we must return a list of the most
likely entities to compose the answer. The main
difference between them is that ground truth for
factoid questions is composed of only one correct
answer and the evaluation method is Mean Recip-
rocal Rank (MRR). However, the ground truth for

list is an actual list of correct answers with vary-
ing length, which uses F-measure as an evaluation
metric. The BioASQ submission format allows
everyone to submit 5 ranked answers for factoid
and 1 to 10 answers for list. For yes/no questions,
the ground truth is simply the yes or no label, us-
ing F-measure as an evaluation metric.

5.1 Yes/No type questions
Although yes/no questions require a simple bi-
nary response, calculating yes/no responses for the
BioASQ question can be challenging:

1. There is an inherent class-bias towards the

questions answered by yes in the dataset;

2. The dataset is quite small for training a com-

plex semantic classiﬁer;

3. An effective model must perform reasoning
and inference using the limited information
it has available, which is extremely difﬁcult
even for non-expert humans.

Due to the nature of the question type, these
questions can not be simply classiﬁed by using
word-level features. Learning the semantic rela-
tionship between the question and the sentences
in the documents is quite elemental to solving this
task. Hence, we present a Natural Language Infer-
ence (NLI)-based system that learns if the asser-
tions made by the questions are true in the context
of the documents. As a part of this system, we
ﬁrst generate assertions from questions and eval-
uate the entailment or contradiction of these as-
sertions using a Recognizing Textual Entailment
(RTE) model. We then use these entailment scores
for all the sentences in the snippets or documents
to heuristically evaluate if the answer to the yes/no
question.
5.1.1 Assertion Extraction
The ﬁrst step towards answering the question is to
identify the assertions made by the question. For
this, we use a statistical natural language parser
to identify the syntactical structure in the ques-
tion. We, then, heuristically generate assertions
from the questions.
Consider the following example question:

Is the monoclonal antibody Trastuzumab (Her-
ceptin) of potential use in the treatment of prostate
cancer?

Upon parsing of this question, we have the
phase constituents of the question. Almost all

112yes/no questions have a standard format that be-
gins with an auxiliary verb followed by a noun
phrase. In this example, we can toggle the ques-
tion word with the ﬁrst noun phrase to generate the
assertion:

The monoclonal antibody Trastuzumab (Her-
ceptin) is of potential use in the treatment of
prostate cancer.

In a similar manner, we then create positive as-
sertions for all yes/no questions. As a simple ex-
tension to this, we can also create negative asser-
tions by using not along with the auxiliary verbs.
5.1.2 Recognizing Textual Entailment
The primary goal of our NLI module is to infer
if any of the sentences among the answer snip-
pets entails or contradicts the assertion posed by
the question. We segmented the answer snippets
for each question to produce a set of assertion-
sentence pairs. To then evaluate if these asser-
tions can be inferred or refuted from the sentences,
we built a Recognizing Textual Entailment (RTE)
model using the InferSent model (Conneau et al.,
2017), which computes sentence embeddings for
every sentence and has been shown to work well
In training InferSent, we experi-
on NLI tasks.
enced two major challenges:

1. The number of assertion-sentence pairs in
BioASQ is too few to train the textual entail-
ment model effectively.

2. The models that are pre-trained on SNLI
(Bowman et al., 2015) datasets use GLOVE
(Pennington et al., 2014) embeddings that
cannot be used for biomedical corpora
which have quite different characteristics and
vocabulary compared to the corpora that
GLOVE was trained on.

However, we have pre-trained embeddings
available that were trained on PubMed and PMC
texts along with Wikipedia articles (Pyysalo et al.,
2013). To leverage these embeddings, we im-
plemented an embedding-transformation method-
ology to projecting the PubMed embeddings to
GLOVE embedding space and then ﬁne tune the
pre-trained InferSent on the BioASQ dataset for
textual entailment. The hypothesis is that, since
both the embeddings had a signiﬁcant fraction
of documents in common (Wikipedia corpus), by
transforming the embeddings from one space to
another, the sentence embeddings from the model

would still represent a lot of the semantic features
of the input sentences that can subsequently used
for classifying textual entailment. For this task,
we explore both linear and non-linear methods of
embedding transformation.

While simple, a linear projection of embeddings
from one space to another has shown to be quite
effective for a lot of multi-domain tasks. By im-
posing an orthogonality constraint on the project
matrix, we model this problem as an orthogonal
Procrustes problem:
Let dp and dg be the embedding dimensions of
PubMed embeddings and GLOVE embeddings re-
spectively.
If Ep and Eg are the matrices of
PubMed embeddings (N × dp) and their corre-
sponding GLOVE embeddings (N × dg) for the
words that both the embeddings have in common
(N), the projection matrix (dg × dp) can be com-
puted as,

W ∗ = arg min

W

(cid:107)W E

p − E
(cid:124)

g(cid:107)
(cid:124)

subject to the constraint that W is orthogonal. The
solution to this optimization problem is given by
(cid:124)
g Ep,
using the singular value decomposition of E
i.e.W ∗ = U V
(cid:124) With this
simple linear transformation, we then computed
the transformed embeddings for all the words in
the PubMed embeddings that are not present in the
GLOVE embeddings.

(cid:124)
g Ep = U ΣV

(cid:124) where E

We also explore a non-linear transformation
using a feed-forward neural network where the
the objective is to learn function f such that,
f (ep; θ) = eg where, ep and eg are PubMed and
GLOVE embeddings respectively. We model f
using a deep neural network with parameters θ,
and train using the common words in both the em-
beddings.

The transformed embeddings from these mod-
els were used in conjunction with the pre-trained
InferSent model to encode the semantic features of
the biomedical sentences as sentence embeddings.
Subsequently, we employ these sentence embed-
dings of the assertion-sentence pairs for a partic-
ular question to train a three-way neural classiﬁer
to predict if the relationship between the two is en-
tailment, contradiction or neither.

It is worth noting here that the embedding trans-
formation techniques that we implemented are not
speciﬁc to the NLI tasks and, in fact, enable trans-
fer learning of a much broader set of tasks on
smaller datasets like BioASQ by using the pre-

113trained models on large datasets of other domains
and ﬁne-tuning on the smaller dataset.

5.1.3 Classiﬁcation
As a ﬁnal step, we use the textual entailment re-
sults for each assertion-sentence pair generated
to heuristically classify the answer as yes or no.
Since our system comprises multiple stages with
the errors of each cascading to the ﬁnal stage, we
do not get perfect entailment results for the pairs.
However, since we have a lot of pairs, we aggre-
gate these entailment scores to compute the overall
entailment or contraction scores to reduce the ef-
fect of accumulated errors for individual pairs on
classiﬁcation.

We used a simple unsupervised approach for
classiﬁcation by just comparing the overall entail-
ment and contradiction scores, i.e.
if the total
number of snippet sentences that entail the asser-
tion are Ne and the total number of snippet sen-
tences that contradict are Nc, then,

(cid:40)

answerq =

yes
no

if Ne ≥ Nc
otherwise

The end-to-end architecture of our system from
the input questions and snippets to the answer is
shown Figure 2.

5.1.4 Experimental Details
For parsing the questions, we used BLLIP
reranking parser (Charniak and Johnson, 2005)
(Charniak-Johnson parser) and used the model
GENIA+PubMed for biomedical text. For train-
ing the textual entailment classiﬁer using In-
ferSent’s sentence embeddings, we used Stan-
ford’s SNLI dataset (Bowman et al., 2015) to
achieve a test-set accuracy of 84.7%.

5.1.5 Results
The performance of the system on yes/no ques-
tions on the training set of phase 5b has been tab-
ulated in table 2. While the accuracies are better
than a random classiﬁer, the task is far from be-
ing solved. Nonetheless, the classiﬁer does handle
the class bias in the training data and performance
similarly on both the categories of answers. More-
over, this classiﬁer achieved the second best test
accuracy of 65.6% on phase 5 of BioASQ 5b (Ta-
ble 4). While we implemented a simple heuristic
based answer-classiﬁer, we believe that a super-
vised classiﬁer using the sentence embeddings as

Category Accuracy (%)
56.5 (252/444)
58.9 (33/56)
57.0 (285/500)

Overall

Yes
No

Table 2: Class-wise accuracies on yes/no ques-
tions in training set of BioASQ Phase 5b

well as ﬁne-tuning of the textual entailment clas-
siﬁer on BioASQ dataset would considerably en-
hance the overall performance of the system.

5.2 Factoid & List Type Questions
Most of the state-of-the-art models for this task
involve training end-to-end deep neural architec-
tures to identify a subset of entities (or phrases)
from the relevant snippets that are most likely to
answer the question. But, owing to the small
size of the dataset, we cannot effectively train
such models on the BioASQ dataset. Hence, we
adopted a two-stage approach that ﬁrst ﬁnds a set
of entities that could potentially answer the ques-
tion and a supervised classiﬁer to rank the entities
on the basis of their likelihood of answering the
question.

For devising the model and evaluation, we pri-
marily focused on factoid type questions since the
methodology for the list-type question would be
largely similar and different only in the number of
top entities returned.
5.2.1 Candidate Selection
We found that the most critical step in the an-
swer generation process is to identify the set of
potential answer candidates that can be fed into
a classiﬁer or ranker to identify the best candi-
dates. At ﬁrst, in order to accomplish this, we used
Named Entity Recognition (NER) taggers to form
a set of candidate answers. The taggers that we
used include Gram-CNN (Zhu et al., 2017), Ling-
Pipe(Carpenter, 2007) and PubTator (Wei et al.,
2013). To analyze the effectiveness of these tag-
gers, we performed an analysis on BioASQ train-
ing set 5b by evaluating the fraction of questions
whose answers are included in the candidate entity
set by the taggers.

Table 3 shows the relative performances of the
three taggers, their union as well as intersection
on train dataset of BioASQ 5b factoid type ques-
tions. A question is exactly answered if a tagger
tags an entity that matches an answer exactly, and
it is partially answered if there is a non-zero over-

114Figure 2: The complete system for yes/no answer classiﬁcation using a question and relevant snippets

lap with an entity tagged and an answer for the
question. We can notice that PubTator and Ling-
Pipe have a good recall with relatively low preci-
sion, while Gram CNN has high recall but low pre-
cision. However, the ﬁnal results with the Named
Entity Taggers were not aligned with our expec-
tations. This is mostly because the answers for
BioASQ are usually a combination of BioNERs
and complementary words, making it hard to de-
ﬁne a pruning method that is able to yield satisfac-
tory results. Surprisingly, a group of candidates
formed of the 100 most frequent n-grams (n from
1 to 4) from the snippets’ sentences were a bet-
ter candidate group than the NER approach for
our supervised ranking method (with NER taggers
used as features instead of candidate entities).
5.2.2 Classiﬁcation Features
Upon computing the set of candidate answers, we
use the question q, set of relevant snippet sen-
tences S and entity type ti to devise a feature vec-
tor for each individual entity ei that comprises the
following features:

• BM25 Score: The BM25 scores for all the
sentences are computed with the question as
the query. Then, the scores of the sentence
that contain the entity are aggregated to com-
pute the BM25 score for the entity, i.e.

(cid:88)

s∈S

ScoreBM 25(ei) =

ScoreBM 25(ei) · 1(s, ei)

where 1(s, ei) is 1 iff sentence s has entity ei.
• Indri Score: Computed in the same manner
• Number of Sentences: Number of sentences

as BM25 score in (i)
s ∈ S that contain the entity ei

• NER Tagger: A multinomial feature that rep-
resents which tagger among PubTator, Ling-
Pipe and GramCNN the entity was extracted
with. This feature is included to identify the
relative strengths of the different taggers.
• Tf Idf: The aggregate Tf-Idf scores of the en-
tity with S as the set of documents
• Entity Type: Is a boolean feature that is 1 if
the type of the entity (for example, gene) is
present in the question, and 0 otherwise.
• Relative Frequency: The amount of times the
entity appears on the snippets’ sentences di-
vided by the total appearance of all of the rel-
evant entities.
• Query Presence: Is a boolean feature that is
1 if the query contains the entity completely
and 0 otherwise.

NER Tags

PubTator
Gram CNN
LingPipe
Union

Intersection

% of questions

Exactly
Partially
Answered Answered

% of
tokens
extracted

32.05
34.90
26.67
49.04
16.29

72.15
99.03
76.75
99.65
38.00

52.27
94.97
11.06
99.25
3.33

Table 3: Baseline recall of different NER Taggers
measured by the fraction of questions that can be
answered by an ideal classiﬁer if the candidates
are chosen using the tagger. We also measure pre-
cision as the fraction of total unique tokens from
the documents that are tagged.

5.2.3 Unsupervised Ranking
As a baseline, we ﬁrst present an unsupervised
ranking system for the candidate answers. In this

115Model

Exact Answers Exact Answers Exact Answers
Yes/No type
Accuracy (%)

List type
F1 score

Factoid type

MRR

(Chandu et al., 2017)
(Peng et al., 2015)
(Wiese et al., 2017)

Sarrouti and Alaoui (2017)

BioAMA(Ours)

-

0.714

-

0.461
0.653

-

0.272
0.392
0.207
0.195

-

0.187
0.361
0.243
0.234

Ideal Answers

All types
ROUGE-2

0.653

-
-

0.577
0.721

Table 4: Comparison of our model with other state of the art approaches

lap with the actual answers.

Once we rank the entities, we use a naive ap-
proach of merely taking top 5 entities as answers
for factoid type and top 10 for list-type. One
could, however, devise a separate model for iden-
tifying the number of top entities to return as an-
swers for the list-type answers.

We found that using just the NER entities as the
answer candidates, the classiﬁer could achieve an
MRR of 0.06 on factoid type questions and an F-
measure of 0.18 for list type questions. However,
by having all the n-grams (n = 1, 2, 3, 4) from the
snippets as candidate answers and using NER tags
as LeToR features, the performance was improved
to an MRR of 0.195 on factoid type questions and
an F1 score of 0.234 on list type questions. The
results are summarized in Table 4.

6 Conclusion and Future Work
In this paper, we present a framework for tackling
both ideal and exact answer type questions and
obtain state of the art results on the ideal answer
type questions on the BioASQ dataset. For exact
answers, we incorporate neural entailment mod-
els along with a novel embedding transformation
technique for answering yes/no questions, and em-
ploy LeToR ranking models to answer factoid/list
based questions. For ideal answers, we improve
the IR component of extractive summarization.
Although this improves ROUGE scores consider-
ably, the human readability aspect of the generated
summary answer is not greatly improved. As fu-
ture directions, we believe that effective abstrac-
tive summarization based approaches like Pointer
Generator Networks (See et al., 2017) and Re-
inforcement Learning based techniques (Paulus
et al., 2017) would improve the human readability
of ideal answers. We aim to continue our research
in this direction to achieve a good balance between
ROUGE score and human readability.

Figure 3: Unsupervised generation of factoid/list
type answers using NER taggers and BM25 re-
trieval model

system, the snippet sentences are ﬁrst ranked using
the BM25 model. Then, for each entity, a score is
computed by aggregating the BM25 scores of the
sentences in which the entity is present. The ra-
tionale for this is that the entities in the top ranked
sentences are more likely to be the answers. This
entity score (which is equivalent to the BM25
score described in 5.2.2) is then used to rank the
entities and return the top k entities as answers to
the question. The overall unsupervised system is
shown in Figure 3.

5.2.4 Learning To Rank
In order to rank the candidate entities in a su-
pervised way, we use a ranking classiﬁer based
on the features described in 5.2.2. For ranking,
we choose point-wise ranking classiﬁers over pair-
wise and list-wise, because it yields similar results
to ranking methods with a less time-consuming
and computationally expensive approach. We use
a traditional SVM-Light (Joachims, 1998) imple-
mentation for point-wise ranking. The data for su-
pervision is derived from the actual answers and
candidate entities are ranked based on their over-

116Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

S. Pyysalo, F. Ginter, H. Moen, T. Salakoski, and
S. Ananiadou. 2013. Distributional semantics re-
sources for biomedical text processing. In Proceed-
ings of LBM 2013, pages 39–44.

Dragomir Radev, Y Hong Qi, Harris Wu, and Weiguo
Fan. 2003. Evaluating web-based question answer-
ing systems.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. Squad: 100, 000+ ques-
tions for machine comprehension of text. CoRR,
abs/1606.05250.

Mourad Sarrouti and Said Ouatik El Alaoui. 2017.
A biomedical question answering system in bioasq
2017. In BioNLP 2017, pages 296–301. Association
for Computational Linguistics.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. CoRR, abs/1704.04368.

Trevor Strohman, Donald Metzler, Howard Turtle, and
Indri: a language-model

W. Bruce Croft. 2005.
based search engine for complex queries.

T. Srensen. 1948. A method of establishing groups of
equal amplitude in plant sociology based on simi-
larity of species and its application to analyses of
In Kongelige
the vegetation on danish commons.
Danske Videnskabernes Selskab, pages 1–34.

Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu.
2013.
Pubtator: a web-based text mining tool
for assisting biocuration. Nucleic acids research,
41(W1):W518–W522.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017.
Fastqa: A simple and efﬁcient neu-
ral architecture for question answering. CoRR,
abs/1703.04816.

Georg Wiese, Dirk Weissenborn, and Mariana L.
Neves. 2017. Neural question answering at bioasq
5b. CoRR, abs/1706.08568.

Qile Zhu, Xiaolin Li, Ana Conesa, and Ccile Pereira.
2017. Gram-cnn: a deep learning approach with lo-
cal context for named entity recognition in biomedi-
cal text. Bioinformatics, page btx815.

References
Georgios Balikas, Anastasia Krithara, Ioannis Partalas,
and George Paliouras. 2015. Bioasq: A challenge on
large-scale biomedical semantic indexing and ques-
In Revised Selected Papers from
tion answering.
the First International Workshop on Multimodal Re-
trieval in the Medical Domain - Volume 9059, pages
26–39, New York, NY, USA. Springer-Verlag New
York, Inc.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the 2015 Conference on
ence.
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Bob Carpenter. 2007. Lingpipe for 99.99% recall
In Proceedings of the Second
of gene mentions.
BioCreative Challenge Evaluation Workshop, vol-
ume 23, pages 307–309.

Khyathi Chandu, Aakanksha Naik, Aditya Chan-
drasekar, Zi Yang, Niloy Gupta, and Eric Nyberg.
2017.
Tackling biomedical text summarization:
Oaqa at bioasq 5b. In BioNLP 2017, pages 58–66.
Association for Computational Linguistics.

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-ﬁne n-best parsing and maxent discriminative
In ACL 2005, 43rd Annual Meeting of
reranking.
the Association for Computational Linguistics, Pro-
ceedings of the Conference, 25-30 June 2005, Uni-
versity of Michigan, USA, pages 173–180.

Alexis Conneau, Douwe Kiela, Holger Schwenk,
Lo¨ıc Barrault, and Antoine Bordes. 2017.
Su-
pervised learning of universal sentence representa-
tions from natural language inference data. CoRR,
abs/1705.02364.

Jan Frederik Forst, Anastasios Tombros, and Thomas
Roelleke. 2009. Less is more: Maximal marginal
relevance as a summarisation feature. In Advances
in Information Retrieval Theory, pages 350–353,
Berlin, Heidelberg. Springer Berlin Heidelberg.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In European conference on machine
learning, pages 137–142. Springer.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, page 10.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. CoRR, abs/1705.04304.

Shengwen Peng, Ronghui You, Zhikai Xie, Beichen
Wang, Yanchun Zhang, and Shanfeng Zhu. 2015.
The fudan participation in the 2015 bioasq chal-
lenge: Large-scale biomedical semantic indexing
and question answering. In CLEF.

117
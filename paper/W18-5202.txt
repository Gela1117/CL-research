End-to-End Argument Mining for Discussion Threads
Based on Parallel Constrained Pointer Architecture

Gaku Morio and Katsuhide Fujita

Tokyo University of Agriculture and Technology

2-24-16, Koganei, Tokyo, Japan

morio@katfuji.lab.tuat.ac.jp, katfuji@cc.tuat.ac.jp

Abstract

Argument Mining (AM) is a relatively recent
discipline, which concentrates on extracting
claims or premises from discourses, and infer-
ring their structures. However, many existing
works do not consider micro-level AM studies
on discussion threads sufﬁciently. In this pa-
per, we tackle AM for discussion threads. Our
main contributions are follows: (1) A novel
combination scheme focusing on micro-level
inner- and inter- post schemes for a discus-
sion thread. (2) Annotation of large-scale civic
discussion threads with the scheme. (3) Par-
allel constrained pointer architecture (PCPA),
a novel end-to-end technique to discriminate
sentence types, inner-post relations, and inter-
post interactions simultaneously.1 The exper-
imental results demonstrate that our proposed
model shows better accuracy in terms of re-
lations extraction, in comparison to existing
state-of-the-art models.
Introduction

1
Argument Mining (AM) is a discipline which
concentrates on extracting claims or premises,
and inferring their structures from a discourse.
In (Palau and Moens, 2009; Stab and Gurevych,
2014; Peldszus and Stede, 2013), they construed
an argument as the pairing of a single claim and
a (possibly empty) set of premises, which justiﬁes
the claim.

Generally,

identifying structures

for argu-
ment components (i.e., premises and claims)
is categorized as a micro-level approach, and
among complete arguments as a macro-level
approach. There are some micro-level approaches
(Palau and Moens,
Stab and Gurevych,
2014, 2017), however, few AM studies aggres-
sively consider a scheme of micro-level reply-to

2009;

1Available at:

https://github.com/EdoFrank/EMNLP2018-ArgMining-Morio
including source codes.

interactions in a thread.
Though Hidey et al.
(2017) provided a micro-level thread structured
dataset,
they considered an entire thread as a
discourse. Thus, they allowed a premise that links
to a claim in another post, while a post should be
considered as a stand-alone discourse because a
writer for each post is different. Also, we need
to consider post-to-post
interactions with the
stand-alone assumption as a backdrop. Moreover,
the dataset of (Hidey et al., 2017) with only 78
threads is too small to apply state-of-the-art neural
discrimination models.

In addition to the shortage of micro-level ano-
tations for discussion threads, no empirical study
on end-to-end discrimination models which tackle
discussion threads exist, to the best of our knowl-
edge.

Motivated by the weaknesses above, this pa-
per commits to the empirical study for discussion
threads. Our main three contributions are as fol-
lows: (1) A novel combination scheme to apply
AM to discussion threads. We introduce inner-
post and inter-post schemes in combination. This
combination enables us to discriminate arguments
per post, rather than per thread as in (Hidey et al.,
2017). In the former scheme, a post is assumed
as a stand-alone discourse and a micro-level an-
notation is provided.
In the second scheme, we
introduce inter-post micro-level interactions. The
introduction of the interactions allows us to cap-
ture informative argumentative relations between
posts.
(2) Large-scale online civic discussions
are annotated by the proposed scheme. Speciﬁ-
cally, we provide two phase annotation, and eval-
(3) A parallel
uate inter-annotator agreements.
constrained pointer architecture (PCPA) is pro-
posed, which is a novel end-to-end neural model.
The model can discriminate types of sentences
(e.g., claim or premise), inner-post relations and
inter-post interactions, simultaneously. In particu-

Proceedingsofthe5thWorkshoponArgumentMining,pages11–21Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics11Figure 1: Example of our scheme for a thread.

lar, our PCPA achieved a signiﬁcant improvement
on challenging relation extractions in comparison
to the existing state-of-the-art models (Eger et al.,
2017; Potash et al., 2017). An advantage of our
model is that the constraints of a thread structure
are considered. The constraints make our archi-
tectures effective at learning and inferring, unlike
existing pointer models.

While our dataset of discussion threads will
make further advances in AM, the proposed PCPA
will make end-to-end AM studies going forward.

2 Related Works

Stab and Gurevych (2017) argue that the task of
AM is divided into the following three subtasks:
(cid:15) Component identiﬁcation focuses on separation of
argumentative and non-argumentative text units and
identiﬁcation of argument component boundaries.
(cid:15) Component classiﬁcation addresses the function of ar-
gument components.
It aims at classifying argument
components into different types, such as claims and
premises.
(cid:15) Structure identiﬁcation focuses on linking arguments
or argument components. Its objective is to recognize
different types of argumentative relations, such as sup-
port or attack relations.

The structure identiﬁcation can also be divided to
macro- and micro-level approaches. The macro-
level approach as in (Boltuži´c and Šnajder, 2014;
2014; Murakami and Raymond,
Ghosh et al.,
2010) addresses
relations between complete
the micro-structure
arguments
ignores
and
of arguments (Stab and Gurevych, 2017).
In
(Ghosh et al., 2014),
introduced
a scheme to represent relations between two
posts by target and callout; however, their study
discards micro-level
in arguments
because of their macro-level annotation. The
micro-level approach as in (Palau and Moens,
2009; Stab and Gurevych, 2014, 2017) focuses
on the relations between argument components.

the authors

structures

In (Palau and Moens, 2009), arguments are con-
sidered as trees.
In (Stab and Gurevych, 2017),
the authors also represented relations of argument
components in essays as tree structures. However,
they addressed discourses of a single writer (i.e.,
an essay writer) rather than multiple authors in
a discussion thread. Therefore, we can’t simply
apply their scheme to our study.

Recently,

the advances of automatic detec-
tion of argument structures have been seen
in the discipline of AM. Some recent pa-
pers (Lippi and Torroni, 2015; Eckle-Kohler et al.,
2015) propose argument component identiﬁcation
to extract argumentative components in the entire
discourse. These works (Persing and Ng, 2016;
Eger et al., 2017; Potash et al., 2017) showed link
extraction task to ﬁnd argumentative relations be-
tween argument components.

End-to-end discrimination models are also
highlighted in AM. The reason is low er-
ror propagation compared with the other ends
(pipeline).
The pipeline models have to
discriminate argument component identiﬁcation
and link extraction subtasks independently, and
thus cause the error propagation (Eger et al.,
2017). The authors propose manners to apply
multi-task learning (Søgaard and Goldberg, 2016;
Martínez Alonso and Plank, 2017) and LSTM-
ER (Miwa and Bansal, 2016)
to the end-to-
end AM. Another end-to-end work for AM,
Potash et al. (2017) argues that Pointer Networks
(Vinyals et al., 2015; Katiyar and Cardie, 2017)
which incorporate a sequence-to-sequence model
in their classiﬁer is a state-of-the-art model for ar-
gument component type prediction and link ex-
traction tasks.

123 Argument Mining for Discussion

Thread
3.1 Scheme
In this work, we present a novel scheme combin-
ing inner-post scheme of a stand-alone post with
inter-post scheme that considers a reply-to argu-
mentative relation. In the inner-post scheme (e.g.,
claim/premise types and inner-post
relations),
"one-claim" approach from (Stab and Gurevych,
In the inter-post scheme,
2017) is adopted.
the micro-level
interaction in the spirit of
(Ghosh et al., 2014) is employed. The deﬁnitions
of inner-post relation and inter-post interaction are
follows:
(cid:15) Inner-post relation (IPR) is a directed argumentative re-
lation in a post. Each IPR:(target   source) indicates
that the source component is either a justiﬁcation for or
a refutation of the target component. Thus, a source
should be a premise, and each premise has a single out-
going link to another premise or claim (Eger et al., 2017).
(cid:15) Target is a head of IPI that has been called out by a sub-
sequent claim in another post that replies to the post of the
target.
(cid:15) Callout is a tail of IPI that refers back to a prior target. In
addition to referring back to the target, a callout must be a
claim.2
(cid:15) Inter-post interaction (IPI) is the micro-level relation-
ship of two posts: parent post and child post that replies to
the parent post. A relation (parent   child) represents
the child is a callout and parent is a target.

Figure 1 shows our combination scheme for a dis-
cussion thread.

3.2 Dataset
To develop a sufﬁcient AM corpus for discussion
threads, we have annotated an original large-scale
online civic discussion (Morio and Fujita, 2018a).
The civic discussion data is obtained by an online
civic engagement on the COLLAGREE (Ito et al.,
2014; Morio and Fujita, 2018b) including a thread
structure. The discussion was held from the end of
2016 to the beginning of 2017, and co-hosted by
the government of Nagoya City, Japan. The accu-
mulated data includes 204 citizens, 399 threads,
1327 posts, 5559 sentences and 120241 tokens
spelled in Japanese.3 To the best of our knowl-
2To restrict a callout to a claim makes our problem more
simple because the number of outgoing links from a claim
becomes one at a maximum. Thus, we introduced the restric-
tion.

3The average of the number of posts per thread is 3.33
(standard deviation is 3.29), the depth of threads is 1.09 (stan-
dard deviation is 1.19), the number of sentences per post is
4.19 (standard deviation is 3.33) and the number of words
per sentence is 21.63 (standard deviation is 19.92).

edge, this work is the ﬁrst approach which anno-
tates large-scale civic discussions for AM.4

3.3 Annotation Design
In (Peldszus and Stede, 2013), the authors argue
that the annotation task for AM contains the fol-
lowing three subtasks: (1) segmentation, (2) seg-
ment classiﬁcation and (3) relationship identiﬁca-
tion. The segmentation requires extensive human
resources, time, and cost. Therefore, we apply a
rule-based technique for the segmentation. Then,
we consider each sentence as an argument com-
ponent candidate (ACC). For classifying the argu-
ment component, the ACC types (claim, premise
or non-argumentative (NonArg)) for each ACC are
annotated. Finally, the relationship identiﬁcation
needs to annotate IPRs and IPIs.

Using multiple processes for multiple annota-
tion subtasks is common (Meyers and Brashers,
2010; Stab and Gurevych, 2014, 2017). To anno-
tate our data, we provide two phases. In the ﬁrst
phase, we concentrate on annotating ACC type
and IPR, and create a temporal gold standard. In
the second phase, IPI is annotated using the tem-
poral gold standard.

We employed a majority vote to create the gold
standard. All three annotators independently an-
notated in this work. The procedure of the ﬁrst
phase for compiling the temporal gold standard is
as follows.

A1: Each ACC type is decided on a majority vote. When the
ACC type of the sentence cannot be decided by majority
vote, NonArg is assigned to them.

A2: Each IPR (link existence) is decided on a majority vote.
A3: Merging the results from A1 and A2, and obtaining trees
where root is a claim. Thus, we have trees to the number
of claims in a post.

A4: Eliminating premise tags that do not belong to any trees,

assigning them to NonArg, and eliminating their IPR.

First, we attempt

3.4 Annotation Result
Inter-annotator agreement for ACC type,
IPR
and IPI annotations are calculated using Fleiss’s
(cid:20) (Fleiss, 1971).
to eval-
the ﬁrst phase anno-
uate the agreement of
tations, however,
IPR is relatively
low: 0:420. The annotators are less likely to
agree on serial arguments (Stab and Gurevych,
2017) like (premise   premise) relations.5
4Recently, Park and Cardie (2018) provide a similar
dataset of civic engagement, while their dataset doesn’t con-
sider post-to-post relations sufﬁciently.

the (cid:20) of

5Unlike with Persuasive Essays (Stab and Gurevych,
2017), citizen’s documents for civic discussions are seldom

13Corpus

COLLAGREE

Type

Claim
Premise
NonArg
IPR w/ A0
IPI
Claim
Premise
Inner-essay rel

Size
1449
2762
1348
2762
745
1506
3832
3832

(cid:20)
.531
.554
.529
.466
.430
.635
.833

Persuasive Essays

.708-.737
Table 1: Inter-annotator agreement scores for the
two corpora.

Therefore, we introduce an initial process A0,
transforming (premise1   premise2) into
(root claim of premise2   premise2), before
A1.6

Table 1 summarizes the number of each type of
relation and inter-annotator agreement.7 For com-
parison, we also mention the annotation results
of Persuasive Essays (Stab and Gurevych, 2017).
Unlike the essay dataset, our datasets contain
badly-structured writings, resulting in low agree-
ment. However, classiﬁcation tasks can be applied
as (Landis and Koch, 1977) refers to the (cid:20) value
from 0.41 to 0.61 as "moderate agreement". More-
over, the agreement of IPR is improved by provid-
ing the process A0.

4 Discriminating ACC Type, Inner-Post

Relation and Inter-Post Interaction

This section describes the study on our end-to-end
discrimination model, which identiﬁes ACC type,
IPR and IPI for our annotated dataset.

4.1 Thread Representation as a Sequence
If the thread itself contains ﬂow of its argument,
only the thread itself is considered as the desirable
input for a discrimination model. Thus, we de-
scribe a way of representing a thread with an input
sequence.

In this work, we extend the sequence represen-
tation of (Eger et al., 2017; Potash et al., 2017).

two

6For

IPRs

well-structured. Thus, we don’t see the point in provid-
ing a more complex scheme (i.e., allowing (premise  
premise) relations).
f(claim1
 
example,
premise1); (premise1   premise2)g are transformed to
f(claim1   premise1); (claim1   premise2)g.

7Outgoing IPI links are composed of 574 claims, 109
premises, and 62 NonArgs. Considering that a callout should
be a claim, the (claim   claim) interaction accounts for
77% of the total. The results indicate that IPIs are pretty ar-
gumentative. In addition, we annotated support/attack rela-
tions (Cocarascu and Toni, 2017). The results show support
accounts for 86% and attacks for 7% of the total IPIs.

The creation of thread representation as an in-
put sequence consists of the following two steps.
First, we assume each element of the input se-
quence for recurrent neural network is a sentence
representation, rather than a word representation.
Second, we sort the sentence representations by
the thread depth order.
In addition, for each
thread depth, we in turn order them according
to the timestamp of their post, and insert separa-
tor representations. The ﬁrst one makes it pos-
sible to input a short sequence to LSTM units
(Hochreiter and Schmidhuber, 1997). The second
makes a classiﬁer easy to discriminate considering
the hierarchy of a thread and reply relations. Fig-
ure 2 shows an example of a thread representation
as sequence.

4.2 Parallel Constrained Pointer

Architecture

One of the main technical contributions of our ap-
proach is to provide a discrimination model that
classiﬁes ACC type, IPR and IPI simultaneously
via end-to-end learning. A Pointer Network (PN)
for end-to-end AM achieves state-of-the-art re-
sults (Potash et al., 2017), which leads to apply-
ing a PN based technique to our scheme. Unfortu-
nately, the naive PN did not achieve the result ex-
pected (the quantitative results are shown in Sec-
tion 5), because the simple PN is unable to con-
strain its search space for thread structures. For
instance, an inner-post relation classiﬁer could dis-
criminate with no need to search out of its post,
or an inter-post interaction classiﬁer could clas-
sify with no need to search out of the parent post
and child post. Therefore, we propose a novel
neural model named parallel constrained pointer
architecture (PCPA). PCPA provides two parallel
pointer architectures: IPR and IPI discrimination
architectures that adopt the apparent constrains of
threads.

Sentence Representation as Input
First, we introduce the input representation. Given
N threads (T1; : : : ; TN ), we denote Ti’s posts
which are sorted in thread depth order, and then
timestamp order as described in Section 4.1 as
(P (i)
), where Ni represents the number
of posts in Ti. In addition to the thread and post
representations, write (S(i;j)
) for sen-
tences in post P (i)
, where Ni;j represents the num-
ber of sentences in P (i)
. Note that separator rep-

1 ; : : : ; P (i)
Ni

; : : : ; S(i;j)
Ni;j

1

j

j

14Figure 2: Sequence representation from Figure 1. ? and j= denotes a separator representation of thread
depth and posts.

Figure 3: Example of the constrained pointer architecture of inner-post relation (IPR) identiﬁcation, dis-
criminating the IPR target from the ACC "7".

resentations are not considered in the notation.

k

∑

Then, wn is given initially, an embedding vec-
tor of nth word in a sentence S(i;j)
, a sen-
tence representation for an input of LSTM is
represented as: Ak =
n wn, where wn is
gained from bag-of-words (BoW) or word em-
beddings (Mikolov et al., 2013; Pennington et al.,
2014; Stab and Gurevych, 2017). In our study, we
employed BoW and a fully connected layer with
a trainable parameter to learn word embeddings.
Subsequently, we provide Bidirectional LSTM
(BiLSTM) (Graves and Schmidhuber, 2005) be-
cause PN requires encoding steps. At each time
step of the encoder BiLSTM, PCPA considers a
representation of an ACC. Thus, the hidden rep-
resentation ei of BiLSTM becomes the concate-
nation of forward and backward hidden represen-
tations. To simplify the explanation, we denote
the hidden representations of (S(i;j)
) as
(e(i;j)
). For better understanding, we
show notations in Figure 3.

; : : : ; S(i;j)
Ni;j

; : : : ; e(i;j)
Ni;j

1

1

Discriminating Inner-Post Relation
The general PN of (Potash et al., 2017) uses all
hidden states ei. Alternately, PCPA can limit

the states to improve the accuracies, since each
premise has a single outgoing link to another sen-
tence in its post. Hence, we provide an approach
to discriminate IPR using only inner-post hidden
states of the BiLSTM.

Figure 3 shows the example IPR discrimina-
tion in thread Ti; for example, we assume that
the inner-post relation of the sentence written as
"7" in the 3rd ACC of post P (i)
is classiﬁed. The
2
general PN needs to consider all ei, therefore, the
search space is large. On the other hands, our
proposed PCPA can consider (e(i;2)
),
which needs to use the hidden states of its post
only. Therefore, our constrained architecture can
reduce the search space signiﬁcantly.

; e(i;2)

; e(i;2)

1

2

3

In general, given W1, W2, and v1, parameters

of attention model (Luong et al., 2015) for PN,

u(i;j;k)
l

⊤
1 tanh

= v

W1e(i;j)

l + W2e(i;j)

k

(1)

represents a degree that kth ACC in post P (i)
has
an outgoing link to lth ACC. Moreover, we can as-
sume e(i;j)
as a query vector. Supposing the ACC
has no outgoing link, we can consider the ACC
learned to point to itself. Although equation (1)
is real-value, a distribution over the IPR input is

k

j

(

)

15Figure 4: Example of the constrained pointer architecture of inter-post interaction (IPI) identiﬁcation,
discriminating the IPI target that is called out from the ACC "5".

j P (i)

considered by taking softmax function, i.e.,

k

p(yipr

j ) = softmax(u(i;j;k))

(2)
representing the probability that kth ACC in post
P (i)
has an outgoing link to lth ACC in P (i)
.
j
Therefore, the objective for IPR in thread Ti is cal-
culated by taking the sum of log-likelihoods for all
posts:

j

Ni∑

Ni;j∑

log p(yipr

k

j P (i)
j )

(3)

Lipr

i =

j=1

k=1

Discriminating Inter-Post Interaction
As the deﬁnition of target and callout
in our
scheme, IPI exists between a parent post and child
post that replies to the parent. Thus, PCPA can
discriminate IPI with no need to use all of the hid-
den representations of the LSTM. In other words,
it can discriminate IPI without searching outside
of the two posts.

Hence, we design an output layer that requires
only a set of reply pairs in thread Ti. Speciﬁcally,
we assume that R(i) = f(j1; j2);(cid:1)(cid:1)(cid:1)g where j1 ̸=
j2^j1 < j2 for a set of parent-child pairs in thread
Ti. Supposing j1 is the index of a parent post and
j2 represents the index of the child post that replies
to the j1. Note that when thread Ti does not have
any reply pairs, R(i) = ∅. Considering the above,
a technique that is similar to the IPR’s technique
is introduced.

Figure 4 shows the example IPI discrimination
in thread Ti; supposing that we are going to dis-
criminate a target that is called out from ACC "5"
in the ﬁgure. In this case, the search space is lim-
; : : : ; e(i;1)
ited by the parent post (e(i;1)
). More-
over, we add an element e(i;2)
so that a callout can
point itself if there’s no target in its parent post.
The left four outputs in the "Interaction Pointer
Distribution" indicate a discrete probabilistic dis-

1

4

1

)

(

tribution that the callout ACC "5" links to target
sentences in its parent post, and an output on the
far right represents a probability that the callout
links to itself.

The equation (1) uses a query in the PN, so we
in turn concentrate on using a query vector for the
callout in IPI. Herein, we introduce an additional
PN for IPI using new attention parameters, W3,W4
and v2, as:

q(i;j;k)
l

= v

⊤
2 tanh

W3e(i;j)

l + W4e(i;j)

k

(4)

k

]

[

q(i;j1;k); q(i;j2;k)

where e(i;j)
is the query from the callout. Sup-
posing that the reply pair is (j1; j2), a target of
kth ACC of the child post P (i)
is searched. The
j2
is obtained by
expanded vector
concatenating the attention vectors q(i;j1;k) from
the parent post and a vector q(i;j2;k)
from the call-
out. This expansion process is the same as the pro-
cess of (Merity et al., 2016). Finally, given all re-
ply pairs of thread Ti, the log-likelihood is calcu-
lated as follows:

k

k

p(yipi
k

j P (i)

j1

Lipi

i =

; P (i)
j2

∑

Ni;j2∑

(j1;j2)2R(i)

k=1

) = softmax([q(i;j1;k); q(i;j2;k)

])

k

log p(yipi
k

j P (i)

j1

; P (i)
j2

)

(5)

Discriminating ACC Type
At each time step of the BiLSTM, the type classi-
ﬁcation task predicts whether it is claim, premise,
or NonArg. The ACC type of sentence S(i;j)
k
can be classiﬁed by taking softmax of z(i;j)
=
Wtypee(i;j)
k + btype, where Wtype and btype are
parameters. An objective for the type classiﬁer

k

16can also be described by taking the sum of log-
likelihoods for all posts as:

p(ytype

k

Ltype

i =

Ni∑
Ni;j∑
j P (i)

j ) = softmax(z(i;j)
)
j P (i)
j )

log p(ytype

k

k

j=1

k=1

(6)

Joint Learning
Combining objectives of IPR (equation (3)), IPI
(equation (5)) and the ACC type (equation (6)), the
training objective of PCPA is shown as follows:

∑
1
N
(cid:0) (1 (cid:0) (cid:11) (cid:0) (cid:12))Ltype

((cid:0)(cid:11)Lipr

i

i

(cid:0) (cid:12)Lipi

i

Loss =

(7)
where (cid:11) and (cid:12) are hyperparameters which adjust
the weight of tasks in our cost function. Note that
(cid:11); (cid:12) 2 [0; 1] ^ (cid:11) + (cid:12) < 1.

)

i

5 Experiments

5.1 Experimental Settings
Evaluation Metric
For the evaluation of ACC types, IPR and IPI
discrimination, we adopt precision, recall and F1
scores. To obtain the precision and recall, we in-
troduce a way to compute positive and negative
cases by creating relations (Stab and Gurevych,
2017), excluding self-pointers. 8 9

Baselines
First, we employ state-of-the-art PN techniques
from (Potash et al., 2017) as baselines. The use
of these baselines was decided because our model
PCPA (Our Model) employs pointer architec-
tures. As the authors proposed two techniques,
sequence-to-sequence model (PN with Seq2Seq)

8For example, supposing there is a post which contains
three sentences, (S1; S2; S3), and two gold standard IPRs,
(S1   S2) and (S1   S3). This is exactly the case that
positive cases of IPR are f(S1   S2); (S1   S3)g, and
negative cases are all sentence pairs excluding self-pointers.
That is, negatives are f(S2   S1); (S2   S3); (S3  
S1); (S3   S2)g. In this case, self-pointer cases are f(S1  
S1); (S2   S2); (S3   S3)g.

9For IPI, we are also able to create sentence pairs. For
instance, suppose there is a parent post which contains three
sentences (S1; S2; S3), a child post that contains two sen-
tences (S4; S5), and a gold standard IPI, (S2   S5). The
positive case of IPI is exactly f(S2   S5)g, and negative
cases are all sentence pairs excluding self-pointers, that is,
f(S1   S4); (S1   S5); (S2   S4); (S3   S4); (S3  
S5)g.

and w/o sequence-to-sequence model (PN with-
out Seq2Seq), we have the two models for com-
parison.

To analyze how a non PN model works,
multi-task learning is employed to the baseline
(Søgaard and Goldberg, 2016) (STagBLSTM) by
(Eger et al., 2017). STagBLSTM is composed of
shared BiLSTM layers for subtasks, and output
layers for each subtask. In (Eger et al., 2017), the
authors provided a BIO tagging task, however, the
task is not required in our work because BiLSTM
handles an input as sentence representation rather
than as word representation. In this paper, we use
one BiLSTM.10

To show end-to-end learning models are ef-
fective for AM on thread structures, we provide
the following three task speciﬁc baselines. First,
feature-based SVM (Stab and Gurevych, 2017)
(SVM - T) is introduced. T indicates each subtask
of the claim classiﬁer, premise classiﬁer, IPR clas-
siﬁer, and IPI classiﬁer. In addition, random for-
est (RF - T) and the logistic regression technique
(Peldszus and Stede, 2015) (Simple - T) are also
introduced. For each task speciﬁc model, BoW
features the top 500 most frequent words 11.

We assume that each output of PN with
Seq2Seq, PN without Seq2Seq or STagBLSTM
does not satisfy the constraints as a self-pointer.
This is because inappropriate outputs with con-
straint violations of IPR and IPI by these ap-
proaches will happen, i.e., they can predict IPI out
of parent and child posts. The assumption main-
tains the false positive (FP) of baselines, since a
self-pointer which results from a chance is not
counted as FP. This condition gives the base-
lines the advantage of precision over our models.
Therefore, this assumption is convincing.

The following describes our implementation de-
tails. The implementation of neural models are by
Chainer (Tokui et al., 2015). The hyperparameters
are the same as (Potash et al., 2017) for the PN
baselines and our models12. In the interest of time,

10Though there are some variation models other than the
single BiLSTM model, our preliminary experiments show a
non-signiﬁcant improvement.

11In fact (Stab and Gurevych, 2017) and employs rich fea-
tures such as structural features. We only use BoW for com-
parison because the properties of COLLAGREE corpus sub-
stantially differ from their corpus.

12Hidden input dimension size 512, hidden layer size 256
for the BiLSTMs, hidden layer size 512 for the LSTM de-
coder of PN without Seq2Seq, and high dropout rate of 0.9
(Srivastava et al., 2014; Zarrella and Marsh, 2016). All mod-
els are trained with the Adam optimizer (Kingma and Ba,

17Model
type

Joint
learning

Model name

Our Model
Our Model - Hyp
STagBLSTM
PN with Seq2Seq
PN without Seq2Seq

Task
speciﬁc

SVM - T
RF - T
Simple - T

Type classiﬁcation

Claim

Premise

NonArg

F1
58.5
58.1
54.2
58.3
60.1

F1
68.7
71.5
65.6
70.8
71.3

F1
36.0
58.8
56.9
48.6
53.1

IPR

Precision

33.8
*45.8
14.3
35.7
36.6

Precision

Link extraction
IPR
IPI
F1
*40.8
*44.3
14.9
27.2
35.0

19.6
*30.4
21.0
13.0
26.5

53.3
41.0
41.1

64.4
66.8
66.1

52.3
38.3
38.3

13.8

0
0

22.4

0
0

6.4
100
0

IPI
F1
*24.8
*26.9
12.6
19.4
20.8

11.5
1.4
0

Our Model w/o separator
STagBLSTM w/o separator
PN with Seq2Seq w/o separator
PN without Seq2Seq w/o separator

Joint
learning
w/o
separator
Table 2: Top: Our models vs.
joint baselines (%). * indicates signiﬁcant. at p < 0:01, two-sided
Wilcoxon signed rank test (Derryberry et al., 2010), compared with each baseline. Middle: Perfor-
mances of task speciﬁc baselines. Bottom: Performances of joint models w/o separator representations.

43.1
51.8
40.7
43.4

66.3
66.1
67.8
67.6

29.6
55.2
52.7
53.7

30.0
13.9
30.4
29.5

36.1
14.5
23.2
21.1

9.9
16.1
10.8
19.0

13.7
10.8
14.6
6.0

we ran 50 epochs, and used the trained model for
testing. The COLLAGREE dataset is divided into
training threads and testing threads at 8 : 2.
In
addition, we use the following hyperparameters in
equation (7): (cid:11) = (cid:12) = 1=3. However, total loss
of Lipr and Lipi tends to enlarge since they have
to calculate a sum of the sentence pairs. Hence,
we provide a model with tuned hyperparameters
(cid:11) = (cid:12) = 0:15 (Our Model - Hyp) for compari-
son.

5.2 Experimental Results
Table 2 summarizes the results of our models and
baselines. For each model, we showed the best F1
score in the table. Due to limitations of space, we
omitted recalls and some precisions. Surprisingly,
all models performed as well as we expected in our
dataset, in spite of low agreements (see Table 1).
Although the basis of the ACC type classiﬁer of
PCPA is the same as the PN model, our model with
tuned hyperparameters is better at NonArg identi-
ﬁcation than the baseline PN models.

Both of our models signiﬁcantly outperform all
baselines for the IPR and IPI discrimination tasks.
"Our Model - Hyp" achieves F1 +9:3% in IPR
identiﬁcation in comparison with the best baseline
PN without Seq2Seq. This is the most important
result because it indicates that incorporating con-
strains of thread structures with the PNs makes re-
lation classiﬁers easy to learn and discriminate.

STagBLSTM shows lower scores in terms of
both IPR and IPI identiﬁcation, implying the difﬁ-
culty of the use of the multi-task learning of BiL-
2014) with a mini batch size of 16.

Model

Our Model
PN with Seq2Seq
PN without Seq2Seq

IPR - F1
(cid:6)0:7
(cid:6)2:3
(cid:6)2:7

IPI - F1
(cid:6)1:8
(cid:6)1:2
(cid:6)3:9

Table 3: Standard deviations of F1 scores (%)

STM. In addition, Table 2 (Middle) also illustrates
that most neural models yield better F1 scores in
comparison with the task speciﬁc models. In addi-
tion, the logistic regression and RF are overﬁtted,
despite that cross validations are employed. Thus,
end-to-end learning assumes an important role for
AM, even in thread structures.

Effectiveness of Separator Representation
To demonstrate the effectiveness of the separator
representations, we conducted an experiment. In
Table 2 (Bottom), the models without the sepa-
rator input representations are indicated as "w/o
separator". It shows that separator representations
dramatically improve scores of PN based models.
This remarkable result is from the ability to learn
the structural information of a thread by encoding
separators in the BiLSTM.

Stability
To analyze the stability of our models, we compare
standard deviations among three selected models.
Table 3 shows standard deviations for the three
models. These results indicate that our model has
lower standard deviations for IPR than baseline
PN models. The reason for this is the size of
search space: our models can effectively limit the
search space based on thread structures.

18Model

Our Model
Our Model with Param Share

IPR - F1
*39.6
36.7

IPI - F1
*22.6
11.9

Table 4: The effect of parameter sharing of the two
pointer architectures.

(a) IPR

(b) IPI

Figure 5: Performances on different thread depths.

Analysis for Parallel Design
Next, we show how our models improve their per-
formance by employing our parallel pointer ar-
chitecture. Herein, we provide a new model of
PCPA with a single PN (Our Model with Param
Share), which shares v1, W1 and W2 in equa-
tion (1) and v2, W3 and W4 in equation (4), re-
spectively. Table 4 demonstrates the mean of F1
scores for our model and Our Model with Param
Share. Note that the average performances are
lower than the best performances in Table 2. The
scores indicate that sharing the two pointer ar-
chitecture parameters is not effective in our pro-
posed model. We estimate this is because poor
association (Caruana, 1997) between the IPR and
IPI identiﬁcation tasks exists. Therefore, our ap-
proach of using two parallel pointer architectures
is effective.

Performance Specialized in Threads
We examine how our models are specialized in
thread structures. Speciﬁcally, we limit the threads
in test datasets by speciﬁc thresholds, and then an-
alyze performance transitions. We conduct two
experiments as the thread depth is limited (Fig-
ure 5a and 5b). While the baselines performances
decrease as the thread depth increases, our model
keeps its F1 score because of the separators and
the search space. The separator representations for
an input increase according to the thread depth,
and the baseline PN models need to use wider
range of hidden states in comparison with the
PCPA model. In other words, our models are ex-
tremely effective, even for deeper threads.

We also limit the threads that we can use in
test data by the number of posts (Figure 6a, and

(a) IPR

(b) IPI

Figure 6: Performances on different number of
posts. When the horizontal value is 1, we test us-
ing threads which contains [1-5] posts.

6b). For discriminating IPR, our model increas-
ingly outperforms others in accordance with the
number of posts. Figure 6b indicates that the dif-
ference between our model and baselines is mini-
mal. This is because the number of posts does not
affect the thread depth, necessarily. Most of COL-
LAGREE’s threads have a depth of at most 2. In
other words, Figure 6b also implies the depth of
threads affects the improvement of IPI identiﬁca-
tions.

6 Conclusion

This paper presented an end-to-end study on dis-
cussion threads for argument mining (AM). We
proposed an AM scheme that is composed of
micro-level inner- and inter- post scheme for a dis-
cussion thread. The annotation result shows we
acquire the valid and pretty argumentative corpus.
To structuralize the discourses of threads automat-
ically, we propose a neural end-to-end AM tech-
nique. Speciﬁcally, we presented a novel tech-
nique to utilize constraints of the thread struc-
ture for pointer networks. The experimental re-
sults demonstrated that our proposed model out-
performed state-of-the-art baselines in terms of re-
lation identiﬁcations.

Possible future work includes enhancing our
scheme for less restricted conditions, i.e., multiple
targets from one callout.

Acknowledgments

This work was supported by CREST, JST (JP-
MJCR15E1), Japan and JST AIP-PRISM Grant
Number
We thank
Takayuki Ito, Eizo Hideshima, Takanori Ito and
Shun Shiramatsu for providing us with the COL-
LAGREE data.

JPMJCR18ZL,

Japan.

19References
Filip Boltuži´c and Jan Šnajder. 2014. Back up your
stance: Recognizing arguments in online discus-
In Proceedings of the First Workshop on
sions.
Argumentation Mining, pages 49–58, Baltimore,
Maryland. Association for Computational Linguis-
tics.

Rich Caruana. 1997. Multitask learning. Mach.

Learn., 28(1):41–75.

Oana Cocarascu and Francesca Toni. 2017. Identify-
ing attack and support argumentative relations us-
ing deep learning. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1374–1379. Association for Com-
putational Linguistics.

DeWayne R. Derryberry, Sue B. Schou, and W. J.
Conover. 2010. Teaching rank-based tests by em-
phasizing structural similarities to corresponding
Journal of Statistics Education,
parametric tests.
18(1).

Judith Eckle-Kohler, Roland Kluge,

and Iryna
Gurevych. 2015. On the role of discourse markers
for discriminating claims and premises in argumen-
In Proceedings of the 2015 Con-
tative discourse.
ference on Empirical Methods in Natural Language
Processing, pages 2236–2242, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Steffen Eger,

Johannes Daxenberger,

and Iryna
Gurevych. 2017. Neural end-to-end learning for
In Proceed-
computational argumentation mining.
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 11–22, Vancouver, Canada. Association
for Computational Linguistics.

Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.

Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,
Mark Aakhus, and Matthew Mitsui. 2014. Analyz-
ing argumentative discourse units in online interac-
tions. In Proceedings of the First Workshop on Ar-
gument Mining, hosted by the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, ArgMining@ACL 2014, June 26, 2014, Balti-
more, Maryland, USA, pages 39–48.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional lstm
and other neural network architectures. NEURAL
NETWORKS, pages 5–6.

Christopher Hidey, Elena Musi, Alyssa Hwang,
Smaranda Muresan, and Kathy McKeown. 2017.
Analyzing the semantic types of claims and
In Pro-
premises in an online persuasive forum.
ceedings of the 4th Workshop on Argument Mining,
pages 11–21, Copenhagen, Denmark. Association
for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Takayuki Ito, Yuma Imi, Takanori Ito, and Eizo
Hideshima. 2014. Collagree: A faciliator-mediated
large-scale consensus support system. In Proceed-
ings of the 2nd International Conference of Collec-
tive Intelligence.

Arzoo Katiyar and Claire Cardie. 2017. Going out on
a limb: Joint extraction of entity mentions and re-
lations without dependency trees. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 917–928. Association for Computational Lin-
guistics.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1).

Marco Lippi and Paolo Torroni. 2015.

Context-
independent claim detection for argument mining.
In Proceedings of the 24th International Conference
on Artiﬁcial Intelligence, IJCAI’15, pages 185–191.
AAAI Press.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
In EMNLP, pages
neural machine translation.
1412–1421. The Association for Computational Lin-
guistics.

Héctor Martínez Alonso and Barbara Plank. 2017.
When is multitask learning effective? semantic se-
quence prediction under varying data conditions. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 1, Long Papers, pages 44–53, Va-
lencia, Spain. Association for Computational Lin-
guistics.

Stephen Merity, Caiming Xiong, James Bradbury, and
Pointer sentinel mixture

Richard Socher. 2016.
models. CoRR, abs/1609.07843.

Renee A. Meyers and Dale Brashers. 2010. Extend-
ing the conversational argument coding scheme: Ar-
gument categories, units, and coding procedures.
Communication Methods and Measures, 4(1-2):27–
45.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality.
In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

20Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1105–1116. Asso-
ciation for Computational Linguistics.

Isaac Persing and Vincent Ng. 2016. End-to-end ar-
gumentation mining in student essays. In Proceed-
ings of Human Language Technologies: The 2016
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 1384–1394.

Gaku Morio and Katsuhide Fujita. 2018a. Annotating
online civic discussion threads for argument mining.
In 2018 IEEE/WIC/ACM International Conference
on Web Intelligence (WI), page (to appear).

Gaku Morio and Katsuhide Fujita. 2018b. Predicting
argumentative inﬂuence probabilities in large-scale
In Companion Proceed-
online civic engagement.
ings of the The Web Conference 2018, WWW ’18,
pages 1427–1434, Republic and Canton of Geneva,
Switzerland. International World Wide Web Confer-
ences Steering Committee.

Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: Classifying positions in online debates
from reply activities and opinion expressions.
In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ’10,
pages 869–875, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
ﬁcation and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tiﬁcial Intelligence and Law, ICAIL ’09, pages 98–
107, New York, NY, USA. ACM.

Joonsuk Park and Claire Cardie. 2018. A corpus of
erulemaking user comments for measuring evalua-
bility of arguments. In Proceedings of the Eleventh
International Conference on Language Resources
and Evaluation, LREC 2018, Miyazaki, Japan, May
7-12, 2018.

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. Int. J. Cogn. Inform. Nat. Intell., 7(1):1–
31.

Andreas Peldszus and Manfred Stede. 2015. Joint pre-
diction in mst-style discourse parsing for argumen-
tation mining. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 938–948, Lisbon, Portugal. As-
sociation for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In Proceedings of the 2014 Con-
representation.
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2017. Here’s my point: Joint pointer architecture for
argument mining. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1375–1384. Association for Com-
putational Linguistics.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, volume 2, pages 231–235. Association for
Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. Journal of Machine Learning Re-
search, 15:1929–1958.

Christian Stab and Iryna Gurevych. 2014. Annotating
argument components and relations in persuasive es-
In COLING 2014, 25th International Con-
says.
ference on Computational Linguistics, Proceedings
of the Conference: Technical Papers, August 23-29,
2014, Dublin, Ireland, pages 1501–1510.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015. Chainer: a next-generation open
source framework for deep learning. In Proceedings
of Workshop on Machine Learning Systems (Learn-
ingSys) in The Twenty-ninth Annual Conference on
Neural Information Processing Systems (NIPS).

Pointer networks.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015.
In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 2692–2700. Curran Associates,
Inc.

Guido Zarrella and Amy Marsh. 2016. Mitre at
semeval-2016 task 6: Transfer learning for stance
detection. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016),
pages 458–463, San Diego, California. Association
for Computational Linguistics.

21
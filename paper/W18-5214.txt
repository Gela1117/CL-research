Using context to identify the language of face-saving

Nona Naderi

Graeme Hirst

Department of Computer Science

Department of Computer Science

University of Toronto

Toronto, Ontario, Canada
nona@cs.toronto.edu

University of Toronto

Toronto, Ontario, Canada
gh@cs.toronto.edu

Abstract

We created a corpus of utterances that attempt
to save face from parliamentary debates and
use it to automatically analyze the language of
reputation defence. Our proposed model that
incorporates information regarding threats to
reputation can predict reputation defence lan-
guage with high conﬁdence. Further exper-
iments and evaluations on different datasets
show that the model is able to generalize to
new utterances and can predict the language
of reputation defence in a new dataset.

1

Introduction

Goffman (1967) deﬁnes face, or reputation, as
“the positive social value a person effectively
claims for himself by the line others assume he
has taken during a particular contact. Face is
an image of self delineated in terms of approved
social attributes”. Criticisms and persuasive at-
tacks pose threats to reputation or face and they
are common in all social interactions. Allegations
are often made against organizations (e.g., compa-
nies and governments) and individuals (e.g., med-
ical practitioners and politicians), and various ar-
gumentation tactics and persuasive strategies are
used in response to these allegations to attempt
to defend the respondent’s reputation and thereby
save face. Previous studies on reputation defence
mostly use manual content analysis, such as the
studies by Benoit and Henson (2009) and Zhang
and Benoit (2009) on political cases, and by Pen-
man (1990) and Tracy (2011) on courtroom cases.
While these studies reveal much about reputation
defence strategies in various social settings, they
do not analyze in detail the actual language used
in the defence of reputation.

Here, we examine political speeches and inves-
tigate whether we can detect the language of rep-
utation defence. We created a corpus of reputa-

tion defence,1 in which the annotations are based
on the structure of parliamentary debate. This
corpus is based on the oral question period of a
Westminster-style parliamentary system, speciﬁ-
cally that of Canada, where the government of the
day is held accountable for its actions and tries to
defend its reputation.2 Using this naturally anno-
tated data lets us avoid the subjectivity of man-
ual analysis, any interpretation by the annotators,
and any annotation inconsistencies. We investi-
gate whether we can predict the language of rep-
utation defence and whether the context in which
the reputation defence occurs can help in identi-
fying this language. We ﬁrst perform experiments
on a sampled dataset from Canadian parliamentary
proceedings of 1994–2014. We then explore the
performance of our approaches on two different
governments. We show that the context of reputa-
tion defence is effective in its recognition.

2 Related work

Reputation defense is more broadly related to
Aristotelian ethos (Aristotle, 2007) or one’s credi-
bility that is reﬂected through the use of language.
Previous studies on face-saving and reputation
management focused on identifying various per-
suasive strategies and their effectiveness (Benoit,
1995; Coombs and Holladay, 2008; Burns and
Bruner, 2000; Sheldon and Sallot, 2008). In the
NLP ﬁeld, Naderi and Hirst (2017) performed a
manual annotation analysis on reputation defence
strategies in Parliament and proposed a computa-
tional model to identify strategies of denial, ex-
cuse, justiﬁcation, and concession. Naderi and
Hirst (2018) further proposed two approaches to

1The data is freely available at http://www.cs.

toronto.edu/˜nona/data/data.html

2https://www.ourcommons.ca/

About/Compendium/Questions/c_d_
principlesguidelinesoralquestions-e.htm

Proceedingsofthe5thWorkshoponArgumentMining,pages111–120Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics111automatically annotate unlabeled speeches with
defence strategies. Another related NLP study fo-
cuses on extracting ethos from the United King-
dom’s parliamentary debates; Duthie and Budzyn-
ska (2018) used a set of features, such as sen-
timents and part-of-speech tags, to extract nega-
tive and positive references. Here, instead, we are
interested in studying whether we can classify a
speech as reputation defence or not, and whether
the context can improve this classiﬁcation.

3 Reputation defence
The main purpose of the oral question period in a
Westminster-style parliamentary system is to hold
the government accountable for its actions and
to highlight the inadequacies of the government.3
Members of the opposition and government back-
benchers both may ask questions, and government
ministers must respond. The questions asked by
the opposition members are confrontational, in-
tended to criticize or embarass the government,
and are considered reputation threats; the answers
to these questions by government ministers try to
defend the government’s choices and the minis-
ters’ reputations. Therefore, these questions and
answers are a rich dataset for characterizing the
language of reputation attack and the language
of reputation defence. Government backbenchers
can also pose questions. However, these questions
are most often friendly and promotional questions,
and the answers given to these questions try to pro-
mote the government’s plans. Thus these ques-
tions and their answers are ordinary reputation-
building or reputation-enhancing pairs. They thus
act as negative examples.

This dichotomy between the two types of ques-
tions in Parliament is supported by qualitative
studies such as those of Pe´rez de Ayala (2001),
Ilie (2006), and Bates et al. (2012). Pe´rez de Ay-
ala (2001) describes Question Time in the U.K.
House of Commons as a ”face-threatening genre”
and examines politeness strategies used in the
face-threatening language of a set of questions.
Bates et al.’s (2012) analysis shows that govern-
ment backbenchers ask either questions that allow

3The Westminster system originated in the United King-
dom and is used in Commonwealth nations, such as Canada,
Australia, India, and New Zealand. The tradition of question
time for government accountability is practiced under differ-
ent names in these countries; in the United Kingdom, it is
known as oral questions, in Canada as oral question period,
in Australia and New Zealand as question time, and in India
as question hour.

the minister to talk about the government’s poli-
cies and positions, or questions that are straight-
forward to answer. While concerns with repu-
tation are of particular importance not only for
politicians but are salient in all social encounters,
gathering a dataset of reputation threats and de-
fences from encounters other than parliamentary
settings is challenging. Hence, we use the avail-
able parliamentary proceedings for characterizing
these languages.

The following question posed by the opposition
in the Canadian Parliament and the Minister’s re-
ply to it is an example of a reputation threat and
the defence made in response. In the example, the
[Deputy] Prime Minister is confronted by an op-
position member with a persuasive attack, and he
tries to defend and justify the actions of the gov-
ernment:4

Example 3.1 Q. Mr. Speaker, the former ﬁnance
minister continues to amaze the crowds with his
dance of the veils, with the ethics counsellor stand-
ing just off stage catching whatever is shed. The
ﬁrst layer was the blind trust that no one could
see through. Next came blind management. Now
we are down to the last and ﬂimsiest layer, the su-
pervisory agreement. Could the Prime Minister
explain why the former ﬁnance minister was al-
lowed the opportunity for hands on management
by the ethics counsellor while all other ministers
adhered to the stricter blind trust or blind man-
agement agreements?

A. Mr. Speaker, the arrangements that were in
place were those that were appropriate to the cir-
cumstances and, in fact, reﬂect the views of the
Parker commission that reviewed these matters in
the past. The former minister complied entirely
with the requirements before him.

The next example shows a non-threatening
question and answer pair, where the question is
posed by a government backbencher.5

Example 3.2 Q. Mr. Speaker, my question is for
the Minister of the Environment. Recently we have
been reading more and more articles in the media
concerning high levels of sulphur in fuels, air pol-
lution and health problems that result from these
high levels. On this issue could the minister tell the

42003-02-20, John Reynolds (Q) and John Manley,
Deputy Prime Minister, representing the Prime Minister (A).
52001-06-04, Shawn Murphy (Q) and David Anderson

(A).

112House what actions are being taken to deal with
the issue of high sulphur levels in fuels in Canada?
A. Mr. Speaker, the announcement I made ear-
lier this year covers gasoline, diesel and fuel oils
outside road fuels.
It will reduce the amount of
sulphur in gasoline from its average now of 360
parts per million to 30 parts per million.
In on
road diesel, the ﬁgure will go from 500 parts per
million to 15. The dates for this are the end of
2004 for gasoline and June 1, 2006, for diesel.

4 Data

We extracted our Canadian data from the Lipad6
dataset of the Canadian parliamentary proceedings
(Hansard) from 1994 to 2014. This data consists
of the proceedings of the 35th to 41st Canadian par-
liaments. We focused on only the ﬁrst question
and answer pair of each topic of discussion dur-
ing the oral question period of parliament sessions
in order to minimize dependency on the broader
topical context. We created a balanced corpus
by randomly sampling the same number of ques-
tions posed by the opposition members (reputation
threats) as those asked by the government back-
benchers (friendly non-threats). This resulted in
9,048 pairs of questions and answers on more than
1,600 issues over the 20-year period.

To further analyze reputation defence strategies
used by different governments, we extracted the
question and answer pairs from parliaments with
different governing parties. The Liberal Party was
the government in the 36th, 37th, and 38th Parlia-
ments, and the Conservative Party was the govern-
ment in the 39th, 40th, and 41st Parliaments. This
allows us to examine the language of reputation
defence used by different political ideologies. Fur-
thermore, by training and testing models on par-
liaments with different governing parties, we can
ensure that the models are not affected by the ide-
ology of the speaker and the topic of day or interest
of the accuser. Table 1 shows the statistics of these
datasets, which, unlike the 1994–2014 dataset, are
not balanced.

5 Reputation threat analysis

A principled analysis of the language of face-
threats or accusations themselves falls outside the
scope of this work, but here we characterize the

6LInked PArliamentary Data, https://www.lipad.

ca

Party
Liberal
Conservative

Parliaments Opposition Government
36, 37, 38
39, 40, 41

11,090
11,504

1,736
2,004

Table 1: Corpus statistics; Party shows the govern-
ing party; Opposition shows the number of questions
asked by the opposition members and their respective
answers, Government shows the number of questions
asked by the government backbenchers and their re-
spective answers.

differences between the questions asked by op-
position members (reputation threats) and ques-
tions asked by government backbenchers (friendly
non-threats). We randomly sampled 3,400 ques-
tions asked by the oppositions and 3,400 questions
asked by the government backbenchers. We per-
formed our analysis using Linguistic Inquiry and
Word Count (LIWC) (Tausczik and Pennebaker,
2010), which is widely used in social science stud-
ies. Table 2 presents the ratio of averages between
reputation threats and non-threat questions for a
set of LIWC features, including anger, negative
and positive emotions, achievement, and cognitive
processes. Ratios greater than 1.0 indicate fea-
tures that are more prominent in reputation threats
and ratios less than 1.0 indicate features that are
more prominent in non-threats. The results show
that, unsurprisingly, anger and negative emotions
used more in reputation threats than non-threats,
whereas positive emotions are used more in non-
threats. These features are motivated by theories,
such as Brown and Levinson (1987) and Parting-
ton (2003) that recognize varying degrees of po-
liteness in threatening or saving the addressee’s
face. Achievements are used more in non-threats
and cognitive processes are used more in reputa-
tion threats. This is consistent with theories (Mul-
holland, 2003) that recognize mentioning the con-
sequences of the fault as one mode of accusation.

6 Approach

Convolutional Neural Networks (CNN) have been
shown to be effective for classiﬁcation tasks (Kim,
2014). Here, we used a CNN model to represent
the question and answer pairs for binary classiﬁ-
cations of face-saving language. We ﬁrst repre-
sented each word in the question and the answer
with its associated pre-trained embedding. We
then applied a convolution operation to each pos-
sible window of x words from the question and the
answer to produce a feature map, similar to the ap-

113Feature
Anger
Negative emotion
Positive emotion
Achievement
Cognitive processes

Ratio Text
1.15
1.35
0.69
0.82
1.20

Opp: Prime Minister has the annoying habit of blindly exonerating . . .
Opp: We all know there is a nasty trade dispute going on between . . .
Gov: . . . presenting new and exciting opportunities . . .
Gov: . . . foundation has successfully concluded agreements with . . .
Opp: . . . Minister of the Environment ought to read the U.S. . . .

Table 2: Ratios of linguistic features in opposition questions to government backbenchers’ questions. Text shows
an example for each feature. Opp shows an opposition question and Gov shows a government backbencher’s
question.

proach of Kim (2014). We then applied a sliding
Max Pooling and concatenated the representation
of the question and the answer. We used 20 and 10
ﬁlters for the ﬁve-fold cross-validation and cross-
parliament experiments, respectively. We used ﬁl-
ter windows of 3 and 4, a dropout of 0.8, and
mini-batch sizes of 32 and 50 for ﬁve-fold cross-
validation and cross-parliament experiments, re-
spectively.

Recurrent neural networks have been used ef-
fectively in NLP for sequence modeling. Here,
we further used two long short-term memory
(LSTM) (Hochreiter and Schmidhuber, 1997) net-
works7 with 128 units to represent questions and
answers, separately. The LSTM layers were then
passed to a dropout layer (Hinton et al., 2012)
with a rate of 0.6. We then merged the two rep-
resentations. For all our Neural Network models,
we initialized our word representations using the
publicly available GloVe pre-trained word embed-
dings (Pennington et al., 2014)8 (300-dimensional
vectors trained on Common Crawl data), and re-
stricted the vocabulary to the 5,000 most-frequent
words. The models were trained with binary cross-
entropy with the Adam optimizer (Kingma and
Ba, 2014) for 10 and 5 epochs for ﬁve-fold cross-
validation and cross-parliament experiments, re-
spectively. We also tried encoding the questions
and answers using a layer of Gated Recurrent
Units (GRU) (Cho et al., 2014) with shared pa-
rameters, but this model performed worse than the
other models, and for brevity we do not report the
results here.

We further trained an SVM classiﬁer (using the
scikit-learn package (Pedregosa et al., 2011)) with
all possible combinations of words extracted from
the cross-product of questions and answers to cap-

7Using https://keras.io/
8https://nlp.stanford.edu/projects/

glove/

ture the interaction between reputation threat and
reputation defence. The features are tuples of
word pairs from question and answer pairs. We
removed word pairs that occurred fewer than 80
times in the datasets. Our use of this set of fea-
tures is inspired by the effectiveness of word pairs
in classifying discourse relations (Biran and McK-
eown, 2013; Pitler et al., 2009) regardless of their
sparsity issue.

7 Evaluation and results

We approach the recognition of the face-saving
language as a binary supervised classiﬁcation task.
Our baselines are majority class (which is always
answers given to the opposition questions), an
SVM model trained with answer unigram vectors
(weighted using tf–idf, represented with the no-
tation ‘-Answers’ in the result tables), and one
layer of GRU to model answer sequences. Since
reputation defence is expressed in response to the
reputation threat, we further considered the ques-
tion as the context of the reputation defence and
trained an SVM model with question and answer
unigrams (weighted using tf-idf, represented by
the notation ‘-Questions&Answers’ in the result
tables). For comparison, we further include the re-
sults of an SVM model trained on only unigrams
from questions (‘-Questions’). We also use one
layer of GRU to model the concatenation of ques-
tion and answer pairs as one sequence. The SVM
model trained on word pairs is represented with
the notation ‘-Questions×Answers’ in the result
tables.

In the cross-parliament setting, we used the
36th, 37th, and 38th parliaments with Liberal gov-
ernments and the 39th, 40th, and 41st parliaments
with Conservative governments. We ﬁrst per-
formed a ﬁve-fold cross-validation on the Lib-
eral and Conservative governments individually
(three parliaments each), and then performed a

114Model

(1) Canada 1994–2014; Opposition: 4,524; Government: 4,524

Accuracy F1

Precision Recall

Majority
Unigrams-Answers
Unigrams-Question&Answers
Unigrams-Questions
1 GRU(128)-Answers
1 GRU(128)-Questions&Answers
CNN(128)-Questions&Answers
2 LSTMs(128)-Questions&Answers
Word-pairs-Questions×Answers

Majority
Unigrams-Answers
Unigrams-Questions&Answers
Unigrams-Questions
1 GRU(128)-Answers
1 GRU(128)-Questions&Answers
CNN(128)-Questions&Answers
2 LSTMs(128)-Questions&Answers
Word-pairs-Questions×Answers

50.00
76.57
88.00
90.10
81.60
94.39
91.40
92.26
91.46

86.47
88.57
92.77
93.59
90.89
95.72
94.50
94.11
95.06

76.57
88.00
90.10
82.64
94.23
91.16
91.92
91.46

88.26
92.59
93.43
94.91
97.52
96.87
96.52
94.95

76.59
88.01
90.11
77.27
93.94
90.54
93.34
91.47

88.10
92.50
93.43
91.53
96.52
95.12
97.23
94.98

76.57
88.00
90.10
89.99
94.91
92.41
91.04
91.46

88.57
92.77
93.59
98.70
98.66
98.81
95.99
95.06

(2) Parliaments 36, 37, 38; Opposition: 11,090; Government: 1,736

(3) Parliaments 39, 40, 41; Opposition: 11,504; Government: 2,004

Majority
Unigrams-Answers
Unigrams-Questions&Answers
Unigrams-Questions
1 GRU(128)-Answers
1 GRU(128)-Questions&Answers
CNN(128)-Questions&Answers
2 LSTMs(128)-Questions&Answers
Word-pairs-Questions×Answers

85.16
87.27
95.87
97.45
91.05
98.33
97.10
97.11
97.48

86.95
95.75
97.41
94.93
99.02
98.31
98.27
97.43

86.82
95.78
97.42
91.63
98.77
97.50
98.98
97.45

87.27
95.87
97.45
98.63
99.30
99.20
97.63
97.48

Table 3: The performance of different models for binary classiﬁcation of reputation defence language using ﬁve-
fold cross-validation on (1) a balanced set from 1994–2014; (2) three Liberal governments; (3) three Conservative
governments.

115cross-parliament classiﬁcation. For all datasets
and models, we randomly used 10% of the training
data as the development set. We evaluated the per-
formance of reputation defence classiﬁcation us-
ing the metrics Accuracy, Precision, Recall, and
F1. Table 3 shows the results of ﬁve-fold cross-
validation on a balanced set from all parliaments in
the period 1994–2014, on just the Liberal govern-
ments, and on just the Conservative governments.
Both CNN and LSTM models improve the classi-
ﬁcation compared to the baselines. In general, we
can see that all the models that rely only on the
answer or reputation defence perform poorer than
the models that rely also on the questions. The
best model achieves an accuracy and F1 measure
of above 98% on the parliaments with Conserva-
tive governments. The highest accuracy and F1
measure on the Liberal dataset is above 95% and
97%, respectively.

the results of

Table 4 shows

the cross-
parliament classiﬁcation. We trained the mod-
els on all Liberal parliaments, and tested them
on all Conservative governments, and then vice
versa. The SVM model trained using question-
and-answer unigrams is a strong baseline. Both
the CNN and LSTM models improved F1 mea-
sure compared to the baseline models. On the
cross-parliament classiﬁcation setting, again the
models trained on both questions and answers per-
form better. The overall performance of the neu-
ral net models across parliaments is poorer than
the classiﬁcation performance within parliaments.
This can be explained by the differences in fram-
ing strategies used in the language of defence by
the two parties, which each defend their actions
and choices from their own point of view. The
SVM model trained on the words extracted from
the cross-product of questions and answers (word-
pairs) achieves the best accuracy, reaching an ac-
curacy and F1 measure above 92% across parlia-
ments. These results show that reputation defence
language can be detected with high accuracy re-
gardless of differences in ideologies and framing
strategies.

An error analysis shows that most errors oc-
curred in the classiﬁcation of answers to non-
threat questions. One reason for this is that while
the government ministers do not defend them-
selves in the answers in response to the govern-
ment backbenchers, they do try to enhance their

image. Consider the following example9:

Example 7.1 Q. Mr. Speaker, my question is for
the Minister of the Environment. Over the week-
end, the leader of the Bloc Qu´eb´ecois had the
temerity to claim that the 2005 budget did not
serve the interests of the people in Quebec. I know
full well that the environment is very important to
the people in my riding. Could the minister tell the
House how the environmental initiatives contained
in the budget will beneﬁt Quebec?

A. Mr. Speaker, Quebeckers are impatiently
awaiting the greenest budget since Confederation.
Very successful contacts have been established
with the Government of Quebec for the use of the
partnership fund. Projects are sprouting up all
over for the climate fund, for new investments, for
national parks and for investment in renewable
and wind energy. Mayors are waiting for green
investments for cities and municipalities through
the new deal, the green municipal fund, the Ener-
Guide program for cities and so on. Quebec must
not be blocked, but greened even more.

We further examined the cases where a repu-
tation defence was erroneously assigned a non-
defence label. These cases require real-world
knowledge to determine that they are indeed repu-
tation defence. Here is an example10:

Example 7.2 Q. Mr. Speaker, this country was
built upon common interests by and for the people
here. We cannot allow the House of Commons to
introduce a bill which, in reality, provides a recipe
for destroying this country. Does the government
realize that this draft bill is an avowal of failure by
this government as far as the future of the federa-
tion is concerned?

A. No, Mr. Speaker. This bill is a follow-up
to the Supreme Court judgment referring back
to the political stakeholders the responsibility to
establish the conditions of clarity under which
they would agree to negotiate the secession of a
province from Canada, and it seems to me that
one of those stakeholders is the Canadian House
of Commons.

The models that rely on only the answer have par-
ticular difﬁculty in distinguishing these cases.

92005-05-31, David Smith (Q) and St´ephane Dion (A).
101999-12-13, Andr´e Bachand (Q) and St´ephane Dion (A).

116Accuracy F1

Precision Recall

82.63
89.23
91.07
91.21
90.83
92.32
91.88
93.36

85.16
82.22
89.60
91.56
84.02
83.48
85.86
85.27
93.59

Model
Train 36, 37, 38 (Opp: 11,090; Gov: 1,736) and test 39, 40, 41 (Opp: 11,504; Gov: 2,004)
Majority
Unigram-Answers
Unigrams-Questions&Answers
Unigrams-Questions
GRU(128)-Answers
GRU(128)-Questions&Answers
CNN(128)-Questions&Answers
2 LSTMs(128)-Questions&Answers
Word-pairs-Questions×Answers
Train 39, 40, 41 (Opp: 11,504; Gov: 2,004) and test 36, 37, 38 (Opp: 11,090; Gov: 1,736)
Majority
Unigram-Answers
Unigrams-Questions&Answers
Unigrams-Questions
GRU(128)-Answers
GRU(128)-Questions&Answers
CNN(128)-Questions&Answers
2 LSTMs(128)-Questions&Answers
Word-pairs-Questions×Answers

86.47
86.95
90.34
91.14
86.29
85.58
86.75
86.72
92.95

85.44
89.10
90.52
92.58
92.14
92.73
92.78
92.31

86.95
90.34
91.14
99.71
98.75
98.55
99.45
92.95

82.22
89.60
91.56
98.25
96.84
99.10
98.66
93.59

83.10
89.02
91.04
85.23
85.65
86.53
86.10
93.33

84.87
89.40
90.42
86.49
86.49
87.67
87.10
92.62

Table 4: The performance of different models for binary classiﬁcation of reputation defence in the cross-parliament
setting. Opp shows the number of opposition members’ questions and their respective answers and Gov shows the
number of government backbenchers’ questions and their respective answers.

Model

Precision Recall
Train 36, 37, 38 and test 39, 40, 41 (balanced, 3400 instances train and 3400 test)

Accuracy F1

Majority
Unigrams-Answers
+NRC Emotion (anger+pos+neg)
+Bigrams
+Vagueness cue words
Word-pairs-Questions×Answers

67.94
69.77
73.41
73.85
83.97
Train 39, 40, 41 and test 36, 37, 38 (balanced, 3400 instances train and 3400 test)

50.00
67.94
69.77
73.41
73.85
83.97

67.92
69.70
73.33
73.75
83.95

67.99
69.94
73.73
74.22
84.14

Majority
Unigrams-Answers
+NRC Emotion (anger+pos+neg)
+Bigram
+Vagueness cue words
Word-pairs-Questions×Answers

50.00
71.24
71.71
73.71
73.88
83.77

70.68
71.14
72.91
73.91
83.67

72.99
73.57
76.88
76.98
84.82

71.24
71.71
73.71
73.88
83.77

Table 5: The performance of different models for binary classiﬁcation of reputation defence in the cross-parliament
setting with the balanced data (1700 instances of each class).

1178 Analyzing the language of defence

To help discover more about the underlying struc-
ture of the data, we conducted an exploratory fea-
ture analysis. We created two balanced datasets
from the two governments, where each dataset
consists of 3,400 question and answer pairs (1,700
questions asked by opposition members and 1,700
questions asked by government backbenchers).
The question and answer pairs were selected ran-
domly. In this setting, we focused only on the text
of the answers or reputation defence.

We consider emotions, such as positive, nega-
tive, and anger. For extracting these features, we
used the NRC Word-Emotion Association Lexi-
con (NRC Emotion lexicon)11. This lexicon pro-
vides manually assigned association scores for ba-
sic emotions including anger, fear, joy, sadness,
disgust, anticipation, trust, surprise, and senti-
ments (positive and negative) (Mohammad and
Turney, 2013).
It consists of 14,182 unigrams
that are manually annotated through crowdsourc-
ing. We compute the total association scores of
the lexicon words in the answer for each class of
emotions and sentiments.

We further examined the NRC VAD Lexicon12
for our analysis. This lexicon provides valence
(positiveness–negativeness / pleasure / displea-
sure), arousal (active–passive), and dominance
(dominant–submissive) scores for 20K English
words (Mohammad, 2018). These dimensions
have been used for analysis of human interaction
(Burgoon and Hale, 1984). We use the total score
of each dimension in the answer as a feature. We
also consider vagueness cue words (Bhatia et al.,
2016; Lebanoff and Liu, 2018). This set of fea-
tures (40 cue words) is represented by the fre-
quency of the vagueness cues in the answer. The
use of these features is motivated by theories such
as that of Fraser (2012) that suggest that hedge
words can be used to avoid face-threatening acts.
We also use bigrams as additional features. We
performed the classiﬁcation using SVM. The re-
sults of the binary classiﬁcation of face-saving lan-
guage on the balanced data of the cross-parliament
setting is presented in Table 5.

The only emotion that contributed to the classi-
ﬁcation was anger. The positive impact of anger

11http://saifmohammad.com/WebPages/

NRC-Emotion-Lexicon.htm

12http://saifmohammad.com/WebPages/

nrc-vad.html

l
a
u
t
c
A

Non-defence
Defence

Predicted
Non-defence Defence
340
1,151

1,360
549

Table 6: Confusion matrix for the best performing
model that relies only on features extracted from an-
swers, including unigrams and bigrams, NRC emo-
tions (anger+pos+neg), and vagueness cues. Trained
on 36,37,38 (3,400 instances) and tested on 39,40,41
(3,400 instances).

l
a
u
t
c
A

Non-defence
Defence

Predicted
Non-defence Defence
332
1,487

1,368
213

Table 7: Confusion matrix for the model trained on
word pairs. Trained on 36,37,38 (3,400 instances) and
tested on 39,40,41 (3,400 instances).

on the classiﬁcation performance is in line with
theories such as those of Mulholland (2003) and
Benoit (1995) that ﬁnd that attacking the accuser
is a type of face-saving strategy. Both positive
and negative sentiments also improved the perfor-
mance of the classiﬁcation, as did vagueness cues
and bigrams. However, using valence, arousal,
and dominance hurt the performance.

The confusion matrices for the best model
trained on the features extracted from the answers
(unigrams and bigrams + NRC Emotions includ-
ing negative and positive sentiments and anger +
vagueness cues) and the model trained on word
pairs are presented in Tables 6 and 7, respectively.
Both models are trained on 3,400 instances from
the 36th, 37th, and 38th parliaments and tested on
3,400 instances from the 39th, 40th, and 41st par-
liaments.

9 Conclusion

Face-saving language is employed in everyday hu-
man interaction. In this study, we introduced the
task of automatically recognizing the language of
face-saving. We created a corpus of reputation-
defence language on various issues from parlia-
mentary proceedings that is freely available. We
further presented two neural network approaches
to classify this language. We showed that the
context of reputation defence is important for this
classiﬁcation task. Our results supported our an-
notation decision based on the adversarial struc-

118ture of the parliament and showed that our cor-
pus is appropriate for analyzing the language of
reputation defence. A practical application of our
model will be to analyze human behavior and to
examine the effectiveness of reputation defence in
various social settings.

Acknowledgements

This research is ﬁnancially supported by an On-
tario Graduate Scholarship, the Natural Sciences
and Engineering Research Council of Canada, and
the University of Toronto. We thank the anony-
mous reviewers for their thoughtful comments and
suggestions.

References
Aristotle. 2007. On Rhetoric: A Theory of Civic Dis-
course (translated by G.A. Kennedy). Oxford Uni-
versity Press.

Soledad Pe´rez de Ayala. 2001.

FTAs and Erskine
May: Conﬂicting needs? Politeness in question
time. Journal of Pragmatics, 33(2):143–169.

Stephen R. Bates, Peter Kerr, Christopher Byrne, and
Liam Stanley. 2012. Questions to the Prime Minis-
ter: A comparative study of PMQs from Thatcher to
Cameron. Parliamentary Affairs, 67(2):253–280.

William L. Benoit. 1995. Accounts, Excuses, and
Apologies: A Theory of Image Restoration Strate-
gies. State University of New York Press, Albany.

William L. Benoit and Jayne R. Henson. 2009. Pres-
ident Bush’s image repair discourse on Hurricane
Katrina. Public Relations Review, 35(1):40–46.

Jaspreet Bhatia, Travis D. Breaux, Joel R. Reidenberg,
and Thomas B. Norton. 2016. A theory of vague-
In Requirements
ness and privacy risk perception.
Engineering Conference (RE), 2016 IEEE 24th In-
ternational, pages 26–35. IEEE.

Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
In Proceedings of the 51st Annual
ambiguation.
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 69–73,
Soﬁa, Bulgaria. Association for Computational Lin-
guistics.

Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some Universals in Language Usage, vol-
ume 4. Cambridge University Press.

Judee K. Burgoon and Jerold L. Hale. 1984. The fun-
damental topoi of relational communication. Com-
munication Monographs, 51(3):193–214.

Judith P. Burns and Michael S. Bruner. 2000. Revisit-
ing the theory of image restoration strategies. Com-
munication Quarterly, 48(1):27–39.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259,.

W. Timothy Coombs and Sherry J. Holladay. 2008.
Comparing apology to equivalent crisis response
strategies: Clarifying apology’s role and value in
crisis communication. Public Relations Review,
34(3):252–257.

Rory Duthie and Katarzyna Budzynska. 2018. A
Deep Modular RNN Approach for Ethos Mining.
In Proceedings of the Twenty-Seventh International
Joint Conference on Artiﬁcial Intelligence, IJCAI-
18, pages 4041–4047. International Joint Confer-
ences on Artiﬁcial Intelligence Organization.

Pragmatic competence: The
Bruce Fraser. 2012.
In Gunther Kaltenb¨ock, Wiltrud
case of hedging.
Mihatsch, and Stefan Schneider, editors, New Ap-
proaches to Hedging, pages 15–34. Brill.

Erving Goffman. 1967. Interaction Ritual: Essays on

face-to-face interaction. Aldine.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012.
Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Cornelia Ilie. 2006. Parliamentary discourses. In Keith
Brown, Anne H. Anderson, Laurie Bauer, Margie
Berns, Graeme Hirst, and Jim Miller, editors, Ency-
clopedia of Language and Linguistics, second edi-
tion, pages 188–196. Elsevier, Oxford.

Yoon Kim. 2014.

Convolutional neural networks
In Proceedings of the
for sentence classiﬁcation.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar. Association for Computational Lin-
guistics.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Logan Lebanoff and Fei Liu. 2018. Automatic detec-
tion of vague words and sentences in privacy poli-
cies. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
Brussels, Belgium. Association for Computational
Linguistics.

119Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
Journal of Lan-
puterized text analysis methods.
guage and Social Psychology, 29(1):24–54.

Karen Tracy. 2011. A facework system of minimal po-
liteness: Oral argument in appellate court. Journal
of Politeness Research. Language, Behaviour, Cul-
ture, 7(1):123–145.

Ernest Zhang and William L. Benoit. 2009. Former
Minister Zhang’s discourse on SARS: Government’s
image restoration or destruction? Public Relations
Review, 35(3):240–246.

Saif Mohammad. 2018. Obtaining reliable human rat-
ings of valence, arousal, and dominance for 20,000
English words. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 174–184.
Association for Computational Linguistics.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
Computational Intelligence, 29(3):436–465.

Joan Mulholland. 2003. A Handbook of Persuasive

Tactics: A Practical Language Guide. Routledge.

Nona Naderi and Graeme Hirst. 2017. Recognizing
reputation defence strategies in critical political ex-
changes. In Proceedings of the International Con-
ference Recent Advances in Natural Language Pro-
cessing, RANLP 2017, pages 527–535, Varna, Bul-
garia.

Nona Naderi and Graeme Hirst. 2018. Automati-
cally labeled data generation for classiﬁcation of
In Proceedings of
reputation defence strategies.
the Eleventh International Conference on Language
Resources and Evaluation (LREC 2018), Paris,
France. European Language Resources Association
(ELRA).

Alan Partington. 2003. The linguistics of political ar-
gument: The spin-doctor and the wolf-pack at the
White House. Routledge.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Robyn Penman. 1990. Facework & politeness: Mul-
tiple goals in courtroom discourse. Journal of Lan-
guage and Social Psychology, 9(1-2):15–38.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the 2014
word representation.
Conferene on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.

Catherine A. Sheldon and Lynne M. Sallot. 2008. Im-
age repair in politics: Testing effects of communica-
tion strategy and performance history in a faux pas.
Journal of Public Relations Research, 21(1):25–50.

120
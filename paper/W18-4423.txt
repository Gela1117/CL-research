Filtering Aggression from the Multilingual Social Media Feed

Sandip Modha

DA-IICT, Gandhinagar
sjmodha@gmail.com

Prasenjit Majumder
DA-IICT, Gandhinagar

prasenjit.majumder@gmail.com

Thomas Mandl

University of Hildesheim, Hildesheim

mandl@uni-hildesheim.de

Abstract

This paper describes the participation of team DA-LD-Hildesheim from the Information Retrieval
Lab(IRLAB) at DA-IICT Gandhinagar, India in collaboration with the University of Hildesheim,
Germany and LDRP-ITR, Gandhinagar, India in a shared task on Aggression Identiﬁcation work-
shop in COLING 2018. The objective of the shared task is to identify the level of aggression from
the User-Generated contents within Social media written in English, Devnagiri Hindi and Ro-
manized Hindi. Aggression levels are categorized into three predeﬁned classes namely: ‘Overtly
Aggressive‘, ‘Covertly Aggressive‘ and ‘Non-aggressive‘. The participating teams are required
to develop a multi-class classiﬁer which classiﬁes User-generated content into these pre-deﬁned
classes. Instead of relying on a bag-of-words model, we have used pre-trained vectors for word
embedding. We have performed experiments with standard machine learning classiﬁers. In addi-
tion, we have developed various deep learning models for the multi-class classiﬁcation problem.
Using the validation data, we found that validation accuracy of our deep learning models outper-
form all standard machine learning classiﬁers and voting based ensemble techniques and results
on test data support these ﬁndings. We have also found that hyper-parameters of the deep neural
network are the keys to improve the results.

1 Introduction
Social media become the popular platform for the common man to the celebrities to discuss or to give
their opinions about any real-world events. In the last a few years, the growth of social media users was
enormous. With this large user base, a massive amount of User-generated content is posted continuously
on Social Media. Social media gives freedom of speech and anonymity to its users. However, often
Social media users abuse this liberty to spread abuses and hate through the posts or comments. On many
occasions, these User-generated contents are offensive or actively aggressive in nature. Many times, such
contents written in a way that might defame or insult individuals or groups of people without actually
using any explicit hate-related or abusive words. Genuine social media users may become the victim
of such abusive or hate comments. Recently, many cases of suicide have been reported by mainstream
media due to trolling or cyberbullying in social media.

Day by day, Anti-Social behavior like Abuse, trolling and cyberbullying is becoming more common
than before on the Social media platform. It is high time for researcher, industry to develop a system
which identiﬁes problematic posts. Social media posts might contain words which can be considered as
either highly or open aggressive or have hidden aggression. Sometimes posts do not have any aggres-
sion. Based on these, posts or comments are classiﬁed into three classes namely: ‘Overtly Aggressive‘,
‘Covertly Aggressive‘ and ‘Non-aggressive‘ by the track organizers (Kumar et al., 2018a). Henceforth, in
rest of paper, we will denote these classes by these abbreviations namely: OAG, CAG, NAG respectively.
Table 1 shows the sample posts belonging to these classes.

Our approach for the shared task TRAC (Kumar et al., 2018a) is based on Machine Learning and
Deep learning. Track organizers have provided training and validation data as per the schedule. Ini-
tially, we have implemented all standard machine learning classiﬁer along with voting based ensemble
technique and prepare baseline results on validation data. Thereafter, we have started to develop deep

ProceedingsoftheFirstWorkshoponTrolling,AggressionandCyberbullying,pages199–207SantaFe,USA,August25,2018.199Id Text
1
2
3 Woman shame on u

Class-label
Do you see further downside in bank nifty till expiry
NAG
Demonitisation like a medicine, it might be sour but slowly and steadily CAG
OAG

Table 1: sample post for each class

learning model for the multi-class classiﬁcation. Since Dataset was created from Facebook posts and
comments, we have chosen fastText pre-trained vector for the word-embedding. Our deep learning mod-
els were based on Bidirectional LSTM, Convolution neural network. In section 3.2 we will discuss our
approaches. It is important to note that we did not engage in any speciﬁc pre-processing on the text
before they were fed into the deep neural net.

Results declared by track organizers show that our deep learning models perform top in the Social
media Hindi dataset, third in English Facebook dataset, ﬁfth in Facebook Hindi dataset. So, overall our
models perform fairly well across all the datasets.

The rest of this paper is organized as follows. In section 2 we brieﬂy discuss related work in this area.
In section 3, we present the corpus statistics and methodology. In section 4, we present results and give
the brief analysis. In section 5, we will give our ﬁnal conclusions along with future works.

2 Related Work
Hate speech and sentiment analysis are the well studied are in the ﬁeld of Natural Language Process-
ing. Aggression identiﬁcation shared task (Kumar et al., 2018a) is more speciﬁc than hate speech and
sentiment analysis task. In (Xu et al., 2012) authors introduce cyberbullying to the NLP community.
They have performed various binary classiﬁcation on tweets text with bullying perspective to determine
whether the user is cyberbully or not, from the sentiment perspective. They reported binary classiﬁcation
accuracy around 81%.

In (Kwok and Wang, 2013), authors have tried to classify tweets against black or not. They have
collected two classes (racist and non-racists) of tweets and used the Naive Bayes classiﬁer for binary
classiﬁcation. The average accuracy for the binary classiﬁcation was around 76 %. (Djuric et al., 2015)
also build a binary classiﬁer to classify in between hate speech and clean user comments on a website.
Authors have proposed to learn distributed low-dimensional representations of comments using word
embedding model such as paragraph2vec model. Authors have created corpus comprises 56,280 com-
ments containing hate speech and 895456 clean comments from Yahoo ﬁnance. They have reported AUC
score around 0.8007 using CBOW paragraph2vec model against 0.7889 in the bag-of-words model using
term frequency.

In (Burnap and Williams, 2015), authors explore cyber hate on Twitter. They have collected tweets for
the speciﬁc domain in a two-week time window. Collection of 450,000 tweets was annotated as hateful
or genuine. They have performed binary classiﬁcation using SVM, BLR, RFDT, Voting base ensemble
and hey achieved best F1-score of 0.77 in the voted ensemble. In (Malmasi and Zampieri, 2017), authors
have used NLP based lexical approach to address the multi-class classiﬁcation problem. They have used
character N-gram, word N-gram and word skip-gram feature for the classiﬁcation.

In (Schmidt and Wiegand, 2017), authors have described the key areas that have been explored to
detect hate speech. They have surveyed different types of features used for hate speech classiﬁcation.
They have categorized features in Simple surface features, word generalization features, sentiment fea-
tures, linguistic features, lexical resources features, Knowledge-based features, and Meta-Information
features. Simple surface features include features like character level unigram/n-gram, word generaliza-
tion features include features like the bag-of-words, clustering, word embedding, paragraph embedding.
Linguistic features include PoS tag of tokens. list of bad words or hate words can be considered as lexical
resources. Existing knowledge base like ConceptNET, Bullyspace can be used as features.

Another type of problematic post is classiﬁed in the shared task eRisk(Losada and Crestani, 2016)
within the Cross-Language Evaluation Forum (CLEF). Here, risk situations regarding health and safety

200are of interest and the research is dedicated to identifying such situations from social media data. In
the ﬁrst classiﬁcation task, posts which indicate depression of a user need to be found. To simulate the
identiﬁcation of such condition early, the shared task provides 10% of the user data for 10 weeks.

Most of the approaches discussed above are the lexical approaches. Our approach is based on pre-
trained word embedding. We have developed deep learning models which implicitly learn features from
the text. Some of the lexical features like the length of the post, number of unique words are added
explicitly in our model but the system performance was degraded.

3 Methodology and Data
In this section, ﬁrst, we describe the datasets used in the experiment in 3.1 and in section 3.2 we will
discuss our approaches in details

3.1 Dataset
Track organizers provided 15,001 aggression-annotated Facebook Posts and Comments each in Hindi
(Romanized and Devanagari script) and English for training and validation (Kumar et al., 2018b). Table
2 shows a detail description of the training and validation Dataset.

Class

NAG
CAG
OAG
Total

English Corpus

Hindi Corpus

# Training

# Validation # Training

# Validation

5052
4240
2708
12000

1233
1057
711
3001

2275
4869
4856
12000

538
1246
1217
3001

Table 2: Class distribution in the Training Dataset

Table 2 shows that there is class-Imbalance in the training data. For English corpus posts from NAG
class are the highest and OAG class is lowest. Similarly, for Hindi corpus, posts from OAG class are
the highest and NAG class is lowest. We will discuss the effect of class Imbalance on results in the
methodology section. Table 3 gives details of the test data corpus. It is worth to note that track organizers
provided an open domain Facebook corpus for training and validation but for testing, in addition to
Facebook English and Hindi corpus, a Twitter corpus dataset was provided from the completely different
domain for both English and Hindi languages.

Test Dataset
Facebook English Corpus
Twitter English Corpus
Facebook mixed script Hindi Corpus
Twitter mixed script Hindi Corpus

# of posts
916
1257
970
1194

Table 3: Test Data Corpus statistics

3.2 Methodology
In this section, we describe our various runs in details. First, we have implemented all standard machine
learning classiﬁers like Multinomial Naive Bayes, Logistic Regression, SGD, KNN, SVC, Decision Tree,
Random forest, various voting based ensemble soft and hard classiﬁer with different text representation
schemes like count based and TF/IDF to prepare baseline results. On the Facebook English Validation
dataset, we got the best-weighted F1 = 0.57 in Logistic Regression with TF/IDF text representation
scheme. KNN classiﬁer gives worst weighted F1 = 0.36 on English Validation data. However soft voting
based Ensemble of Logistic Regression, Naive Bayes, and Random Forest gives best weighted F1 =0.58
with count-based text representation scheme. As discussed in section 3.1, there is a class imbalance in
the training Dataset. We have performed the experiments with equal class labels of training data but

201weighted F1 decreased substantially on validation data. So, we concluded that there is no need to tackle
the class imbalance for this dataset.

3.2.1 Word Embedding for Text representation
Initially, we have implemented both variants of Word2vec namely CBOW and Skip-gram for word-
embedding, but the accuracy was around 50%. We obtained the pre-trained Glove vector with different
dimensions, but the accuracy improved only marginally. Finally, we settled with fastText (Mikolov et
al., 2018) which is an extension of Word2vec. The fastText consider each word as N-gram characters.
A word vector for a word is computed from the sum of the n-gram characters. Word2vec and Glove
consider each word as a single unit and provide a word vector for each word. Since Facebook users
make a lot of mistakes in spelling, typos, fastText is more convenient than Glove and Word2vec. In the
followings, the main advantages of fastText are given over other embedding techniques.

1. fastText can generate word embedding for a word which is not processed during the training from its
N-gram character features. Word2vec and Glove can’t generate word embedding for unseen word.

2. For the rare word, fastText generates better word embedding than Word2vec and Glove due to the

n-gram character features

3.2.2 Model Architecture
Track organizers had provided four Datasets from the open domain of Facebook and Twitter for the test-
ing. For each dataset, we have submitted three runs. In this subsection, we will discuss the architecture
of our deep neural network models and machine learning models which we developed during the training
phase and tested on validation data.

Bidirectional LSTM This is our ﬁrst deep neural network model which we developed to submit our
ﬁrst run. There is two LSTM layer instead of one in Bidirectional LSTM. Bidirectional LSTM is the
extension of the single LSTM. Model parameters are as follows: Maximum features 10000 words; length
of the sequence is 500. We believe that embedding layer is the most critical layer for the model. fastText
(Mikolov et al., 2018) pre-trained vector is used for word embedding with embed size is 300. We have
designed simple bidirectional LSTM with two fully connected layers. We add dropout to the hidden
layer to counter overﬁtting. There are 50 memory units in LSTM and hidden layer. Adam optimization
algorithm is used to update the weight matrix. 3 epochs are sufﬁcient for the model to get overﬁtted.

Single LSTM with higher dropout: This model is based on traditional LSTM model with higher
dropout. The ﬁrst layer is an embedding layer with Maximum features 10000, The length of the sequence
is 1024(maximum length of comment in the dataset). We believe that embedding layer is the most critical
layer for the model. fastText (Mikolov et al., 2018) pre-trained vectors are used for word embedding with
embed size is 300. There are 64 memory units in LSTM layer plus one dense layer with 256 nodes. We
add dropout around 0.5 to counter overﬁtting in the hidden layer. Adam optimization algorithm is used
to update the weight matrix. 3 epochs are sufﬁcient to get the best validation accuracy around 58.4 % on
English corpus and 61.1 % on Hindi corpus.

Model based on Convolution Neural Network This model is based on Convolution Neural Network.
The ﬁrst layer is an embedding layer with maximum features 10000; the length of the sequence is
1024(maximum length of comment in the dataset). The fastText (Mikolov et al., 2018) pre-trained vec-
tors are used for word embedding with embed size is 300. We have added a one-dimensional convolution
layer with 100 ﬁlters of height 2 and stride 1 to target biagrams. In addition to this, Global Max Pooling
layer added. Pooling layer fetches the maximum value from the ﬁlters which are feed to the dense layer.
There are 256 nodes in the hidden layer without any dropout. Validation accuracy is around 58.9 % on
English corpus and 62.4 % on Hindi corpus.

202Model based on Convolution Neural Network with different Filter height This model is by and
large same with previous CNN model except for different one-dimensional ﬁlters with height 2,3,4 to
target bigrams, trigrams, and four-grams features. After convolution layer and max pool layer, model
concatenate max pooled result from each of one-dimensional convolution layer, then build one output
layer on top of them. We have implemented this model from (Zhang and Wallace, 2015). There are 256
nodes in the hidden layer with 0.2 dropouts. Validation accuracy is around 57.6 % on English corpus and
62.4 % on Hindi corpus.

Model based Bidirectional GRU and Convolution Neural Network This model is the hybrid model
of Recurrent Neural Network and Convolution Neural network. The model contains one embedding layer
with pre-trained weight matrix from fastText, max features is 10000 and embed size is 300 followed by
bidirectional GRU layer with 128 units and one- dimensional convolution layer with 64 ﬁlters having
ﬁlter height 2. Validation accuracy is around 59 % on English corpus.

Voting based ensemble model This model is voting based ensemble model of Bidirectional LSTM,
Single LSTM, CNN, Logistic Regression, BIGRU with CNN, the soft ensemble of (random forest clas-
siﬁer, Naive Bayes classiﬁer, Logistic Regression).

Model based on Logistic Regression During the training phase, we got best validation accuracy from
using logistic regression among all standard classiﬁer. TF/IDF gives better accuracy than count based
text representation scheme. We have set logistic regression parameters like N-gram, minimum document
frequency using grid search. We have done little pre-processing on corpus like Non-ASCII character
removal. Stop words are not removed. Validation accuracy is around 57.8 % on English corpus and
59.18 % on Hindi corpus.

4 Results

In this section, we ﬁrst present results on the validation dataset. Table 4 and 5 show results on the
English and the Hindi validation data corpus respectively.

Classiﬁer
Naive Bayes
Logistic Regression
KNN
Linear SVC
Decision Tree
SGD
Random Forest
Soft ensemble
Hard Ensemble
LSTM NN + fasttext
Convolution NN + fasttext
Convolution Ngram+fasttext
BIGRU+FASTTEXT
Bidirectional LSTM + fasttext

Accuracy Precision Recall F1 (weighted) Text Repre. scheme
0.5734
0.5768
0.4415
0.5631
0.4841
0.5691
0.5101
0.5894
0.5751
0.584
0.589
0.576
0.59
0.5928

count based
TF/IDF
Count based
TF/IDF
count base
TF/IDF
TF/IDF
count based
count based
word embedding
word embedding
word embedding
word embedding
word embedding

0.57
0.56
0.38
0.56 0
0.48
0.57
0.49
0.58
0.57
0.59
0.58
0.58
na
0.58

0.58
0.56
0.42
0.56
0.48
0.57
0.5
0.59
0.57
0.59
0.58
0.6
na
0.59

0.58
0.56
0.44
0.56
0.48
0.57
0.51
0.59
0.58
0.58
0.59
0.58
na
0.58

Table 4: Results on English (Facebook) validation data

As we look at the result on validation data, we obtained best weighted F1 score and validation accuracy
for English dataset in the soft ensemble of Logistic Regression, Random Forest, and SGD and in the
Bidirectional LSTM. For Hindi corpus, we got the best weighted F1 score and validation accuracy in
Convolution Neural Network.

203Classiﬁer
Naive Bayes
Logistic Regression
KNN
Linear SVC
Decision Tree
SGD
Random Forest
Soft ensemble
Hard Ensemble
LSTM NN + fasttext
Convolution NN + fasttext
Convolution Ngram NN

Accuracy Precision Recall F1 (weighted) Text Repre. scheme
0.5718
0.5991
0.3318
0.5784
0.5211
0.5924
0.5511
0.5981
0.5944
0.611
0.624
0.624

TF/IDF
TF/IDF
Count based
TF/IDF
count base
TF/IDF
TF/IDF
TF/IDF
TF/IDF
word embedding
word embedding
word embedding

0.59
0.62
0.43
0.58
0.52
0.60
0.56
0.61
0.60
0.6454
0.6278
0.63

0.57
0.6
0.33
0.58
0.52
0.59
0.55
0.60
0.59
0.6111
0.6241
0.62

0.56
0.59
0.34
0.58 0
0.52
0.59
0.55
0.60
0.59
0.6042
0.6244
0.62

Table 5: Results on Hindi (Facebook) validation data

System
Random Baseline
BiDirectional LSTM with fastText-run-1
LSTM with higher dropout-run2
N-Gram CNN-run-3
top team saroyehun

F1 (weighted)
0.3535
0.5959
0.6178
0.5580
0.6424

Table 6: Results for the English (Facebook) dataset

Table 6, 7, 8 and 9 shows our models results on different test datasets which are created from
Facebook and Twitter posts. Figure 1, 2, 3 and 4 show the confusion matrices for the each dataset.
One can observe that weighted F1 score on the test data is better than the validation dataset. Posts in
the datasets are related to different events. Without any given context, our model gives a weighted
F1 score around 0.6178 for English Facebook dataset and 0.6081 for Hindi Facebook Dataset. The
Hindi and English Social media test datasets were created from the Twitter with 3 or 4 domain like #
JNUShutdown, #Cricket2015, #demonetization. We have trained models on Facebook comments or
posts and tested on Twitter posts. It is worth to note that there are lexical differences between Twitter
posts and Facebook posts. Twitter posts are 140 characters long and the majority contain user mentions,
external URL, and Hashtags while most of Facebook posts are longer than Twitter posts does not have
Hashtags or user mentions in the text. However, our model gives a weighted F1 score around 0.5520 for
English Twitter dataset and for mixed script Hindi Twitter dataset, our model gives F1 weighted around
0.4992 in model based on Convolution Neural network.

4.1 Result Analysis
In this subsection, we will present result analysis. The results on validation data show that models based
on LSTM and CNN marginally outperform (around 2 % to 3%) standard machine learning classiﬁers
with respect to weighted F1- score and accuracy on Facebook English corpus and Hindi corpus. Table
10 shows tweets which belong to the CAG class are classiﬁed under the NAG class. Table 11 shows the
tweets which belong to NAG class are classiﬁed under the OAG class.

The main reasons for the posts which are failed to classiﬁed under CAG class are the unavailability of
the context and difference with the wisdom of the human assessor. Same reasons can be applied to the
tweets which are classiﬁed in OAG but actually they belong to NAG class.

204System
Random Baseline
BIGRU-CNN-withfasttext-run-1
CNN-withfastText-run-2
BiDirectional LSTM with fastText-run-3
Top team vista.ue

F1 (weighted)
0.3477
0.5486
0.5520
0.5423
0.6008

Table 7: Results for the English (Social Media(Twitter) Dataset

System
Random Baseline
CNN-withfastText-run-1
N-Gram CNN-run-2
logistic Regression with Tf/IDF -run-3
top team na14

F1 (weighted)
0.3571
0.6081
0.5965
0.6034
0.6450

Table 8: Results for the Hindi (Facebook) Dataset

Figure 1: Confusion Ma-
trix for EN-FB task

Figure 2: Confusion Ma-
trix for EN-TW task

Figure 3: Confusion Ma-
trix for HI-FB task

Figure 4: Confusion Ma-
trix for HI-TW task

205System
Random Baseline
CNN-withfastText-run-1
LSTM with higher dropout-run2
voting based ensemble
top team DA-LD-Hildesheim

F1 (weighted)
0.3206
0.4992
0.4579
0.4797
0.4992

Table 9: Results for the Hindi (Social Media(Twitter)) Dataset.

Post-id

Text

facebook-
467581

facebook-
442501

modi should learn from him how
what to communicate common
people not to corporate people.
The name and the meaning has
changed of RBI to reverse bank
of India.

Actual
Class
CAG

Predicted
Class
NAG

CAG

NAG

Table 10: Tweets which are hard to classify in CAG class

Post-id

Text

facebook-
408314,

facebook-
355578,

facebook-
400906

Pakistan is a terrorist country
and no body would like to play
with Pakistan.
Gandhi Killer Ram (Nathu)
Temple in India Big Shame
4India.

You have to goes for this facts in
U N O in USA and then you can
claims for Kashmir.

Actual
Class
NAG

Predicted
Class
OAG

NAG

OAG

NAG

OAG

Table 11: Tweets belongs to NAG but classiﬁed under OAG class

2065 Conclusion

After performing exhaustive experiments, we conclude that Deep Neural Network with proper word
embedding marginally outperforms all standard machine learning classiﬁer and ensemble techniques.
The critical parameters for the models are the batch size and learning rate. We also concluded that higher
drop out will help to counter model overﬁtting and improvise a standard evaluation metric. CNN and
LSTM are the better models for these datasets. On the English test corpus, we obtained a better F1 score
for NAG class and poor F1-score for CAG class which supports the previous (Malmasi and Zampieri,
2017) ﬁndings. For the Facebook Hindi test corpus, the same seems not to be true. We obtained a
better F1 score for CAG class than NAG class. In the future, we will focus on various pre-trained word
embedding models and study how the word is represented by this model. we have planned to develop
deeper Neural nets and identify optimal parameters using grid search. It is also to be noted that the model
leads to poor result on test data created from the different source than the training corpus source.

Acknowledgements

We would like to acknowledge undergraduate students of LDRP-ITR Mr.Daksh Patel, Mr.Aditya Patel,
Mr.Yash Madhani, Mr.Kishan Thakkar, and Miss. Krishna Tank for their support in post hoc analysis
and they have prepared bad-word ﬁles for both the English and Hindi languages(Romanized and
Devanagari). In the future, we would like to incorporate these lexical resources features in our models.

References
Pete Burnap and Matthew L Williams. 2015. Cyber hate speech on twitter: An application of machine classiﬁca-

tion and statistical modeling for policy and decision making. Policy & Internet, 7(2):223–242.

Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, and Narayan Bhamidipati.
2015. Hate speech detection with comment embeddings. In Proceedings of the 24th International Conference
on World Wide Web Companion, pages 29–30. International World Wide Web Conferences Steering Committee.

Ritesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and Marcos Zampieri. 2018a. Benchmarking Aggression Identiﬁ-
cation in Social Media. In Proceedings of the First Workshop on Trolling, Aggression and Cyberbulling (TRAC)
Santa Fe, USA.

Ritesh Kumar, Aishwarya N. Reganti, Akshit Bhatia, and Tushar Maheshwari. 2018b. Aggression-annotated
Corpus of Hindi-English Code-mixed Data. In Proceedings of the 11th Language Resources and Evaluation
Conference (LREC), Miyazaki, Japan.

Irene Kwok and Yuzhou Wang. 2013. Locate the hate: Detecting Tweets Against Blacks. In Twenty-Seventh AAAI

Conference on Artiﬁcial Intelligence

David E. Losada and Fabio Crestani. 2016. A test collection for research on depression and language use. In

Conference Labs of the Evaluation Forum, pages 28–39. Springer.

Shervin Malmasi and Marcos Zampieri. 2017. Detecting Hate Speech in Social Media. In Proceedings of the

International Conference Recent Advances in Natural Language Processing (RANLP), pages 467–472.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in
In Proceedings of the International Conference on Language

pre-training distributed word representations.
Resources and Evaluation (LREC 2018)

Anna Schmidt and Michael Wiegand. 2017. A Survey on Hate Speech Detection Using Natural Language Pro-
cessing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media.
Association for Computational Linguistics, pages 1–10, Valencia, Spain.

Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu, and Amy Bellmore. 2012. Learning from bullying traces in social
Proceedings of the 2012 conference of the North American chapter of the association for computa-

tional linguistics: Human language technologies, pages 656–666. Association for Computational Linguistics.

Ye Zhang and Byron Wallace. 2015. A sensitivity analysis of (and practitioners’ guide to) convolutional neural

networks for sentence classiﬁcation. arXiv preprint arXiv:1510.03820

207
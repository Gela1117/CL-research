Automatically Detecting the Position and Type of Psychiatric Evaluation

Report Sections

Deya M. Banisakher, Naphtali Rishe, Mark A. Finlayson

School of Computing and Information Sciences

Florida International University

{dbani001,rishe,markaf}@fiu.edu

Miami, FL 33199

Abstract

Psychiatric evaluation reports represent a rich
and still mostly-untapped source of informa-
tion for developing systems for automatic di-
agnosis and treatment of mental health prob-
lems. These reports contain free-text struc-
tured within sections using a convention of
headings. We present a model for automati-
cally detecting the position and type of differ-
ent psychiatric evaluation report sections. We
developed this model using a corpus of 150
sample reports that we gathered from the Web,
and used sentences as a processing unit while
section headings were used as labels of section
type. From these labels we generated a uniﬁed
hierarchy of labels of section types, and then
learned n-gram models of the language found
in each section. To model conventions for sec-
tion order, we integrated these n-gram mod-
els with a Hierarchical Hidden Markov Model
(HHMM) representing the probabilities of ob-
served section orders found in the corpus, and
then used this HHMM n-gram model in a de-
coding framework to infer the most likely sec-
tion boundaries and section types for docu-
ments with their section labels removed. We
evaluated our model over two tasks, namely,
identifying section boundaries and identifying
section types and orders. Our model signif-
icantly outperformed baselines for each task
with an F1 of 0.88 for identifying section
types, and a 0.26 WindowDiff (Wd) and 0.20
and (Pk) scores, respectively, for identifying
section boundaries.

Introduction

1
With the exponential growth of free text in elec-
tronic health records (EHRs)—which includes
mental health documents—it is ever more impor-
tant to develop natural language processing (NLP)
models that automatically understand and parse
such text. When incorporated in other systems,
these models may aid (1) clinical decision sup-

port, (2) the extraction of key population informa-
tion and trends, and (3) precision medicine efforts
where personalized information and trends are ex-
tracted and used in the treatment process (Demner-
Fushman et al., 2009; Hripcsak et al., 2003).

The majority of clinical NLP work has focused
on semantic parsing of clinical notes found in
EHRs. There are several challenges in automatic
understanding of unstructured text in EHRs, en-
compassing many levels of linguistic processing:
identifying document layouts, their discourse or-
ganization, mapping lexical information to seman-
tic concepts found in biomedical ontologies, as
well as understanding inter-concept co-reference
and temporal relations (Li et al., 2010). These
challenges are also present for mental health NLP
applications.

We present an approach to automatically model
the discourse structure of psychiatric reports as
well as segment these reports into various sec-
tions. Our model learns the section types, po-
sitions, and sequence and can automatically seg-
ment unlabeled text in a psychiatric report into
the corresponding sections. We hypothesize that
knowledge of the ordering of the sections can im-
prove the performance of a section classiﬁer and a
text segmenter. To test this hypothesis, we train a
Hierarchical Hidden Markov Model (HHMM) that
categorizes sections in psychiatric reports into one
of 25 pre-deﬁned section labels.

The remainder of this paper is organized as fol-
lows: we ﬁrst introduce psychiatric reports and
their various types and conventions (§2). Next, we
discuss the task deﬁnition in detail (§3). We then
describe our approach including the corpus used,
and the two main components of our model (§4).
Additionally, we present and discuss the baselines
and experiments performed as well as the results
obtained from those experiments (§5). We follow
this with a review of related work on document

Proceedingsofthe9thInternationalWorkshoponHealthTextMiningandInformationAnalysis(LOUHI2018),pages101–110Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics101section identiﬁcation and text segmentation (§6).
Finally, we conclude and specify our contributions
(§7).

2 Psychiatric Evaluation Reports

A mental health assessment is the process through
which a psychiatrist or a psychologist obtains
and organizes necessary information about men-
tal health patients. This process usually involves
a series of psychological and medical tests (clin-
ical and non-clinical), examinations, and inter-
views (Reeves and Rosner, 2016). These proce-
dures serve the purpose of making a diagnosis that
then guides a treatment or a treatment plan (Asso-
ciation, 2018).

The output of a mental health assessment is a
mental health report. Psychiatric reports are sim-
pler subtype of this document type, and mainly
consist of long-form unstructured text.
They
are the end product of psychiatric assessments
in which psychiatrists summarize the information
they gathered, as well as integrate the patient his-
tory, their evaluation, patient diagnosis, and sug-
gested treatments or future steps (Groth-Marnat,
2009; Goldﬁnger and Pomerantz, 2013). There
are several types of psychiatric reports that vary
depending on the type and purpose of assessment:
Psychiatric evaluation reports, crisis evaluation re-
ports, daily SOAP reports (Subjective, Objective,
Assessment, Plan), mental status exam reports,
and mini mental status exam reports, to name a
few (Association, 2006). Our study focuses on
psychiatric evaluation reports. Although there
is no one strict format, there are general guide-
lines that psychiatrists follow when writing psy-
chiatric evaluation reports. Drawing from the gen-
eral psychiatric evaluation domains, these reports
start with the patient’s identifying information,
followed by the patient’s chief complaints, pre-
senting illness and its history, personal and fam-
ily’s medical history, mental status examination,
and ending with the psychiatric medical diagnosis
and treatment plan. This information is typically
structured into an ordered list of headed sections
(Association, 2006). Table 1 contains a detailed
list of the main sections of a psychiatric evalua-
tion report in general order of appearance. Not all
listed sections appear in all psychiatric evaluation
reports, and they also do not necessarily appear in
the same order, although there is usually a general
pattern to the order.

Family History: Her mother was depressed and was
treated. Her mother is currently age 55 . . . There is no
family history of bipolar disorder, anxiety . . . Medical
history in the family is signiﬁcant for her son, age 4, who
is having seizures . . . and several paternal great aunts
had breast cancer.

Figure 1: Excerpt from a psychiatric report showing
an example of implicitly including two different sec-
tions within another (namely, FAMILY PSYCHIATRIC
HISTORY in the ﬁrst underlined portion, and FAMILY
MEDICAL HISTORY in the second underlined portion
within FAMILY HISTORY).

3 Task Deﬁnition

Our goal was to build models that learn the sec-
tion structure of an evaluation psychiatric report.
As discussed earlier, a psychiatric evaluation re-
port consists of several sections, often ordered in
a usual way. Therefore the task we tackle here
is to segment and classify blocks of unstructured
text (at the sentence level) drawn from psychi-
atric evaluation reports into their appropriate sec-
tion types. We assume that the reports follow the
general guidelines of psychiatric evaluation report
writing discussed in (§2).

There are four main challenges in section classi-
ﬁcation of clinical notes and mental health reports.
First, labels that psychiatrists use to designate sec-
tions are ambiguous and various (Li et al., 2010),
for example, a section titled IDENTIFICATION
OF PATIENT by one psychiatrist might be named
REFERRAL DATA or IDENTIFYING INFORMA-
TION by another. Second, psychiatrists often omit
some sections entirely or include them implicitly
within other sections or under other labels, for ex-
ample, the section CHILDHOOD EVENTS can be
included in a larger section such as FAMILY HIS-
TORY while STRENGTHS AND SUPPORTS can
be listed within Mental Status. Figure 1 shows an
example. Third, the sections’ order can be differ-
ent between different psychiatric reports. Fourth,
some section labels are omitted or skipped, es-
pecially if the information that would be placed
in that section is not relevant to the patient being
evaluated.

Additionally, With the section labels removed
from the reports, our segmentation task was to ﬁnd
the section boundaries using sentences as the pro-
cessing unit. This task is similar to topic shift de-
tection in meeting minute, newscasts, and doctor-

102patient counseling conversations (both, written
and spoken). Psychiatric reports are highly struc-
tured , with speciﬁc types of information (e.g.,
prescribed medications) found in particular sec-
tions (e.g., Treatment Plan), and with various gen-
eral conventions for what information should ap-
pear in which sections, and in what order. How-
ever, the segmentation task is not trivial as it faces
the same aforementioned challenges. Addition-
ally, one must ﬁnd highly distinctive features to
distinguish individual sentences (and thus, bound-
aries) in various sections as some of these sec-
tions can contain similar linguistic and structural
features and may even contain similar topic key-
language in FAMILY PSYCHIATRIC
words (e.g.
HISTORY and SOCIAL HISTORY.

We identify the subtasks of this problem as (1)
learning and building a model for the sections’
order and presence in a report, (2) learning and
building models that describe the distinctive fea-
tures of the various section types, and (3) apply-
ing a combination of these two model to simulta-
neously identifying section boundaries and label
section types.

4 Approach

1, . . . , O∗

Given the sequential nature of the reports’ sec-
tions, we treat this ordering task as a sequence
labeling task. That is, given a psychiatric re-
port with n sections S = (S1, . . . , Sn), de-
termine the optimal sequence of section labels
O∗ = (O∗
n) among all possible section
sequences. Hidden Markov Models (HMMs) have
been used successfully for sequence labeling in
a wide variety of applications, including speciﬁ-
cally natural language processing and medical in-
formatics.
In our problem formulation and ap-
proach, we follow and combine work presented
by Sherman and Liu (2008) and Li et al. (2010).
Both of these approaches used HMM-based mod-
els coupled with section or topic-speciﬁc n-gram
models to segment text. Sherman and Liu (2008)
focused on segmenting sentences within meeting
minutes into a set of predeﬁned topics, while Li
et al. (2010) focused on identifying sections within
a clinical note documents. We take a supervised
learning approach where we learn the HMM pa-
rameters using a labeled corpus. Our implementa-
tion was generally guided by the work described
in Barzilay and Lee (2004) and (Rabiner, 1989).
To overcome the challenges outlined in (§3), we

ﬁrst created a uniﬁed hierarchy of standardize sec-
tion labels types, based on observations in a 150
report corpus that we assembled. Second, while
Li et al. (2010) focused on the section level when
building their n-gram language models, we focus
on the sentence level, similar to Sherman and Liu
(2008). Additionally, to model the inclusion of
some sections within others as discussed in (§3)
we built a two-level Hierarchical HMM (HHMM)
(Bui et al., 2004) in which some states contain
HMM models for their implicit subsections. This
is in contrast to the approach presented by Li et al.
(2010), who used a ﬂat HMM, disregarding any
hierarchy within the clinical notes’ sections. The
HHMM model was ﬁrst proposed by Fine et al.
(1998) as a strict tree structure where each state
in the HHMM is an HHMM itself. This approach
was extended and tailored by researchers for var-
ious tasks such as the approach proposed by Bui
et al. (2004) who relaxed the original model to ﬁt
general HMM structures and implementations.

In summary, to tackle the ﬁrst subtask from
(§3) we built a two-level HHMM that models the
positions and order of the reports’ sections. To
tackle the second subtask, we built language mod-
els (namely, n-gram models) per section type that
describe distinctive lexical information for each of
those sections. We then couple the HHMM with
the n-gram models where the HHMM and HMM
states represent the known section labels, while
the states’ observations are the n-grams contained
within each of the individual sections. Finally,
to tackle the the third subtask, that is identifying
section boundaries, we follow a decoding scheme
using the Viterbi algorithm (discussed brieﬂy in
§4.4).

In the remainder of this section we describe
the corpus we collected and annotated. Next, we
present the two components of the HHMM model,
that is, the states (modeling the section order) and
the observations (modeling the section language).
Finally we brieﬂy discuss the process by which we
use the model to identify section boundaries.

4.1 Corpus

To the best of our knowledge there is no corpus of
psychiatric reports annotated with section labels,
so we created our own. We collected 150 pub-
licly available psychiatric evaluation report sam-
ples by crawling the web through custom search
engines (Google Custom Search Engine for Med-

103Table 1: List of possible sections in a psychiatric report used in the corpus.

plicit subsections in our data, namely, MEDI-
CAL HISTORY, FAMILY HISTORY, and MEN-
TAL STATUS. For example, some reports con-
taining the section MENTAL STATUS might in
turn include information in that section about both
MENTAL STATUS EXAM and STRENGTHS AND
SUPPORTS. In this case we identiﬁed these im-
plicit subsection boundaries (that is, the bound-
aries were not identiﬁed with a section header) and
labeled those subsections with both the parent and
child label. Table 1 lists the the parent sections that
sometimes included other sections implicitly (ﬁrst
column), the uniﬁed list of section types found in
the collected reports (second column), word and
sentence level statistics (columns 3-5), and per-
centage of reports containing those sections in the
corpus (last two columns). For both of these stages
we used all 150 reports.

ical Transcriptionists1 and GoogleMT2) and other
sources 3. The reports we selected were complete
and adhere to the general guidelines for psychi-
atric report writing discussed in the previous sec-
tions. Some of the reports were anonymized sam-
ples of real reports, while others were mock re-
ports written for educational purposes.

We prepared the corpus in two stages. First, we
standardized the labels’ names, selecting a single
uniform name for each section type and mapping
corresponding section labels found in the corpus
to those names. For example, some reports con-
tained the section SCHOOL while others listed it
as EDUCATION. Here we selected EDUCATION
as the uniform section label across all reports.

Second, we created a hierarchy for the sec-
tion names which reﬂected implicit embedded sec-
tions types that we found in the corpus. There
were only three section types that included im-

1https://cse.google.com/cse/publicurl?

cx=010964806533120826279:kyuedntb2fy

2https://www.googlemt.com/#gsc.tab=0
3http://www.medicaltranscriptionsamples.

com/

http://mtsamples.com/
https://medword.com/psychiatry5.html
http://www.medicaltranscriptionsamplereports.

com/

http://onwe.bioinnovate.co/

psychological-assessment-example/

Following standard procedure for supervised
machine learning, we split our corpus under a
cross-validation paradigm into two sets for train-
ing and testing, where 80% of the reports were
used in training and 20% for testing.
This
amounted to 120 and 30 reports for training and
testing respectively.

104Parent Label Section Label # Words # Sentences Avg. Sent. Length % Present % Implicit - IDENTIFYING DATA 12 2 6 100 - CHIEF COMPLAINT 27 3 9 100 - MEDICAL HISTORY HISTORY OF PRESENT ILLNESS 232 29 8 95 10 PSYCHIATRIC HISTORY 85 8 11 82 36 SUBSTANCE ABUSE HISTORY 98 10 10 88 44 REVIEW OF SYMPTOMS 150 19 8 96 51 - SURGERIES 28 3 7 33 - ALLERGIES 4 2 2 98 - CURRENT MEDICATIONS 40 9 4 100 - FAMILY HISTORY BIRTH AND DEVELOPMENTAL HISTORY 59 5 10 31 51 ABUSE HISTORY / TRAUMA 110 9 12 79 34 FAMILY PSYCHIATRIC HISTORY 44 5 9 73 80 FAMILY MEDICAL HISTORY 48 7 7 92 38 SOCIAL HISTORY 80 7 11 76 45 PREGNANCY 29 3 8 47 64 - SPIRITUAL BELIEFS 12 2 5 24 - EDUCATION 32 3 8 68 - EMPLOYMENT 31 3 9 79 - LEGAL 10 1 5 20 - MENTAL STATUS MENTAL STATUS 155 18 9 95 11 STRENGTHS AND SUPPORTS 8 1 8 71 43 - FORMULATION 35 4 8 62 - DIAGNOSES 63 12 5 100 - PROGNOSIS 8 2 3 74 - TREATMENT PLAN 121 12 10 100 -            4.2 Modeling the Section Orders
As discussed before, we built an HHMM where
each state corresponds to a distinct section la-
bel. We introduce the terms state and parent state
when discussing the HHMM. A state is simply
an HMM state corresponding to a distinct section.
A parent state is an HHMM state corresponding
to a collection of ordered sections. To account
for sections listed implicitly, we created a two-
level HHMM where parent states contained states
representing the ordered subsections found in the
parent state section. Thus our model contained
25 states and three parent states corresponding to
information in Table 1. The ﬁrst HHMM layer
contained both states and parent states, while the
second layer contained a total of 12 states corre-
sponding to the potential implicit subsections for
the three parent states. In our HHMM, each par-
ent state is simply an HMM itself. Thus our dis-
cussion of HMM parameter calculation applies to
both states and parent states.

Our model learned transition probabilities from
the labeled corpus. The state transition prob-
abilities capture constraints on section order-
ings. We estimated the probabilities between each
state s using Equation 1. Additionally, to account
for sparsity (that is, unseen section orders) we
smoothed the probabilities by the total number of
section labels tS following Laplace smoothing.

P (sj|si) =

count(si, sj) + 1
count(si) + tS

(1)

The second level HMM models contained
within the parent states follow the same scheme in
probability estimation, but differ in the smoothing
parameter (tS). Here, the total number of section
labels tS depends on the number of subsections in
each of the parent states. For example, the parent
state MEDICAL HISTORY contains a total of four
subsections or states, and thus its HMM model is
smoothed by tS = 4. Finally, all of the model’s
states were linked with empty transitions in ad-
dition to self-looping ones to account for missing
sections as well as a section continuation, respec-
tively (i.e. indicating a section shift or a continua-
tion).

4.3 Modeling Section Language
To tackle the second subtask identiﬁed in (§3), we
built n-gram language models (Jain et al., 2015)
that captured distinctive lexical information con-

tained within the individual sections. This, in
turn, helped classify unknown blocks of text (that
is, text unseen previously by the trained mod-
els) within a report into their respective sections.
We opted to use bigrams as our training corpus
because higher n-gram models were extremely
sparse, and had poor performance. This is con-
sistent with signiﬁcant research showing that in
most applications bigrams work well and better
than others (Reynar, 1998).

We built independent bigram models for each
section type in the reports, using only text from
that section type. Additionally, for each of the
three section types represented by the parent states
(discussed above) we built bigram models using
text found in all of the contained subsections. A
common problem that arises with n-gram models
is sparsity of phrases or words. This is especially
the case when training on a small corpus. Given
our relatively small corpus, our models were quite
sparse at ﬁrst, however, we used Laplace Smooth-
ing as a solution.

Similar to transition probabilities, our HHMM
learned observation probabilities from the labeled
corpus. We trained a bigram model for each state
s of the HHMM. Equation 2 shows the computa-
tion for the likelihood of a sentence sequence wk
0
(i.e., a long sequence of words) to be generated
by a state s. Equation 3 shows the computation
for estimating the speciﬁc state bigram probabil-
ity along with Laplace smoothing counts for the
corresponding section S (VS represents the vocab-
ulary size for that section state).

P (wk

0|s) =

Ps(wi+1|wi)

0

Ps(wi+1|wi) =

countS(wi+1
) + 1
countS(wi) + |VS|

i

(2)

(3)

We used a rule-based approach to detect uni-
formly structured sections containing only stan-
dard medical terms such as medications and addi-
tional key terms. The sections mapped with hard-
coded rules are the CURRENT MEDICATIONS
and the standard DSM-IV multiaxal assessment
contained within the DIAGNOSIS section, one of
which is illustrated in Figure 2. We recognize that
this standard has been dropped with the introduc-
tion of DSM-5 in 2013, however, our dataset fol-
lows the older standard as most psychiatric reports

k−1(cid:89)

105in existence do since the new standard is relatively
new.

Figure 2: Example of DSM-IV multiaxal diagnosis as-
sessment.

For the MEDICATIONS section we used pub-
licly available datasets containing lists of medica-
tions (eMedicineHealth, 2018), and the U.S. Na-
tional Library of Medicine’s RxNorm dataset (Liu
et al., 2005). String-matching was additionally
used to locate the DIAGNOSIS sections as our al-
gorithm would search for the key headers “Axis I,
II, III, IV, V”.

Therefore we generated 26 bigram models, one
for each section type (except for the two rule-
based types) plus three parent section types.

4.4 Decoding
We integrated the bigram models with the HHMM
and then used this bigram-HHMM model in a de-
coding framework to infer the most likely sec-
tion boundaries and section types for documents
with their section labels removed. We used the
Viterbi algorithm and applied the following equa-
tion to obtain the most likely labeling of sections
O∗, where n is the section index, and kn is the
word index for section n:

P (s)P (wkn

0 |s)
0 |s1)×
P (s1)P (wkn

O∗ = arg max

s

= arg max
s1s2...sn

n(cid:89)

P (si|si−1)P (wkn

1 |si)

i=0

5 Results and Discussion

As discussed above, we randomly split the corpus
into training and testing sets in a cross-validation
setup, using ten folds, resulting in 120 reports for
training and 30 for testing in each fold. Our mod-
els were trained to learn a total of 25 distinct sec-
tions. Here we present our evaluation methods
and results, describing our baseline approaches, as
well as the performance of both the baselines and
our method averaged across the test sets.

5.1 Evaluation Methods
There are two problems that our system solves: 1)
the section labeling problem —applying the cor-
rect section type to each section—and 2) the sec-
tion segmentation problem—identifying the cor-
rect section boundaries. We evaluate our system’s
performance on these two problems separately.

For the section ordering, we evaluated the per-
formance of the model on each section using the
F1 measure averaged across all folds. As for
the boundary detection problem, we use the Win-
dowDiff (Wd) (Pevzner and Hearst, 2002) and Pk
(Beeferman et al., 1999) metrics. These metrics
compare the number of segmentation boundaries
between a system’s output and a gold standard by
observing a scrolling window of text in the docu-
ment, and run from 0 to 1, with scores closer to 0
being better. Wd increases (gets worse) when the
boundaries are different. Similarly Pk increases
when a section type transition (i.e., a section type
for this study) is different. The Wd score repre-
sents the probability that the number of boundaries
found by the system is different from that in the
gold standard, while the Pk score represents the
probability that any two sentences are incorrectly
listed as being in the same section.

5.2 Baseline Methods
We compared our system’s performance in ﬁnd-
ing the correct labels of sections in a report to
two baseline methods. The ﬁrst method was in-
troduced as a baseline by Li et al. (2010). This
method uses bigrams to independently classify
each section, disregarding any section order in-
formation. For the second baseline, we followed
the primary approach proposed by Li et al. (2010)
which is a ﬂat HMM model built similarly to
our model as described previously (§4), but op-
erates on a section level rather than a sentence
level. Li’s method ignores hierarchical informa-
tion where some report sections are implicitly in-
cluded within other sections. Our implementation
of this model included 25 states corresponding to
each section within the reports. Both of these
methods assume that the section boundaries are
given, and as such they only generate a sequence
labeling for section types.

We compared our system’s performance in
identifying section boundaries to two other base-
line methods. The ﬁrst is LCSeg—a popular text
segmentation baseline (Galley et al., 2003). LC-

106Table 2: Section type identiﬁcation results (precision, recall and F1 scores) per section as well as micro and macro
averages. Parent sections are in bold.

Seg assumes that a topic change in written text oc-
curs when chains of frequent repetitions of words
begin and end.
It rewards shorter chains over
longer ones and further rewards chains with more
repeated terms. Finally, the lexical cohesion be-
tween two chains is evaluated using a cosine simi-
larity. The second method is TopicTiling—an aug-
mentation of the well-known TextTiling algorithm
(Hearst, 1994). TopicTiling (Riedl and Biemann,
2012) is LDA-based and represents segments as
dense vectors of terms contained in dominant top-
ics (as opposed to sparse term vectors).

5.3 Results

For the section labeling problem, our model
equaled or outperformed both baselines in all
the sections. Table 2 shows the precision, re-
call, and F1 scores for the two baselines and our
model. The DIAGNOSIS section saw the best per-
formance due to a rule-based approach. Simi-
larly, CURRENT MEDICATIONS achieved high
scores due to the use of dictionaries. All three

models performed the worst in identifying the LE-
GAL section. We suspect that this is due to the
low prevalence of this section and its content in
the dataset. Similarly, sections with lower preva-
lence saw lower performance than others. Both
baselines performed well in identifying the IDEN-
TIFYING DATA and DIAGNOSIS sections due to
their highly distinctive language. Our model per-
formed better for all implicit subsections, and sig-
niﬁcantly better for two (i.e., PREGNANCY and
BIRTH AND DEVELOPMENTAL HISTORY. Fi-
nally, our model performed exactly the same as the
Flat HMM baseline for the three parent types, as
our model reduces to the Flat HMM in these cases
and because the ﬂat HMM model assumes a ﬁxed
general ordering of the sections.

Since the report sections vary in size, we com-
puted both macro- and micro-averaged precision,
recall, and F -measure (last two rows in Table 2).
Our model’s micro-averaged F -measure is above
90% which is signiﬁcantly higher than both the
Flat-HMM and the independent bigram baselines

107SectionIndependent BigramFlat HMMHHMMPRF1PRF1PRF1IDENTIFYING DATA0.830.810.820.960.940.950.980.950.97CHIEF COMPLAINT0.680.650.670.880.740.800.940.890.91MEDICAL HISTORY0.660.660.650.930.880.900.930.880.90HISTORY OF PRESENT ILLNESS0.690.670.680.910.860.880.940.860.90PSYCHIATRIC HISTORY0.650.600.620.740.850.790.930.860.89SUBSTANCE ABUSE HISTORY0.690.690.690.880.800.840.950.830.89REVIEW OF SYMPTOMS0.80.670.730.790.860.820.940.870.90SURGERIES0.40.310.350.790.510.620.850.640.73ALLERGIES0.60.800.690.900.860.880.880.910.89CURRENT MEDICATIONS0.870.740.800.900.840.870.910.930.92FAMILY HISTORY0.600.560.580.920.860.890.920.860.89BIRTH AND DEVELOPMENTAL HISTORY0.680.500.570.710.680.690.890.800.84ABUSE HISTORY / TRAUMA0.420.330.370.870.770.820.960.810.88FAMILY PSYCHIATRIC HISTORY0.570.590.580.920.870.890.920.900.91FAMILY MEDICAL HISTORY0.650.600.620.920.890.900.940.890.91SOCIAL HISTORY0.670.690.680.660.890.760.930.810.87PREGNANCY0.60.670.630.890.510.650.920.800.86SPIRITUAL BELIEFS0.730.460.560.900.90.900.930.880.90EDUCATION0.660.610.630.710.770.740.920.840.88EMPLOYMENT0.650.620.630.910.880.890.920.860.89LEGAL0.160.620.260.670.610.640.720.680.70MENTAL STATUS0.560.720.620.850.940.890.850.940.89MENTAL STATUS EXAM0.640.630.640.830.960.890.850.960.90STRENGTHS AND SUPPORTS0.420.820.560.800.920.860.820.920.87FORMULATION0.560.710.630.860.780.820.920.820.87DIAGNOSES0.880.760.810.960.950.960.980.980.98PROGNOSIS0.660.620.640.840.820.830.900.860.88TREATMENT PLAN0.740.830.780.950.930.940.970.930.95Macro-Average0.620.640.620.850.820.830.910.860.88Micro-Average0.620.620.620.860.830.840.930.910.92performing at 85% and 62% respectively. Sim-
ilar to Li et al. (2010), both our HHMM and
the Flat-HMM baseline seemed to neither overﬁt
nor underﬁt, which is indicated by higher micro-
averaged compared to the macro-averaged scores.
As for the boundary detection problem, and
similar to the evaluation in Sherman and Liu
(2008), we performed two experiments for the
baselines since both baselines require a parame-
ter representing the number of boundaries (num-
ber of topics minus one). In the ﬁrst experiment
we allowed the parameter to be chosen by LCSeg
and TopicTiling, respectively, while in the second
experiment, we provide the algorithms with the
correct number of boundaries (i.e., number of sec-
tions minus one). Our model however, needs no
prior information regarding the number of sections
present in a given report. Table 3 shows the Wd
and Pk scores for all three approaches. Our sys-
tem again outperformed both baselines indicated
by lower Wd and Pk error rates overall. Both
baselines performed better when the number of
boundaries is known—an expected result. In fact,
TopicTiling outperformed our approach by a small
margin when provided with the correct parameter
value. We note, however, that when running open
loop on new text, the number of sections will be
unknown, so this result does not reﬂect how we
envision the approach being used.

Table 3: Section boundary identiﬁcation results.

6 Related Work
As discussed above, our work simultaneously
solves two problems within a psychiatric evalua-
tion report: identifying section types and identify-
ing section boundaries. The ﬁrst problem has been
referred to as argumentative zoning (Teufel et al.,
1999; Li et al., 2010; Denny et al., 2009), while
the second is a type of text segmentation problem
(Hearst, 1994; Riedl and Biemann, 2012). Argu-
mentative zoning refers to classifying text sections
into mutually exclusive categories. Work on this

task is mostly centered around identifying scien-
tiﬁc article sections (e.g., abstract, introduction,
methodology, etc.) (Teufel, 1999).

Our work is a combination and extension of
Li et al. (2010)’s work on identifying section
types within clinical notes and Sherman and Liu
(2008)’s work on text segmentation of meeting
minutes. Both approaches integrated n-gram lan-
guage models into HMMs. The former mod-
eled HMM emissions at the section level using
bigrams, while the later modeled the emissions
at the sentence level and used unigrams and tri-
grams. Other approaches followed similar strate-
gies in segmenting story text and in creating gener-
ative models for detecting story boundaries (Mul-
bregt et al., 1998; Yamron et al., 1998). More re-
cently, Yu et al. (2016) used a hybrid deep neural
network combined with a Hidden Markov Model
(DNN-HMM) to segment speech transcripts from
broadcast news to a sequence of stories.

More broadly, there has been some work on ap-
plying NLP in the mental health domain. How-
ever, due to lack of readily available clinical
data (e.g. clinical reports), researchers have fo-
cused on non-clinical sources (e.g., social me-
dia) (Chapman et al., 2011). Several algorithms
were developed to detect speciﬁc emotions from
suicide notes and online journals (Pestian et al.,
2012; Strapparava and Mihalcea, 2008), while
twitter data was used to detect distress and sui-
cide ideation (Homan et al., 2014; O’Dea et al.,
2015). Additionally, twitter data was used to mea-
sure mood valence and detect depression (Sadilek
et al., 2013; De Choudhury et al., 2013; Cop-
persmith et al., 2015). Facebook data was used
to measure emotion contagion and to predict
post-partum depression (Coviello et al., 2014;
De Choudhury et al., 2014). Instead of social me-
dia and publicly available, non-clinical data Al-
thoff et al. (2016) used counseling conversations
gathered using a messaging service and developed
discourse analysis methods to measure the corre-
lation of outcomes with various linguistic aspects.

7 Contributions

To the best of our knowledge, our work represents
the only attempt at detecting the position and type
of psychiatric report sections.
In this paper we
present an approach that applies and extends ear-
lier work on document section discovery and seg-
mentation. We collected a corpus of psychiatric

108# of Boundaries Algorithm Pk Wd System Choice LCSeg 0.29 0.37 TopicTiling 0.27 0.33 Provided LCSeg 0.25 0.33 TopicTiling 0.20 0.25  HHMM 0.20 0.26   documents and created a uniﬁed hierarchy of sec-
tion labels. We built an n-gram-based HHMM
model that successful detects the order of sec-
tions as well as their boundaries within a given re-
port. We evaluated our model’s performance over
two separate tasks, namely the section ordering
task and the section boundary identiﬁcation. Our
model outperformed baselines for both of those
tasks. Finally, our approach further conﬁrms that
learning the section ordering of a psychiatric re-
port yields better performance for boundary iden-
tiﬁcation and text segmentation.

Acknowledgments

We thank our colleagues at the Cognition, Narra-
tive, and Culture Laboratory (Cognac Lab), espe-
cially Mohammed Aldawsari and Victor Yarlot for
their contributions to the overall idea, approach,
and evaluation.

References
Tim Althoff, Kevin Clark, and Jure Leskovec. 2016.
Large-scale analysis of counseling conversations:
An application of natural language processing to
mental health. Transactions of the Association for
Computational Linguistics, 4:463–476.

American Psychiatric Association. 2006. American
Psychiatric Association Practice Guidelines for the
Treatment of Psychiatric Disorders: Compendium
2006. American Psychiatric Association Publish-
ing, Washington, DC.

American Psychiatric Association. 2018. What is
Available from: https://www.

psychiatry?
psychiatry.org/patients-families/
what-is-psychiatry.
2018).

(Accessed on Jul 1,

Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applica-
tions to generation and summarization. In Proceed-
ings of the 2004 North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies Conference (HLT-NAACL),
pages 113–120.

Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1):177–210.

Hung H. Bui, Dinh Q. Phung, and Svetha Venkatesh.
2004. Hierarchical hidden markov models with
the
general state hierarchy.
19th National Conference on Artiﬁcal Intelligence,
AAAI’04, pages 324–329, San Jose, California.

In Proceedings of

Wendy W Chapman, Prakash M Nadkarni, Lynette
Hirschman, Leonard W D’Avolio, Guergana K
Savova, and Ozlem Uzuner. 2011. Overcoming bar-
riers to NLP for clinical text: The role of shared
tasks and the need for additional creative solutions.
Journal of the American Medical Informatics Asso-
ciation, 18(5):540–543.

Glen Coppersmith, Mark Dredze, Craig Harman,
Kristy Hollingshead, and Margaret Mitchell. 2015.
CLPsych 2015 shared task: Depression and PTSD
In Proceedings of the 2nd Workshop
on twitter.
on Computational Linguistics and Clinical Psychol-
ogy: From Linguistic Signal to Clinical Reality
(CLPsych), pages 31–39.

Lorenzo Coviello, Yunkyu Sohn, Adam D.

I.
Kramer, Cameron Marlow, Massimo Franceschetti,
Nicholas A. Christakis, and James H. Fowler. 2014.
Detecting emotional contagion in massive social
networks. PLOS ONE, 9(3):1–6.

Munmun De Choudhury, Scott Counts, Eric J. Horvitz,
and Aaron Hoff. 2014. Characterizing and pre-
dicting postpartum depression from shared facebook
data. In Proceedings of the 17th ACM Conference
on Computer Supported Cooperative Work & Social
Computing, CSCW ’14, pages 626–638, Baltimore,
MD.

Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013. Predicting depres-
sion via social media. In Proceedings of the 7th In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM), volume 13, pages 1–10, Boston,
MA.

Dina Demner-Fushman, Wendy W Chapman, and
Clement J McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of Biomedical Informatics, 42(5):760–772.

Joshua C. Denny, Anderson Spickard, III, Kevin B.
Johnson, Neeraja B. Peterson, Josh F. Peterson, and
Randolph A. Miller. 2009. Evaluation of a method
to identify and categorize section headers in clinical
documents. Journal of the American Medical Infor-
matics Association, 16(6):806–815.

eMedicineHealth. 2018. Medications and drugs listing.

https://www.emedicinehealth.com/
medications-drugs/article_em.htm.
(Accessed on Feb 18, 2018).

Shai Fine, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. Machine Learning, 32(1):41–62.

Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics (ACL), volume 1, pages
562–569, Sapporo, Japan.

109Karen Goldﬁnger and Andrew M Pomerantz. 2013.
Psychological Assessment and Report Writing.
Sage, Thousand Oaks, CA.

R Reeves and R Rosner. 2016. Forensic Psychiatry and
Forensic Psychology: Forensic Psychiatric Assess-
ment. Elsevier.

Jeffrey C Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.

Martin Riedl and Chris Biemann. 2012. Topictiling: A
In Pro-
text segmentation algorithm based on lda.
ceedings of ACL 2012 Student Research Workshop,
ACL ’12, pages 37–42, Jeju Island, Korea.

Adam Sadilek, Christopher Homan, Walter S Lasecki,
Vincent Silenzio, and Henry Kautz. 2013. Model-
ing ﬁne-grained dynamics of mood at scale. WSDM,
Rome, Italy, pages 3–6.

M. Sherman and Yang Liu. 2008. Using hidden
markov models for topic segmentation of meeting
transcripts. In Proceedings of the 2008 IEEE Spoken
Language Technology Workshop, pages 185–188.

Carlo Strapparava and Rada Mihalcea. 2008. Learn-
In Proceed-
ing to identify emotions in text.
ings of the ACM Symposium on Applied Comput-
ing (SAC), SAC ’08, pages 1556–1560, Fortaleza,
Ceara, Brazil.

Simone Teufel. 1999. Argumentative zoning: Infor-
mation Extraction from Scientiﬁc Text. Ph.D. thesis,
University of Edinburgh, Edinburgh, Scotland, UK.

Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
In Proceedings of the
tation in research articles.
ninth conference on European chapter of the Associ-
ation for Computational Linguistics (EACL), pages
110–117, Bergen, Norway.

J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van
Mulbregt. 1998. A hidden markov model approach
In Pro-
to text segmentation and event tracking.
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
volume 1, pages 333–336 vol.1.

Jia Yu, Xiong Xiao, Lei Xie, Chng Eng Siong, and
Haizhou Li. 2016. A dnn-hmm approach to story
In INTERSPEECH, San Francisco,
segmentation.
USA.

Gary Groth-Marnat. 2009. Handbook of Psychological

Assessment. John Wiley & Sons, Hoboken, NJ.

Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’94, pages 9–16, Las Cruces, NM.

Christopher Homan, Ravdeep Johar, Tong Liu, Megan
Lytle, Vincent Silenzio, and Cecilia Ovesdotter Alm.
2014. Toward macro-insights for suicide preven-
tion: Analyzing ﬁne-grained distress at scale.
In
Proceedings of the Workshop on Computational Lin-
guistics and Clinical Psychology: From Linguistic
Signal to Clinical Reality, pages 107–117.

George Hripcsak, Suzanne Bakken, Peter D Stetson,
and Vimla L Patel. 2003. Mining complex clini-
cal data for patient safety research: A framework for
event discovery. Journal of Biomedical Informatics,
36(1-2):120–130.

Kush Jain, Priya Khatri, and Garima Indolia. 2015.
Chunked n-grams for sentence validation. Procedia
Computer Science, 57:209–213.

Ying Li, Sharon Lipsky Gorman, and No´emie Elhadad.
2010. Section classiﬁcation in clinical notes using
supervised hidden markov model. In Proceedings of
the 1st ACM International Health Informatics Sym-
posium IHI, pages 744–750, Arlington, VA.

S. Liu, Wei Ma, R. Moore, V. Ganesan, and S. Nel-
son. 2005. Rxnorm: Prescription for electronic drug
information exchange. IT Professional, 7(5):17–23.

Paul van Mulbregt, Ira Carp, Lawrence Gillick, Steve
Lowe, and Jon Yamron. 1998. Text segmentation
and topic tracking on broadcast news via a hidden
markov model approach. In Fifth International Con-
ference on Spoken Language Processing.

Bridianne O’Dea, Stephen Wan, Philip J. Batterham,
Alison L. Calear, Cecile Paris, and Helen Chris-
In-
tensen. 2015. Detecting suicidality on twitter.
ternet Interventions, 2:183–188.

John P. Pestian, Pawel Matykiewicz, Michelle Linn-
Gust, Brett South, Ozlem Uzuner, Jan Wiebe,
K. Bretonnel Cohen, John Hurdle, and Christopher
Brew. 2012. Sentiment analysis of suicide notes:
A shared task. Biomedical Informatics Insights,
5s1:BII.S9042.

Lev Pevzner and Marti A Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19–
36.

L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257–286.

110
Investigating Domain-Speciﬁc Information for

Neural Coreference Resolution on Biomedical Texts

Hai-Long Trieu1, Nhung T. H. Nguyen2, Makoto Miwa1,3 and Sophia Ananiadou2

1Artiﬁcial Intelligence Research Center (AIRC),

National Institute of Advanced Industrial Science and Technology (AIST), Japan
2National Centre for Text Mining, University of Manchester, United Kingdom

3Toyota Technological Institute, Japan

long.trieu@aist.go.jp, makoto-miwa@toyota-ti.ac.jp
{nhung.nguyen, Sophia.Ananiadou}@manchester.ac.uk

Abstract

Existing biomedical coreference resolu-
tion systems depend on features and/or
rules based on syntactic parsers. In this pa-
per, we investigate the utility of the state-
of-the-art general domain neural coref-
erence resolution system on biomedical
texts. The system is an end-to-end sys-
tem without depending on any syntactic
parsers. We also investigate the domain
speciﬁc features to enhance the system for
biomedical texts. Experimental results on
the BioNLP Protein Coreference dataset
and the CRAFT corpus show that, with
no parser information, the adapted sys-
tem compared favorably with the systems
that depend on parser information on these
datasets, achieving 51.23% on the BioNLP
dataset and 36.33% on the CRAFT corpus
in F1 score.
In-domain embeddings and
domain-speciﬁc features helped improve
the performance on the BioNLP dataset,
but they did not on the CRAFT corpus.

Introduction

1
Deep neural systems have recently achieved the
state-of-the-art performance on coreference reso-
lution tasks in the general domain (Clark and Man-
ning, 2016; Wiseman et al., 2016; Lee et al., 2017).
These systems do not heavily rely on manual fea-
tures since the networks automatically build ad-
vanced features from the input. Such an attribute
has made deep neural systems preferable to tradi-
tional manual feature-based systems.

In the biomedical domain, coreference informa-
tion has been shown to enhance the performance
of entity and event extraction (Miwa et al., 2012;
Choi et al., 2016a). Most of work in this domain
use rule-based or hybrid approaches (Nguyen

et al., 2011, 2012; Miwa et al., 2012; D’Souza
and Ng, 2012; Li et al., 2014; Choi et al., 2016b;
Cohen et al., 2017). These systems rely on syn-
tactic parsers to extract hand-crafted features and
rules, e.g., rules based on predicate argument
structure (Nguyen et al., 2012; Miwa et al., 2012)
or features based on syntax trees (D’Souza and
Ng, 2012). These rules are designed speciﬁ-
cally for each type of coreference, such as noun
phrases, relative pronouns, and non-relative pro-
nouns. Moreover, several rules are restricted to
speciﬁc entities of the training corpus, e.g., pro-
tein entities for the BioNLP Protein Coreference
dataset (Nguyen et al., 2011).1

Given the fact that deep learning methods can
produce the state-of-the-art performance on gen-
eral texts, we are motivated to apply such methods
to biomedical texts. We therefore raise three re-
search questions in this paper:
• How does a general domain neural sys-
tem with no parser information perform on
biomedical domain?
• How we can incorporate domain-speciﬁc in-
• Which performance range the system is in

formation into the neural system?

comparison with existing systems?

In order to address these questions, we directly ap-
ply the end-to-end neural coreference resolution
system by Lee et al. (2017) (Lee2017) to biomed-
ical texts. We then investigate domain speciﬁc
features such as domain-speciﬁc word embed-
dings, grammatical number agreements between
mentions, i.e., mentions are singular or plural,
and agreements of MetaMap (Aronson and Lang,
2010) entity tags of mentions. These features do
not rely on any syntactic parsers. Moreover, these
features are also general for any biomedical cor-
pora and not restricted to the corpora we use.

1http://2011.bionlp-st.org/home/

protein-gene-coreference-task

ProceedingsoftheBioNLP2018workshop,pages183–188Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics183We evaluated the Lee2017 system on two
datasets:
the BioNLP Protein Coreference
dataset (Nguyen et al., 2011) and CRAFT (Cohen
et al., 2017). Our experimental results have re-
vealed that the system could achieve reasonable
performance on both corpora. The system out-
performed several systems on the BioNLP dataset
that employed rule-based (Choi et al., 2016b) and
conventional machine learning methods (Nguyen
et al., 2011) using parser information, although
it was not competitive with the state-of-the-art
systems.
Integrating in-domain embeddings and
domain-speciﬁc features into the deep neural sys-
tem improved the performance of both mention
detection and mention linking on the BioNLP
dataset, but the integration could not enhance the
performance on the CRAFT corpus.

2 Methods
In this section, we brieﬂy introduce the baseline
Lee2017 system (Lee et al., 2017) and present
domain-speciﬁc features to adapt the system to
biomedical texts.

2.1 Baseline System
The baseline Lee2017 system treats all spans up to
the maximum length as mention candidates. Each
mention candidate is represented as a concate-
nated vector of the ﬁrst word, the last word, the
soft head word, and the span length embeddings.
The embeddings for the ﬁrst and last words are
calculated from the outputs of LSTMs (Hochre-
iter and Schmidhuber, 1997), while those for soft
head word are calculated from the weighted sum
of the embeddings of words in the span using
an attention mechanism (Bahdanau et al., 2014).
These candidates are ranked based on their men-
tion scores sm calculated as follows:
sm(i) = wm · FFNNm(gi),

(1)

where wm is a weight vector, FFNN denotes a
feed-forward neural network, and gi is the vector
representation of a mention i.

After mentions are decided, the system resolves
coreference by linking mentions back to their an-
tecedent using antecedent scores sa calculated as:
sa(i, j) = wa·FFNNa([gi, gj, gi◦gj, φ(i, j)]),
(2)
where ◦ denotes an element-wise multiplication
and φ(i, j) represents the feature vector between
the two mentions.

2.2 Domain-speciﬁc features
We incorporate the following domain-speciﬁc fea-
tures to enhance the baseline system.
In-domain word embeddings: The input word
embeddings play an important role in deep learn-
ing. Instead of using embeddings trained on gen-
eral domains, e.g., word embeddings provided
with the word2vec tool (Mikolov et al., 2013), we
use 200-dimensional embeddings trained on the
whole PubMed and PubMed Central Open Access
subset (PMC) with a window size of 2 (Chiu et al.,
2016).
Grammatical numbers: We check mentions’
grammatical numbers, i.e., whether each mention
is singular or plural. A mention is singular if its
part-of-speech tag is N N or if it is one of the ﬁve
singular pronouns: it, its, itself, this, and that. A
mention is plural if its part-of-speech tag is N N S
or if it is one of the seven plural pronouns: they,
their, theirs, them, themselves, these, and those.
MetaMap entity tags: We employ MetaMapLite2
to identify all possible entities according to
the UMLS semantic types.3
In cases that
MetaMapLite assigns multiple semantic types for
each entity, we take into account all of the types.
The grammatical numbers and MetaMap entity
tags are incorporated into the network as follows.
We ﬁrstly pre-processed the input and assigned
token-based values for each type of features. For
example, a token may have “singular”, “plural”, or
“unknown” as the number attribute. Meanwhile,
the MetaMap entity tags are distributed to each to-
ken with their position information chosen from
“Begin” and “Inside”. These features are ﬁnally
encoded as a binary vector of φ(i, j) in Equa-
tion 2 that shows whether two mentions i and j
has the number agreement and whether they share
the same MetaMap semantic type.

3 Experiments
3.1 Data
We employed two biomedical corpora: BioNLP
Protein Coreference dataset (Nguyen et al., 2011)
and CRAFT (Cohen et al., 2017). The BioNLP
dataset consists of 1,210 PubMed abstracts se-
lected from the GENIA-MedCo coreference cor-
pus. CRAFT (Cohen et al., 2017) provides coref-

2https://metamap.nlm.nih.gov/

MetaMapLite.shtml

3https://metamap.nlm.nih.gov/Docs/

SemanticTypes_2013AA.txt

184Training set (docs)
Development set (docs)
Test set (docs)
Avg. sent. per doc
Avg. words per doc
Vocabulary size

BioNLP
800
150
260
9.15
258.00
15,900

CRAFT
54
6
7
274.75
8,060.85
27,405

Table 1: Characteristics of BioNLP and CRAFT.

erence annotations of 67 full papers extracted from
PMC. While BioNLP focusses on protein/gene
coreference, CRAFT covers a wider range of
coreference relations such as events, pronomi-
nal anaphora, noun phrases, verbs, and nominal
premodiﬁers corefernce.
In the CRAFT corpus,
coreference is divided into two types:
identity
chains (a set of base noun phrases and/or appos-
itives that refer to the same thing in the world) and
appositive relations (two noun phrases that are ad-
jacent and not linked by a copula). We use only
the identity chains.

The BioNLP dataset was ofﬁcially divided into
training, development, and test sets. Regarding
CRAFT, we randomly divided it into three subsets
in a ratio of 8:1:1 for training, development, and
test, respectively. Detailed characteristics of the
two corpora as well as these three sets are reported
in Table 1. It is noticeable that CRAFT is a corpus
of full papers, which makes it more challenging
for text mining tools than the BioNLP dataset—a
corpus of abstracts (Cohen et al., 2010).

3.2 Settings
We ﬁrst directly applied the Lee2017 system to
the corpora. Lee2017 used two pretrained embed-
dings in general domains provided by Pennington
et al. (2014) and Turian et al. (2010), and all de-
fault features such as speaker, genre, and distance.
To train the Lee2017 system, we employed the
same hyper-parameters as reported in Lee et al.
(2017) except for a threshold ratio. Although
Lee2017 used the ratio λ = 0.4 to reduce the
number of mentions from the list of candidates,
we tuned it on the BioNLP development set and
used λ = 0.7.

We then investigate the impact of each feature
on the biomedical texts by preparing the following
four systems:
• Lee2017:

general embeddings,

speaker,

genre, and distance features

Prec.
BioNLP
81.15
Lee2017
81.01
PubMed
79.23
PubMed-SG
PubMed+MM
80.41
81.91
PubMed+Num
PubMed+MM+Num 81.04
Prec.
CRAFT
Lee2017
70.76
70.93
PubMed
71.98
PubMed-SG
PubMed+MM
71.11
72.79
PubMed+Num
PubMed+MM+Num 71.60

Rec.
63.81
66.12
65.73
67.17
66.31
66.69
Rec.
48.71
46.90
50.24
47.91
42.55
45.00

F1 (%)
71.44
72.81
71.85
73.20
73.29
73.17
F1 (%)
57.70
56.46
59.18
57.25
53.70
55.27

Table 2: Results of mention detection on the de-
velopment set of BioNLP and CRAFT. The high-
est numbers are shown in bold.

tures as Lee2017

• PubMed: biomedical embeddings, same fea-
• PubMed-SG: PubMed with no speaker and
• PubMed+*: PubMed with the MetaMap fea-
ture (MM) and/or the grammatical number
feature (Num).

genre features

For evaluation, we calculated precision, recall,
and F1 on MUC, B3, and CEAFφ4 using the
CoNLL scorer (Pradhan et al., 2014). For the
BioNLP dataset, we also employed the scorer pro-
vided by the shared task organisers to make fair
comparisons with previous work. We reported the
performance on two sub-tasks: (1) mention detec-
tion, i.e., to identify coreferent mentions, such as
named entities, prepositions or noun phrases, and
(2) mention linking, i.e., to link these mentions if
they refer to the same thing. The result of the ﬁrst
task affects that of the second one.

3.3 Results
Results on the development sets of the two corpora
are presented in Table 2 for mention detection and
Table 3 for mention linking (see Appendix A for
detailed scores in different metrics).
Regarding the BioNLP dataset,

the Lee2017
system performed reasonably well even when it
did not use any domain-speciﬁc features. Re-
placing general embeddings by the biomedical
ones improved F1 score in general (Lee2017 v.s.
PubMed). Removing speaker and genre fea-
tures (-SG) did not help enhance the performance.

185System
Lee2017
PubMed
PubMed-SG
PubMed+MM
PubMed+Num
PubMed+MM+Num

BioNLP CRAFT
61.25
33.85
33.92
62.51
34.85
61.47
63.41
33.91
31.28
63.16
63.12
32.77

Table 3: Average F1 scores (%) of mention linking
on the development set of BioNLP and CRAFT.

Adding MetaMap’s tags (+MM) or the number
feature (+Num) produced slightly better scores in
comparison to PubMed. However, combining the
two features at the same time was not as effective
as expected. Among the proposed features, the
agreement on MetaMap entity tags (+MM) was
the strongest one on the BioNLP dataset.

The impact of the features was quite different on
the CRAFT corpus. As shown in Table 2, intro-
ducing biomedical embeddings (PubMed) show
slightly worse F1 score on mention detection than
Lee2017 but it also show a slight improvement
on mention linking. Removing speaker and genre
features (-SG) boosted the performance. How-
ever, adding domain-speciﬁc features all harmed
the performance. As a result, PubMed-SG showed
the best score on the CRAFT development set.

Results in Tables 2 and 3 justify the fact that
the CRAFT corpus is more challenging than the
BioNLP dataset. The scores of the experimented
systems on the CRAFT corpus were always lower
than those on the BioNLP dataset. This is reason-
able because (1) CRAFT consists of full papers
that are signiﬁcantly longer than abstracts, (2) it
covers a wide range of anaphors, and (3) its iden-
tity chains can be arbitrarily long.

We applied the best performing system on each
development set, i.e., PubMed+MM for BioNLP
and PubMed-SG for CRAFT, to its test set, and
reported the results in Tables 4 and 5 with show-
ing the performance in previous work for compar-
ison. Table 4 reveals that the neural system out-
performed ﬁve systems that used SVM and rule-
based approaches including the best system on
the shared task, and the system could compete
with Nguyen et al. (2012)’s. Meanwhile, on the
CRAFT corpus (Table 5), we could only produce
better performance than the general state-of-the-
art system, especially due to the low precision.

System
TEES (BioNLP ST)
ConcordU (BioNLP ST)
UZurich (BioNLP ST)
UUtah (BioNLP ST)
Choi et al. (2016b)
PubMed+MM
Nguyen et al. (2012)
Miwa et al. (2012)
D’Souza and Ng (2012)

Prec Rec
67.2
14.4
19.4
63.2
21.5
55.5
73.3
22.2
50.0
46.3
47.5
55.6
52.5
50.2
62.7
50.4
67.2
55.6

F1 (%)
23.8
29.7
31.0
34.1
48.1
51.2
51.3
55.9
60.9

Table 4: Results of mention linking on the test set
of the BioNLP dataset. The F-scores are in as-
cending order.

System
General state-of-the-art
Rule-based
Union of the two output
PubMed-SG

Prec. Rec.
0.93
0.08
0.29
0.78
0.35
0.78
0.44
0.31

F1
0.14
0.42
0.46
0.36

Table 5: B3 scores of mention linking on the
CRAFT test set in comparison with the three sys-
tems by Cohen et al. (2017). This is not a fair
comparison as our system only addressed identity
chains and the test set is different from theirs.

4 Conclusion
We have applied a neural coreference system to
biomedical texts and incorporated domain-speciﬁc
features to enhance the performance. Experimen-
tal results on two biomedical corpora, the BioNLP
dataset and the CRAFT corpus, have shown that
(1) the neural system performed reasonably well
with no parser information, (2) the in-domain
embeddings and domain-speciﬁc features did not
consistently perform well on the two corpora, and
(3) the system could attain better performance
than several rule-based and traditional machine
learning-based systems on the BioNLP dataset.

As future work, we would like to investigate
feature representations to make input features use-
ful to a target domain. We will also incorporate
rules in the existing systems into the network.

Acknowledgments
This research has been carried out with fund-
ing from AIRC/AIST and results obtained from
a project commissioned by the New Energy and
Industrial Technology Development Organization
(NEDO).

186Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient Estimation of Word Represen-
tations in Vector Space. CoRR, abs/1301.3781.

Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012.
Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759–1765.

N. L. T. Nguyen, J.-D. Kim, and J. Tsujii. 2011.
Overview of bionlp 2011 protein coreference shared
In Proceedings of the BioNLP Shared Task
task.
2011 Workshop, pages 74–82, Portland, Oregon,
USA. Association for Computational Linguistics.

Ngan Nguyen, Jin-Dong Kim, Makoto Miwa, Takuya
Matsuzaki, and Junichi Tsujii. 2012. Improving pro-
tein coreference resolution by simple semantic clas-
siﬁcation. BMC Bioinformatics, 13(1):304.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global Vectors for Word
In EMNLP, volume 14, pages
Representation.
1532–1543.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring Coreference Partitions of Predicted Men-
In Proceed-
tions: A Reference Implementation.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 30–35. Association for Computational
Linguistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ’10, pages 384–
394.

Sam Wiseman, Alexander M. Rush, and Stuart M.
Shieber. 2016. Learning global features for coref-
erence resolution. CoRR, abs/1604.03035.

A Detailed results
We report detailed results of mention linking on
the development set of the two corpora in Table 6
and Table 7. Due to the long running time of the
scorer, we were not able to report CEAFφ4 scores
for CRAFT.

References
Alan R Aronson and Franc¸ois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association, 17(3):229–236.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Billy Chiu, Gamal K. O. Crichton, Anna Korhonen,
and Sampo Pyysalo. 2016. How to Train good Word
Embeddings for Biomedical NLP. In Proceedings of
the 15th Workshop on Biomedical Natural Language
Processing, pages 166–174.

Miji Choi, Haibin Liu, William Baumgartner, Justin
Zobel, and Karin Verspoor. 2016a. Coreference
resolution improves extraction of biological expres-
sion language statements from texts. Database,
2016:baw076.

Miji Choi, Justin Zobel, and Karin Verspoor. 2016b. A
categorical analysis of coreference resolution errors
in biomedical texts. Journal of biomedical informat-
ics, 60:309318.

Kevin Clark and Christopher D. Manning. 2016.
resolution by learning
CoRR,

Improving coreference
entity-level distributed representations.
abs/1606.01323.

K. Bretonnel Cohen, Helen L. Johnson, Karin Ver-
spoor, Christophe Roeder, and Lawrence E. Hunter.
2010. The structural and content aspects of abstracts
versus bodies of full text journal articles are differ-
ent. BMC Bioinformatics, 11(1):492.

K. Bretonnel Cohen, Arrick Lanfranchi, Miji Joo-
young Choi, Michael Bada, William A. Baumgart-
ner, Natalya Panteleyeva, Karin Verspoor, Martha
Palmer, and Lawrence E. Hunter. 2017. Coreference
annotation and resolution in the colorado richly an-
notated full text (craft) corpus of biomedical journal
articles. BMC Bioinformatics, 18(1):372.

Jennifer D’Souza and Vincent Ng. 2012. Anaphora
resolution in biomedical literature: a hybrid ap-
proach. In BCB, pages 113–122. ACM.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
In Proceedings of the 2017 Conference on
lution.
Empirical Methods in Natural Language Process-
ing, pages 188–197, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Lishuang Li, Liuke Jin, Zhenchao Jiang, Jing Zhang,
and Degen Huang. 2014. Coreference resolution
In BIBM, pages 12–14. IEEE
in biomedical texts.
Computer Society.

187System
Prec.
65.31
Baseline
65.44
PubMed
63.02
PubMed-SG
66.17
PubMed+MM
PubMed+Num
66.81
PubMed+MM+Num 65.73

MUC
Rec.
45.03
47.04
46.58
48.30
47.83
47.37

F1
53.30
54.74
53.57
55.84
55.75
55.06

Prec.
71.50
71.12
68.72
71.62
72.27
71.68

B3
Rec.
50.27
51.85
51.72
52.95
52.23
52.66

F1
59.03
59.98
59.02
60.89
60.64
60.72

Prec.
77.06
77.25
76.11
76.70
78.15
77.53

CEAFφ4

Rec.
66.54
68.85
68.01
70.53
68.63
70.04

F1
71.41
72.81
71.83
73.49
73.08
73.59

Avg.
F1 (%)
61.25
62.51
61.47
63.41
63.16
63.12

Table 6: Results of mention linking on the BioNLP development set.

Prec.
System
45.46
Baseline
47.36
PubMed
PubMed-SG
46.04
46.22
PubMed+MM
PubMed+Num
47.31
PubMed+MM+Num 46.85

MUC
Rec.
27.17
27.34
28.49
27.37
23.40
25.41

F1
34.02
34.67
35.20
34.38
31.31
32.95

Prec.
44.29
44.89
43.33
43.70
48.70
45.78

B3
Rec.
27.17
26.30
28.67
27.09
23.01
25.30

F1
33.68
33.17
34.50
33.44
31.25
32.59

Avg. F1 (%)

33.85
33.92
34.85
33.91
31.28
32.77

Table 7: Results of mention linking on the CRAFT development set.

188
Comparison of Representations of Named Entities for Multi-label

Document Classiﬁcation with Convolutional Neural Networks

Lidia Pivovarova

and Roman Yangarber

University of Helsinki, Finland
Department of Computer Science

first.last@cs.helsinki.fi

Abstract

in the context of

We explore representations for multi-
token names
the
Reuters topic and sector classiﬁcation
tasks (RCV1). We ﬁnd that: the best way
to treat names is to split them into tokens
and use each token as a separate feature;
NEs have more impact on sector classiﬁca-
tion than on topic classiﬁcation; replacing
all NEs with special entity-type tokens is
not an effective strategy; representing to-
kens by different embeddings for proper
names vs. common nouns does not im-
prove results. We highlight the improve-
ments over state-of-the-art results that our
CNN models yield.

Introduction

1
This paper addresses large-scale multi-class text
classiﬁcation tasks: categorizing articles in the
Reuters news corpus (RCV1) according to topic
and to industry sectors. A topic is a broad news
category, e.g., “Economics,” “Sport,” “Health.”
A sector deﬁnes a narrower business area, e.g.,
“Banking,” “Telecommunications,” “Insurance.”

We use convolutional neural networks (CNNs),
which take word embeddings as input. Typically
word embeddings are built by treating a corpus as
a sequence of tokens, where named entities (NEs)
receive no special treatment. Yet NEs may be im-
portant features in some classiﬁcation tasks: com-
panies, e.g., are often linked to particular industry
sectors, and certain industries are linked to loca-
tions. Thus company and location names may be
important features for sector classiﬁcation.

RCV1 is much smaller than corpora typically
used to build word embeddings. Thus we utilize
external resources—a corpus of approximately 10
million business news articles, collected using the

PULS news monitoring system (Pivovarova et al.,
2013). While nominally RCV1 contains gen-
eral news, it is skewed toward business; many of
the topic labels are business-related (“Markets”,
“Commodities”, “Share Capital,” etc.). Thus, we
expect our business corpus to help in learning fea-
tures for the Reuters classiﬁcation tasks.

We compare several NE representation to ﬁnd
the most suitable name features for each task. We
use the PULS NER system (Grishman et al., 2003;
Huttunen et al., 2002a,b) to ﬁnd NEs and their
types—company, location, person, etc. We com-
pare various representations of NEs, by building
embeddings, and training CNNs to ﬁnd the best
representation. We also compare building embed-
dings on the RCV1 corpus vs. using much larger
external corpora.

2 Data and Prior Work

RCV1 (Lewis et al., 2004) is a corpus of about
800K Reuters articles from 1996–1997 with man-
ually assigned sector and topic labels. Both clas-
siﬁcations are multi-label—each document may
have zero or more labels. While all documents
have topic labels, only 350K have sector labels.

While RCV1 appears frequently in published
research, few authors tackle the full-scale classi-
ﬁcation problem. Typically they use subsets of the
data: (Daniely et al., 2017; Duchi et al., 2011) use
only the four most general topic labels;
(Dredze
et al., 2008) use 6 sector categories to explore bi-
nary classiﬁcation,
(Daniels and Metaxas, 2017)
use a subset of 6K articles. Even when the entire
dataset is used, the training-text split varies across
papers, because the “original” split (Lewis et al.,
2004) is impractical for most purposes: 23K in-
stances for training, and 780K for testing.

Another problem that complicates comparison
is the lack of consistency in evaluation metrics

Proceedingsofthe3rdWorkshoponRepresentationLearningforNLP,pages64–68Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics64Figure 1: Architecture of the convolutional neural networks

used to evaluate classiﬁer performance. The most
common measures for multi-class classiﬁcation
are macro- and micro-averaged F-measure, which
we use in this paper. However, others use other
metrics. For example, (Liu et al., 2017) use pre-
cision and cumulative gain at top K—measures
adopted from information retrieval. This is not
comparable with other work, because these met-
rics are used not only to report results, but also
to optimize the algorithms during training. The
notion of the best classiﬁer differs depending on
which evaluation measure is used. Thus, although
RCV1 is frequently used, we ﬁnd few papers di-
rectly comparable to our research, in the sense that
they use the entire RCV1 dataset and report micro-
and macro-averaged F-measure.

To the best of our knowledge, our previous
work (Du et al., 2015) was the only study of the
utility of NEs for RCV1 classiﬁcation. We demon-
strated that using a combination of keyword-based
and NE-based classiﬁers works better than either
classiﬁer alone. In that paper we applied a rule-
based approach for NEs, and did not use NEs as
features for machine learning.

3 Model

The architecture of our CNN is shown in Figure 1.
The inputs are fed into the network as zero-padded
text fragments of ﬁxed size, with each word repre-
sented by a ﬁxed-dimensional embedding vector.
The inputs are fed into a layer of convolutional ﬁl-
ters with multiple widths, optionally followed by
deeper convolutional layers. The results of the last
convolutional layer are max-pooled, producing a

vector with one scalar per ﬁlter. This is fed into a
fully-connected layer with dropout regularization,
with one sigmoid node in the output layer for each
of the class labels. For each class label, a cross-
entropy loss is computed. Losses are averaged
across labels, and the gradient of the loss is back-
propagated to update the weights. This is similar
to the model (Kim, 2014) used for sentiment anal-
ysis. The key differences are that our model uses
an arbitrary number of convolutions, and that we
use sigmoid rather than softmax on output, since
the labels are not mutually exclusive.

To train the model we used a random split: 80%
of the data used for training, 10% development
set, and 10% test set. The development set is used
to determine when to stop training, and to tune a
set of optimal thresholds {θi} for each label i—if
the output probability pi is higher than θi, the la-
bel is assigned to the instance, otherwise it is not.
To ﬁnd the optimal threshold, we optimize the F-
measure for each label. The test set is used to ob-
tain the ﬁnal, reported performance scores.

Our focus is this paper is data representation,
thus we defer the tuning of hyper-parameters for
future work. All experiments use the same net-
work structure: 3 convolution layers with ﬁlter
sizes {3,7,11}, {3,7,11}, and {3,11}, with 512,
256 and 256 ﬁlters of each size, respectively. The
runs differ only in the input embeddings they use.

4 Data Representation

We train the embeddings using GloVe (Pennington
et al., 2014). As features we use lower-cased lem-
mas of all words. The rationale for this is that our

65  A US appeals Court  revived a civil suit accusing Apple of creating a monopoly text representationwith word embeddingsfirst convolutional layerwith multiple filter widthsfeature mapsfeature mapssecond convolutional layerwith multiple filter widthsmax-poolingfully-connected layerwith dropout and sigmoid outputcorpora are relatively small, so the data are sparse
and not sufﬁcient to build embeddings from sur-
face forms. We tune the embeddings while train-
ing the CNN, updating them at each iteration.

our NER system:

We explore several name representations, using
• type: each entity is represented by a spe-
cial token denoting its type—C-company, C-
person, C-location, etc, and C-name if the
type is not determined. The model learns one
embedding for each of these tokens.
• name: each name gets its own embedding;
multi-word names treated as a single token.
• split-name: multi-word names are split into
tokens, and each token has its own embed-
ding; the motivation is that some company
names may contain informative parts—e.g.,
Air Baltic, Delta Airlines—which may indi-
cate that these companies operate in the same
ﬁeld; these name parts may be more useful
than the name as a whole.
• split-name+common: similar to the above,
but tokens inside names and in common con-
text are distinguished; the motivation is that
some words may be used in names with-
out any relation to the company’s line of
business—e.g., Apple, Blackberry—and their
usage inside names should not be mixed with
their usage as common nouns.

In the experiments, we build GloVe embeddings
from two corpora: RCV1 only, and RCV1 plus
our external corpus. For comparison, we also use
200-dimensional embeddings trained on a 6 bil-
lion general corpus (glove-6B), provided by the
GloVe project.0 This corresponds to our split-
name representation mode.

To illustrate the effect of the different token
representations, Table 1 shows ten words near-
est to the sample lemmas: apple and airline.
When name representation is used,
the token
apple is ambiguous,
its nearest neighbors are
both fruit words (pear) and computer words (ap-
ple computer). In type representation, the “com-
puter” meaning disappears, since all mentions of
Apple as company are represented by the special
token C-company. When using glove-6B, the fruit
meaning is absent, and all neighbors are computer-
related words. The token airline does not exhibit
such ambiguity, and all representations produce
similar nearest neighbors.

0https://nlp.stanford.edu/projects/glove/

name

pear
unpasteurized
juice
apple computer
odwalla
strawberry
fruit
macintosh
meat
pear board

name

carrier
ﬂight
british airways
american airlines
air france
passenger
lufthansa
air
united airlines
aircraft

apple

type

pear
unpasteurized
juice
fruit
salmonella
peach
taint
orange
crate
strawberry
airline

type

carrier
ﬂight
passenger
aircraft
airport
air
pilot
route
plane
aviation

glove-6B
iphone
microsoft
intel
macintosh
ipod
ibm
ipad
software
google
itunes

glove-6B
airlines
airways
lufthansa
carrier
ﬂights
ﬂight
pilots
qantas
alitalia
klm

split-name+common

apple

pear
juice
unpasteurized
odwalla NE
fruit
anthrax
salmonella
rotten
unpasteurised
strawberry

apple NE
computer NE
macintosh NE
amelio NE
operating-system
compaq NE
microsoft NE
oracle NE
ibm NE
software
jobs NE

airline

airlines NE
airways NE
carrier
ﬂight
air NE
passenger
lufthansa NE
pilot
aircraft
route

airline NE
malaysian NE
scandinavian NE
airlines NE
system NE
pilots NE
air NE
klm NE
passengers NE
jet NE
tajudin NE

Table 1: Nearest neighbors for sample words using
various word representations.

In the split-name+common representation
mode, each lemma may produce two vectors, one
for a common noun and one for a proper noun
(inside a name). As the table shows, apple as a
common noun has a clear “fruit” meaning; the one
company appearing among the neighbors is a juice
producer, Odwalla. The nearest neighbors for ap-
ple NE, in name context, include IT companies.
The tokens airline and airline NE have no clear
semantic distinction, with similar nearest neigh-
bors. In such cases there is no clear advantage in
using two embeddings rather than one.

We test all of the above name representations
experimentally, to determine which is more useful
in the document classiﬁcation tasks.

5 Results and Discussion

Experimental results are presented in Tables 2
and 3. We compare our results with those found
in related work, described in Section 2, focusing
on micro- and macro-averaged F-measure—µ-F1
and M-F1, respectively. The experimental settings
differ in the various papers, which makes precise
comparison difﬁcult. For example, several pre-
vious papers use the “standard split,” (proposed
in (Lewis et al., 2004)), which contains only 23K

66Algorithm (prior)
SVM (Lewis et al., 2004)
SVM (Zhuang et al., 2005)
Naive Bayes (Puurula, 2012)
Bloom Filters (Cisse et al., 2013)
SVM + NEs (Du et al., 2015)

M-F1
29.7
30.1

µ-F1
51.3
52.0
— 70.5
72.4
63.8

47.8
57.7

RCV1 embeddings

CNN type
CNN name
CNN split-name
CNN split-name+common

CNN type
CNN name
CNN split-name
CNN split-name+common
CNN split-name (Glove-6B)

RCV1 + external corpus

32.2
61.0
63.6
44.3

47.7
55.2
60.7
38.0
55.7

58.4
80.2
82.0
68.3

72.6
78.4
80.3
66.0
78.4

Table 2: Sector classiﬁcation results on RCV1.

training instances, which is not sufﬁcient for learn-
ing word embeddings.

Compared to the reported state-of-the-art results
on Sector Classiﬁcation (Table 2), our best model
yields a 10% gain in µ-F1, (Cisse et al., 2013), and
a 6% gain in M-F1 (Du et al., 2015). The best µ-F1
and M-F1 results are obtained by the same model.1
On Topic Classiﬁcation (Table 3), our µ-F1 re-
sults show a modest improvement of 0.5% in F-
measure—or a 3.5% (averaged) error reduction—
over state of the art (Johnson and Zhang, 2015).2
As seen in Table 2, the best data representa-
tion for Sector Classiﬁcation, is split-name, where
each token has the same embedding regardless
whether it is used in a proper-name or a common-
noun context. The worst performing name repre-
sentation is type, where names are mapped to spe-
cial “concepts” (C-company, C-person etc.), and
each concept has its own embedding. This in-
dicates the importance of the tokens inside the
named entities for Sector Classiﬁcation, and sup-
ports the notion that company names mentioned in
text correlate with sector labels.

Results for Topic Classiﬁcation are in Table 3.
The best data representation is again split-name,
though the difference between representations is
less pronounced than in the case of Sector Classi-
ﬁcation, and using type does not lead to a signif-
icant drop in model performance. This suggests
that proper names are less important for Topic

1In prior work, state of the art was achieved by different

models.

2Interestingly, the best result for M-F1 on Topics is still
in prior work: i.e., these prior models perform better on very
infrequent topics. This is to be explored in future work.

Algorithm (prior)
SVM (Lewis et al., 2004)
ANN (Nam et al., 2014)
CNN (Johnson and Zhang, 2015)
RCV1 embeddings

M-F1
61.9
69.2
67.1

µ-F1
81.6
85.3
85.7

CNN type
CNN name
CNN split-name
CNN split-name+common

CNN type
CNN name
CNN split-name
CNN split-name+common
CNN split-name (Glove-6B)

RCV1 + external corpus

65.5
66.7
66.5
66.6

64.9
66.4
65.7
65.6
65.8

85.5
86.2
86.2
86.2

85.6
86.2
85.9
85.8
85.8

Table 3: Topic classiﬁcation results on RCV1.

(event) classiﬁcation, and supports the intuition
that entity names (e.g., companies) are less corre-
lated with the types of events in which the entities
participate in business news. However, there may
be correlations between industry sectors and top-
ics/events: e.g., mining or petroleum companies
rarely launch new products. This may explain why
the split-name representation appears to be better
for Topic Classiﬁcation. One possible next step is
to build CNNs that jointly model Topics and Sec-
tors; we plan to explore this in future work.

Surprisingly, using external corpora did not im-
prove the models’ performance, as indicated by
both Sector and Topic results (Tables 2 and 3, re-
spectively). This may mean that the genre and the
time period of the news corpus are more relevant
for building embeddings than the size of the cor-
pora. However, other factors may contribute as
well, e.g., our hyper-parameter combination may
not be optimal for these embeddings. Neverthe-
less, the results follow the same pattern: the best
name representation is split-name and the differ-
ence between representations is more pronounced
for Sector than for Topic classiﬁcation.

In conclusion, our contribution is two-fold. On
one classic large-scale classiﬁcation task, sectors,
our proposed CNNs yield substantial improve-
ments over state-of-the-art; on topics—a modest
improvement in µ-F-measure. Further, to the best
of our knowledge, this is the ﬁrst attempt at a sys-
tematic comparison of NE representation for text
classiﬁcation. More effective ways of representing
NEs should be explored in future work, given their
importance for the classiﬁcation tasks, as demon-
strated by the experiments we present in this paper.

67Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and
Yiming Yang. 2017. Deep learning for extreme
multi-label text classiﬁcation. In Proceedings of the
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 115–124. ACM.

Jinseok Nam, Jungi Kim, Eneldo Loza Menc´ıa, Iryna
Gurevych, and Johannes F¨urnkranz. 2014. Large-
scale multi-label text classiﬁcation—revisiting neu-
In Joint european conference on
ral networks.
machine learning and knowledge discovery in
databases, pages 437–452. Springer.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Lidia Pivovarova, Silja Huttunen, and Roman Yangar-
ber. 2013. Event representation across genre.
In
Proceedins of the 1st Workshop on Events: Deﬁ-
nition, Detection, Coreference, and Representation,
NAACL HLT.

Antti Puurula. 2012. Scalable text classiﬁcation with
sparse generative modeling.
In Patricia Anthony,
Mitsuru Ishizuka, and Dickson Lukose, editors, PRI-
CAI 2012: Trends in Artiﬁcial Intelligence, volume
7458 of Lecture Notes in Computer Science, pages
458–469. Springer Berlin Heidelberg.

Dong Zhuang, Benyu Zhang, Qiang Yang, Jun Yan,
Zheng Chen, and Ying Chen. 2005. Efﬁcient text
classiﬁcation by weighted proximal SVM. In Fifth
IEEE International Conference on Data Mining.

References
Moustapha M. Cisse, Nicolas Usunier, Thierry Arti,
and Patrick Gallinari. 2013. Robust Bloom ﬁlters
for large multilabel classiﬁcation tasks. In Advances
in Neural Information Processing Systems, pages
1851–1859.

Zachary Alan Daniels and Dimitris N Metaxas. 2017.
Addressing imbalance in multi-label classiﬁcation
In AAAI, pages
using structured hellinger forests.
1826–1832.

Amit Daniely, Nevena Lazic, Yoram Singer, and Kunal
Talwar. 2017. Short and deep: Sketching and neural
networks.

Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Conﬁdence-weighted linear classiﬁcation. In
Proceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 264–271, New
York, NY, USA. ACM.

Mian Du, Matthew Pierce, Lidia Pivovarova, and Ro-
Improving supervised clas-
man Yangarber. 2015.
siﬁcation using information extraction. In Interna-
tional Conference on Applications of Natural Lan-
guage to Information Systems, pages 3–18. Springer.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159.

Ralph Grishman, Silja Huttunen, and Roman Yangar-
ber. 2003. Information extraction for enhanced ac-
cess to disease outbreak reports. Journal of Biomed-
ical Informatics, 35(4):236–246.

Silja Huttunen, Roman Yangarber, and Ralph Grish-
man. 2002a. Complexity of event structure in IE
scenarios. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei.

Silja Huttunen, Roman Yangarber, and Ralph Grish-
man. 2002b. Diversity of scenarios in information
extraction. In Proceedings of the Third International
Conference on Language Resources and Evaluation
(LREC 2002), Las Palmas de Gran Canaria, Spain.

Rie Johnson and Tong Zhang. 2015. Semi-supervised
convolutional neural networks for text categoriza-
In Advances in neural
tion via region embedding.
information processing systems, pages 919–927.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361–397.

68
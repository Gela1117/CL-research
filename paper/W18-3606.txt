Generating High-Quality Surface Realizations Using Data Augmentation

and Factored Sequence Models

Henry Elder
ADAPT Centre,

Dublin City University, Ireland

Chris Hokamp

Aylien Ltd.

Dublin, Ireland

henry.elder@adaptcentre.ie

chris@aylien.com

Abstract

This work presents state of the art re-
sults in reconstruction of surface realiza-
tions from obfuscated text. We identify
the lack of sufﬁcient training data as the
major obstacle to training high-performing
models, and solve this issue by gener-
ating large amounts of synthetic training
data. We also propose preprocessing tech-
niques which make the structure contained
in the input features more accessible to se-
quence models. Our models were ranked
ﬁrst on all evaluation metrics in the En-
glish portion of the 2018 Surface Realiza-
tion shared task.

1

Introduction

Contextualized Natural Language Generation
(NLG) is a long-standing goal of Natural Lan-
guage Processing (NLP) research. The task of
generating text, conditioned on knowledge about
the world, is applicable to almost any domain.
However, despite recent advances on some tasks,
NLG models still produce relatively low quality
outputs in many settings. Representing the context
in a consistent manner is still a challenge: how can
we condition output on a stateful structure such as
a graph or a tree?

Several shared tasks have recently explored
NLG from inputs with graph-like structures; RDF
triples (Colin et al., 2016), dialogue act-based
meaning representations (Novikova et al., 2017)
and abstract meaning representations (May and
Priyadarshi, 2017).
In each of these challenges,
the input has structure beyond simple linear se-
quences; however, to date, the top results in these
tasks have consistently been achieved using rela-
tively standard sequence-to-sequence models.

The surface realization task (Mille et al., 2018)
is a conceptually simple challenge: given shufﬂed
input, where tokens are represented by their lem-
mas, parts of speech, and dependency features, can
we train a model to reconstruct the original text?
A model that performs well at this task is likely
to be a good starting point for solving more com-
plex tasks, such as NLG from Resource Descrip-
tion Framework (RDF) graphs or Abstract Mean-
ing Representation (AMR) structures. In addition,
training data for the surface realization task can
also be generated in a fully-automated manner.

In this work, we show that training dataset
size may be the major obstacle preventing current
sequence-to-sequence models from doing well at
NLG from structured inputs. Although inputting
the structures themselves is theoretically appeal-
ing (Tai et al., 2015), for some tasks it may be
enough to use sequential inputs by ﬂattening struc-
tures, and providing structural information via in-
put factors, as long as the training dataset is sufﬁ-
ciently large. By augmenting training data using a
large corpus of unannotated data, we obtain a new
state of the art in the surface realization task using
off-the-shelf sequence to sequence models.

In addition, we show that information about
the output word order, implicitly available in the
universal dependency ﬁelds, provides essential in-
formation about the word order of correct output
sequences, conﬁrming that structural information
cannot be discarded without a large drop in perfor-
mance.

The main contributions of this work are:

1. We show how training datasets can be aug-

mented with synthetic data

2. We apply preprocessing steps to simplify the
universal dependency structures, making the
structure more explicit

ProceedingsoftheFirstWorkshoponMultilingualSurfaceRealisation,pages49–53Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics493. We evaluate copy attention models for the

4 Features

surface realization task

2 The Surface Realization Shared Task

In the shallow track of the 2018 surface realization
(SR) shared task, inputs consist of tokens from a
universal dependency (UD) tree provided in the
form of lemmas. The original order of the se-
quence is obfuscated by random shufﬂing1.

Models are evaluated on their ability to recon-
struct the original, unshufﬂed input which gener-
ated the features. In order to do this, models must
make use of structural information in order to re-
order the tokens correctly as well as part-of-speech
and/or dependency parse labels in order to restore
the correct surface realization of lemmas. Note
that we focus upon the English sub-task, where
word order is critical because of the typologically
analytic nature of English, however, for other lan-
guages, restoring word order may be less impor-
tant, while deriving surface realizations from lem-
mas may be much more challenging.

3 Datasets

3.1 Augmenting Training with Synthetic

Datasets

To augment the SR training data, we used sen-
tences from the WikiText corpus (Merity et al.,
2016). Each of these sentences was parsed us-
ing UDPipe (Straka and Straková, 2017) to ob-
tain the same features provided by the SR organiz-
ers. We then ﬁltered this data, keeping only sen-
tences with at least 95% vocabulary overlap with
the in-domain SR training data. Note that the in-
put vocabulary for this task is word lemmas, so at
least 95% of the tokens in each instance in our ad-
ditional training data are lemmas which are also
found in the in-domain data. The order of tokens
in each instance of this additional dataset is then
randomly shufﬂed to simulate the random input
order in the SR data.

We thus obtain 642,960 additional training in-
stances, which are added to the 12,375 instances
supplied by the SR shared task organizers.

1The task organizers also introduced a deep task, but since
ours was the only submission to the deep task, we save our
discussion of this task for future work.

4.1 Leveraging Structured Features

Because we have the dependency parse features
for each input, some (noisy) information about
word order is implicitly available from the parse
information; however, discovering the structural
relationship between the dependency parse fea-
tures and the order of words in the output se-
quence is likely to be challenging for our sequence
to sequence model. Therefore, we re-construct
the original parse tree from the dependency fea-
tures, and perform a depth-ﬁrst search to sort and
reorder the lemmas. This is similar to the lin-
earization step performed by Konstas et al. (2017),
the main difference being we randomly choose
between child nodes instead of using a predeter-
mined order based on edge types.

In order to further augment the available con-
text, we experiment with adding potential delem-
matized forms for each input lemma. The possible
forms for each lemma were found by creating a
map from (lemma, xpos) → form, using the
WikiText dataset. For each input lemma and xpos,
we then check for the pair in the map – if it ex-
ists, the corresponding form is appended to the se-
quence. This makes forms available to the model
for copying.

For some (lemma, xpos) pairs there are mul-
tiple potential forms. When this occurs we add all
potential forms to the input sequence. The map-
ping was found to cover 98.9% of cases in the de-
velopment set.

4.2 Factored Inputs

Factored models were introduced by Alexan-
drescu et al. (2006) as a means of including ad-
ditional features beyond word tokens into neural
language models. The key idea is to create a sep-
arate embedding representation for each feature
type, and to concatenate the embeddings for each
input token to create its dense representation. Sen-
nrich et al. (2016) showed that this technique is
quite effective for neural machine translation, and
some recent work, such as Hokamp (2017) has
successfully applied this technique to related se-
quence generation tasks.

The embedding ej for each input token xj with

50FEATURE

DESCRIPTION

lemma
XPOS
position
UPOS
head position

deprel

the lemma of the surface word
the English part-of-speech label
the position in the sequence
the universal part-of-speech label
the position of the head word according to the
dependency parser
the dependency relation label according to the
dependency parser

VOCABULARY
SIZE
30004
53
103
20
100

EMBEDDING
SIZE
300
16
25
8
25

51

15

Table 1: The features used in the factored models, along with the number of possible values the feature
may take, and the respective embedding size.

POSITION
1
2
3
4
5
6
7
8
9

LEMMA
learn
lot
there
be
about
a
.
Chernobyl
to

XPOS
VERB
NOUN
PRON
VERB
ADP
DET
PUNCT
PROPN
PART

UPOS
VB
NN
EX
VBZ
IN
DT
.
NNP
TO

HEAD POSITION
2
4
4
0
8
2
4
1
1

DEPREL
acl
nsubj
expl
root
case
det
punct
obl
mark

Table 2: An example from the training data, containing all features we use as input factors.

factors F is created as in Eq. 1:

|F|(cid:110)

(1)

ej =

Ekxjk

k=1

where(cid:102) indicates vector concatenation, Ek is the

embedding matrix of factor k, and xjk is a one
hot vector for the k-th input factor. Table 1 lists
each of the factors used in our models, along with
its corresponding embedding size. The embedding
size of 300 for the lemma is set in conﬁguration,
while the embedding sizes of the other features
are set heuristically by OpenNMT-py, using the
heuristic |embeddingk| = |Vk|0.7, where |Vk| is
the vocabulary size of feature k. Table 2 gives an
example from the training data with actual instan-
tiations of each of the features.

5 Model
Models were trained using the OpenNMT-py
toolkit (Klein et al., 2017). The model archi-
tecture is a 1 layer bidirectional recurrent neu-
ral network (RNN) with long short-term memory

(LSTM) cells (Hochreiter and Urgen Schmidhu-
ber, 1997) and attention (Luong et al., 2015). The
model has 450 hidden units in the encoder and de-
coder layers, and 300 hidden units in the word
embeddings which are learned jointly across the
whole model. Dropout of 0.3 is applied between
the LSTM stacks. We use a coverage attention
layer (Tu et al., 2016) with lambda value of 1.

The models are trained using stochastic gradi-
ent descent with learning rate 1. A learning rate
decay of 0.5 is applied at each epoch once perplex-
ity does not decrease on the validation set. Models
were trained for 20 epochs. Output was decoded
using beam search with beam size 5. Unknown to-
kens were replaced with the input token that had
the highest attention value at that time step. The
approach of copying input tokens using attention
is commonly known as a pointer network (Vinyals
et al., 2015). Output from the epoch checkpoint
which performed best on the development set was
chosen for test set submission.

The exploration and choice of hyperparameters
was aided by the use of Bayesian hyperparameter

51optimization platform SigOpt2.

3. providing potential forms via the delemmati-

6 Experiments

We experiment with many different combinations
of input features and training data, in order to
understand which elements of the representation
have the largest impact upon performance.

We limit vocabulary size during training to en-
able the network to generalize to unknown tokens
at test time. When using just the SR training data
we train word embeddings for the 15,000 most
frequent tokens from a possible 23,650 unique to-
kens. When using the combined SR training data
and ﬁltered WikiText dataset we use the 30,000
most frequent tokens from a possible 106,367
unique tokens.

We trained on a single Tesla K40 GPU. Training
time was approximately 1 minute per epoch for the
SR data and 1 hour per epoch for the combined SR
data and ﬁltered WikiText.

7 Results

We report results using automated evaluation met-
ric BLEU (Papineni et al., 2002). On the test
set we additionally report the NIST (Przybocki
et al., 2009) score and the normalized edit distance
(DIST).

SYSTEM
SR Baseline
SR + delemma suggestions
SR + delemma suggestions +
linearization
SR + delemma suggestions +
linearization + additional data

BLEU
21.27
23.75
43.11

68.86

Table 3: Ablation study with BLEU scores for dif-
ferent conﬁgurations on the shallow task develop-
ment set

Table 3 presents the results of the surface real-
ization experiments. We observe three main com-
ponents that drastically improve performance over
the baseline model:

1. augmenting the training set with more data

2. reordering the input using the dependency

parse features

2https://sigopt.com/

zation map

Table 4 gives the ofﬁcial SR 2018 results from
task organizers. Our system, which corresponds
to the best conﬁguration from Table 3 was ranked
ﬁrst across all metrics.

TEAM ID BLEU DIST NIST
12.02
1 (Ours)
9.51
2
7.71
3
12.02
4
10.62
5
10.86
6
7
8.86
9.58
8
AVG
10.15

80.42
70.01
47.63
70.22
77.56
79.29
51.87
65.9
67.86

69.14
28.09
8.04
66.33
50.74
55.29
23.2
29.6
41.3

Table 4: Ofﬁcial results of the surface realization
shared task using BLEU, DIST and NIST as eval-
uation metrics.

8 Related Work

The surface realization task bears the closest re-
semblance to the SemEval 2017 shared task AMR-
to-text (May and Priyadarshi, 2017). Our ap-
proach to data augmentation and preprocessing
uses many insights from Neural AMR (Konstas
et al., 2017). Traditional data-to-text systems use
a rule based approach (Reiter and Dale, 2000).

9 Conclusion

The main takeaway from this work is that data
augmentation improves performance on the sur-
face realization task. Although unsurprising, this
result conﬁrms that sufﬁcient data is needed to
achieve reasonable performance, and that ﬂattened
structural information such as dependency parse
features is insufﬁcient without additional prepro-
cessing to reduce the complexity of the input. The
surface realization task is ostensibly quite simple,
thus it is surprising that baseline sequence to se-
quence models, which perform well in other tasks
such as machine translation, cannot solve this task.
We hypothesize that the lemmatization and shuf-
ﬂing of the input does not provide sufﬁcient in-
formation to reconstruct the input. In sequences
longer than a few words, there is likely to be sig-
niﬁcant ambiguity without additional structural in-

52formation such as parse features. However, recon-
structing the original sequence from unprocessed,
ﬂattened parse information alone is unrealistic us-
ing standard encoder-decoder models.

In future work, we plan to explore more chal-
lenging variants of this task, while also experi-
menting with models that do not require feature-
speciﬁc preprocessing to make use of rich struc-
tural information in the input.

References
Andrei Alexandrescu and Katrin Kirchhoff. 2006.
In Proceedings
Factored neural language models.
of the Human Language Technology Conference
of the NAACL, Companion Volume: Short Papers.
Association for Computational Linguistics, Strouds-
burg, PA, USA, NAACL-Short ’06, pages 1–4.
http://dl.acm.org/citation.cfm?id=1614049.1614050.

Emilie Colin, Claire Gardent, M Yassine, and Shashi
Narayan. 2016. The WebNLG Challenge : Generat-
ing Text from DBPedia Data. The 9th International
Conference on Natural Language Generation .

and

Sepp Hochreiter

Schmidhu-
Short-Term Mem-
Neural Computation 9(8):1735–1780.

ber.
ory.
https://doi.org/10.1162/neco.1997.9.8.1735.

J Urgen

1997.

Long

Chris Hokamp. 2017. Ensembling factored neural ma-
chine translation models for automatic post-editing
and quality estimation. In Proceedings of the Sec-
ond Conference on Machine Translation. Associa-
tion for Computational Linguistics, pages 647–654.
http://aclweb.org/anthology/W17-4775.

Guillaume Klein, Yoon Kim, Yuntian Deng,
Josep Crego, Jean Senellart, and Alexander M.
Rush. 2017.
OpenNMT: Open-source Toolkit
for Neural Machine Translation pages 67–72.
https://doi.org/10.18653/v1/P17-4012.

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural AMR:
Sequence-to-Sequence Models for Parsing and Gen-
eration https://doi.org/10.1145/nnnnnnn.nnnnnnn.

Minh-Thang Luong, Hieu Pham,

and Christo-
pher D. Manning. 2015.
Effective Approaches
to Attention-based Neural Machine Translation
https://doi.org/10.18653/v1/D15-1166.

Jonathan May and Jay Priyadarshi. 2017. SemEval-
2017 Task 9: Abstract Meaning Representa-
SemEval pages
tion Parsing and Generation.
536–545.
http://nlp.arizona.edu/SemEval-
2017/pdf/SemEval090.pdf.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer Sentinel Mixture
Models http://arxiv.org/abs/1609.07843.

Simon Mille, Anja Belz, Bernd Bohnet, Yvette Gra-
ham, Emily Pitler, and Leo Wanner. 2018. The
First Multilingual Surface Realisation Shared Task
(SR’18): Overview and Evaluation Results. In Pro-
ceedings of the 1st Workshop on Multilingual Sur-
face Realisation (MSR), 56th Annual Meeting of the
Association for Computational Linguistics ({ACL}).
Melbourne, Australia, pages 1–10.

Jekaterina Novikova, OndÅ ´Zej Dušek, and Verena
The E2E Dataset: New Chal-
Rieser. 2017.
lenges For End-to-End Generation (August):201–
206. http://arxiv.org/abs/1706.09254.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Au-
tomatic Evaluation of Machine Translation.
In
Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics. Asso-
ciation for Computational Linguistics, Strouds-
burg, PA, USA, ACL ’02,
pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Mark

Kay

Peterson,

Przybocki,

SÃl’bastien
The
Bronsart, and Gregory Sanders. 2009.
NIST 2008 Metrics
translation
challenge—overview, methodology, metrics, and
Machine Translation 23(2):71–103.
results.
https://doi.org/10.1007/s10590-009-9065-6.

for machine

Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press, New York, NY, USA.

Rico Sennrich and Barry Haddow. 2016. Linguis-
tic input features improve neural machine transla-
In Proceedings of the First Conference on
tion.
Machine Translation, WMT 2016, colocated with
ACL 2016, August 11-12, Berlin, Germany. pages
83–91.
http://aclweb.org/anthology/W/W16/W16-
2209.pdf.

Milan Straka and Jana Straková. 2017. Tokenizing,
POS Tagging, Lemmatizing and Parsing UD 2.0
In Proceedings of the CoNLL 2017
with UDPipe.
Shared Task: Multilingual Parsing from Raw Text
to Universal Dependencies. Association for Compu-
tational Linguistics, Vancouver, Canada, pages 88–
99. http://www.aclweb.org/anthology/K/K17/K17-
3009.pdf.

Kai Sheng Tai, Richard Socher,

and Christo-
pher D. Manning. 2015.
Improved Seman-
tic Representations From Tree-Structured Long
Short-Term Memory Networks pages 1556–1566.
https://doi.org/10.1515/popets-2015-0023.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xi-
aohua Liu,
Model-
ing Coverage for Neural Machine Translation
https://doi.org/10.1145/2856767.2856776.

and Hang Li. 2016.

Oriol

Vinyals,

Meire

Navdeep
http://arxiv.org/abs/1506.03134.

Jaitly.

2015.

and
Fortunato,
Pointer Networks

53
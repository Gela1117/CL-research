Unsupervised Word Inﬂuencer Networks from News Streams

Ananth Balashankar
New York University
ananth@nyu.edu

Sunandan Chakraborty

Indiana University
sunchak@iu.edu

Lakshminarayanan Subramanian

New York University

lakshmi@cs.nyu.edu

Abstract

In this paper, we propose a new unsu-
pervised learning framework to use news
events for predicting trends in stock prices.
We present Word Inﬂuencer Networks
(WIN), a graph framework to extract lon-
gitudinal temporal relationships between
any pair of informative words from news
streams. Using the temporal occurrence of
words, WIN measures how the appearance
of one word in a news stream inﬂuences
the emergence of another set of words in
the future. The latent word-word inﬂu-
encer relationships in WIN are the build-
ing blocks for causal reasoning and pre-
dictive modeling. We demonstrate the ef-
ﬁcacy of WIN by using it for unsupervised
extraction of latent features for stock price
prediction and obtain 2 orders lower pre-
diction error compared to a similar causal
graph based method. WIN discovered in-
ﬂuencer links from seemingly unrelated
words from topics like politics to ﬁnance.
WIN also validated 67% of the causal ev-
idence found manually in the text through
a direct edge and the rest 33% through a
path of length 2.

Introduction

1
Stock price prediction using ﬁnancial news events
and social media sentiments have been studied ex-
tensively in literature. Most of these works rely
on extracting rich features from relevant ﬁnan-
cial news of companies (Falinouss, 2007; Kalyani
et al., 2016; Hagenau et al., 2013; Shynkevich
et al., 2015), Twitter sentiments of ﬁnancial terms
(Mao et al., 2011; Rao and Srivastava, 2012;
Bernardo et al., 2018) and market volatility mea-
sures (Balcilar et al., 2017; Sun et al., 2014) as

features to predict trends in their stock prices.
However, none of these approaches tried to ex-
ploit unknown or little known relationships be-
tween news events and stock prices. Previous
works used “known” factors and used them as fea-
tures to predict stock prices by extracting them
from news stories. There might be other unknown
(and non-ﬁnance related) factors potentially inﬂu-
encing stock prices that cannot be discovered us-
ing these methods.

This paper aims to understand unknown and
latent relationships between words that describe
events in news streams to potentially uncover hid-
den links between news events and apply those
new relationships to build a news-driven predictive
model for stock prices. The appearance of these
relationship entities in news, may be well sepa-
rated over time. For example, market volatility is
known to be triggered by recessions; this hidden
relationship may manifest in new streams with a
frequency spike in the word ”recession” followed
by a frequency spike in the word ”volatility”, a few
weeks later. Thus, mining large news datasets can
potentially reveal inﬂuencing factors behind the
surge of a particular word in news. This notion
can be generalized to discover the inﬂuence of one
event to another, where the events are manifested
by speciﬁc words appearing in news.

In this paper, we propose a new framework
– Word Inﬂuencer Networks (WIN) that aims at
detecting the latent relationships between words,
where such relationships are not directly observed.
WIN differs from existing relationship extraction
and representational frameworks across two di-
mensions – (1) unsupervised causal relationships
instead of associative ones that can be used to un-
derstand a path of inﬂuence among news items,
(2) ﬁnding inter-topic inﬂuence relationships out-
side the “context” or the conﬁnes of a single doc-
ument. Construction of WIN can be used to build

ProceedingsoftheFirstWorkshoponEconomicsandNaturalLanguageProcessing,pages62–68Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics62predictive models for numerous news-dependent
variables, including stock prices.

We constructed WIN from a news corpus of
around 700, 000 articles and evaluated it to extract
features for stock price prediction and obtained
two orders lower prediction error compared to a
similar causal graph based method. WIN also val-
idated 67% of the causal evidence found manually
in the text through a direct edge in the network and
the rest through a path of length 2. We also eval-
uated the network qualitatively for sparsity and its
capacity to generate “out of context” inter-topic
word relationships on the entire vocabulary.

2 Related Work

source
Online news articles are a popular
including extrac-
for mining real-world events,
tion of causal
Radinsky and
relationships.
Horvitz (Radinsky and Horvitz, 2013) proposed
a framework to ﬁnd causal relationships between
events to predict future events from News but
caters to a small number of events. Causal re-
lationships extracted from news using Granger
causality have also been used for predicting vari-
ables, such as stock prices (Kang et al., 2017;
Verma et al., 2017; Darrat et al., 2007). A similar
causal relationship generation model has been pro-
posed by Hashimoto et al. (2015) to extract causal
relationships from natural language text. A simi-
lar approach can be observed in (Kozareva, 2012;
Do et al., 2011), whereas CATENA system (Mirza
and Tonelli, 2016) used a hybrid approach consist-
ing of a rule-based component and a supervised
classiﬁer. WIN differs from these approaches as
it explores latent inter-topic causal relationships in
an unsupervised manner from the entire vocabu-
lary of words and collocated N-grams.

Apart from using causality, there are many other
methods explored to extract
information from
news and are used in time series based forecasting.
Amodeo et al. (Amodeo et al., 2011) proposed a
hybrid model consisting of time-series analysis, to
predict future events using the New York Times
corpus. FBLG (Cheng et al., 2014) focused on
discovering temporal dependency from time series
data and applied it to a Twitter dataset mention-
ing the Haiti earthquake. Similar work by Luo et
al. (Luo et al., 2014) showed correlations between
real-world events and time-series data for inci-
dent diagnosis in online services. Other similar
works like, Trend Analysis Model (TAM) (Kawa-

mae, 2011) and Temporal-LDA (TM-LDA) (Wang
et al., 2012) model the temporal aspect of topics in
social media streams like Twitter. Structured data
extraction from news have also been used for stock
price prediction using techniques of information
retrieval in (Ding et al., 2014; Xie et al., 2013;
Ding et al., 2015; Chang et al., 2016; Ding et al.,
2016). Vaca et al. (Vaca et al., 2014) used a collec-
tive matrix factorization method to track emerg-
ing, fading and evolving topics in news streams.
WIN is inspired by such time series models and
leverages the Granger causality detection frame-
work for the trend prediction task.

3 Word Inﬂuence Network

Word Inﬂuence Network (WIN) addresses the dis-
covery of inﬂuence between words that appear in
news text. The identiﬁcation of inﬂuence link be-
tween words is based on temporal co-variance, so
that answers to questions of the form “Does the
appearance of word x inﬂuence the appearance of
word y after δ days?” can be addressed. The inﬂu-
ence of one word on another is determined based
on pairwise causal relationships and is computed
using Granger causality test. Following the iden-
tiﬁcation of Granger causal pairs of words, such
pairs are combined together to form a network of
words, where the directed edges depict potential
inﬂuence between words. The network provides a
more holistic view of the causal information ﬂow
by overcoming a common drawback of pair-wise
Granger causality, when the true relationship in-
volves three or more variables (Maziarz, 2015). In
the ﬁnal network an edge or a path between a word
pair represents a ﬂow of inﬂuence from the source
word to the ﬁnal word and this inﬂuence depicts an
increase in the appearance of the ﬁnal words when
the source word was observed in news data.

The word inﬂuencer network can offer the fol-
lowing that can signiﬁcantly increase the beneﬁts
of using news for analytics – (1) Detection of in-
ﬂuence path, (2) Discovery of unknown facts, (3)
Hypothesis testing and (4) Feature extraction for
experiment design.

4 Methodology

Construction of WIN from the raw unstructured
news data, ﬁnding pairwise causal links and even-
tually building the inﬂuence network involves nu-
merous challenges. In the rest of the section we
discuss the design methodologies used to over-

63come these challenges along with some properties
of the network.
Selecting Informative Words: Only a small per-
centage of the words appearing in news can be
used for meaningful information extraction and
analysis. There are some words that are too fre-
quent and some are too rare to establish any signif-
icant relationship(Manning et al., 1999; Hovold,
2005). Any word whose frequencies were in those
range were removed. Speciﬁcally, we eliminated
too frequent (at least once in more than 50% of
the days) or too rare (appearing in less than 100
articles). These thresholds were determined em-
pirically by looking at the temporal frequency dis-
tribution of the words. Many common English
nouns, adjectives and verbs, whose contribution
to semantics is minimal(Forman, 2003) carry very
little signiﬁcance were also removed from the vo-
cabulary. However, named-entities were retained
for their newsworthiness and a set of trigger words
were retained that depicts events (e.g. ﬂood, elec-
tion) using an existing event trigger detection al-
gorithm. The vocabulary set was enhanced by
adding bigrams that are signiﬁcantly collocated in
the corpus, such as, ‘fuel price’ and ‘prime minis-
ter’ etc. after applying similar ﬁltering methods as
described for words.
Time-series Representation of News Data: Con-
sider a corpus D of news articles indexed by time
t, such that Dt is the collection of news articles
published at time t. Each article d ∈ D is a
collection of words Wd, where ith word wd,i ∈
Wd is drawn from a vocabulary V of size N.
The set of articles published at time t can be ex-
pressed in terms of the words appearing in the
articles as {αt
i is the sum
of frequency of the word wi ∈ V across all ar-
i corresponding to
ticles published at time t. αt
i(cid:80)T
wi ∈ V is deﬁned as, αt
µt
i =
t=1 µt
i
i is normalized by using the
frequency distribution of wi in the entire time pe-
riod. T (wi) represents the time series of the word
wi, where i varies from 1 to N, the vocabulary
size.

N}, where αt

d=1 T F (wd,i). αt

(cid:80)|Dt|

where µt

2, ..., αt

1, αt

i =

4.1 Measuring Inﬂuence between Words
Given two time-series X and Y ,
the Granger
causality test checks whether the X is more effec-
tive in predicting Y than using just Y and if this
holds then the test concludes X “Granger-causes”
Y (Granger et al., 2000). However, if both X and

Y are driven by a common third process with dif-
ferent lags, one might still fail to reject the alter-
native hypothesis of Granger causality. Hence, in
WIN, we explore the possibility of causal links be-
tween all word pairs and detect triangulated rela-
tions to eliminate the risk of ignoring confound-
ing variables, otherwise not considered in Granger
causality test.

Constructing WIN using an exhaustive set of
word pairs can be computationally challenging
and prohibitively expensive when the vocabulary
size is fairly large. This is true in our case, where
even after using a reduced set of words and in-
cluding the collocated phrases, the vocabulary size
is around 39, 000. One solution to this problem
is considering the Lasso Granger method (Arnold
et al., 2007) that applies regression to the neigh-
borhood selection problem for any word, given the
fact that the best regressor for that variable with
the least squared error will have non-zero coefﬁ-
cients only for the lagged variables in the neigh-
borhood. The Lasso algorithm for linear regres-
sion is an incremental algorithm that embodies a
method of variable selection (Tibshirani, 1994). In
our case, if we are determining the inﬂuence link
between a word y to the rest, then,

w = argmin

1
N

Σ(x,y)∈V |w.x − y|2 + λ||w||

(1)

where V is the input vocabulary from the news
dataset, N is the vocabulary size, x is the list of
all lagged variables (maximum lag of 30 days per
word) of the vocabulary and λ is a constant to be
determined. To set λ, we use the method used
in (Meinshausen and B¨uhlmann, 2006). We se-
lect the variables that have non-zero co-efﬁcients
and choose the best lag for a given variable based
on the maximum absolute value of a word’s co-
efﬁcient. We then, draw an edge from all these
words to the predicted word with the annotations
of the optimal time lag (in days) and incrementally
construct the graph as illustrated in Figure 3.

4.2 Topic Inﬂuence Compression
The number of nodes in this version of WIN corre-
sponds to the vocabulary size and it can be hard to
visualize the graph due to its size. To make in-
formation gathering from WIN easier, we make
the graph coarser by clustering the nodes based
on topics. Topics are learned from the origi-
nal news corpus using Latent Dirichlet Allocation
(LDA)(Blei et al., 2003). Inﬂuence is generalized

64to topic level by calculating the weight of inter-
topic inﬂuence relationships as a total number of
edges between vertices of two topics. The strength
of this inﬂuence is deﬁned as,

Φ(θu, θv) =

# Edges between u and v

(|θu| × |θv|)

(2)

where, θu and θv are two topics in our topic
model and |θu| represents the size of topic θu, i.e.
the number of words in the topic whose topic-
probability is greater than 0.001. Φ(θu, θv) is
termed as strong if its value is within the top 1%
of Φ for all topics. Any edge in the original WIN
is removed if there are no strong topic edges be-
tween the corresponding word nodes. This ﬁltered
topic graph has only edges between topics which
have high inﬂuence strength.

5 Evaluation
5.1 Data
The news dataset1 we used for stock price pre-
diction contains news crawled from 2010 to 2013
using Google News APIs and New York Times
data from 1989 to 2007. We construct WIN from
the time series representation of its 12,804 uni-
grams and 25,909 bigrams, as well as the 10 stock
prices2 from 2013 we use for prediction. The
prediction is done with varying step sizes (1,3,5),
which indicates the time lag between the news
data and the day of the predicted stock price in
days.
In order to qualitatively validate that la-
tent inter-topic edges exist in the news stream, we
also constructed WIN from the online archives of
Times of India (TOI), the most circulated Indian
English newspaper. This dataset contains all the
articles published in their online edition between
January 1, 2006 and December 31, 2015 contain-
ing 1,538,932 articles.

Inter-topic edges of WIN

5.2
The inﬂuence network we constructed from the
TOI dataset has 18,541 edges and 7,190 uni-
grams and bi-gram vertices. We split the edges
to inter-topic (9774) edges and intra-topic (8765)
edges. We were interested in the inter-topic non-
associative relationships that WIN is expected to
capture. From Figure 1, we can see many top-
ics (44) do not have inter-topic inﬂuence relation-
ships, but a few topics (5) inﬂuence or are inﬂu-
enced by a large number of topics. Some of these

1https://github.com/dykang/cgraph
2https://ﬁnance.yahoo.com

highly inﬂuential topics are composed of words
describing “Education”, “Economics”, “Politics”,
“Crime” and “Agriculture”, and the maximum
number of inﬂuencer relationships in WIN is from
“Politics” to “Crime”.

Figure 1: Inter-topic word relationships

5.2.1 Links of the network
Inspecting the links and paths of WIN gives us
qualitative insights into the context in which the
word-word relationships were established. Since
WIN is also capable of representing other stock
time series as potential inﬂuencers in the network,
we can use this to model the propagation of shocks
in the market as shown in Figure 2. WIN also
highlights one of the limitations of granger causal-
ity by running on the entire vocabulary as shown
in Figure 3, i.e if an underlying event (slum re-
habilitation) causes two other events at different
time lags (provided relief and coordinate commit-
tee), the link between the two lagged events can be
pruned as it is dependent on the underlying cause.

5.3 Prediction using causal links
To evaluate the causal links generated by WIN,
we use it to extract features for predicting stock
prices using the exact data and prediction setting
used in Kang et al. (2017). Note that the features
and topics were not chosen in an unsupervised
manner in Kang et al. (2017), but rather based
on a semantic parser. Once the features are ex-
tracted from WIN, we use the past values of stock
prices and time series corresponding to the incom-
ing word edges of WIN to predict the future values

HP

8

14

8

FB

AAPL

AMZN

Figure 2: Inter-stock inﬂuencer links

65coordinate committee

19

provided relief

Table 2: Stock price predictive features from WIN

2

21

slum rehabilitation

Figure 3: WIN highlighting the underlying cause

Table 1: Stock price prediction error using WIN

Step size Cbest W INuni W INbi W INboth
1
3
5

0.023
0.023
0.023

0.020
0.022
0.021

0.022
0.022
0.022

1.96
3.78
5.25

Stock symbol
AAPL
AMZN
FB
GOOG
HPQ
IBM
MSFT
ORCL
TSLA
YHOO

Prediction indicators
workplace, shutter, music
healthcare, HBO, cloud

unfriended, troll, politician

advertisers, artiﬁcial intelligence, shake-up

China, inventions, Pittsburg

64 GB, redesign, outage

interactive, security, Broadcom
corporate, investing, multimedia

prices, testers, controversy

entertainment, leadership, investment

of the stock prices using the multivariate regres-
sion equation used to determine Granger Causal-
ity. The results shown in Table 1 is the root mean
squared error (RMSE) calculated on a 30 day win-
dow averaged by moving it by 10 days over the
period and hence is directly comparable to (Kang
et al., 2017)’s CGRAPH - Cbest. The mean abso-
lute error (MAE) for the same set of evaluations
is within 0.003 of the RMSE, which indicates that
the variance of the errors is also low. As com-
pared to their best error, WIN from unigrams, bi-
grams or both obtain two orders lower error and
signiﬁcantly outperforms CGRAPH, which also
includes features from topics and sentiments from
tweets. We attribute this gain to the ﬂexibility of
WIN’s Lasso Granger method to produce sparse
graphs as compared to CGRAPH’s Vector Auto
Regressive model with exogenous variables which
uses a ﬁxed number (10) of incoming edges per
node. This imposes an artiﬁcial bound on spar-
sity thereby losing valuable information. We over-
come this in WIN using a suitable penalty term (λ)
in the Lasso method.

The causal links in WIN are also more generic
(Table 2) than the ones described in CGRAPH.
The nodes of CGRAPH are tuples extracted from
a semantic parser (SEMAFOR) based on evidence
of causality in a sentence. WIN poses no such re-
striction and and derives topical (unfriended, FB)
and inter-topical (healthcare, AMZN), sparse, la-
tent and semantic relationships.

5.4 Causal evidence in WIN
To validate the causal links in WIN, we extracted
word pairs which depicted direct causal relation-
ships in the news corpus. We narrowed down the
search to words surrounding verbs which depict
the notion of causality like “caused”, “effect” and

manually veriﬁed that these word pairs were in-
deed causal. We then searched the shortest path in
WIN between these word pairs. 67% of the word
pairs which were manually identiﬁed to be causal
in the news text through causal indicator words
such as “caused”, were linked in WIN through di-
rect edges, while the rest were linked through an
intermediate relevant node. As seen in Table 3, the
bigram involving the word in the path is relevant
to the context in which the causality is established.
The time lags in the path show that the inﬂuence
between events are at different time lags. We also
qualitatively veriﬁed that two unrelated words are
either not connected or have a path length greater
than 2, which makes the relationship weak.

Table 3: Comparison with manually identiﬁed in-
ﬂuence from news articles

Word pairs
price, project
land, budget
price, land
strike, law
land, bill

Words of the inﬂuence path
price-hike –(19)– power-project
allot-land –(22)– railway-budget

price-hike –(12)– land

terror-strike –(25)– law ministry

land-reform –(25)– bill-pass

6 Conclusions

In this paper, we have presented WIN, a frame-
work that learns latent word relationships from
news streams in an unsupervised manner for stock
price prediction. This prediction model consid-
erably lowers the error as compared to a re-
lated causal graph method by capturing rich inter-
topical features. In future work, we aim to extend
the concept of inﬂuencer network for other types
of text abstraction, like word embeddings and ex-
plore inﬂuencer network based econometric pre-
dictive models.

66References
Giuseppe Amodeo, Roi Blanco, and Ulf Brefeld. 2011.
Hybrid models for future event prediction. CIKM
’11, pages 1981–1984.

Andrew Arnold, Yan Liu, and Naoki Abe. 2007. Tem-
poral causal modeling with graphical granger meth-
In Proceedings of the 13th ACM SIGKDD
ods.
International Conference on Knowledge Discovery
and Data Mining, KDD ’07, pages 66–75, New
York, NY, USA. ACM.

Mehmet Balcilar, Rangan Gupta, and Clement Kyei.
2017. Predicting stock returns and volatility with
investor sentiment indices: A reconsideration using
a nonparametric causalityinquantiles test. Bulletin
of Economic Research, 70(1):74–87.

Ivo Bernardo, Roberto Henriques, and Victor Lobo.
2018. Social market: Stock market and twitter cor-
relation. In Intelligent Decision Technologies 2017,
pages 341–356, Cham. Springer International Pub-
lishing.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.

Ching-Yun Chang, Yue Zhang, Zhiyang Teng, Zahn
Bozanic, and Bin Ke. 2016. Measuring the informa-
tion content of ﬁnancial news. In COLING, pages
3216–3225. ACL.

Dehua Cheng, Mohammad Taha Bahadori, and Yan
Liu. 2014. Fblg: A simple and effective approach
for temporal dependence discovery from time series
data. KDD ’14, pages 382–391.

Ali F. Darrat, Maosen Zhong, and Louis T.W. Cheng.
2007. Intraday volume and volatility relations with
and without public news. Journal of Banking and
Finance, 31(9):2711 – 2729.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2014. Using structured events to predict stock price
movement: An empirical investigation.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2015. Deep learning for event-driven stock predic-
tion. In IJCAI, pages 2327–2333. AAAI Press.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2016. Knowledge-driven event embedding for stock
prediction. In COLING, pages 2133–2142. ACL.

Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011. Minimally supervised event causality iden-
In Proceedings of the Conference on
tiﬁcation.
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 294–303, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Pegah Falinouss. 2007. Stock trend prediction using

news events. Masters thesis.

George Forman. 2003.

An extensive empirical
study of feature selection metrics for text classi-
Journal of machine learning research,
ﬁcation.
3(Mar):1289–1305.

Clive WJ Granger, Bwo-Nung Huangb, and Chin-Wei
Yang. 2000. A bivariate causality between stock
prices and exchange rates: evidence from recent
asianﬂu. The Quarterly Review of Economics and
Finance, 40(3):337–354.

Michael Hagenau, Michael Liebmann, and Dirk Neu-
mann. 2013. Automated news reading: Stock price
prediction based on ﬁnancial news using context-
Decision Support Systems,
capturing features.
55(3):685 – 697.

Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer,
and Jong-Hoon Oh. 2015. Generating event causal-
ity hypotheses through semantic relations. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artiﬁcial Intelligence, AAAI’15, pages 2396–2403.
AAAI Press.

Johan Hovold. 2005. Naive bayes spam ﬁltering using
word-position-based attributes. In CEAS, pages 41–
48.

Joshi Kalyani, H. N. Bharathi, and Rao Jyothi. 2016.
Stock trend prediction using news sentiment analy-
sis. CoRR, abs/1607.01958.

Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen,
and Eduard Hovy. 2017. Detecting and explaining
causes from text for a time series event. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2758–2767,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Noriaki Kawamae. 2011. Trend analysis model: trend
consists of temporal words, topics, and timestamps.
WSDM ’11, pages 317–326.

Zornitsa Kozareva. 2012. Cause-effect relation learn-
ing. In Workshop Proceedings of TextGraphs-7 on
Graph-based Methods for Natural Language Pro-
cessing, TextGraphs-7 ’12, pages 39–43, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu,
Rui Ding, Dongmei Zhang, and Zhe Wang. 2014.
Correlating events with time series for incident di-
agnosis. KDD ’14, pages 1583–1592.

Christopher D Manning, Hinrich Sch¨utze, et al. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.

Huina Mao, Scott Counts, and Johan Bollen. 2011.
Predicting ﬁnancial markets: Comparing survey,
news, twitter and search engine data. Arxiv preprint.

Mariusz Maziarz. 2015. A review of the granger-
causality fallacy. The Journal of Philosophical Eco-
nomics, 8(2):6.

67Nicolai Meinshausen and Peter B¨uhlmann. 2006.
High-dimensional graphs and variable selection with
the lasso. The annals of statistics, pages 1436–1462.

X-Q Sun, Shen H-W, and Cheng X-Q. 2014. Trad-
ing network predicts stock price. Scientiﬁc Reports.
2014;4:3711. doi:10.1038/srep03711.

Paramita Mirza and Sara Tonelli. 2016. Catena: Causal
and temporal relation extraction from natural lan-
In Proceedings of COLING 2016,
guage texts.
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 64–75. The
COLING 2016 Organizing Committee.

Kira Radinsky and Eric Horvitz. 2013. Mining the web
to predict future events. WSDM ’13, pages 255–
264. ACM.

Tushar Rao and Saket Srivastava. 2012. Analyzing
stock market movements using twitter sentiment
analysis. In Proceedings of the 2012 International
Conference on Advances in Social Networks Anal-
ysis and Mining (ASONAM 2012), ASONAM ’12,
pages 119–123, Washington, DC, USA. IEEE Com-
puter Society.

Y. Shynkevich, T. M. McGinnity, S. Coleman, and
A. Belatreche. 2015. Stock price prediction based
on stock-speciﬁc and sub-industry-speciﬁc news ar-
In 2015 International Joint Conference on
ticles.
Neural Networks (IJCNN), pages 1–8.

Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society, Series B, 58:267–288.

Carmen K Vaca, Amin Mantrach, Alejandro Jaimes,
and Marco Saerens. 2014. A time-based collective
factorization for topic discovery and monitoring in
news. WWW ’14, pages 527–538.

Ishan Verma, Lipika Dey, and Hardik Meisheri. 2017.
Detecting, quantifying and accessing impact of news
events on indian stock indices. In Proceedings of the
International Conference on Web Intelligence, WI
’17, pages 550–557, New York, NY, USA. ACM.

Yu Wang, Eugene Agichtein, and Michele Benzi. 2012.
Tm-lda: efﬁcient online modeling of latent topic
transitions in social media. KDD ’12, pages 123–
131. ACM.

Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ´an Creamer. 2013. Semantic frames to predict
stock price movement. In ACL (1), pages 873–883.
The Association for Computer Linguistics.

68
Phonetic Vector Representations for Sound Sequence Alignment

Pavel Sofroniev and Çağrı Çöltekin

Department of Linguistics
University of Tübingen

pavel.sofroniev@student.uni-tuebingen.de, ccoltekin@sfs.uni-tuebingen.de

Abstract

representations of

This study explores a number of data-driven
vector
the IPA-encoded
sound segments for the purpose of sound
sequence alignment. We test
the alter-
native representations based on the align-
ment accuracy in the context of computa-
tional historical linguistics. We show that
the data-driven methods consistently do bet-
ter than linguistically-motivated articulatory-
acoustic features. The similarity scores ob-
tained using the data-driven representations
in a monolingual context, however, performs
worse than the state-of-the-art distance (or sim-
ilarity) scoring methods proposed in earlier
studies of computational historical linguistics.
We also show that adapting representations to
the task at hand improves the results, yielding
alignment accuracy comparable to the state of
the art methods.
Introduction

1
Most studies in computational linguistics or nat-
ural language processing treat the phonetic seg-
ments as categorical units, which prevents analyz-
ing or exploiting the similarities or differences be-
tween these units. Alignment of sound sequences,
a crucial step in a number of different fields of in-
quiry, is one of the tasks that suffers if the segments
are treated as distinct symbols with no notion of
similarity. As a result, alignment algorithms com-
monly employed in practice (e.g., Needleman and
Wunsch, 1970) use a scoring function based on
similarity of the individual units.

The tasks that require or benefit from aligning
sequences are prevalent in computational linguis-
tics, as well as relatively unrelated fields such as
bioinformatics. In this study, we focus on align-
ing phonetically transcribed parallel word lists in
the context of computational historical linguistics,
where alignment of sound sequences is interesting

either on its own (demonstrating differences be-
tween language varieties) or as a necessary step in
a larger application, for example, for inferring the
cognacy of these words or finding synchronic or
diachronic sound correspondences.

The use of similarities between the sound seg-
ments has been common in computational studies
of historical linguistics (Covington, 1996, 1998;
Kondrak, 2000; Kondrak and Hirst, 2002; Kon-
drak, 2003; List, 2012; Jäger, 2013; Jäger and
Sofroniev, 2016). These studies rely on scoring
functions most of which are based on the linguis-
tic knowledge about the sound changes that typi-
cally occur across languages. Another trend shared
by all of the earlier studies is the use of a re-
duced alphabet for representing the sound seg-
ments. Even though the standard way to encode
sound sequences is the International Phonetic Al-
phabet (IPA), using a smaller set of symbols, such
as ASJP (Brown et al., 2013; Wichmann et al.,
2016), seem to help creating scoring functions that
are more useful for historical linguistics.

In the present study, we explore a number of
methods that learn vector representations for IPA
tokens from multi-lingual word-lists, either using
the words in a monolingual context or making use
of the fact that words represent the same concept
in different languages. We use a standard similar-
ity metric over vectors (cosine similarity) for de-
termining the similarities between the segments,
and, in turn, use these similarities for aligning IPA-
transcribed sequences.

Besides providing a more principled method for
measuring distances, compared to only distance in-
formation, vector representations are more useful
for further analysis, and may yield better results in
other computational tasks relying on supervised or
unsupervised machine learning techniques. Vec-
tor representations for phonetic, phonological or
orthographic units have been used successfully in

Proceedingsofthe15thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages111–116Brussels,Belgium,October31,2018.c(cid:13)2018TheSpecialInterestGrouponComputationalMorphologyandPhonologyhttps://doi.org/10.18653/v1/P17111earlier research, e.g., for word segmentation (Ma
et al., 2016), transfer learning of named entity
recognition (Mortensen et al., 2016) and morpho-
logical inflection (Silfverberg et al., 2018).

We compare our methods to a one-hot-encoding
baseline (which is equivalent to symbolic repre-
sentations), linguistically-motivated vectors, and
alignments produced using state-of-the-art scoring
methods. We compare the alignment performance
of these methods on a manually-annotated gold-
standard corpus, using the same alignment algo-
rithm and the same training data where applicable.

2 Methods

Our aim is to learn and use vector representa-
tions for the purposes of sound sequence align-
ment. Once we have vector representations, we
align the two sequences with Needleman-Wunsch
algorithm using the cosine similarity between the
phonetic vectors as the similarity function.

2.1 Baseline Representations
One-hot encoding
is a common method for rep-
resenting categorical data. Under one-hot encod-
ing, given a vocabulary of N distinct segments,
each segment would be represented as a distinct bi-
nary vector of size N, such that exactly one of its
dimensions has value 1 and all other dimensions
have value 0. The method does not yield useful
distance measures as each segment is equidistant
from all the others. We use one-hot encoding as a
proxy for a purely symbolic baseline.

PHOIBLE Online
is an ongoing project aiming
to compile a comprehensive database of the world
languages’ phonological inventories (Moran et al.,
2014). The project also maintains a table of phono-
logical features, effectively mapping each segment
encountered in the database to a unique ternary
feature vector. Feature values are assigned based
on Hayes (2009) and Moisik and Esling (2011),
and indicate either the presence, absence, or non-
applicability of an articulatory-acoustic feature for
each IPA symbol. PHOIBLE feature vectors serve
as a linguistically-informed baseline.

2.2 Data-driven Vector Representations
Our proposed methods include three data-driven
methods to learn vector representations for IPA-
encoded sound segments.

are

are

embeddings

phon2vec
the well-known
word2vec method (Mikolov et al., 2013) applied
to IPA-encoded phonetic segments. The method
learns dense vector representations that maximize
the similarity of segments that appear in similar
contexts. As in original word2vec models, the
context is treated as a bag of words, ignoring the
relative position of each context element.
Position sensitive neural network embeddings
(NN embeddings) are obtained using a simple
feed-forward neural network architecture. Similar
to word2vec skip-gram method, the neural network
tries to predict the context of a word from the word
itself. The hidden layer representations are, then,
used as the representations for the word. Unlike
word2vec, however, the context is not treated as a
bag of phonetic segments. The position of the ele-
ments in the context is significant.
RNN embeddings
a
sequence-to-sequence recurrent neural network
(Cho et al., 2014). Given a pair of sequences, the
network encodes the first sequence into a vector
which is then decoded into an output sequence.
The first layer of the network is an embeddings
layer which converts the input categories to dense
vector representations with a smaller number of
dimensions. The network is trained to ‘translate’
words (as sequences of IPA tokens) between the
languages in the training set, while, in the process,
learning useful representations for IPA tokens.
Once the network is trained, we are interested in
the representations build for each IPA-token by
the embedding layer.

obtained

using

Unlike the other data-driven methods described
above, the RNN embeddings require, and make
use of, multi-lingual nature of the data. However,
crucially, the method does not require any explicit
alignment of the sequences in advance.
2.3 State-of-the-art Scoring Functions
We compare the alignment performance of our
methods to two state-of-the-art scoring functions.
The first one,
the sound-class-based phonetic
alignment (SCA, List, 2012) employs a set of
28 sound classes.
It operates on IPA sequences
by converting the segments into their respective
sound classes, aligning the sound class tokens, and
then converting these back into IPA. The scoring
function is hand-crafted to reflect the perceived
probabilities of sound change transforming a seg-
ment of one class into a segment of another.

112We also compare our results with the alignments
obtained using the method proposed by Jäger
(2013), which uses the ASJP database (Wichmann
et al., 2016) to calculate the pairwise mutual in-
formation (PMI) scores for each pair of ASJP seg-
ments. The method starts with an initial alignment,
and re-aligns the corpus iteratively for obtaining
the final PMI-based scores. The method is data-
driven, but heavily optimized for the task. Since
it does not work with IPA-encoded sequences, we
first convert the IPA sequences to ASJP alphabet,
and convert them back to IPA after alignment.1
3 Experiments and Results
3.1 Data
In order to evaluate the performance of the meth-
ods put forward in the previous section, we use
the Benchmark Database for Phonetic Alignments
(BDPA, List and Prokić, 2014). The database con-
tains 7198 aligned pairs of IPA sequences collected
from 12 source datasets, covering languages and
dialects from 6 language families (detailed infor-
mation about the data set is provided in the Ap-
pendix). The database also features the small set
of 82 selected pairs used by Covington (1996) to
evaluate his method, encoded in IPA.

Our

data

sourced

training

from
NorthEuraLex,
a comprehensive lexicostatis-
tical database that provides IPA-encoded lexical
data for languages of, primarily but not exclu-
sively, Northern Eurasia (Dellert and Jäger, 2017).
At the time of writing the database covers 1016
concepts from 107 languages, resulting in 121 614
IPA transcriptions.
3.2 Experimental Setup
Obtaining
the
phon2vec and neural network methods involves
settings the models’ hyperparameters and training
on a data set of IPA sequences (or pairs thereof).
We tokenize the input sequences using an open
source Python package developed during this
study.2 The phon2vec and NN embeddings are
trained on the set of all tokenised transcriptions in
the training set. For training the RNN, we need
cognates, pairs of words in different languages that
share a common root. As our training set does

representations with

vector

is

1 The IPA to ASJP conversion is lossy. However, the
alignments are not affected since the source IPA symbols are
known during ASJP to IPA conversion.

2https://pypi.python.org/pypi/ipatok.

not include cognacy information, the RNN embed-
dings are trained on the set of tokenised transcrip-
tions of the word pairs constituting probable cog-
nates — pairs in which the words belong to differ-
ent languages, are linked to the same concept, and
have normalised Levenshtein distance lower than
0:5. We have also experimented with thresholds
of 0:4 and 0:6, but setting the cutoff at 0:5 yields
better-performing embeddings.

For each method, we run the respective model
with the Cartesian product of common values for
each hyperparameter, practically performing a ran-
dom search of the hyperparameter space. The val-
ues we have experimented with, as well as the best-
performing combinations thereof, are summarized
in the Appendix. Note that the models are opti-
mized for the respective prediction task they per-
form, not for good alignment performance.

The implementation is realized in the Python
programming language, and makes use of a num-
ber of libraries, including NumPy (Walt et al.,
2011), SciPy (Jones et al., 2001), scikit-learn (Pe-
dregosa et al., 2011), Gensim (Řehůřek and Sojka,
2010), and Keras (Chollet et al., 2015). The source
code used for the experiments reported here is pub-
licly available.3
3.3 Evaluation
In order to quantify the methods’ performance, we
employ an intuitive evaluation scheme similar to
the one used by Kondrak and Hirst (2002): if, for
a given word pair, m is the number of alternative
gold-standard alignments and n is the number of
correctly predicted alignments, the score for that
pair would be n
m. In the common word pair case of
a single gold-standard alignment and a single pre-
dicted alignment, the latter would yield 1 point if
it is correct and 0 points otherwise; partially cor-
rect alignment do not yield points. The percentage
scores are obtained by dividing the points by the
total number of pairs.
3.4 Results and Discussion
The alignment performance of our baselines, pro-
posed methods, as well as PMI and SCA on the
BDPA data sets is summarized in Table 1.

The first point we would like to draw attention
to is that the one-hot encoding scores are consis-
tently lower than those in the other columns. This
is expected because, unlike the other methods, one-

3 https://github.com/pavelsof/ipavec.

113Andean
Bai
Bulgarian
Dutch
French
Germanic
Japanese
Norwegian
Ob-Ugrian
Romance
Sinitic
Slavic
Global
Covington

one-hot
85:66
52:55
60:54
14:16
42:94
39:93
53:56
59:39
59:58
40:48
27:34
76:96
51:83
60:61

phoible
87:31
62:77
80:54
25:65
62:92
51:78
65:04
78:87
77:87
71:28
28:57
90:73
66:64
82:42

phon2vec

97:25
61:25
77:98
26:00
68:94
54:59
73:74
73:69
73:35
63:16
30:75
84:22
66:99
80:18

nn
99:34
74:72
82:55
32:50
74:30
71:83
62:71
83:53
78:04
76:37
72:46
89:89
75:88
82:52

rnn
99:50
75:52
86:70
32:50
77:04
72:55
71:08
89:06
82:55
77:55
74:04
96:81
78:45
82:52

pmi
95:21

–

81:70
36:67
71:98
75:32
68:26
78:11
82:09
84:51

–

89:36
77:36
87:80

sca
99:67
83:45
89:34
42:20
80:90
83:48
82:19
91:77
86:04
95:62
98:95
94:15
84:84
90:24

Table 1: Scores, as percentage of total alignment pairs. Global scores does not include Covington. PMI method
does not handle tonal languages, and its global score is based on the non-tonal language groups.

hot encoding cannot represent the degree of pho-
netic similarity between IPA segments. Viewing
the one-hot encoding scores as a baseline, we con-
clude that the other methods’ distance measures do
indeed contribute to sequence alignment.

The PHOIBLE feature vectors are roughly on
par with the phon2vec embeddings, yielding bet-
ter results than the NN embeddings on two of
the datasets (Japanese and Slavic), and are other-
wise outperformed by the NN and the RNN em-
beddings, as well as PMI and SCA. Part of the
low performance of the PHOIBLE’s vectors can
be due to the fact that PHOIBLE does not pro-
vide feature vectors for all IPA segments in the
BDPA datasets. However, the similar performance
between PHOIBLE vectors and phon2vec and,
clearly better performance achieved by the NN em-
beddings indicates that we can learn (more) useful
linguistic generalizations in a data-driven manner.
Of the data-driven methods, phon2vec yields the
lowest scores, being outperformed by both neu-
ral network models in all datasets except Japanese.
Given that both the phon2vec and the NN embed-
dings are trained on the same data, the consistent
performance difference between phon2vec and NN
embeddings points to usefulness of to the sequen-
tial order of IPA segments. The better performance
of the RNN embeddings over other data driven
methods is not surprising, as they capture useful
information from the multi-lingual data set. Fur-
thermore, the performance of RNN embeddings is
similar to the PMI method, yielding better results
in many data sets.

For all but the Slavic dataset, SCA yields higher
scores than other methods compared in this study.
The score differences exhibit considerable vari-

ance — from less than 1 percent point for the An-
dean dataset up to 26 percent points for the Sinitic
dataset. A possible explanation for this variance
is the fact that not all IPA segments found in
the benchmark datasets are found in the training
data. For example, NorthEuraLex includes a single
tonal language, Mandarin Chinese, and the models
cannot produce meaningful embeddings for most
of the tones encountered in the Sinitic and Bai
datasets. Arguably, a larger training dataset fea-
turing a richer set of IPA segments would produce
better-performing embeddings.
4 Conclusion
In this study we have proposed, implemented, and
evaluated three methods for obtaining vector rep-
resentations of IPA segments for the purposes of
pairwise IPA sequence alignment. Our method
outperforms a linguistically-informed baseline, as
well as a trivial one-hot representation, per-
forms comparably to a state-of-the-art data driven
method. However, the performances of data driven
methods,
including ours, seem to be behind a
linguistically-informed system, SCA. Neverthe-
less, the results of the data-driven methods are not
too far off the mark, and we believe that they could
be significantly improved by using larger and more
diverse training data, and better tuning of the data-
driven methods. This constitutes one direction for
future experiments; another possibility is to train
and use embeddings specific to a particular lan-
guage family or macro-area. Further investigation
is also needed with respect to comparing and eval-
uating the methods, especially in the context of a
larger application, such as cognacy identification
or phylogenetic inference.

114References
Bryan Allen. 2007. Bai Dialect Survey. SIL Interna-

tional.

Jørn Almberg and Kristian Skarbø. 2011. Nordavinden

og sola. En norsk dialektprøvedatabase på nettet.

Cecil H. Brown, Eric W. Holman, and Søren Wich-
mann. 2013. Sound Correspondences in the World’s
Languages. Language, 89(1):4–29.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
Phrase Representations using RNN Encoder-
Decoder
for Statistical Machine Translation.
arXiv:1406.1078 [cs, stat].

François Chollet et al. 2015. Keras.

Gerhard Jäger and Pavel Sofroniev. 2016. Automatic
cognate classification with a support vector machine.
In Proceedings of the 13th Conference on Natural
Language Processing (KONVENS 2016), pages 128–
134.

Grzegorz Kondrak. 2000. A New Algorithm for the
Alignment of Phonetic Sequences. In Proceedings
of the 1st North American Chapter of the Association
for Computational Linguistics Conference, NAACL
2000, pages 288–295, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Grzegorz Kondrak. 2003. Phonetic alignment and sim-
ilarity. Computers and the Humanities, 37(3):273–
291.

Grzegorz Kondrak and Graeme Hirst. 2002. Algorithms
for language reconstruction, volume 63. University
of Toronto Toronto.

Michael A. Covington. 1996. An Algorithm to Align
Words for Historical Comparison. Computational
Linguistics, 22(4):481–496.

Johann-Mattis List. 2012. SCA: Phonetic Alignment
based on sound classes. New Directions in Logic,
Language and Computation, pages 32–51.

Michael A. Covington. 1998. Alignment of Multiple
Languages for Historical Comparison. In Proceed-
ings of the 17th International Conference on Compu-
tational Linguistics - Volume 1, COLING ’98, pages
275–279, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Johannes Dellert and Gerhard Jäger, editors. 2017.
NorthEuraLex (version 0.9). Eberhard Karls Univer-
sität Tübingen, Tübingen.

Rick Derksen, editor. 2008. Etymological dictionary
of the Slavic inherited lexicon. Number 4 in Lei-
den Indo-European Etymological Dictionary Series.
Brill, Leiden and Boston.

Louis Gauchat, Jules Jeanjaquet, and Ernest Tappolet,
editors. 1925. Tableaux phonétiques des patois su-
isses romands. Attinger, Neuchâtel.

Harald Hammarström, Sebastian Bank, Robert Forkel,
and Martin Haspelmath, editors. 2018. Glottolog
3.2. Max Planck Institute for the Science of Human
History, Jena.

Bruce Hayes. 2009.

well.

Introductory Phonology. Black-

Paul Heggarty. 2006. Sounds of the Andean languages.

Jīngyī Hóu, editor. 2004. Xiàndài Hànyǔ fāngyán

yīnkù. Shànghǎi Jiàoyù, Shànghǎi.

Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001.

SciPy: Open source scientific tools for Python.

Gerhard Jäger. 2013.

Phylogenetic Inference from
Word Lists Using Weighted Alignment with Empiri-
cally Determined Weights. Language Dynamics and
Change, 3(2):245–291.

Johann-Mattis List and J Prokić. 2014. A benchmark
database of phonetic alignments in historical linguis-
tics and dialectology. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 288–294.

Jianqiang Ma, Çağrı Çöltekin, and Erhard Hinrichs.
2016. Learning phone embeddings for word seg-
mentation of child-directed speech. In Proceedings
Workshop on Cognitive Aspects of Computational
Language Learning, pages 53–63.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. arXiv:1301.3781 [cs].

Scott R. Moisik and John H. Esling. 2011. The ‘whole
larynx’ approach to laryngeal features. In Proceed-
ings of the International Congress of Phonetic Sci-
ences (ICPhS XVII), pages 1406–1409.

Steven Moran, Daniel McCloy, and Richard Wright, ed-
itors. 2014. PHOIBLE Online. Max Planck Institute
for Evolutionary Anthropology, Leipzig.

David R. Mortensen, Patrick Littell, Akash Bharad-
waj, Kartik Goyal, Chris Dyer, and Lori S. Levin.
2016. PanPhon: A resource for mapping IPA seg-
In Proceed-
ments to articulatory feature vectors.
ings of COLING 2016, the 26th International Con-
ference on Computational Linguistics: Technical Pa-
pers, pages 3475–3484. ACL.

Saul B Needleman and Christian D Wunsch. 1970. A
general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443–453.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,

115D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011.
Scikit-learn: Machine learning in
Journal of Machine Learning Research,
Python.
12:2825–2830.

Jelena Prokić, John Nerbonne, Vladimir Zhobov, Petya
Osenova, Kiril Simov, Thomas Zastrow, and Er-
hard Hinrichs. 2009. The computational analysis of
Bulgarian dialect pronunciation. Serdica journal of
computing, 3(3):269–298.

Colin Renfrew and Paul Heggarty. 2009. Languages

and origins in Europe.

Georges de Schutter, Boudewijn van den Berg, Ton
Goeman, and Thera de Jong. 2005. Morfologische
atlas van de Nederlandse dialecten.

Hattori Shirō. 1973.

Japanese dialects. Diachronic,

areal and typological linguistics, pages 368–400.

Miikka P. Silfverberg, Lingshuang Mao, and Mans
Hulden. 2018. Sound Analogies with Phoneme Em-
beddings. In Proceedings of the Society for Compu-
tation in Linguistics (SCiL) 2018, pages 136–144.

Stefan van der Walt, S. Chris Colbert, and Gael Varo-
quaux. 2011. The NumPy array: A structure for effi-
cient numerical computation. Computing in Science
and Engineering, 13:22–30.

Feng Wang. 2006. Comparison of languages in contact.
The distillation method and the case of Bai. Institute
of Linguistics Academia Sinica, Taipei.

Søren Wichmann, Eric W. Holman, and Cecil H.
Brown, editors. 2016. The ASJP Database (version
17). Available at http://asjp.clld.org/.

M. Zhivlov. 2011. Annotated Swadesh wordlists for the
Ob-Ugrian group (Uralic family). The Global Lexi-
costatistical Database.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.

116
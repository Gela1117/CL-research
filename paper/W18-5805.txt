String Transduction with Target Language Models

and Insertion Handling

Garrett Nicolai‚Ä† and Saeed NajaÔ¨Å‚Ä° and Grzegorz Kondrak‚Ä°

‚Ä†Department of Computer Science

Johns Hopkins University
gnicola2@jhu.edu

‚Ä°Department of Computing Science

University of Alberta

{snajafi, gkondrak}@ualberta.ca

Abstract

Many character-level tasks can be framed as
sequence-to-sequence transduction, where the
target is a word from a natural language. We
show that leveraging target language mod-
els derived from unannotated target corpora,
combined with a precise alignment of the
training data, yields state-of-the art results on
cognate projection, inÔ¨Çection generation, and
phoneme-to-grapheme conversion.

1

Introduction

Many natural language tasks, particularly those in-
volving character-level operations, can be viewed
as sequence-to-sequence transduction (Figure 1).
Although these tasks are often addressed in iso-
lation, they share a common objective ‚Äî in each
case, the output is a word in the target language.

The hypothesis that we investigate in this pa-
per is that a single task- and language-independent
system can achieve state-of-the-art results by
leveraging unannotated target language corpora
that contain thousands of valid target word types.
We focus on low-data scenarios, which present
a challenge to neural sequence-to-sequence mod-
els because sufÔ¨Åciently large parallel datasets are
often difÔ¨Åcult to obtain. To reinforce transduc-
tion models trained on modest-sized collections of
source-target pairs, we leverage monolingual text
corpora that are freely available for hundreds of
languages.

Our approach is based on discriminative string
transduction, where a learning algorithm assigns
weights to features deÔ¨Åned on aligned source and
target pairs. At test time, an input sequence is con-
verted into the highest-scoring output sequence.
Advantages of discriminative transduction include
an aptitude to derive effective models from small
training sets, as wells as the capability to incorpo-
rate diverse sets of features. SpeciÔ¨Åcally, we build

Figure 1: Illustration of four character-level sequence-
to-sequence prediction tasks. In each case, the output
is a word in the target language.

upon DIRECTL+ (Jiampojamarn et al., 2010), a
string transduction tool which was originally de-
signed for grapheme-to-phoneme conversion.

We present a new system, DTLM, that com-
bines discriminative transduction with character
and word language models (LMs) derived from
large unannotated corpora. Target language mod-
eling is particularly important in low-data scenar-
ios, where the limited transduction models often
produce many ill-formed output candidates. We
avoid the error propagation problem which is in-
herent in pipeline approaches by incorporating the
LM feature sets directly into the transducer.

In addition, we bolster the quality of trans-
duction by employing a novel alignment method,
which we refer to as precision alignment. The idea
is to allow null substrings (nulls) on the source
side during the alignment of the training data,
and then apply a separate aggregation algorithm
to merge them nulls with adjacent non-empty sub-
strings. This method yields precise many-to-many
alignment links that lead to improved transduction
accuracy.

Proceedingsofthe15thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages43‚Äì53Brussels,Belgium,October31,2018.c(cid:13)2018TheSpecialInterestGrouponComputationalMorphologyandPhonologyhttps://doi.org/10.18653/v1/P1743‡πÄ‡∏ü‡πÄ‡∏ó‡∏≠‡∏£‡πå  feather+1PL;PRES Feder /f…õ√∞…ö/  feather Phoneme-to-Grapheme Conversion Transliteration Inflection  Generation Cognate   Projection Sequence-to-sequence Transduction The contributions of this paper include the
following.
(1) A novel method of incorporat-
ing strong target language models directly into
discriminative transduction.
(2) A novel ap-
proach to unsupervised alignment that is partic-
ularly beneÔ¨Åcial in low-resource settings.
(3)
An extensive experimental comparison to pre-
vious models on multiple tasks and languages,
which includes state-of-the-art results on inÔ¨Çec-
tion generation, cognate projection, and phoneme-
(4) Publicly available
to-grapheme generation.
implementation of the proposed methods.
(5)
Three new datasets for cognate projection.

2 Baseline methods

In this section, we describe the baseline methods,
including the alignment of the training data, the
feature sets of DirecTL+ (henceforth DTL), and
reranking as a way of incorporating corpus statis-
tics.

2.1 Alignment

Before a transduction model can be derived from
the training data, the pairs of source and target
strings need to be aligned, in order to identify
atomic substring transformations. The unsuper-
vised M2M aligner (Jiampojamarn et al., 2007)
employs the Expectation-Maximization (EM) al-
gorithm with the objective of maximizing the joint
likelihood of its aligned source and target pairs.
The alignment involves every source and target
character. The pairs of aligned substrings may
contain multiple characters on both the source and
target sides, yielding many-to-many (M-M) align-
ment links.

DTL excludes insertions from its set of edit op-
erations because they greatly increase the com-
plexity of the generation process,
to the point
of making it computationally intractable (Barton,
1986). Therefore, the M2M aligner is forced to
avoid nulls on the source side by incorporating
them into many-to-many links during the align-
ment of the training data. Although many-to-
many alignment models are more Ô¨Çexible than 1-1
models, they also generally require larger paral-
lel datasets to produce correct alignments. In low-
data scenarios, especially when the target strings
tend to be longer than the source strings, this ap-
proach often yields sub-optimal alignments (e.g.,
the leftmost alignment in Figure 2).

Figure 2:
Examples of different alignments in
phoneme-to-letter conversion. The underscore denotes
a null substring.

2.2 Features
DTL is a feature-rich, discriminative character
transducer, which searches for a model-optimal
sequence of character transformation operations
for its input. The core of the engine is a dy-
namic programming algorithm capable of trans-
ducing many consecutive characters in a single
operation, also known as a semi-Markov model.
Using a structured version of the MIRA algo-
rithm (McDonald et al., 2005), the training pro-
cess assigns weights to each feature, in order to
achieve maximum separation of the gold-standard
output from all others in the search space.

DTL uses a number of feature templates to as-
sess the quality of an operation: source context,
target ùëõ-gram, and joint ùëõ-gram features. Con-
text features conjoin the rule with indicators for all
source character ùëõ-grams within a Ô¨Åxed window
of where the rule is being applied. Target n-grams
provide indicators on target character sequences,
describing the shape of the target as it is being pro-
duced, and may also be conjoined with the source
context features. Joint ùëõ-grams build indicators
on rule sequences, combining source and target
context, and memorizing frequently-used rule pat-
terns. An additional copy feature generalizes the
identity function from source to target, which is
useful if there is an overlap between the input and
output symbol sets.

2.3 Reranking
The target language modeling of DTL is limited to
a set of binary ùëõ-gram features, which are based
exclusively on the target sequences from the par-
allel training data. This shortcoming can be reme-
died by taking advantage of large unannotated cor-
pora that contain thousands of examples of valid
target words.

Nicolai et al. (2015) propose to leverage corpus
statistics by reranking the ùëõ-best list of candidates
generated by the transducer. They report con-
sistent modest gains by applying an SVM-based

44w …î k …ô z   w …î _ k _ …ô z  w …î k  …ô z  w a l k e r s  w a l k e r s  w a l k e r s   reranker, with features including a word unigram
corpus presence indicator, a normalized character
language model score, and the rank and normal-
ized conÔ¨Ådence score generated by DTL. How-
ever, such a pipeline approach suffers from er-
ror propagation, and is unable to produce output
forms that are not already present in the ùëõ-best list.
In addition, training a reranker requires a held-out
set that substantially reduces the amount of train-
ing data in low-data scenarios.

3 Methods

In this section, we describe our novel extensions:
precision alignment, character-level
lan-
guage modeling, and corpus frequency. We make
the new implementation publicly available.1

target

3.1 Precision Alignment
We propose a novel alignment method that pro-
duces accurate many-to-many alignments in two
stages. The Ô¨Årst step consists of a standard 1-1
alignment, with nulls allowed on either side of the
parallel training data. The second step removes
the undesirable nulls on the source side by merg-
ing the corresponding 0-1 links with adjacent 1-1
links. This alignment approach is superior to the
one described in Section 2.1, especially in low-
data scenarios when there is not enough evidence
for many-to-many links.2

Our precision alignment is essentially a 1-1
alignment with 1-M links added when necessary.
In a low-resource setting, an aligner is often un-
able to distinguish valid M-M links from spurious
ones, as both types will have minimal support in
the training data. On the other hand, good 1-1
links are much more likely to have been observed.
By limiting our Ô¨Årst pass to 1-1 links, we ensure
that only good 1-1 links are posited; otherwise, an
insertion is predicted instead. On the second pass,
the aligner only needs to choose between a small
number of alternatives for merging the insertions,
increasing the likelihood of a good alignment, and,
subsequently, correct transduction.

Consider the example in Figure 2 where 5
source phonemes need to be aligned to 7 target
letters. The baseline approach incorrectly links the

1http://github.com/GarrettNicolai/DTLM and /M2MP
2The improvement in the alignment quality is relative to
the performance of our transduction system, as we demon-
strate in Section 4.6 ‚Äî the alignments are not necessarily
optimal from the linguistic point of view.

else

ùê∂ùêº+=1

if ùë° > 0 and ùë•ùë° == _ then

1: Algorithm:ForwardInsertionMerging
2: Input: (ùë•ùëá ,ùë¶ùëâ )
3: Output:(ùõºùëá +1,ùëâ +1)
4: ùê∂ùêº = 0, ùëÉ ùêº = 0
5: for ùë° =0; ùë° ‚â§ ùëá do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

ùëÉ ùêº = ùê∂ùêº
ùê∂ùêº = 0
if ùë° ‚àí ùê∂ùêº == 0 then

ùõºùë°,ùë£ = 0
if ùë° > 0 and ùë£ > 0 then
for ùëò = 0; ùëò ‚â§ ùëÉ ùêº do

for ùë£=0; ùë£ ‚â§ ùëâ do

if ùë° > 0 or ùë£ > 0 then

ùõºùë°,ùë£+= ùõø(ùë•ùë°
ùë°‚àíùê∂ùêº‚àíùëò, ùë¶ùë£
ùõºùë°‚àíùê∂ùêº‚àíùëò‚àí1,ùë£‚àíùê∂ùêº‚àíùëò‚àí1

ùë£‚àíùê∂ùêº‚àíùëò)*

ùõºùë°,ùë£ = 1
ùëêùëúùëõùë°ùëñùëõùë¢ùëí {insertions at the start of the word}

Figure 3: The forward step of M2M, modiÔ¨Åed to merge
insertions to adjacent characters.

letter ‚Äòa‚Äô with the phoneme /w/ (the leftmost align-
ment in the diagram). Our Ô¨Årst-pass 1-1 alignment
(in the middle), correctly matches /O/ to ‚Äòa‚Äô, while
‚Äòl‚Äô is treated as an insertion. On the second pass,
our algorithm merges the null with the preceding
1-1 link. By contrast, the second insertion, which
involves /@/, is merged with the substitution that
follows it (the rightmost alignment).

Figure 3 demonstrates how we modify the for-
ward step to merge insertions with adjacent sub-
stituions; similar modiÔ¨Åcations are made for the
backward step, expectation, and decoder. The in-
put consists of a source string x of length ùëá , and
a target string y of length ùëâ . Both x and y may
contain underscores, which represent nulls from
the Ô¨Årst alignment pass. The ùõº score represents
the sum of the likelihoods of all paths that have
been traversed through source character ùë° and tar-
get character ùë£. In a 1-1 alignment, all ùõº scores ac-
cumulate along the diagonal, while in a many-to-
many alignment, other cells of the ùõº matrix may
be Ô¨Ålled. Our precision alignment is a compromise
between these two methods: we consider adjacent
characters, but force the ùõº score to accumulate on
the diagonal. By allowing insertions and deletions
in the Ô¨Årst pass, we force x and y to be of equal
length. We then perform a 1-1 alignment, expand-
ing the alignment size only when the source char-
acter is a null.

We supplement the forward algorithm of M2M
with two counters: PI is the number of adjacent
insertions immediately to the left of the current
character, while CI is the number of insertions that

45have been encountered since the last substitution.
The loop at line 18 executes the ùõº score accumu-
lation, where ùõø is the likelihood of a speciÔ¨Åc se-
quence alignment, effectively merging insertions
with adjacent substitutions. An extended example
that illustrates the operation of the algorithm is in-
cluded in the Appendix.

3.2 Character-level language model
In order to incorporate a stronger character lan-
guage model into DTL, we propose an additional
set of features that directly reÔ¨Çect the probabil-
ity of the generated subsequences. We train a
character-level language model on a list of types
extracted from a raw corpus in the target lan-
guage, applying Witten-Bell smoothing and back-
off for unseen ùëõ-grams. During the generation
process, the transducer incrementally constructs
target sequences character-by-character. The nor-
malized log-likelihood score of the current output
sequence is computed according to the character
language model.

For consistency with other sets of features, we
convert these real-valued scores into binary indi-
cators by means of binning. Development exper-
iments led to the creation of bins that represent a
normal distribution around the mean likelihood of
words. Features Ô¨Åre in a cumulative manner, and
a Ô¨Ånal feature Ô¨Åres only if no bin threshold is met.
For example, if a sequence has a log-likelihood of
-0.85, the feature for -0.9 Ô¨Åres, as does the one for
-0.975, and -1.05, etc.

3.3 Corpus frequency counts
We also extend DTL with a feature set that can
be described as a unigram word-level language
model. The objective is to bias the model towards
generating output sequences that correspond to
words observed in a large corpus. Since an out-
put sequence can only be matched against a word
list after the generation process is complete, we
propose to estimate the Ô¨Ånal frequency count for
each preÔ¨Åx considered during the generation pro-
cess. Following Cherry and Suzuki (2009) we use
a preÔ¨Åx trie to store partial words for reference in
the generation phase. We modify their solution by
also storing the count of each preÔ¨Åx, calculated as
the sum of all of the words in which the preÔ¨Åx oc-
curs.

As with our language model features, unigram
features are binned. A unigram feature Ô¨Åres if the

Figure 4: An example of transduction with target lan-
guage models. Black cells represent Ô¨Åring features.

count of the generated sequence surpasses the bin
threshold, in a cumulative manner.

We found that the quality of the target uni-
gram set can be greatly improved by language-
based corpus pruning. Although unannotated cor-
pora are more readily available than parallel ones,
they are often noisier. SpeciÔ¨Åcally, crowd-sourced
corpora such as Wikipedia contain many English
words that can unduly inÔ¨Çuence our unigram fea-
tures. In order to mitigate this problem, we prepro-
cess our corpora by removing all word unigrams
that have a higher probability in an English corpus
than in a target-language corpus.

Consider an example of how our new features
beneÔ¨Åt a transduction model, shown in Figure 4.
Note that although we portray the extensions as
part of a pipeline, their scores are incorporated
jointly with DTL‚Äôs other features. The top-ùëõ
list produced by the baseline DTL for the input
phoneme sequence /pI@s/ fails to include the cor-
rect output pierce. However, after the new lan-
guage model features are added, the correct form
makes its way to the top predictions. The new fea-
tures combine with the original features of DTL,
so that the high unigram count of piece is not suf-
Ô¨Åcient to make it the top prediction on the right

46p…™…ôs DirecTL+ Language Model Unigram Model Candidate Prediction Score piess 4.07 pies 4.06 piuss 3.97 Candidate LM Score Bin Prediction Score pies -0.88 8 18.12 piers -0.94 9 14.97 pierce -0.97 9 13.21 Candidate Unigram count Bin Prediction Score pious 23.9 10 14.88 pierce 30.7 10 13.12 piece 283.7 8 12.95                    2.09 pies piers                    1.90 pierce                    1.90 pious               1.49 pierce               1.49 piece               1.64 Candidate Prediction Score pierce 18.35 piers 18.27 press 16.93 pies 16.55 piece 14.05  side of the diagram. Only when both sets of new
features are incorporated does the system manage
to produce the correct form, as seen at the bottom
of the diagram.

4 Experiments

In this section, we present the results of our exper-
iments on four different character-level sequence-
to-sequence tasks: transliteration, inÔ¨Çection gen-
eration, cognate projection, and phoneme-to-
grapheme conversion (P2G). In order to demon-
strate the generality of our approach, the exper-
iments involve multiple systems and datasets, in
both low-data and high-data scenarios.

Where low-data resources do not already exist,
we simulate a low-data environment by sampling
an existing larger training set. Low-data training
sets consist of 100 training examples, 1000 de-
velopment examples, and 1000 held-out examples,
except for cognate projection, where we limit the
development set to 100 training examples, and the
held-out set to the remaining examples. An output
is considered correct if it exactly matches any of
the targets in the reference data.

4.1 Systems
We evaluate DTLM, our new system, against two
strong baselines and two competitive tools. Pa-
rameter tuning was performed on the same devel-
opment sets for all systems.

We compare against two baselines. The Ô¨Årst
is the standard DTL, as described in Section 2.2.
The second follows the methodology of Nicolai
et al. (2015), augmenting DTL with a reranker
(DTL+RR), as described in Section 2.3. Both
baselines use the default 2-2 alignment with dele-
tions produced by the M2M aligner. We train
the reranker using 10-fold cross validation on
the training set, using the reranking method of
Joachims (2002). Due to the complexity of its
setup on large datasets, we omit DTL+RR in such
scenarios. Except where noted otherwise, we train
4-gram character language models using the CMU
toolkit3 with Witten-Bell smoothing on the Uni-
Morph corpora of inÔ¨Çected word forms.4 Word
counts are determined from the Ô¨Årst one million
lines of the corresponding Wikipedia dumps.

We also compare against Sequitur (SEQ), a
generative string transduction tool based on joint

3http://www.speech.cs.cmu.edu/SLM
4unimorph.org

source and target ùëõ-grams (Bisani and Ney, 2008),
and a character-level neural model (RNN). The
neural model uses the encoder-decoder architec-
ture typically used for NMT (Sutskever et al.,
2014). The encoder is a bi-directional RNN ap-
plied to randomly initialized character embed-
dings; we employ a soft-attention mechanism to
learn an aligner within the model. The RNN is
trained for a Ô¨Åxed random seed using the Adam
optimizer, embeddings of 128 dimensions, and
hidden units of size 256. We use a beam of size 10
to generate the Ô¨Ånal predictions. We experimented
with the alternative neural approach of Makarov
et al. (2017), but found that it only outperforms our
RNN when the source and target sides are largely
composed of the same set of symbols; therefore,
we only use it for inÔ¨Çection generation.

4.2 Transliteration
Transliteration is the task of converting a word
from a source to a target script on the basis of the
word‚Äôs pronunciation.

Our low-resource data consists of three back-
transliteration pairs from the 2018 NEWS Shared
Task: Hebrew to English (HeEn), Thai to English
(ThEn), and Persian to English (PeEn). These
languages were chosen because they represent
back-transliteration into English. Since the orig-
inal forms were originally English, they are much
more likely to appear in an English corpus than if
the words originated in the source language. We
report the results on the task‚Äôs 1000-instance de-
velopment sets.

Since transliteration is mostly used for named
entities, our language model and unigram counts
are obtained from a corpus of named entities. We
query DBPedia5 for a list of proper names, dis-
carding names that contain non-English charac-
ters. The resulting list of 1M names is used to train
the character language model and inform the word
unigram features.

The results in Table 1 show that our pro-
posed extensions have a dramatic impact on low-
resource transliteration.
In particular, the seam-
less incorporation of the target language model
not only simpliÔ¨Åes the model but also greatly im-
proves the results with respect to the reranking ap-
proach. On the other hand, the RNN struggles to
learn an adequate model with only 100 training ex-
amples.

5https://wiki.dbpedia.org

47System
DTL
DTL+RR
DTLM
RNN
SEQ

HeEn ThEn PeEn
13.2
8.7
13.6
19.0
26.1
36.7
5.4
2.6
8.5
7.8

1.1
2.7
9.6
1.3
4.4

Table 1: Word-level accuracy on transliteration (in %)
with 100 training instances.

System HeEn ThEn PeEn
21.9
DTL
23.6
36.8
DTLM 38.7
26.7
25.8
RNN
SEQ
25.5
31.2

37.0
48.0
43.8
44.9

Table 2: Word-level accuracy on transliteration (in %)
with complete training sets.

We also evaluate a larger-data scenario. Using
the same three languages, we replace the 100 in-
stance training sets with the ofÔ¨Åcial training sets
from the 2018 shared task, which contain 9,447,
27,273, and 15,677 examples for HeEn, TnEn, and
PeEn, respectively. The language model and fre-
quency lists are the same as for the low-resource
experiments. Table 2 shows that DTLM outper-
forms the other systems by a large margin thanks
to its ability to leverage a target word list. Addi-
tional results are reported by NajaÔ¨Å et al. (2018b).

InÔ¨Çection generation

4.3
InÔ¨Çection generation is the task of producing an
inÔ¨Çected word-form, given a citation form and
a set of morphological features. For example,
given the Spanish inÔ¨Ånitive liberar, with the
tag V;IND;FUT;2;SG, the word-form liberar√°s
should be produced.

In recent years, inÔ¨Çection generation has at-
tracted much interest (Dreyer and Eisner, 2011;
Durrett and DeNero, 2013; Nicolai et al., 2015;
Ahlberg et al., 2015). Aharoni and Goldberg
(2017) propose an RNN augmented with hard at-
tention and explicit alignments for inÔ¨Çection, but
have difÔ¨Åculty consistently improving upon the
results of DTL, even on larger datasets. Fur-
thermore, their system cannot be applied to tasks
where the source and target are different lan-
guages, due to shared embeddings between the en-
coder and decoder. Ruzsics and Samardzic (2017)
incorporate a language model into the decoder of

System
DTL
DTLM
CLUZH

Average

40.7
49.0
40.9

Table 3: Word-level accuracy (in %) on inÔ¨Çection gen-
eration with 100 training instances.

a canonical segmentation system. Our model dif-
fers in that we learn the inÔ¨Çuence of the language
model during training, in conjunction with DTL‚Äôs
other features. Deutsch et al. (2018) place a hard
constraint on the decoder, so that it only produces
observed derivational forms. We instead imple-
ment a soft constraint, encouraging candidates that
look like real words, but allowing the model to
generalize to unseen forms.

Our

inÔ¨Çection data comes from the 2017
CoNLL‚ÄìSIGMORPHON Shared Task on Rein-
Ô¨Çection (Cotterell et al., 2017). We use the
datasets from the low-resource setting of the in-
Ô¨Çection generation sub-task, in which the train-
ing sets are composed of 100 source lemmas with
inÔ¨Çection tags and the corresponding inÔ¨Çected
forms. We supplement the training data with
100 synthetic ‚Äúcopy‚Äù instances that simply trans-
form the target string into itself. This modiÔ¨Åca-
tion, which is known to help in transduction tasks
where the source and target are nearly identical,
is used for the inÔ¨Çection generation experiments
only. Along with the training sets from the shared
task, we also use the task‚Äôs development and test
sets, each consisting of 1000 instances.

Since Sequitur is ill-suited for this type of trans-
duction, we instead train a model using the method
of the CLUZH team (Makarov et al., 2017), a
state-of-the-art neural system that was the winner
of the 2017 shared task. Their primary submis-
sion in the shared task was an ensemble of 20 indi-
vidual systems. For our experiments, we selected
their best individual system, as reported in their
system paper. For each language, we train models
with 3 separate seeds, and select the model that
achieves the highest accuracy on the development
set.

Table 3 shows that DTLM improves upon
CLUZH by a signiÔ¨Åcant margin. The Appendix
contains the detailed results for individual lan-
guages. DTLM outperforms CLUZH on 46 of the
52 languages. Even for languages with large mor-
phological inventories, such as Basque and Polish,

48with the sparse corpora that such inventories sup-
ply, we see notable gains over DTL. We also see
large gains for languages such as Northern Sami
and Navajo that have relatively small Wikipedias
(fewer than 10,000 articles).

DTLM was also evaluated as a non-standard
submission in the low-data track of the 2018
Shared Task on Universal Morphological InÔ¨Çec-
tion (Cotterell et al., 2018). The results reported
by NajaÔ¨Å et al. (2018a) conÔ¨Årm that DTLM sub-
stantially outperforms DTL on average. Further-
more, a linear combination of DTLM and a neural
system achieved the highest accuracy among all
submissions on 34 out of 103 tested languages.

4.4 Cognate projection
Cognate projection, also referred to as cognate
production, is the task of predicting the spelling
of a hypothetical cognate in another language.
For example, given the English word difÔ¨Åculty,
the Spanish word diÔ¨Åcultad should be produced.
Previously proposed cognate projection systems
have been based on SVM taggers (Mulloni, 2007),
character-level SMT models (Beinborn et al.,
2013), and sequence labeling combined with a
maximum-entropy reranker (Ciobanu, 2016).

In this section, we evaluate DTLM in both low-
and high-resource settings. Our low-resource data
consists of three diverse language pairs. The Ô¨Årst
set corresponds to a mother-daughter historical re-
lationship between reconstructed Vulgar Latin and
Italian (VL-IT) and contains 601 word pairs manu-
ally compiled from the textbook of Boyd-Bowman
(1980). English and German (EN-DE), close sib-
lings from the Germanic family, are represented
by 1013 pairs extracted from Wiktionary. From
the same source, we also obtain 438 Slavic word
pairs from Russian and Polish (RU-PL), which are
written in different scripts (Cyrillic vs. Latin). We
make the new datasets publicly available.6

The results are shown in Table 4. Of the sys-
tems that have no recourse to corpus statistics,
the RNN appears crippled by the small training
size, while SEQ is competitive with DTL, espe-
cially on the difÔ¨Åcult EN-DE dataset. On the other
hand, the other two systems obtain substantial im-
provements over DTL, but the gains obtained by
DTLM are 2-3 times greater than those obtained
by DTL+RR. This demonstrates the advantage of
incorporating the language model features directly

6http://github.com/GarrettNicolai/CognateData

System EN-DE RU-PL VL-IT
DTL
39.2
43.6
DTL+RR
52.5
DTLM
RNN
15.7
36.9
SEQ

4.3
7.1
17.7
2.2
9.2

23.5
32.8
43.9
1.7
22.3

Table 4: Word-level accuracy (in %) on cognate pro-
jection with 100 training instances.

System EN-ES EN-DE EN-RU
BZG-13
45.7
DTL
30.3
DTLM 56.8
RNN
34.3

17.2
24.3
33.5
20.5

8.3
13.3
45.9
15.0

Table 5: Word-level accuracy (in %) on large-scale
cognate projection.

into the training phase over simple reranking.

Our high-resource data comes from a previous
study of Beinborn et al. (2013). The datasets
were created by applying romanization scripts
and string similarity Ô¨Ålters to translation pairs ex-
tracted from Bing. We Ô¨Ånd that the datasets
are noisy, consisting mostly of lexical loans from
Latin, Greek, and English, and include many com-
pound words that share only a single morpheme
(i.e., informatics and informationswissenschaft).
In order to alleviate the noise, we found it bene-
Ô¨Åcial to disregard all training pairs that could not
be aligned by M2M under the default 2-2 link set-
ting.

Another problem in the data is the overlap be-
tween the training and test sets, which ranges from
40% in EN-ES to 94% in EN-EL. Since we believe
it would be inappropriate to report results on con-
taminated sets, we decided to ignore all test in-
stances that occur in the training data. (Unfortu-
nately, this makes some of the test sets too small
for a meaningful evaluation.) The resulting dataset
sizes are included in the Appendix. Along with the
datasets, Beinborn et al. (2013) provide the pre-
dictions made by their system. We re-calculate the
accuracy of their predictions (BZG-13), discard-
ing any forms that were present in the training set,
and compare against DTL and DTLM, as well as
the RNN.

Table 5 shows striking gains. While DTL and
the RNN generally improve upon BZG-13, DTLM
vastly outstrips either alternative. On EN-RU,

49DTLM correctly produces nearly half of poten-
tial cognates, 3 times more than any of the other
systems. We conclude that our results constitute a
new state of the art for cognate projection.

4.5 Phoneme-to-grapheme conversion
Phoneme-to-grapheme (P2G) conversion is the
task of predicting the spelling of a word from a se-
quence of phonemes that represent its pronuncia-
tion (Rentzepopoulos and Kokkinakis, 1996). For
example, a P2G system should convert [t r ae n z
d ah k sh ah n] into transduction. Unlike the op-
posite task of grapheme-to-phoneme (G2P) con-
version, large target corpora are widely available.
To the best of our knowledge, the state of the art
on P2G is the joint ùëõ-gram approach of Bisani and
Ney (2008), who report improvements on the re-
sults of Galescu and Allen (2002) on the NetTalk
and CMUDict datasets.

Our low-resource dataset consists of three lan-
guages: English (EN), Dutch (NL), and Ger-
man (DE), extracted from the CELEX lexical
database (Baayen et al., 1995).

Table 6 shows that our modiÔ¨Åcations yield sub-
stantial gains for all three languages, with consis-
tent error reductions of 15-20% over the reranking
approach. Despite only training on 100 words, the
system is able to convert phonetic transcriptions
into completely correct spellings for a large frac-
tion of words, even in English, which is notorious
for its idiosyncratic orthography. Once again, the
RNN is hampered by the small training size.

We also evaluate DTLM in a large-data sce-
to replicate the P2G ex-
nario. We attempt
periments reported by (Bisani and Ney, 2008).
The data comes from three lexicons on which
we conduct 10-fold cross validation: English
NetTalk (Sejnowski and Rosenberg, 1993), French
Brulex (Mousty and Radeau, 1990), and English
CMUDict (Weide, 2005). These corpora contain
20,008, 24,726, and 113,438 words, respectively,
in both orthographic and phonetic notations. We
note that CMUDict differs from the other two lex-
icons in that it is much larger, and contains pre-
dominantly names, as well as alternative pronun-
ciations. When the training data is that abundant,
there is less to be gained from improving the align-
ment or the target language models, as they are al-
ready adequate in the baseline approach.

Table 7 shows the comparison of the results.
The P2G results obtained by Sequitur in our exper-

System
EN
DTL
13.9
DTL+RR 25.3
39.6
DTLM
RNN
2.7
15.9
SEQ

NL
30.6
32.6
43.7
6.6
30.5

DE
33.5
51.5
60.5
14.1
28.6

Table 6: Word-level accuracy (in %) on phoneme-to-
grapheme conversion with 100 training instances.

DTL
DTLM
RNN
SEQ

NetTalk Brulex CMU
48.3
49.0
48.0
48.6

61.0
75.2
55.8
62.7

68.0
76.8
67.1
71.5

Table 7: Word-level accuracy (in %) on phoneme-to-
grapheme conversion with large training sets.

iments are slightly lower than those reported in the
original paper, which is attributable to differences
in data splits, tuned hyper-parameters, and/or the
presence of stress markers in the data. Sequitur
still outperforms the baseline DTL, but DTLM
substantially outperforms both Sequitur and the
RNN on both the NetTalk and Brulex datasets,
with smaller gains on the much larger CMUDict.
We conclude that our results advance the state of
the art on phoneme-to-grapheme conversion.

4.6 Ablation
Table 8 shows the results of disabling individ-
ual components in the low-resource setting of the
P2G task. The top row reproduces the full DTLM
system results reported in Table 6. The remain-
ing three show the results without the character-
level LM, word unigram, and precision alignment,
respectively. We observe that the accuracy sub-
stantially decreases in almost every case, which
demonstrates the contribution of all three compo-
nents.

In a separate experiment on the English G2P
dataset, we investigate the impact of the align-
ment quality by applying several different align-
ment approaches to the training sets. When M2M
aligner uses unconstrained alignment,
it favors
long alignments that are too sparse to learn a
transduction model, achieving less than 1% accu-
racy. Kubo et al. (2011)‚Äôs MPALIGNER, which em-
ploys a length penalty to discourage such overlong
substring matches, improves moderately, achiev-

50System
DTLM
-Language model
-Freq
-Precision

EN
39.6
37.8
22.0
34.9

NL
43.7
38.2
37.1
43.7

DE
60.5
48.5
56.7
46.7

Table 8: Ablation test on P2G data with 100 training
instances.

ing 27.5% accuracy, while constraining M2M
to 2-2 improves further, to 34.9%. The accu-
racy increases to 39.6% when the precision align-
ment is employed. We conclude that in the low-
resource setting, our proposed precision alignment
is preferable to both MPALIGNER and the standard
M2M alignment.

4.7 Error Analysis
The following error examples from three different
tasks demonstrate the advantages of incorporating
the character-level LM, word frequency, and pre-
cision alignment, respectively. For the purpose
of insightful analysis, we selected test instances
for which DTLM produces markedly better output
than DTL.

In inÔ¨Çection generation, the second person plu-
ral form of knechten is correctly predicted as
knechtetet, instead of knechttet. In this case, our
character language model derived from a large text
corpus rightly assigns very low probability to the
unlikely 4-gram sequence chtt, which violates
German phonotactic constraints.

In the phoneme-to-grapheme conversion task,
[tIlEm@tri] is transduced to telemetry, instead of
tilemetry. In English, a reduced vowel phoneme
such as [I] may correspond to any vowel letter. In
this example, DTLM is able to successfully lever-
age the occurrence of the correct word-form in a
raw corpus.

In cognate projection,

the actual cognate of
Kenyan is kenianisch, rather than kenyisch. This
prediction can be traced to the alignment of the
adjectival sufÔ¨Åx -an to -anisch in the training data.
The match, which involves a target substring of
considerable length, is achieved through a merger
of multiple insertion operations.

The errors made by DTLM fall into a few differ-
ent categories. Occasionally, DTLM produces an
incorrect form that is more frequent in the corpus.
For example, DTLM incorrectly guesses a sub-
junctive form of the verb versetzen to be the high-

frequency versetzt, rather than the unseen verset-
zet. More important, the transducer is incapable
of generalizing beyond source-target pairs seen in
training. For example, consider the doubling of
consonants in English orthography (e.g. betting).
Unlike the RNN, DTLM incorrectly predicts the
present participle of rug as *ruging, because there
is no instance of the doubling of ‚Äòg‚Äô in the training
data.

5 Conclusion

We have presented DTLM: a powerful language-
and task-independent transduction system that can
leverage raw target corpora. DTLM is particularly
effective in low-resource settings, but is also suc-
cessful when larger training sets are available. The
results of our experiments on four varied trans-
duction tasks show large gains over alternative ap-
proaches.

Acknowledgments

program,

The Ô¨Årst author was supported in part by the
Defense Advanced Research Projects Agency‚Äôs
Incidents
(DARPA) Low Resource Emergent
(LORELEI)
contract No.
HR0011-15-C0113. Any opinions and conclu-
sions expressed in this material are those of the
authors, and do not necessarily reÔ¨Çect the views of
the Defense Advanced Research Projects Agency
(DARPA).

under

The second and third authors were supported
by the Natural Sciences and Engineering Research
Council of Canada (NSERC).

We thank the members of the University of Al-
berta teams who collaborated with us in the con-
text of the 2018 shared tasks on transliteration
and morphological reinÔ¨Çection: Bradley Hauer,
Rashed Rubby Riyadh, and Leyuan Yu.

References
Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inÔ¨Çection generation with hard monotonic atten-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2004‚Äì2015, Vancouver,
Canada. Association for Computational Linguistics.

Malin Ahlberg, Markus Forsberg, and Mans Hulden.
2015. Paradigm classiÔ¨Åcation in supervised learning
of morphology. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-

51guage Technologies, pages 1024‚Äì1029. Association
for Computational Linguistics.

Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX Lexical Database. Release
2 (CD-ROM). Linguistic Data Consortium, Univer-
sity of Pennsylvania, Philadelphia, Pennsylvania.

G. Edward Barton. 1986. Computational complexity
In Proceedings of the
in two-level morphology.
24th Annual Meeting of the Association for Compu-
tational Linguistics, pages 53‚Äì59, New York, New
York, USA. Association for Computational Linguis-
tics.

Lisa Beinborn, Torsten Zesch, and Iryna Gurevych.
2013. Cognate production using character-based
machine translation. In Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 883‚Äì891, Nagoya, Japan. Asian
Federation of Natural Language Processing.

Maximilian Bisani and Hermann Ney. 2008.

Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech communication, 50(5):434‚Äì451.

Peter Boyd-Bowman. 1980. From Latin to Romance in

sound charts. Georgetown University Press.

Colin Cherry and Hisami Suzuki. 2009. Discriminative
In Proceed-
substring decoding for transliteration.
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1066‚Äì1075.
Association for Computational Linguistics.

Alina Maria Ciobanu. 2016. Sequence labeling for
cognate production. In Knowledge-Based and Intel-
ligent Information and Engineering Systems: Pro-
ceedings of the 20th International Conference KES-
2016, pages 1391‚Äì1399.

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
G√©raldine Walther, Ekaterina Vylomova, Arya D.
McCarthy, Katharina Kann, Sebastian Mielke, Gar-
rett Nicolai, Miikka Silfverberg, David Yarowsky,
Jason Eisner, and Mans Hulden. 2018. The CoNLL‚Äì
SIGMORPHON 2018 shared task: Universal mor-
the
phological reinÔ¨Çection.
CoNLL‚ÄìSIGMORPHON 2018 Shared Task: Univer-
sal Morphological ReinÔ¨Çection, Brussels, Belgium.
Association for Computational Linguistics.

In Proceedings of

Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
G√©raldine Walther, Ekaterina Vylomova, Patrick
Xia, Manaal Faruqui, Sandra K√ºbler, David
Yarowsky, Jason Eisner, and Mans Hulden. 2017.
CoNLL‚ÄìSIGMORPHON 2017 shared task: Uni-
versal morphological reinÔ¨Çection in 52 languages.
In Proceedings of the CoNLL‚ÄìSIGMORPHON 2017
Shared Task: Universal Morphological ReinÔ¨Çection,
pages 1‚Äì30, Vancouver. Association for Computa-
tional Linguistics.

Daniel Deutsch, John Hewitt, and Dan Roth. 2018. A
distributional and orthographic aggregation model

In Proceed-
for english derivational morphology.
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1938‚Äì1947. Association for Computa-
tional Linguistics.

Markus Dreyer and Jason Eisner. 2011. Discovering
morphological paradigms from plain text using a
In Proceedings
Dirichlet process mixture model.
of the Conference on Empirical Methods in Natural
Language Processing, pages 616‚Äì627. Association
for Computational Linguistics.

Greg Durrett and John DeNero. 2013.

Supervised
learning of complete morphological paradigms. In
HLT-NAACL, pages 1185‚Äì1195.

Lucian Galescu and James F. Allen. 2002. Pronunci-
ation of proper names with a joint n-gram model
for bi-directional grapheme-to-phoneme conversion.
In Seventh International Conference on Spoken Lan-
guage Processing.

Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010.
Integrating joint n-gram features
into a discriminative training network. In NAACL-
HLT, pages 697‚Äì700, Los Angeles, CA. Association
for Computational Linguistics.

Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
In Human Language Technologies
conversion.
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 372‚Äì
379. Association for Computational Linguistics.

Thorsten Joachims. 2002. Optimizing search engines
In Proceedings of the
using clickthrough data.
eighth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 133‚Äì
142. ACM.

Keigo Kubo,

Hiromichi Kawanami,

Hiroshi
Saruwatari, and Kiyohiro Shikano. 2011. Un-
constraned many-to-many alignment for automatic
In Proceedings of the
pronunciation annotation.
Asia-PaciÔ¨Åc Signal and Information Processing
Association Annual Summit and Conference.

Peter Makarov, Tatiana Ruzsics, and Simon Clematide.
2017. Align and copy: UZH at SIGMORPHON
2017 shared task for morphological reinÔ¨Çection.
arXiv preprint arXiv:1707.01355.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.

Philippe Mousty and Monique Radeau. 1990. Brulex.
Une base de donn√©es lexicales informatis√©e pour
le fran√ßais √©crit et parl√©. L‚Äôann√©e Psychologique,
90(4):551‚Äì566.

52Andrea Mulloni. 2007. Automatic prediction of cog-
nate orthography using support vector machines.
In Proceedings of the ACL 2007 Student Research
Workshop, pages 25‚Äì30, Prague, Czech Republic.
Association for Computational Linguistics.

Saeed NajaÔ¨Å, Bradley Hauer, Rashed Rubby Riyadh,
Leyuan Yu, and Grzegorz Kondrak. 2018a. Combin-
ing neural and non-neural methods for low-resource
In Proceedings of the
morphological reinÔ¨Çection.
CoNLL‚ÄìSIGMORPHON 2018 Shared Task: Univer-
sal Morphological ReinÔ¨Çection, Brussels, Belgium.
Association for Computational Linguistics.

Saeed NajaÔ¨Å, Bradley Hauer, Rashed Rubby Riyadh,
Leyuan Yu, and Grzegorz Kondrak. 2018b. Com-
parison of assorted models for transliteration.
In
Proceedings of the Seventh Named Entities Work-
shop, pages 84‚Äì88. Association for Computational
Linguistics.

Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.
2015. InÔ¨Çection generation as discriminative string
In Proceedings of the 2015 Confer-
transduction.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 922‚Äì931. Association for
Computational Linguistics.

Panagiotis A. Rentzepopoulos and George K. Kokki-
nakis. 1996.
EfÔ¨Åcient multilingual phoneme-to-
grapheme conversion based on HMM. Computa-
tional Linguistics, 22(3):351‚Äì375.

Tatyana Ruzsics and Tanja Samardzic. 2017. Neu-
ral sequence-to-sequence learning of internal word
structure. In Proceedings of the 21st Conference on
Computational Natural Language Learning (CoNLL
2017), pages 184‚Äì194, Vancouver, Canada. Associ-
ation for Computational Linguistics.

TJ Sejnowski and CR Rosenberg. 1993. NETtalk cor-
pus. URL< ftp://svrftp. eng. cam. ac. uk/pub/comp.
speech/dictionaries.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104‚Äì3112.

Robert Weide. 2005. The Carnegie Mellon pronounc-

ing dictionary [cmudict. 0.6].

53
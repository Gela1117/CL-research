An Argument-Annotated Corpus of Scientiﬁc Publications

Anne Lauscher,1,2 Goran Glavaˇs,1 and Simone Paolo Ponzetto1

1Data and Web Science Research Group

University of Mannheim, Germany

2Web-based Information Systems and Services

{anne, goran, simone}@informatik.uni-mannheim.de

Stuttgart Media University, Germany

lauscher@hdm-stuttgart.de

Abstract

Argumentation is an essential feature of scien-
tiﬁc language. We present an annotation study
resulting in a corpus of scientiﬁc publications
annotated with argumentative components and
relations. The argumentative annotations have
been added to the existing Dr. Inventor Cor-
pus, already annotated for four other rhetorical
aspects. We analyze the annotated argumenta-
tive structures and investigate the relations be-
tween argumentation and other rhetorical as-
pects of scientiﬁc writing, such as discourse
roles and citation contexts.

1

Introduction

With the rapidly growing amount of scientiﬁc liter-
ature (Bornmann and Mutz, 2015), computational
methods for analyzing scientiﬁc writing are becom-
ing paramount. To support learning-based mod-
els for automated analysis of scientiﬁc publica-
tions, potentially leading to better understanding
of the different rhetorical aspects of scientiﬁc lan-
guage (which we dub scitorics), researchers pub-
lish manually-annotated corpora. To date, existing
manually-annotated scientiﬁc corpora already re-
ﬂect several of these aspects, such as sentential
discourse roles (Fisas et al., 2015), argumenta-
tive zones (Teufel et al., 1999, 2009; Liakata et al.,
2010), subjective aspects (Fisas et al., 2016), and
citation polarity and purpose (Jochim and Sch¨utze,
2012; Jha et al., 2017; Fisas et al., 2016).

As tools of persuasion (Gilbert, 1976, 1977),
scientiﬁc publications are abundant with argumen-
tation. Yet, somewhat surprisingly, there is no pub-
licly available corpus of scientiﬁc publications (in
English), annotated with ﬁne-grained argumenta-
tive structures.
In order to support comprehen-
sive analyses of rhetorics in scientiﬁc text (i.e., sci-
torics), argumentative structure of scientiﬁc publi-
cations should not be studied in isolation, but rather

in relation to other rhetorical aspects, such as the
discourse structure. This is why in this work we
contribute a new argumentation annotation layer to
an existing Dr. Inventor Corpus (Fisas et al., 2016),
already annotated for several rhetorical aspects.

Contributions. We propose a general argument
annotation scheme for scientiﬁc text that can cover
various research domains. We next extend the
Dr.
Inventor corpus (Fisas et al., 2015, 2016)
with an annotation layer containing ﬁne-grained
argumentative components and relations. Our ef-
forts result in the ﬁrst argument-annotated corpus
of scientiﬁc publications (in English), which al-
lows for joint analyses of argumentation and other
rhetorical dimensions of scientiﬁc writing. We
make the argument-annotated corpus publicly avail-
able.1 Finally, we offer an extensive statistical and
information-theoretic analysis of the corpus.

2 Related Work

Researchers have offered a plethora of argument
annotation schemes and corpora for various do-
mains, including Wikipedia discussions (Biran and
Rambow, 2011), on-line debates (e.g., Abbott et al.,
2016; Habernal and Gurevych, 2016), e-markets
(e.g., Islam, 2007), persuasive essays (Stab and
Gurevych, 2017), news editorials (Al Khatib et al.,
2016), and law (Wyner et al., 2010). The corpus of
Reed et al. (2008) covers multiple domains, includ-
ing news and political debates.

The work on argumentative annotations in scien-
tiﬁc writing is, however, much scarcer. Pioneering
annotation efforts of Teufel and Moens (1999a,b);
Teufel et al. (1999) focused on discourse-level ar-
gumentation (dubbed argumentative zones), denot-
ing more the rhetorical structure of the publica-

1http://data.dws.informatik.

uni-mannheim.de/sci-arg/compiled_corpus.
zip

Proceedingsofthe5thWorkshoponArgumentMining,pages40–46Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics40tions than ﬁne-grained argumentation, i.e., there
are no (1) ﬁne-grained argumentative components
(at sub-sentence level) and no (2) relations between
components, giving rise to an argumentation graph.
Blake (2010) distinguishes between explicit and
implicit claims, correlations, comparisons, and ob-
servations in biomedical publications. In contrast,
we are not interested in how the claim is made,
but rather on what are the claims (and what is
not a claim) and how they are mutually connected.
Green et al. (2014); Green (2014, 2015, 2016) pro-
posed methods for identifying and annotating ar-
gumentative structures in scientiﬁc publications,
but released no publicly available annotated corpus.
In the effort most similar to ours, Kirschner et al.
(2015) annotated arguments in a corpus of educa-
tional research publications. Besides being quite
small, this corpus is also written in German.

3 Annotation Scheme

A number of theoretical frameworks of argumen-
tation have been proposed (Walton et al., 2008;
Anscombre and Ducrot, 1983, inter alia).2 Among
the most widely used is the model of Toulmin
(2003), from which we start in this work as well,
because of its relative simplicity and adoption in
artiﬁcial intelligence and argument mining (Bench-
Capon, 1998; Verheij, 2005; Kirschner et al., 2015).
The Toulmin model, originally developed for the
legal domain, recognizes six types of argumenta-
tive components: claim, data, warrant, backing,
qualiﬁer, and rebuttal.

We conducted a preliminary annotation study us-
ing the Toulmin model with two expert annotators
on a small corpus subset. Annotators did not iden-
tify any warrant, backing, qualiﬁer, nor rebuttal
components. The annotators also pointed to the
interlinked argumentative structure of publications
in which claim were often used as ground for (sup-
porting or conﬂicting) another claim. Not foreseen
by the Toulmin model, we realized that the rela-
tions between argumentative components can be of
different nature. Finally, the annotators recognized
two distinct claim types: those presented as com-
mon knowledge (or state of the art) in the research
area and those relating to authors’ own research.

Following the above observations from the pre-
liminary annotation, we simplify the annotation
scheme by removing the non-observed component

2For an extensive overview, we refer the reader to (Benta-

har et al., 2010)

types. Our ﬁnal annotation scheme has the follow-
ing types of argumentative component:
(1) Own Claim is an argumentative statement that
closely relates to the authors’ own work, e.g.:

”Furthermore, we show that by simply
changing the initialization and target velocity,
the same optimization procedure leads to
running controllers.”

(2) Background Claim is an argumentative state-
ment relating to the background of authors’ work,
e.g., about related work or common practices in the
respective research ﬁeld, e.g.:

”Despite the efforts, accurate modeling of
human motion remains a challenging tasks.”

::::::::::

memory::::and::::::::

(3) Data component represents a fact that serves
as evidence for or against a claim. Note that refer-
ences or (factual) examples can also serve as data,
e.g.:
”[...], due to :::::::
graphics:::::::::
hardware
constraints nearly all video game character
animation is still done using traditional SSD.”
We follow Bench-Capon (1998) and allow for
links between the arguments. We introduce three
different relations types, similar to Dung (1995).
(1) A Supports relation holds between components
a and b if the assumed veracity of b increases with
the veracity of a;
(2) A Contradicts relation holds between compo-
nents a and b if the assumed veracity of b decreases
with the veracity of a;
(3) The Semantically Same relation is annotated
between two mentions of effectively the same claim
or data component. This relation can be seen as
argument coreference, analogous to entity (Lee
et al., 2011, 2017) and event coreference (Glavaˇs
and ˇSnajder, 2013; Lu and Ng, 2018).

It is important to emphasize that we do not bind
the spans of our argumentative components to sen-
tence boundaries, but rather allow for argumenta-
tive components of arbitrary span lengths, ranging
from a single token to multiple sentences.

4 Annotation study
Dataset. Believing that argumentation needs to
be studied in combination with other rhetorical as-
pects of scientiﬁc writing, we enriched the existing
Dr. Inventor corpus (Fisas et al., 2015, 2016), con-
sisting of 40 publications from computer graphics,
with argumentative information. The Dr. Inventor

41Annotation Layer

Discourse Role

Citation Purpose

Subjective Aspect

Summarization Relevance

Labels
Background
Challenge
Approach
Outcome
Future Work
Criticism
Comparison
Use
Substantiation
Basis
Neutral
Advantage
Disadvantage
Adv.-Disadv.
Disadv.-Adv.
Novelty
Common Practice
Limitation
Totally irrelevant
Should not appear
May appear
Relevant
Very relevant

%
20
5
57
16
2
23
9
11
1
5
53
33
16
3
1
13
32
2
66
6
14
6
8

Table 1: Annotation layers of the Dr. Inventor Cor-
pus with label distributions.

corpus has four layers of rhetorical annotations: (1)
discourse roles, (2) citation purposes with associ-
ated citation contexts, (3) judgments of subjective
aspects, and (4) annotations of sentence relevance
for a summary. Table 1 summarizes the different
annotation layers and their label distributions.
Annotation Process. We hired one expert3 and
three non-expert annotators4 for our annotation
study. We trained the annotators in a calibration
phase, consisting of ﬁve iterations, in each of which
all annotators annotated one publication. After
each iteration we computed the inter-annotator
agreement (IAA), discussed the disagreements,
and, if needed, adjourned the annotation guide-
lines.5 We measured the IAA in terms of the F1-
measure because (1) it is easily interpretable and
straight-forward to compute and (2) it can account
for spans of varying length, allowing for computing
relaxed agreements in terms of partial overlaps.6
The evolution of IAA over the ﬁve calibration it-
3A researcher in computational linguistics, not in computer

graphics.

4Humanities and social sciences scholars.
5http://data.dws.informatik.

uni-mannheim.de/sci-arg/annotation_
guidelines.pdf

6Note that the chance-corrected measures, e.g., Cohen’s
Kappa, approach F1-measure when the number of negative
instances grows (Hripcsak and Rothschild, 2005).

Figure 1: IAA evolution over the ﬁve calibration
phases (purple for argumentative components; blue
for relations; dark for the strict agreements; light
for the relaxed agreements).

erations is depicted in Figure 1, in two variants:
(1) A strict version in which components have to
match exactly in span and type and relations have
to match exactly in both components, direction and
type of the link and (2) a relaxed version in which
components only have to match in type and overlap
in span (by at least half of the length of the shorter
of them). Expectedly, we observe higher agree-
ments with more calibration. The agreement on
argumentative relations is 23% lower than on the
components, which we think is due to the high am-
biguity of argumentation structures, as previously
noted by Stab et al. (2014). That is, given an ar-
gumentative text with pre-identiﬁed argumentative
components, there are often multiple valid interpre-
tations of an argumentative relation between them,
i.e., it is “[...] hard or even impossible to iden-
tify one correct interpretation” (Stab et al., 2014).
Additionally, disagreements in component identiﬁ-
cation are propagated to relations as well, since the
agreement on a relation implies the agreement on
annotated components at both ends of the relation.

5 Corpus Analysis
We ﬁrst study the argumentation layer we annotated
in isolation. Afterwards, we focus on the interrela-
tions with other rhetorical annotation layers.
Analysis of Argumentation Annotations. Ta-
ble 2 lists the number of components and rela-
tions in total and on average per publication. The
number of own claims roughly doubles the amount
of background claims, as the corpus consists only
of original research papers, in which the authors
mainly emphasize their own contributions. Interest-

42907,943907,943907,943907,943907,943;339908897.9399080,#0,9438897.9#0,94380,Category

Component

Relation

Label
Background claim 2,751
Own claim
5,445
Data
4,093
Supports
5,790
Contradicts
696
Semantically same
44

Total Per Publication
68.8 ± 25.2
136.1 ± 46.0
102.3 ± 32.1
144.8 ± 43.1
17.4 ± 9.1
1.1 ± 1.81

Criterion
Diameter
Max In-Degree
# standalone claims
# unsupp. claims
# unconn. subgraphs
# comp. per subgraph

Min Max Avg (µ) Std (σ)
0.71
1.97
21.40
29.14
35.78
1.5

5
2
3
11
27 127
39 180
78 231
1
17

3.05
6.33
63.00
94.38
147.23
2.09

Table 2: Total and per-publication distributions of
labels of argumentative components and relations
in the extended Dr. Inventor Corpus.

Table 4: Graph-based analysis of the argumentative
structures identiﬁed in the extended Dr. Inventor
Corpus (per publication).

Label
Background claim
Own claim
Data

Min Max Avg (µ)
87.46
85.70
25.80

340
500
244

5
3
1

Std (σ)
43.74
44.03
27.59

Table 3: Statistics on length of argumentative com-
ponents (in number of characters) in the extended
Dr. Inventor Corpus.

ingly, there are only half as many data components
as claims. We can see two reasons for this – ﬁrst,
not all claims are supported and secondly, claims
can be supported by other claims. There are many
more supports than contradicts relations. This is
intuitive, as authors mainly argue by providing sup-
porting evidence for their own claims.

Table 3 shows the statistics on length of argu-
mentative components. While the background
claims and own claims are on average of similar
length (85 and 87 characters, respectively), they
are much longer than data components (average of
25 characters). This is intuitive given the domain
of the corpus, as facts in computer science often
require less explanation than claims. For example,
we noticed that authors often refer to tables and ﬁg-
ures as evidence for their claims. Similarly, when
claiming weaknesses or strengths of related work,
authors commonly provide references as evidence.
The argumentative structure of an individual pub-
lication corresponds to a forest of directed acyclic
graphs (DAG) with annotated argumentative com-
ponents as nodes and argumentative relations as
edges. Thus, to obtain further insight into struc-
tural properties of argumentation in scientiﬁc pub-
lications, in Table 4 we provide graph-based mea-
sures like the number of connected components
(i.e., subgraphs), the diameter, and the number of
standalone claims (i.e., nodes without incoming
or outgoing edges) and unsupported claims (i.e.,
nodes with no incoming supports edges).
Our

annotators identiﬁed an average of 141 connected
component per publication, with an average diame-
ter of 3. This indicates that either authors write very
short argumentative chains or that our annotators
had difﬁculties noticing long-range argumentative
dependencies.

On the one hand, there are at least 27 standalone
claims in each publication, that is claims, that are
not connected with any other components. On the
other hand, the maximum in-degree of a claim in a
publication, on average, is 6, indicating that there
are claims for which a lot of evidence is given. Intu-
itively, the claims for which more evidence is given
should be more prominent. We next run PageRank
(Page et al., 1999) on argumentation graphs of in-
dividual publications to identify most prominent
claims. We list a couple of examples of claims with
highest PageRank scores in Table 5. Somewhat
unexpectedly, in 30 out of 40 publications in the
dataset the highest ranked claim was a background
claim. This suggests that in computer graphics
authors emphasize more research gaps and motiva-
tion for their work than they justify its impact (for
which empirical results often sufﬁce).
Links to Other Rhetorical Aspects. We next in-
vestigate the interdependencies between the newly
added argumentative annotations and the existing
rhetorical annotations of the Dr. Inventor Corpus.
An inspection of dependencies between different
annotation layers in the corpus may indicate the
usefulness of computational approaches that aim
to exploit such interrelations. E.g., Bjerva (2017)
recently showed that the measure of mutual infor-
mation strongly correlates with performance gains
obtained by multi-task learning models.

In this work, we employ the measure of normal-
ized mutual information (NMI) (Strehl and Ghosh,
2003) to assess the amount of information shared
between the ﬁve annotation layers. NMI is a variant
of mutual information scaled to the interval [0, 1]

43Type

background claim

own claim

Pub.
A13
A21

A39

Claim with maximal PageRank score
’physical validity is often sacriﬁced for performance’
’a tremendous variety of materials exhibit this type of behavior’
’the solution to the problem of asymmetry is to modify the CG method so that
it can operate on equation (15), while procedurally applying the constraints
inherent in the matrix W at each iteration’

Table 5: Claims with maximum PageRank score in a publication.

AC
–
0.22
0.08
0.04
0.18

DR
–
–
0.11
0.10
0.10

SA
–
–
–
0.13
0.04

SR
–
–
–
–
0.01

AC
DR
SA
SR
CC

Table 6: Normalized mutual information between
different annotation layers.

through normalization with the entropy of each of
the two label sets. In Table 6 we show the NMI
scores for all pairs of annotations layers: argument
components (AC), discourse roles (DR), citation
contexts (CC), subjective aspects (SA), and sum-
mary relevances (SR). The strongest association
is found between argumentative components (AC)
and discourse roles (DR). Looking at the labels of
these two annotation layers, this seems plausible –
background claim (AC) is likely to appear in a sen-
tence of discourse role background (DR). Similarly,
own claims more frequently appear in sections de-
scribing the outcomes of the work. To conﬁrm
this intuition, we computed co-occurrence matri-
ces for pairs of label sets – indeed, the AC label
own claim most frequently appears together with
the discourse role approach and outcome, and the
background claim with discourse roles background
and challenge. Consider the following sentence:

”With the help of modeling tools or capture
devices, complicated 3D character models
are widely used in the ﬁelds of entertainment,
virtual reality, medicine, etc.”

It contains a general claim about the research area
(i.e., it is a background claim) and it also offers
background information in terms of the discourse
role. A similar set of intuitive label alignments
justiﬁes the higher NMI score between argumenta-
tive components (AC) and citation contexts (CC):
citation contexts often appear in sentences with a
background claim. Again, this is not surprising, as
authors typically reference other publications and

in order to motivate their work:

”An improvement based on addition of
auxiliary joints has been also proposed in
[ :::::Weber::::::
2000]. Although this reduces the
artifacts, the skin to joints relationship must
be re-designed after joint addition.”

In the above example, the wave-underlined text,
i.e. the citation, serves as the data for the under-
lined text which is the background claim stating a
research gap in the referenced work. At the same
time, the underlined text can be seen as the citation
context with the reference as target.

6 Conclusion

We presented an annotation scheme for argumen-
tation analysis in scientiﬁc publications. We anno-
tated the Dr. Inventor Corpus (Fisas et al., 2015,
2016) with an argumentation layer. The resulting
corpus, which is, to the best of our knowledge, the
ﬁrst argument-annotated corpus of scientiﬁc publi-
cations in English, enables (1) computational analy-
sis of argumentation in scientiﬁc writing and (2) in-
tegrated analysis of argumentation and other rhetor-
ical aspects of scientiﬁc text. We further provided
corpus statistics and graph-based analysis of the ar-
gumentative structure of the annotated publications.
Finally, we analyzed the dependencies between dif-
ferent rhetorical aspects, which can inform compu-
tational models aiming to jointly address multiple
aspects of scientiﬁc discourse. In the future, we
plan to extend the corpus with publications from
other domains and develop computational models
for the integrated analysis of scientiﬁc writing.

Acknowledgments

This research was partly funded by the German Re-
search Foundation (DFG), grant number EC 477/5-
1 (LOC-DB). We thank our annotators for their
very dedicated annotation effort.

44References
Rob Abbott, Brian Ecker, Pranav Anand, and Mari-
lyn A Walker. 2016. Internet argument corpus 2.0:
An sql schema for dialogic social media and the cor-
pora to go with it. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation, pages 4445–4452, Portoroˇz, Slowenia.
European Language Resources Association.

Khalid Al Khatib, Henning Wachsmuth, Johannes
Kiesel, Matthias Hagen, and Benno Stein. 2016.
A news editorial corpus for mining argumentation
In Proceedings of COLING 2016, the
strategies.
26th International Conference on Computational
Linguistics: Technical Papers, pages 3433–3443,
Osaka, Japan. The COLING 2016 Organizing Com-
mittee.

Jean-Claude Anscombre and Oswald Ducrot. 1983.
L’argumentation dans la langue. Editions Mardaga.

Trevor JM Bench-Capon. 1998. Speciﬁcation and im-
plementation of toulmin dialogue game. In Proceed-
ings of the 11th Conference on Legal Knowledge
Based Systems, pages 5–20, Groningen, Netherlands.
Foundation for Legal Knowledge Based Systems.

Jamal Bentahar, Bernard Moulin, and Micheline
Blanger. 2010. A taxonomy of argumentation mod-
els used for knowledge representation. Artiﬁcial In-
telligence Review, 33(3):211–259.

Or Biran and Owen Rambow. 2011.

Identifying jus-
In Fifth IEEE Inter-
tiﬁcations in written dialogs.
national Conference on Semantic Computing, pages
162–168, Palo Alto, CA, USA. IEEE.

Johannes Bjerva. 2017. Will my auxiliary tagging
task help? estimating auxiliary tasks effectivity in
In Proceedings of the 21st
multi-task learning.
Nordic Conference on Computational Linguistics,
pages 216–220, Gothenburg, Sweden. Association
for Computational Linguistics.

Catherine Blake. 2010. Beyond genes, proteins, and
abstracts: Identifying scientiﬁc claims from full-text
biomedical articles. Journal of Biomedical Informat-
ics, 43(2):173–189.

Lutz Bornmann and R¨udiger Mutz. 2015. Growth rates
of modern science: A bibliometric analysis based
on the number of publications and cited references.
Journal of the Association for Information Science
and Technology, 66(11):2215–2222.

Phan Minh Dung. 1995. On the acceptability of argu-
ments and its fundamental role in nonmonotonic rea-
soning, logic programming and n-person games. Ar-
tiﬁcial Intelligence, 77(2):321–357.

Beatriz Fisas, Francesco Ronzano, and Horacio Sag-
gion. 2016. A multi-layered annotated corpus of
In Proceedings of the Interna-
scientiﬁc papers.
tional Conference on Language Resources and Eval-
uation, pages 3081–3088, Portoroˇz, Slovenia. Euro-
pean Language Resources Association.

Beatriz Fisas, Horacio Saggion, and Francesco Ron-
zano. 2015. On the discoursive structure of com-
In Proceedings of
puter graphics research papers.
The 9th Linguistic Annotation Workshop, pages 42–
51, Denver, CO, USA. Association for Computa-
tional Linguistics.

G Nigel Gilbert. 1976. The transformation of research
ﬁndings into scientiﬁc knowledge. Social Studies of
Science, 6(3-4):281–306.

G Nigel Gilbert. 1977. Referencing as persuasion. So-

cial Studies of Science, 7(1):113–122.

Goran Glavaˇs and Jan ˇSnajder. 2013. Exploring coref-
erence uncertainty of generically extracted event
In International Conference on Intelli-
mentions.
gent Text Processing and Computational Linguistics,
pages 408–422. Springer.

Nancy Green. 2014. Towards creation of a corpus
for argumentation mining the biomedical genetics re-
search literature. In Proceedings of the First Work-
shop on Argumentation Mining, pages 11–18, Balti-
more, Maryland. Association for Computational Lin-
guistics.

Nancy Green. 2015. Annotating evidence-based ar-
In 2015 IEEE
gumentation in biomedical text.
International Conference on Bioinformatics and
Biomedicine, pages 922–929, Washington, D.C.,
USA. IEEE.

Nancy Green. 2016.

Implementing argumentation
schemes as logic programs. In The 16th Workshop
on Computational Models of Natural Argument, vol-
ume 30, New York, USA. CEUR-WS.

Nancy Green, E Cabrio, S Villata, and A Wyner. 2014.
Argumentation for scientiﬁc claims in a biomedical
research article. In Proceedings of the Workshop on
Frontiers and Connections between Argumentation
Theory and Natural Language Processing, pages 21–
25, Forl-Cesena, Italy. CEUR-WS.

Ivan Habernal and Iryna Gurevych. 2016. Which ar-
gument is more convincing? Analyzing and predict-
ing convincingness of web arguments using bidirec-
In Proceedings of the 54th Annual
tional LSTM.
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), page 1122, Berlin,
Germany. Association for Computational Linguis-
tics.

George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and reliability in infor-
mation retrieval. Journal of the American Medical
Informatics Association, 12(3):296–298.

Khandaker Shahidul Islam. 2007. An Approach to Ar-
gumentation Context Mining from Dialogue History
in an e-Market Scenario. In Proceedings of the 2Nd
International Workshop on Integrating Artiﬁcial In-
telligence and Data Mining - Volume 84, AIDM
’07, pages 73–81, Darlinghurst, Australia, Australia.
Australian Computer Society, Inc.

45Rahul Jha, Amjad-Abu Jbara, Vahed Qazvinian, and
Dragomir R. Radev. 2017. NLP-driven citation anal-
ysis for scientometrics. Natural Language Engineer-
ing, 23(1):93–130.

In Pro-
from the Discourse Structure Perspective.
ceedings of the Workshop on Frontiers and Connec-
tions between Argumentation Theory and Natural
Language Processing, pages 21–25.

Alexander Strehl and Joydeep Ghosh. 2003. Cluster en-
sembles – a knowledge reuse framework for combin-
ing multiple partitions. Journal of Machine Learn-
ing Research, 3:583–617.

Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
In Proceedings of the
tation in research articles.
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, pages 110–
117, Bergen, Norway. Association for Computa-
tional Linguistics.

Simone Teufel and Marc Moens. 1999a. Argumenta-
tive classiﬁcation of extracted sentences as a ﬁrst
step towards ﬂexible abstracting. In Advances in au-
tomatic Text Summarization, pages 155–171, Cam-
bridge, MA, USA. MIT Press.

Simone Teufel and Marc Moens. 1999b. Discourse-
level argumentation in scientiﬁc articles: Human
In Towards Standards
and automatic annotation.
and Tools for Discourse Tagging, Workshop, Mary-
land, MA, USA. Association for Computational Lin-
guistics.

Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: Evidence from chemistry and com-
In Proceedings of the 2009
putational linguistics.
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3, pages 1493–1502, Ed-
inburgh, Scotland. Association for Computational
Linguistics.

Stephen E. Toulmin. 2003. The Uses of Argument, up-

dated edition. Cambridge University Press.

Bart Verheij. 2005. Evaluating Arguments Based on
Toulmins Scheme. Argumentation, 19(3):347–371.

Douglas Walton, Chris Reed, and Fabrizio Macagno.
2008. Argumentation Schemes. Cambridge Univer-
sity Press.

Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches
to Text Mining Arguments from Legal Cases.
In
Semantic Processing of Legal Texts, pages 60–79.
Springer Berlin Heidelberg.

Charles Jochim and Hinrich Sch¨utze. 2012. Towards
a generic and ﬂexible citation classiﬁer based on
In Proceedings of
a faceted classiﬁcation scheme.
the 24th International Conference on Computational
Linguistics, pages 1343–1358, Mumbai, India. The
COLING 2012 Organizing Committee.

Christian Kirschner, Judith Eckle-Kohler, and Iryna
Gurevych. 2015. Linking the thoughts: Analysis
of argumentation structures in scientiﬁc publications.
In Proceedings of the 2nd Workshop on Argumenta-
tion Mining, pages 1–11, Denver, CO, USA. Associ-
ation for Computational Linguistics.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the ﬁfteenth conference on
computational natural language learning: Shared
task, pages 28–34. Association for Computational
Linguistics.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
In Proceedings of the 2017 Conference on
lution.
Empirical Methods in Natural Language Processing,
pages 188–197.

Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin R Batchelor. 2010. Corpora for the Con-
ceptualisation and Zoning of Scientiﬁc Papers.
In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.

Jing Lu and Vincent Ng. 2018. Event coreference res-
In

olution: A survey of two decades of research.
IJCAI, pages 5479–5486.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Chris Reed, Raquel Mochales Palau, Glenn Rowe,
and Marie-Francine Moens. 2008. Language re-
In Proceedings
sources for studying argument.
of the Sixth International Conference on Language
Resources and Evaluation, pages 2613–2618, Mar-
rakesh, Marocco. European Language Resources As-
sociation.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Christian Stab, Christian Kirschner, Judith Eckle-
Kohler, and Iryna Gurevych. 2014. Argumentation
Mining in Persuasive Essays and Scientiﬁc Articles

46
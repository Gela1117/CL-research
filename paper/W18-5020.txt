A Context-aware Convolutional Natural Language Generation model for

Dialogue Systems

Sourab Mangrulkar

National Institute of Technology Goa
100rabmangrulkar@gmail.com

Suhani Shrivastava

National Institute of Technology Goa

suhani1396@gmail.com

Veena Thenkanidiyoor

National Institute of Technology Goa

veenat@nitgoa.ac.in

Dileep Aroor Dinesh

Indian Institute of Technology Mandi
addileep@iitmandi.ac.in

Abstract

language generation (NLG)

Natural
is
an important component in spoken dia-
log systems (SDSs). A model for NLG
involves sequence to sequence learning.
State-of-the-art NLG models are built us-
ing recurrent neural network (RNN) based
sequence to sequence models (Duˇsek and
Jurcicek, 2016a). Convolutional sequence
to sequence based models have been used
in the domain of machine translation but
their application as natural language gen-
erators in dialogue systems is still unex-
plored. In this work, we propose a novel
approach to NLG using convolutional neu-
ral network (CNN) based sequence to se-
quence learning. CNN-based approach al-
lows to build a hierarchical model which
encapsulates dependencies between words
via shorter path unlike RNNs.
In con-
trast to recurrent models, convolutional
approach allows for efﬁcient utilization of
computational resources by parallelizing
computations over all elements, and eases
the learning process by applying constant
number of nonlinearities. We also pro-
pose to use CNN-based reranker for ob-
taining responses having semantic corre-
spondence with input dialogue acts. The
proposed model is capable of entrainment.
Studies using a standard dataset shows the
effectiveness of the proposed CNN-based
approach to NLG.
Introduction

1
In task-speciﬁc spoken dialogue systems (SDS),
the function of natural language generation (NLG)
components is to generate natural language re-
sponse from a dialogue act (DA) (Young et al.,
2009). DA is a meaning representation specify-
ing actions along with various attributes and their

values. NLG plays a very important role in real-
izing the overall quality of the SDS. Entrainment
to users way of speaking is essential for generat-
ing more natural and high quality natural language
responses. Most of the approaches for incorpo-
rating entrainment are rule-based models. Recent
advances have been in the direction of develop-
ing a fully trainable context aware NLG model
(Duˇsek and Jurcicek, 2016a). However, all these
approaches are based on recurrent sequence to se-
quence architecture.

Convolutional neural networks are largely un-
explored in the domain of NLG for SDS inspite
of having several advantages (Waibel et al., 1989;
LeCun and Bengio, 1995). Recurrent networks de-
pend on the computations of previous time step
and thus inhibits parallelization within a sequence.
Convolutional networks on the other hand, allows
parallelization within a sequence resulting in ef-
ﬁcient use of GPUs and other computational re-
sources (Gehring et al., 2017). Multi-block (multi-
layer) convolutional networks enable controlling
the upper bound on the effective context size and
form a hierarchical structure.
In contrast to the
sequential structure of RNNs, hierarchical struc-
ture provides shorter paths for modeling long-
range dependencies. Recurrent networks apply
variable number of nonlinearities to the inputs,
whereas convolutional networks apply ﬁxed num-
ber of nonlinearities which simpliﬁes the learning
(Gehring et al., 2017).

In this paper, we present a novel approach
of using convolutional sequence to sequence
model (ConvSeq2Seq) for the task of NLG. Con-
vSeq2Seq generator is an encoder decoder model
where convolutional neural networks (CNNs) are
used to build both encoder and decoder states. It
uses multi-step attention mechanism.
In the de-
coding phase, beam search is implemented and n-
best natural language responses are chosen. The
n-best beam search responses from ConvSeq2Seq

ProceedingsoftheSIGDIAL2018Conference,pages191–200,Melbourne,Australia,12-14July2018.c(cid:13)2018AssociationforComputationalLinguistics191generator may have some missing and/or irrele-
vant information. To address this, we propose to
rank the n-best outputs from ConvSeq2Seq gener-
ator using convolutional reranker (CNN reranker).
CNN reranker implements one dimensional con-
volution on beam search responses and generates
binary vectors. These binary vectors are used
to penalize the responses having missing and/or
irrelevant information. We evaluate our model
on the Alex Context natural language generation
(NLG) dataset of Duˇsek and Jurcicek (2016a) and
demonstrate that our model outperforms the RNN-
based model of Duˇsek and Jurcicek (2016a) (TGen
model) in automatic metrics. Training time of pro-
posed model is observed to be signiﬁcantly lower
than TGen model. The main contributions of this
work are (i) ConvSeq2Seq generator for NLG and
(ii) CNN-based reranker for ranking n-best beam
search responses for obtaining semantically appro-
priate responses with respect to input DA.

The rest of this paper is organized as follows.
Section 2 gives a brief review of different ap-
proaches to NLG. In Section 3, proposed convolu-
tional natural language generator (ConvSeq2Seq)
is described along with CNN reranker. The ex-
perimental studies are presented in Section 4 and
conclusions are given in Section 5.

2 Related Work

Natural language generation (NLG) task is divided
into two phases: sentence planning and surface re-
alization. Sentence planning generates intermedi-
ate structure such as dependency trees or templates
modeling the input semantic symbols. Surface re-
alization phase converts the intermediate structure
into the ﬁnal natural language response.

Conventional approaches to NLG are rule based
approaches (Stent et al., 2004; Walker et al.,
2002). Most recent NLG approaches include se-
quence to sequence RNN models (Wen et al.,
2015a,b; Duˇsek and Jurcicek, 2016b,a). Sequence
to sequence learning is to map the input sequence
to a ﬁxed sized vector using one RNN, and then to
map the vector to the target sequence with another
RNN. In (Wen et al., 2015a), a sequence to se-
quence RNN model is used with some decay factor
to avoid vanishing gradient problem. The n-best
outputs generated by the model are ranked using a
CNN-based reranker. The model also uses a back-
ward sequence to sequence RNN reranker to fur-
ther improve the performance. Model proposed by

Wen et al. (2015b) is a statistical language gener-
ator based on a semantically controlled long-short
term memory (LSTM) structure. The LSTM gen-
erator can learn from unaligned data by jointly op-
timizing sentence planning and surface realization
using a simple cross entropy training criterion, and
language variation can be easily achieved by sam-
pling from output candidates.

Model proposed by Duˇsek and Jurcicek (2016b)
serves as a sequence to sequence generation model
for SDS which doesn’t take into account the con-
text. The model uses single layer sequence to se-
quence RNN encoder decoder architecture along
with attention mechanism to generate n-best out-
put utterances. It then uses RNN reranker to rank
the n-best outputs of generator to get the utterance
which best describes the input DA. The model can
also be used to generate deep syntax trees which
can be converted to output utterance using a sur-
face realization mechanism. This model is context
unaware because it takes into account only the in-
put DA and no preceding user utterance(s). This
leads to generation of very rigid responses and also
inhibits ﬂexible interactions. Context awareness
adapts/entrains to the user’s way of speaking and
thereby generates responses of high quality and
naturalness. The semantic meaning which is re-
quired to be given in response to a query is very
well modelled if context awareness is taken into
account. This leads to generation of more infor-
mative response.

Model proposed by Duˇsek and Jurcicek (2016a)
serves as a baseline sequence to sequence gener-
ation model (TGen model) for SDS which takes
into account the context. The model takes into
account the preceding user utterance while gener-
ating natural language output. The model imple-
mented three modiﬁcations to the model proposed
by Duˇsek and Jurcicek (2016b). The ﬁrst modi-
ﬁcation was prepending context to the input DAs.
The second modiﬁcation was implementing a sep-
arate encoder for user utterances/contexts. The
third modiﬁcation was implementing a N-gram
match reranker. This reranker is based on n-gram
precision scores and promotes responses having
phrase overlaps with user utterances (Duˇsek and
Jurcicek, 2016a).

In the next section, we present the proposed
CNN-based sequence to sequence generator for
NLG.

192Figure 1: Pipeline of the proposed convolutional NLG model.

3 Proposed Approach
The pipeline of the proposed approach for NLG
is shown in Figure 1. Input DA with prepended
context is ﬁrst given to convolutional sequence
to sequence generator (ConvSeq2Seq) to get n-
best natural language responses or hypotheses (n
is beam size). These n-best hypotheses and binary
vector representation of input DA are given as in-
put to CNN reranker to get the misﬁt penalties of
the hypotheses. The n-best hypotheses and con-
text user utterance are given as input to the N-gram
match reranker to get bigram precision scores of
hypotheses. Final rank of each hypothesis i where
1 ≤ i ≤ n is calculated as follows:

ranki = log probabilityi
+(ω ∗ bigram precisioni)
−(W ∗ misf it penaltyi)

Here, we get log probabilities from ConvSeq2Seq
generator, bigram precision scores from N-gram
match reranker and misﬁt penalties from CNN
reranker. Here, ω and W are constants. We im-
plement the N-gram match reranker as given by
Duˇsek and Jurcicek (2016a). We describe the pro-
posed convolutional sequence to sequence gener-
ator in Section 3.1 and convolutional reranker in
Section 3.2.

3.1 ConvSeq2Seq Generator
The proposed sequence to sequence generator is
based on convolutional sequence to sequence ap-
proach proposed by Gehring et al. (2017)1.
It
1We use the implementation in the pytorch framework

(Gehring et al., 2017)

is a CNN-based encoder decoder architecture.
Figure 2 shows the working of proposed Con-
vSeq2Seq generator on an input instance from
training dataset.
In this architecture, CNNs are
used to compute the encoder states and decoder
states. This architecture is based on succession
of convolutional blocks/layers. Input sequence is
represented as a combination of word and posi-
tion embeddings. These embeddings are operated
upon by ﬁrst convolutional block and gated lin-
ear units (GLUs) to get the outputs for the ﬁrst
block. This can be seen in Figure 2 where only
one convolutional block is shown for representa-
tion purpose. The output from ﬁrst block is input
to the second convolutional block and this succes-
sion follows till the last convolutional block.

Stacking of several convolutional layers/blocks
allows to increase and control the effective context
size. For example, stacking 10 layers of convolu-
tional blocks, each having a kernel width of k=4,
results in effective context size of 31 elements.
Each output is dependent on 31 inputs. Stack-
ing of several convolutional layers/blocks results
in a hierarchical structure. In hierarchical struc-
ture, nearby elements interact at lower blocks and
distant elements at higher blocks.
It provides a
shorter path for modeling long-range dependen-
cies and eases discovery of compositional struc-
ture in sequences compared to sequential structure
of RNNs. For example, to model dependencies
between n words, only O ( n
k ) convolutional op-
erations would be required by CNN in contrast
to O(n) operations in RNN. RNNs over-process
the ﬁrst word and under-process the last word,

193Figure 2: Working of ConvSeq2Seq generator on an input instance from training dataset. Here, for
representation purpose, encoder and decoder consists of only one convolutional layer with kernel width
k=3. The encoder input sequence x =(how, far, is, that, inform, distance, X-distance) comprises of user
context “how far is that” prepended to input DA “inform distance X-distance”.

whereas a constant number of kernels and nonlin-
earities are applied to the inputs of CNN which
eases the learning process (Gehring et al., 2017).

The ConvSeq2Seq model uses position embed-
dings in addition to word embeddings in order to
get a sense of which part of the input sequence
it is currently processing (Gehring et al., 2017).
Let e = (w1 + p1,. . . ,ws + ps) be the input se-
quence representation, where w = (w1,. . . ,ws) and
p = (p1,. . . ,ps) are the the word embeddings and
positional embeddings of the input sequence x =
(x1,. . . ,xs) (having s elements) to the encoder net-
work respectively.
Intermediate states are com-
puted based on a ﬁxed number of input elements.

2

1,. . . ,zl

i+k/2] + bl

z) + zl−1

i−k/2, . . . , zl−1

In encoding phase, input is padded with k−1
ele-
ments on the left and right side with zero vectors.
For each block l, the output zl = (zl
s) is com-
puted as follows:
z[zl−1

i = ν(Wl
zl
Here, [zl−1
i+k/2] is the input A ∈ Rkd
z ∈ R2d
from the previous block, Wl
are parameters of convolution kernel and d is em-
bedding dimension. Let B ∈ R2d be the output of
convolution kernel. ν() is gated linear unit(GLU)
which is the nonlinearity function applied to the
output B of convolution kernel. Let zu be the en-
coder output from the last block u.

i−k/2, . . . , zl−1

z ∈ R2d×kd, bl

i

194Figure 3: 19 classes of CNN reranker.

Let g = (g1,. . . ,gt) be the representation of the
sequence that is being fed to the decoder network.
Computation of g is similar to that of encoder net-
Input to decoder is padded with k-1 ele-
work.
ments on both left and right side with zero vectors
to prevent decoder from having access to future
information. As a result, last k-1 intermediate de-
coder outputs are removed. In decoding phase, for
each block l, the output hl = (hl
t) is com-
puted as follows:

1,. . . ,hl

] + bl
q)

i

q[hl−1
i−k+1, . . . , hl−1
i = ν(Wl
ql
i = Wl
dql
i + bl
dl
d + gi
(cid:80)s
exp(dl
i.zu
j )
s(cid:88)
m=1 exp(dl
i.zu
m)

al
ij =

cl
i =

ij(zu
al
j + ej)
i + hl−1

i + ql

i

j=1
hl
i = cl

(1)
(2)

(3)

(4)

(5)

1, . . . , ql

t) is the intermediate decoder
Here, ql = (ql
output and its computation is similar to that of en-
coder network. For computing attention, current
intermediate decoder state ql
i is combined with the
embedding of the previous target element gi as
shown in Equation (2). Equation (3) computes at-
tention of i-th decoder state and j-th encoder out-
put element for the l-th decoder block. Equa-
tion (4) computes the conditional input which is
weighted sum of combination of encoder outputs
and input embeddings. Equation (5) computes the
current decoder output which is combination of
conditional input, intermediate decoder output and
i be the de-
previous layer decoder output. Let hL
coder output of i-th element and the ﬁnal decoding
block L. Distribution over T possible next target
elements yi+1 is computed as follows:

p(yi+1|y1, . . . , yi, x) = ζ(WohL

i + bo) ∈ RT

Here, ζ is softmax function, Wo and bo are the
weights and bias of fully connected linear layer.

3.2 CNN Reranker
The n-best beam search responses from Con-
vSeq2Seq model may have missing information

information.

and/or irrelevant
CNN reranker
reranks the n-best beam search responses and
heavily penalizes those responses which are not
semantically in correspondence with the input DA.
Responses having missing information and/or ir-
relevant information are heavily penalized. Con-
volutional networks are excellent feature extrac-
tors and have achieved state-of-the-art results in
many text classiﬁcation and sentence-level classi-
ﬁcation tasks such as sentiment analysis, question
classiﬁcation, etc (Kim, 2014; Kalchbrenner et al.,
2014). This classiﬁer takes as input a natural lan-
guage response and outputs a binary vector. Each
element of binary vector is a binary decision on
the presence of DA type or slot-value combina-
tions. For the dataset which we have used (Duˇsek
and Jurcicek, 2016a), there are 19 such classes of
DA types and slot-value combinations. These 19
classes are shown in Figure 3.

Input DAs are converted to similar binary vec-
tor. Hamming distance between the classiﬁer out-
put and binary vector representation of input DA
is considered as reranking penalty. The weighted
reranking penalties of all the n-best responses are
subtracted from their log-probabilities similar to
Duˇsek and Jurcicek (2016a).

The architecture and working of the CNN
reranker on an input instance from training dataset
is shown in Figure 4. It is based on the CNN ar-
chitecture proposed for sentence classiﬁcation by
Kim (2014). Input is a natural language response
x = (x1, x2, . . . , xn) where xi’s are word embed-
dings each having m dimensions, resulting in a in-
put matrix of n×m dimensions. Each ﬁlter has the
width equal to the size of word embeddings, i.e.,
m and its height speciﬁes the number of words it
will operate on. This one dimensional convolution
is followed by applying activation function and 1-
max pooling. The resulting feature vector has the
dimension equal to the total number of ﬁlters. This
penultimate layer is operated upon by a logistic
layer to output the binary vector. Given penulti-
mate layer feature vector t, the output binary vec-
tor y is computed as:

y = σ(t.Wf + b)

195Figure 4: Architecture and working of the CNN reranker on an input instance from training dataset.

Here, σ is sigmoid activation function, Wf is the
weight matrix and b is the bias vector.

The model proposed by Wen et al. (2015a)
implements a CNN reranker
that uses one-
dimensional ﬁlters where convolutional operations
are carried out on segments of words.
It uses
padding vectors. Proposed CNN reranker uses
two-dimensional ﬁlters which operate on complete
words rather than segments of words. This is more
intuitive and meaningful. Also, no padding is re-
quired. The feature vector from proposed CNN
reranker is v-dimensional whereas CNN reranker
by Wen et al. (2015a) outputs longer feature vec-
tors having dimension equal to v ∗ m, where v
= total number of ﬁlters and m = embedding
size. Thus, proposed CNN reranker requires lesser
number of computations.

4 Experimental Studies
The studies in this work are performed on
Alex Context natural language generation (NLG)
dataset (Duˇsek and Jurcicek, 2016a). This dataset
is intended for fully trainable NLG systems in
task-oriented spoken dialogue systems (SDS). It
is in the domain of public transport information
and has four dialogue act (DA) types namely re-
quest, inform, iconﬁrm and inform no match. It
contains 1859 data instances each having 3 tar-
get responses. Each data instance consists of a
preceding context (user utterance), source mean-
ing representation and target natural language re-
sponses/sentences. Data is delexicalized and split
into training, validation and test sets as done by
Duˇsek and Jurcicek (2016a). For training and val-
idation, the three paraphrases are used as separate

196instances. For evaluation they are used as three
target references.

Input to our ConvSeq2Seq generator is a DA
prepended with user utterance. This allows en-
trainment of the model to the user utterances. A
single dictionary is used for context utterances
and DA tokens. Our model is trained by min-
imizing cross-entropy error using Nesterov Ac-
celerated Gradient (NAG) optimizer (Nesterov,
1983). The hyper-parameters are chosen by cross-
validation method. Based on our experiments on
validation set, we use maximum sentences per
batch 20, learning rate 0.07, minimum learning
rate 0.00001, maximum number of epochs 2000,
learning rate shrink factor 0.5, clip-norm 0.5, en-
coder embedding dimension 100, decoder embed-
ding dimension 100, decoder output embedding
dimension 100 and dropout 0.3. Encoder part in-
cludes 10 layers/blocks, each having 100 units and
kernel width of 7. Decoder part includes 10 lay-
ers, each having 100 units and kernel width of 7.
For generating outputs on test set, we choose batch
size 128 and beam size 20.

For our CNN reranker, all the possible combi-
nations of DA tokens and its values are considered
as classes. We have 19 such classes. Each input
is a natural language sentence and each output is a
set of class labels. Training is done by minimizing
cross-entropy loss using Adam optimizer (Kingma
and Ba, 2015). Cross-entropy error is measured on
validation set after every 100 steps. Misclassiﬁca-
tion penalty for CNN reranker is set to 100. Based
on our experiments, we choose embedding dimen-
sion 128, ﬁlter sizes (3,5,7,9), number of ﬁlters 64,
dropout keep probability 0.5, batch size 100, num-
ber of epochs 100 and L2 regularization, λ=0.05.
The performance of the proposed ConvSeq2Seq
model for NLG is compared with that of TGen
model (Duˇsek and Jurcicek, 2016a). For com-
parison, we have considered NIST (Doddington,
2002), BLEU (Papineni et al., 2002), METEOR
(Denkowski and Lavie, 2014), ROUGE L (Lin,
2004) and CIDEr metrics (Vedantam et al., 2015).
For this study, we have considered script “mteval-
v13a-sig.pl” (version 13a) that implements these
metrics. This script was used for E2E NLG chal-
lenge (Novikova et al., 2017). We focus on the
evaluations using this version. Our model has also
been evaluated using the metric script “mteval-
v11b.pl” (version 11b) to compare our results with
those stated in (Duˇsek and Jurcicek, 2016a). The

13a version takes into account the closest refer-
ence length with respect to candidate length for
calculation of brevity penalty. This is in accor-
dance with IBM BLEU. On the contrary, 11b ver-
sion takes shortest reference length for measuring
brevity penalty. This is the reason behind higher
BLEU scores in the 11b version when compared to
13a version. Both the models have been evaluated
on ﬁve different metrics, with NIST and BLEU
scores being of atmost importance.

We have used N-gram match reranker with the
weight ω set to 1 based on experiments done on
validation set. When using 11b version for evalu-
ating automatic metrics, weight ω is set to 5.

4.1 Studies of the models using 13a version of

the metrics

The comparison of the performance of the pro-
posed model with that of TGen model using the
13a version of the metric implementation is given
in Table 1.
It is seen from Table 1 that there is
a slight improvement in the scores of our Con-
vSeq2Seq generator after using CNN reranker.
However, scores improve signiﬁcantly when N-
gram match reranker is used in addition to CNN
reranker. An improvement of 3.32 BLEU points is
seen. The best scores are obtained when ω is set
to 1 for N-gram match reranker.

ConvSeq2Seq model in combination with CNN
reranker and N-gram match reranker outperforms
TGen model with N-gram match reranker in all
the metrics, with a difference of 0.65 in terms
of NIST score which is 8% more than the TGen
NIST score on this setup. ConvSeq2Seq model
with CNN reranker outperforms TGen model with
RNN reranker in all the metrics, with a differ-
ence of 1.8 in terms of NIST score which is 27%
more than the TGen NIST score on this setup. In
the domain of NLG, NIST score is found to have
highest correlation with human based judgments
when compared to other metrics (Belz and Reiter,
2006). In Table 1, the bold numbers indicate the
best scores.

4.2 Studies of the models using 11b version of

the metrics

The comparison of the performance of the pro-
posed model with that of TGen model using the
11b version of the metric implementation is given
in Table 2. A slight improvement in the scores
of our ConvSeq2Seq generator after using CNN
reranker is seen in Table 2 except for BLEU score.

197Evaluation metric version 13a
Baseline Model: TGen(RNN+RNN)
Prepending context (using RNN reranker)
+ N-gram match reranker
ConvSeq2Seq(CNN + CNN)
Prepending context
+ CNN reranker
+ CNN reranker + N-gram match reranker

NIST BLEU METEOR ROUGE L CIDEr

6.660
7.956

62.13
65.49

8.450
8.474
8.608

62.08
62.23
65.55

0.4434
0.4655

0.4670
0.4692
0.4762

0.7269
0.7547

0.7557
0.7561
0.7654

3.6956
3.8515

3.7281
3.7412
3.8725

Table 1: Different automatic metric scores of TGen model (RNN+RNN) and proposed model
(CNN+CNN) on test data using evaluation metric version 13a.

Evaluation metric version 11b
Baseline Model: TGen(RNN+RNN)
Prepending context (using RNN reranker)
+ N-gram match reranker
ConvSeq2Seq(CNN + CNN)
Prepending context
+ CNN reranker
+ CNN reranker + N-gram match reranker

NIST BLEU METEOR ROUGE L CIDEr

6.456
7.772

63.87
69.26

-
-

8.450
8.474
7.920

63.02
62.93
69.60

0.4670
0.4692
0.4534

-
-

0.7557
0.7561
0.7238

-
-

3.7281
3.7412
3.6955

Table 2: Different automatic metric scores of TGen model (RNN+RNN) and proposed model
(CNN+CNN) on test data using evaluation metric version 11b.

We see an improvement of 6.7 BLEU points when
using N-gram match reranker with ω set to 5. A
decrease in scores of other metrics is seen. These
inconsistencies are due to the way brevity penalty
is calculated for computing BLEU scores in 11b
version of metric implementation.

BLEU and NIST scores of the TGen model
given in Table 2 match with that represented
in (Duˇsek and Jurcicek, 2016a). The scores of
our model shows slight improvement over TGen
model.

The studies done to compare the proposed
model with the TGen model, show the effective-
ness of considering the CNN-based approach to
NLG. Studies also show that CNN reranker out-
performs the RNN reranker. Further, CNN-based
model is expected to take less time to train when
compared to RNN-based model. We compare the
time taken by the models in the next section.

4.3 Studies on the models based on training

time

In this section, we compare the proposed model
with that of TGen based on time taken for train-
ing. All the experiments were performed on 8GB
Nvidia GeForce GTX 1080 GPU. The time taken
for training ConvSeq2Seq generator is approxi-
mately 4 minutes. The time taken for training

CNN reranker is approximately 2 minutes. The
time taken for training TGen model is approxi-
mately 128 minutes which is 21 times more than
our ConvSeq2Seq generator in combination with
CNN reranker. This shows the effectiveness of
using convolutional neural network in building a
model for NLG than using recurrent neural net-
work based approach used in TGen.

5 Conclusion and Future Work
In this paper a novel approach to natural language
generation (NLG) using convolutional sequence
to sequence learning is proposed. The convolu-
tional model for NLG is found to encapsulate de-
pendencies between words in a better way than re-
current neural network (RNN) based sequence to
sequence learning. It is also seen that the convo-
lutional approach makes efﬁcient use of compu-
tational resources. The proposed model in com-
bination with CNN reranker and N-gram match
reranker is capable of entraining to users’ way
of speaking. Studies conducted on a standard
dataset shows the effectiveness of proposed ap-
proach which outperforms the conventional RNN-
based approach.

In future, we propose to perform human based
evaluations to support the present performance of
the model.

198References
A. Belz and E. Reiter. 2006.

Comparing Auto-
matic and Human Evaluation of NLG Systems.
In 11th Conference of
the European Chapter
of the Association for Computational Linguistics.
http://www.aclweb.org/anthology/E06-1040.

M. Denkowski and A. Lavie. 2014. Meteor Universal:
Language Speciﬁc Translation Evaluation for Any
Target Language. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation. Associa-
tion for Computational Linguistics, pages 376–380.
https://doi.org/10.3115/v1/W14-3348.

G. Doddington. 2002.

Automatic Evaluation of
Machine Translation Quality Using N-gram
In Proceedings of
Co-occurrence Statistics.
the Second International Conference on Hu-
man
Technology Research. Mor-
gan Kaufmann Publishers
San Fran-
cisco, CA, USA, HLT ’02, pages 138–145.
http://dl.acm.org/citation.cfm?id=1289189.1289273.

Language

Inc.,

O. Duˇsek and F. Jurcicek. 2016a. A Context-aware
Natural Language Generator for Dialogue Systems.
In Proceedings of the 17th Annual Meeting of the
Special Interest Group on Discourse and Dialogue.
Association for Computational Linguistics, pages
185–190. https://doi.org/10.18653/v1/W16-3622.

O. Duˇsek and F. Jurcicek. 2016b.

Sequence-to-
Sequence Generation for Spoken Dialogue via Deep
In Proceedings of the
Syntax Trees and Strings.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics, pages 45–51.
https://doi.org/10.18653/v1/P16-2008.

J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N.
Dauphin. 2017. Convolutional Sequence to Se-
In ICML. PMLR, volume 70 of
quence Learning.
Proceedings of Machine Learning Research, pages
1243–1252.

N. Kalchbrenner, E. Grefenstette, and P. Blunsom.
2014. A Convolutional Neural Network for Mod-
In Proceedings of the 52nd An-
elling Sentences.
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Associa-
tion for Computational Linguistics, pages 655–665.
https://doi.org/10.3115/v1/P14-1062.

Y. Kim. 2014. Convolutional Neural Networks for
the
Sentence Classiﬁcation.
2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, pages 1746–1751.
https://doi.org/10.3115/v1/D14-1181.

In Proceedings of

D. P. Kingma and J. L. Ba. 2015. Adam: A Method for
In Proceedings of Inter-
Stochastic Optimization.
national Conference on Learning Representations.
pages 1–13.

Y. LeCun and Y. Bengio. 1995. Convolutional net-
works for images, speech, and time series. MIT
Press, Cambridge, MA, USA, 1st edition.

C.-Y. Lin.

2004.
for Automatic
In
http://www.aclweb.org/anthology/W04-1013.

ROUGE: A Package
Summaries.
Out.

Evaluation
Summarization

of
Branches

Text

Y. Nesterov. 1983. A method for unconstrained con-
vex minimization problem with the rate of conver-
gence o(1/k2) . Doklady AN USSR 269:543–547.
https://ci.nii.ac.jp/naid/20001173129/en/.

J. Novikova, O. Duˇsek, and V. Rieser. 2017. The E2E
Dataset: New Challenges for End-to-End Genera-
tion. In Proceedings of the 18th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue. Saarbr¨ucken, Germany. ArXiv:1706.09254.
https://arxiv.org/abs/1706.09254.

Bleu:

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002.
a Method for Automatic Eval-
In Proceed-
uation of Machine Translation.
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics. pages 311–318.
http://www.aclweb.org/anthology/P02-1040.

A. Stent, R. Prasad, and M. Walker. 2004. Trainable
Sentence Planning for Complex Information Pre-
In Proceed-
sentation in Spoken Dialog Systems.
ings of the 42nd Annual Meeting on Association for
Computational Linguistics. Association for Compu-
tational Linguistics, Stroudsburg, PA, USA, ACL
’04. https://doi.org/10.3115/1218955.1218966.

R. Vedantam, C. L. Zitnick, and D. Parikh. 2015.
CIDEr: Consensus-based image description evalu-
In CVPR. IEEE Computer Society, pages
ation.
4566–4575.

A. Waibel, T. Hanazawa, G. Hinton, K. Shikano,
and K. J. Lang. 1989. Phoneme recognition us-
IEEE Transac-
ing time-delay neural networks.
tions on Acoustics, Speech, and Signal Processing
37(3):328–339. https://doi.org/10.1109/29.21701.

M. A. Walker, O. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken dialogue
using boosting. Computer Speech & Language
16(3-4):409–433.
https://doi.org/10.1016/S0885-
2308(02)00027-X.

T.-H. Wen, M. Gasic, D. Kim, N. Mrksic, P.-H. Su,
D. Vandyke, and S. Young. 2015a. Stochastic Lan-
guage Generation in Dialogue using Recurrent Neu-
ral Networks with Convolutional Sentence Rerank-
In Proceedings of the 16th Annual Meeting
ing.
of the Special Interest Group on Discourse and Di-
alogue. Association for Computational Linguistics,
pages 275–284. https://doi.org/10.18653/v1/W15-
4639.

199T.-H. Wen, M. Gasic, N. Mrkˇsi´c, P.-H. Su, D. Vandyke,
and S. Young. 2015b.
Semantically Condi-
tioned LSTM-based Natural Language Genera-
In Proceed-
tion for Spoken Dialogue Systems.
ings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1711–1721.
https://doi.org/10.18653/v1/D15-1199.

S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2009. The Hid-
den Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language 24(2):150.
https://doi.org/10.1016/j.csl.2009.04.001.

200
An Evaluation of Information Extraction Tools for Identifying Health

Claims in News Headlines

School of Economics and Management

School of Information Studies

Shi Yuan

Beihang University

Beijing, China

Bei Yu

Syracuse University
Syracuse, NY, USA

ystone1025@buaa.edu.cn

byu@syr.edu

Abstract

This study evaluates the performance of four information extraction tools (extractors) on iden-
tifying health claims in health news headlines. A health claim is deﬁned as a triplet: IV (what
is being manipulated), DV (what is being measured) and their relation. Tools that can identify
health claims provide the foundation for evaluating the accuracy of these claims against author-
itative resources. The evaluation result shows that 26% headlines do not include health claims,
and all extractors face difﬁculty separating them from the rest. For those with health claims,
OPENIE-5.0 performed the best with F-measure at 0.6 level for extracting “IV-relation-DV”.
However, the characteristic linguistic structures in health news headlines, such as incomplete
sentences and non-verb relations, pose particular challenge to existing tools.

1

Introduction

Mass media is a major source of information about health-related research, policies, and business. On
average, four in ten American adults reported following health news stories closely. Thus, the quality
of health news plays an important role in public understanding of health science (Brodie et al., 2003).
However, inaccuracy in health news has raised concerns among scientists, journalists, and the general
public. The lack of training has been identiﬁed as the main cause of low-quality health news stories
(Sarri et al., 1998; Voss, 2002).

This study aims for using NLP techniques to identify inaccuracy in health news reporting. The re-
sulting tools may be used for monitoring the quality of health news and providing training examples for
journalists and readers. To achieve this ultimate goal, we propose a two-step solution: ﬁrst extract the
health claims in news stories and then verify these claims against reliable sources, such as original re-
search publications. In this study we focus on the ﬁrst step to extract health claims from news headlines,
because headlines serve the role of attracting readers to click and read the whole story (NPR, 2018), and
thus the inaccuracies in headlines may be more consequential than those in the main stories.

FDA (2009) deﬁned a health claim as “the relationship between a substance and a disease or health-
related condition”. Sumner et al.
(2014) deﬁned it more broadly as a triplet of three elements: In-
dependent Variable (IV), Dependent Variable (DV), and their relation. IV is deﬁned as what is being
manipulated, DV as what is being measured, and relation as the words that describe the link between
IV and DV. They have applied this deﬁnition for manually examining exaggerations in health-related
claims in science publications, press releases and news articles. Therefore, we adopt this deﬁnition to
represent health claims. For example, in the headline “Drug suppresses spread of breast cancer caused
by stem-like cells”, the “IV-relation-DV” is “drug; suppresses; spread of breast cancer”.

Entity-relation extraction has been a fundamental task in the area of information extraction. Some
general-purpose tools have been developed to extract entities and relations, such as OPENIE-5.0
(Mausam, 2016), OLLIE (Mausam et al., 2012) and REVERB (Fader et al., 2011; Etzioni et al., 2011).
Some tools were developed for speciﬁc domains, such as SemRep for extracting relations in biomedi-
cal publications (Rindﬂesch and Marcelo, 2003). However, the health news headlines sometimes have

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/

ProceedingsoftheWorkshoponEventsandStoriesintheNews,pages34–43SantaFe,NewMexico,USA,August20,2018.34unique grammatical structures compared to regular sentences with “subject-predicate-object” structures.
For example, the noun “prevalence” describes the relation in the headline “Prevalence of estrogen recep-
tor mutations in patients with metastatic breast cancer”. Therefore, existing information extraction tools
may encounter challenges in identifying “IV-relation-DV” from health news headlines.

In this paper, we evaluate popular information extraction tools for identifying health claims in the
format of “IV-relation-DV” triples in health news headlines. The result is expected to shed light on the
directions for improving the existing tools. The rest of paper is organized as follows. Section 2 reviews
the state-of-the-art information extraction tools. Section 3 describes the construction of the benchmark
data set. Section 4 presents the evaluation methods and results. Section 5 discusses the challenges that
current tools face and offers suggestions for adaptation. Section 6 concludes the paper.

2 Related Work on Entity-relation Extraction

Studies in open information extraction aim for recognizing general-purpose “subject-relation-object”
triplets from text. Based on the order of entity and relation identiﬁcation, existing methods can be
summarized into three types: identify entities ﬁrst and then relations, identify relations ﬁrst and then
entities, and simultaneously identify both.

The ﬁrst type of studies identiﬁes entities ﬁrst and then speciﬁes some patterns to extract relations.
REXTOR (Katz and Lin, 2000) is an early system that uses grammar rules for entity and relation ex-
traction. Later, W OEP arse (Wu and Weld, 2010), trained on Wikipedia articles, used dependency route
patterns to decide whether two entities have relations. Besides, Relnoun (Pal and Mausam, 2016) was
developed to extract relations from compound noun phrases, such as “Collins; be director of; NIH” in
“Collins, the director of NIH”. The designs of these methods raise some questions regarding their adapt-
ability to our task. For REXTOR, the grammar rules would link entities to several types of relations that
do not apply to our task, such as “is subject of”. W OEP arse would match entities to what appear in
Wikipedia; however, IVs and DVs in our task may be new and thus not yet included in Wikipedia. For
Relnoun, health claims in our task are not usually expressed in compound noun phrases.

Different from the ﬁrst method, some studies prefer recognizing relations ﬁrst. For example, REVERB
(Fader et al., 2011; Etzioni et al., 2011) ﬁrst extracts the longest word sequence that satisﬁes certain
syntactic and lexical constraints as a relation. It then searches for the entities as two noun phrases that
are nearest to the relation phrase, one on left, one on right. Experiment results have shown that REVERB
has 30% higher AUC than W OEP arse based on the precision-recall curve (Etzioni et al., 2011). Similar
to REVERB, SRLIE (Christensen et al., 2011) is also a verb-centric system. It ﬁrst identiﬁes all verbs and
their modiﬁers, and then extracts the verbs with at least two modiﬁers as relations. However, REVERB
may mistakenly identify modiﬁers to IVs and DVs as entities when they are closer to the relation verbs.
In Comparison, SRLIE tends to ignore the modiﬁers to IVs and DVs.

The third method is designed for simultaneously identifying entities and relations. This method usu-
ally depends on “subject-relation-object” patterns. For example, OLLIE (Mausam et al., 2012) uses
dependency parsing patterns to identify triplets. Before OLLIE, TEXTRUNNER (Banko et al., 2007),
a CRF-based system, uses part-of-speech tags for triplet identiﬁcation. Empirical results showed that
OLLIE performed better than REVERB based on precision-yield curve (Mausam et al., 2012), and RE-
VERB better than TEXTRUNNER based on precision-recall curve (Etzioni et al., 2011). Furthermore,
considering the need for identifying numerical relations, BONIE (Saha et al., 2017) applied numerical
dependency patterns to extract triplets that a number or a quantity-unit phrase, such as “Hong Kong; has
labour force of; 3.5 million” in “Hong Kong’s labour force is 3.5 million”.

Based on OLLIE, a new generation extractor named OPENIE-5.0 has been developed (Mausam,
2016).
It combined SRLIE, Relnoun, BONIE and ListExtraction (Extraction from conjunctive sen-
tences). Compared by precision-yield curve, OPENIE-5.0 is better than both OLLIE and REVERB
(Mausam, 2016).

Overall, the aforementioned extractors rely on structural information in complete sentences to iden-
tify triplets. However, news headlines are often times not complete sentences. Therefore, whether the
applicability of these extractors remain an open question.

35In addition, because health news involves many biomedical concepts, we also reviewed information
extractors in the biomedicine domain. SemRep (Rindﬂesch and Marcelo, 2003) is the state-of-the-art
tool to identify semantic predications from biomedical text. It is a widely-used, rule-based system. The
rules were derived from phrase structures (e.g., appositive structures). Furthermore, SemRep also relies
on UMLS, a biomedical knowledge database, to identify concepts and relations. Besides SemRep, Dey
et al. (2007) also proposed a system, which summarized PubMed articles by combining entity-relation
structures into a network. The method for entity-relation structure identiﬁcation depends on dependency
relation rules. In comparison, SemRep suits our task better because the goal of Dey et al. (2007) is for
text summarization instead of information extraction.

Based on the above review of the strength and weakness of the information extraction tools, we choose
to evaluate four tools that best suit our task for extracting “IV-relation-DV” triplets in health news head-
lines: two representative systems of different methods (REVERB and OLLIE), a combination system
(OPENIE-5.0) and a speciﬁc tool tailored to biomedicine (SemRep).

3 Benchmark Dataset Construction

We created a benchmark data set for evaluating the information extraction tools. This section describes
the process of data collection, annotation, and validation.

3.1 Data Collection
ScienceDaily.com is a large website that aggregates science news. We collected all health news
from ScienceDaily.com in 2016 and 2017, and selected all news articles with headlines including
two common diseases “breast cancer” and “diabetes”. The ﬁnal collection contains 564 news articles,
including 212 news headlines on breast cancer and 352 on diabetes. Those news headlines have been all
annotated health claims manually.

3.2 Data Annotation Schema
We developed an annotation schema that includes three types of health claims: the ﬁrst type does not
describe a health claim, the second type describes a health claim between an IV and a DV, and the third
type describes a health claim among multiple quasi-IVs (Sumner et al., 2014). Speciﬁcally, we deﬁne
the annotation schema as:

• Health Claim or Not: Label a headline as “1” if it describes a health claim, otherwise label “0”.
For example, the headline “Diabetics who use verapamil have lower glucose levels, data show” is
labeled as “1”, but the headline “Better breast cancer drugs?” is labeled as “0”. Sometimes health
claims are phrased as questions in headlines, such as “Can mindful eating help lower risk of type 2
diabetes, cardiovascular disease?” For such cases, we also label them as “1”.

• IV: What is being manipulated, e.g., “Fasting-mimicking diet” is the IV in the headline “Fasting-
mimicking diet may reverse diabetes”. The annotated IV should include all relevant words, including
the modiﬁers. For example, the IV of “Teen girls with a family history of breast cancer do not
experience increased depression or anxiety” is “Teen girls with a family history of breast cancer”.
Label “0” if no IV is found.

• DV: What is being measured, e.g., “diabetes” in in the headline “Fasting-mimicking diet may reverse
diabetes”. Label “0” if no DV is found. Similar to the IV annotation, the annotated DV should
include all relevant modiﬁers. For example, the DV of “Smoking can hamper common treatment for
breast cancer” is “common treatment for breast cancer”.

• Relation: The statement of relation between IV and DV. The annotated relation should include all
modiﬁers, like modal verbs (e.g., “can”), negative words (e.g., “not”), preposition combinations
(e.g., “associated with”) and verb combinations (e.g., “help reduce”). For example, the relation is
“found to switch” in “Breast cancer cells found to switch molecular characteristics”.

36• Multiple IVs: Sometimes multiple quasi-IVs were mentioned if they are correlated (Sumner et al.,
2014). In these cases, they are described in the same phrase, and thus impossible to separate as two
independent phrases. For example, “heart hormones, obesity, and diabetes” in the headline “New
links between heart hormones, obesity, and diabetes”.

Inter-coder Agreement

3.3
We randomly chose 100 news headlines to evaluate inter-coder reliability. Two annotators annotated
them separately. Inter-coder agreement was then calculated using Cohen’s Kappa. We ﬁrst evaluate the
agreement on whether a headline describes a health claim (see Table 1). The Cohen’s Kappa for this
annotation task is 0.71, indicating substantial agreement. The main disagreement is on headlines with
non-verb words to express relations, such as “behind” in “Identifying a genetic mutation behind sporadic
Parkinson’s disease”, which was neglected occasionally.

Annotator B

Annotator A No Health Claim

No Health Claim Health Claim
9
77
Table 1: Confusion matrix from whether a headline describe a relation.

Health Claim

14
0

We then compared inter-coder agreement on IV, DV, and relation annotations on the 77 headlines
with health claims identiﬁed by both annotators. Since these annotations are text snippets rather than
categories, we convert the original annotations to ﬁve categories: “IV”, “DV”, “Relation”, “Multiple IVs”
and “No Annotation”, and then examine each text snippet that has been annotated by either annotator,
assigning it to the annotator’s chosen category. Since annotations from two annotators may not be totally
the same, we consider two annotations are the same if they share the main keywords or phrases. For
example, consider a headline “Breast, ovarian cancer may have similar origins, study ﬁnds”. One person
annotated it as “IV-relation-DV” structure, “Breast, ovarian cancer; may have; similar origins” while the
other annotated as “relation-Multiple IVs” structure, “may have similar origins; Breast, ovarian cancer”.
The two annotations correspond to each other as “IV” vs. “Multiple IVs”, “Relation” vs. “Relation” and
“DV” vs. “Relation”. The confusion matrix was then generated accordingly (Table 2), and Cohen’s
Kappa is 0.89.

There are mainly two types of disagreement on IV, DV, and relation annotations. One is how to distin-
guish “IV-relation-DV” and “relation-Multiple IVs” structures on headlines with multiple IVs or DVs,
such as “Breast, ovarian cancer may have similar origins, study ﬁnds”. The other type of disagreements
is whether a preposition phrase should be “DV” or not. For example, as for the headline “A novel cancer
immunotherapy shows early promise in preclinical studies”, one annotator annotated “preclinical stud-
ies” as “DV”, but the other thought “preclinical studies” an adverbial modiﬁer of “shows early promise
in” and no “DV” in that headline.

Annotator A

IV
Relation
DV
Multiple IVs
No Annotation

Annotator B

IV Relation DV Multiple IVs No Annotation
2
68
1
0
0
3
0
0
0
0

3
75
2
0
0

2
1
71
0
0

2
0
1
0
0

Table 2: Confusion matrix from IV, DV and relation annotations.

3.4 The Annotated Dataset
After the inter-coder reliability check, the disagreements were resolved through discussion. Then one
annotator annotated the remaining news headlines. Among the 564 headlines, 416 (74%) describe health
claims and 148 (26%) do not. Each of the 416 headlines with health claims was annotated as one
“IV-relation-DV” triplet with a few exceptions. Five headlines were annotated with “Multiple IVs”

37(e.g., “Little to no association between butter consumption, chronic disease or total mortality”); two
described more than one “IV-relation-DV” triplet (e.g., “Epigenetic modiﬁcation increases susceptibility
to obesity and predicts fatty liver”). We have also identiﬁed six types of linguistic structures in health
news headlines that might confuse the extractors:

• Non-verb relation: such as “New potential treatment for cancer metastasis identiﬁed”.
• Relation with modal verb: such as “Smoking can hamper common treatment for breast cancer”.
• IVs and DVs with prepositional phrases: such as “Sugars in Western diets” in “Sugars in Western

diets increase risk for breast cancer tumors and metastasis”.

• Multiple IVs or DVs in parallel phrases: parallel phrases mean to contain more than one IVs or
DVs, such as “breast, ovarian cancer” in “Breast, ovarian cancer may have similar origins, study
ﬁnds”.

• Headlines containing both reporting verb and another verb to describe the relation: such as “ﬁnd”
and “treat” in “Scientists ﬁnd ‘outlier’ enzymes, potential new targets to treat diabetes, inﬂamma-
tion”.

• Headlines as incomplete sentences: such as “Sugar-sweetened drinks linked to increased visceral

fat”.

The 416 headlines with health claims include 113 with incomplete sentence structure (27%), 39 with
reporting verbs (9%), 39 non-verb relations (9%), 108 with modal verbs (26%), 107 with prepositional
phrases in IVs (26%), 182 with prepositional phrases in DVs (44%), 18 with parallel phrases in IVs (4%),
42 with parallel phrases in DVs (10%). One headline may include multiple characteristics.

As a robustness check, we further compared the linguistic characteristics of the headlines about two
diseases: “breast cancer” and “diabetes”, and found no signiﬁcant difference (see Figure 1). Therefore,
we consider the linguistic characteristics of health news headlines independent of disease types.

Figure 1: Distribution of different linguistic structures on breast cancer and diabetes.

4 Experiment Method and Result

Four systems were compared in terms of their performance in extracting “IV-relation-DV” from health
news headlines. Section 4.1 describes the evaluation method. Section 4.2 evaluates performance on
identifying headlines without health claims, and Section 4.3 on headlines with health claims. Section 4.4
evaluates performance on cases with special linguistic structures.

384.1 Evaluation Method
We chose two methods for this evaluation: the ﬁrst one calculates the precision, recall and F-measure by
manually comparing the machine annotations against the gold standard; the second method automatically
calculates the BLEU scores between machine annotations and the gold standard.

Precision, Recall and F-measure are traditional evaluation methods in information retrieval. For in-
formation extraction task, Fader et al. (2011) and Etzioni et al. (2011) deﬁned precision as the fraction
of returned extractions that are correct, and recall as the fraction of correct extractions in the total cor-
pus. For each extracted triplet, we manually check whether it is correct or not. The correct extraction is
deﬁned as keywords or phrases match in IV, DV and relation.

For robustness check we choose the second evaluation method as the BLEU score, which can be auto-
matically calculated. As a popular measure in machine translation, it evaluates the similarity between the
machine translations and the gold standard. The BLEU score divides each translated sentence into several
n-grams and compares differences between a machine translation and a professional human translation
based on those n-grams (Papineni et al., 2002).

In our task, we consider each manually-annotated “IV-relation-DV” triplet as a reference translation,
and each machine-extracted triplet as a candidate translation. For each extractor, we will calculate a
BLEU score. The higher BLEU score is, the better performance an extractor would achieve. The maxi-
mum number of n-grams varies from 2 to 4 and the weights for each n-gram are equal. In addition, we
apply “Add-one Smoothing” technique proposed in Lin and Och (2004) to avoid the BLEU score being
0 when n grows bigger.

4.2 Headlines without Health Claim
In our benchmark data set, 26% headlines do not contain health claims. Correct extractions should return
no triplets for such headlines. To evaluate the extractors on this task, we deﬁne precision as the correct
results with no triplets among all results with no triplets, and recall as the correct results with no triplets
among all headlines without claims. Table 3 shows the confusion matrix from each extractor and Table
4 shows the precision, recall and F-measure. The result shows that no extractors performed well in this
task with all F-measures below 0.5 with little variation. Furthermore, OLLIE and OPENIE-5.0 had better
precisions, but REVERB and SemRep had better recalls.

REVERB

OLLIE

Results with no triplets
Results with triplets

Results with no triplets
Results with triplets

OPENIE-5.0

Results with no triplets
Results with triplets

SemRep

Results with no triplets
Results with triplets

114
34

Headlines without claims Headlines with claims
208
208
Headlines without claims Headlines with claims
107
309
Headlines without claims Headlines with claims
60
356
Headlines without claims Headlines with claims
262
154

115
33

77
71

60
88

Table 3: Confusion matrix from different tools.

Information Extractor Precision Recall F-measure
.48
REVERB
OLLIE
.46
.45
OPENIE-5.0
SemRep
.44

.35
.42
.50
.31

.77
.52
.41
.78

Table 4: Precision, Recall and F-measure for different tools on headlines without health claim.

An analysis of the false positive extractions shows that the main problem is the broad deﬁnition of re-
lation in the three general-purpose tools, and thus many verbs that do not describe health claims were still

39identiﬁed as relations. For example, consider a news headline without health claim, “New study explores
concerns of African American breast cancer survivors”, the triplet is “New study; explores; concerns of
African American breast cancer survivors” from REVERB, OLLIE, OPENIE-5.0. In contrast, SemRep
is stricter on the deﬁnition of relations, which results in higher recall, but at the same time low precision
with many headlines with health claims not identiﬁed as well.

4.3 Headlines with Health Claim
In this section, we evaluate the extractors’ performance on headlines with health claims. The manual
evaluation (Table 5) shows that OPENIE-5.0 ranked highest in F-measure at .62, followed by OLLIE
at .53, REVERB at .41 and SemRep at .13.
In Table 6, OPENIE-5.0 also ranked highest in BLEU
score, followed by REVERB and OLLIE, regardless of the maximum number of n-grams. Overall,
OPENIE-5.0 is the best tool for extracting “IV-relation-DV” in headlines with health claims, leaving
some room for improvement. The results also show that the SemRep, the tool dedicated to relation
extraction in biomedical literature, does not generalize well to health news. In addition, REVERB and
OLLIE achieved similar precision level to OPENIE-5.0, but their recalls are relatively lower.

Information Extractor Precision Recall F-measure
.41
REVERB
.53
OLLIE
.62
OPENIE-5.0
SemRep
.13

.61
.62
.67
.23

.31
.46
.57
.08

Table 5: Precision, Recall and F-measure for different tools on headlines with health claim.

Information Extractor BLEU Scores (N=2) BLEU Scores (N=3) BLEU Scores (N=4)
REVERB
.65
.54
OLLIE
.69
OPENIE-5.0
SemRep
.10

.66
.61
.74
.17

.66
.58
.71
.13

Table 6: BLEU scores for different tools on headlines with health claim.

Table 3 has shown the number of false negative extractions from all tools. SemRep has 262, the
highest number, followed by REVERB, 208, OLLIE, 107, and OPENIE-5.0, 60. Among those headlines,
24 headlines are the most challenging, because none of the extractors was able to identify the triplets.
71% of them (17 out of 24) are incomplete sentences. Since OPENIE-5.0 is the best performing system,
we examined the missing triplets in its output and found 58% (35 out of 60) are incomplete sentences.
Therefore, incomplete sentences are particularly challenging for the extractors.

Our benchmark data set includes only ﬁve headlines with multiple IVs and two headlines with multiple
relations. Because these are likely difﬁcult cases, we particularly checked the extractors’ performance
on these headlines. For the ﬁve headlines annotated with “relation-Multiple IVs” structure, REVERB
and SemRep both return no triplets. OPENIE-5.0 returns two triplets and OLLIE returns three, but none
of those triplets were correct. For headlines with more than one “IV-relation-DV” triplets, OPEINIE-5.0
and REVERB return one correct triplet “Epigenetic modiﬁcation; predicts; fatty liver” of “Epigenetic
modiﬁcation increases susceptibility to obesity and predicts fatty liver”, while others return no triplet or
wrong triplets.

Impact of Linguistic Structures in Headlines

4.4
We then further examined the impact of the speciﬁc linguistic structures described in Section 3.4 on
individual extractors (Table 7). If ranking the task difﬁculty by the best F-measure for each linguistic
type, identifying non-verb relation is the most challenging with best F-measure at 0.19 by SemRep.
Second, identifying triplets in incomplete sentences is also challenging with best F-measure at 0.30 by
OLLIE and 0.28 by OPENIE-5.0. The extractors performed slightly better on two tasks with best F-
measure at 0.40 level: identifying multiple IVs or DVs in parallel phrases, and identifying actual verb
relations when reporting verbs are used. The extractors performed best on the tasks of identify-ing

40verb relations (including modal verbs) and identifying prepositional phrases in IVs and DVs with best
F-measures over 0.6.
Structure Type

Non-verb Relation

Verb Relation

Modal Verb

Prepositional Phrase in IV

Prepositional Phrase in DV

Parallel phrases for Multiple IVs

Parallel phrases for Multiple DVs

Headlines with Reporting Verbs

Incomplete Sentences

Information Extractor Precision Recall F-measure
REVERB
0
.07
OLLIE
OPENIE-5.0
0
.19
SemRep
.44
REVERB
OLLIE
.57
.67
OPENIE-5.0
.12
SemRep
.57
REVERB
OLLIE
.70
.88
OPENIE-5.0
.17
SemRep
REVERB
.24
.51
OLLIE
.61
OPENIE-5.0
.06
SemRep
REVERB
.46
.61
OLLIE
.67
OPENIE-5.0
.13
SemRep
REVERB
.09
.25
OLLIE
.36
OPENIE-5.0
SemRep
.07
.30
REVERB
.35
OLLIE
.43
OPENIE-5.0
SemRep
.10
.28
REVERB
.20
OLLIE
.39
OPENIE-5.0
SemRep
.15
.15
REVERB
.30
OLLIE
OPENIE-5.0
.28
.16
SemRep

0
.10
0
.33
.64
.66
.71
.22
.70
.76
.88
.34
.35
.57
.65
.10
.67
.68
.72
.19
.25
.29
.40
.11
.67
.41
.46
.16
.36
.23
.41
.29
.50
.40
.35
.29

0
.05
0
.13
.34
.50
.63
.08
.48
.65
.88
.11
.18
.46
.57
.05
.35
.55
.63
.10
.06
.22
.33
.06
.19
.31
.41
.07
.23
.18
.38
.10
.09
.24
.24
.11

Table 7: Performance on different linguistic structures.

5 Challenges to Individual Extractors
Based on the above results, we summarize the main challenges for each extractor and offer suggestions
for improvement.

SemRep: SemRep outputs signiﬁcantly fewer triplets than the other tools. It even missed the cases
with clear verb structures, such as “Smoking can hamper common treatment for breast cancer”. The
restriction may be attributed to SemRep’s strict deﬁnition on some entities and relations. For example,
“associated with” is deﬁned as the relation between a gene and a disease only (Kilicoglu et al., 2011).
Therefore, loosening the deﬁnition on some entities and relations might be helpful.

REVERB: Since REVERB is a verb-based extractor, the ﬁrst suggestion is to add some rules for
processing headlines with non-verb relations. In addition, the ability to recognizing IV and DV in com-

41plicated noun phrases should also be improved.
In such cases, REVERB missed the head nouns in
complicated noun phrases. For example, the IV extracted by REVERB is “humans” in healine “One of
the most common viruses in humans may promote breast cancer development”.

OLLIE: Headlines with reporting verb bring a big problem to OLLIE. OLLIE tends to extract the
reporting verbs as the relations while ignore the actual verb relations. For example, for the headline “Sci-
entists ﬁnd ‘outlier’ enzymes, potential new targets to treat diabetes, inﬂammation”, OLLIE identiﬁed
“Scientists; ﬁnd; ‘outlier’ enzymes” as the triplet, but missed the actual health claim that the enzymes
may be able to treat diabetes.

OPENIE-5.0: OPENIE-5.0 faces the challenges of non-verb relations and incomplete sentences.
Especially, for headlines with “A linked to B” structure, OPENIE-5.0 can only identify triplets with-
out DVs. For example, OPENIE-5.0 identiﬁed “Sugar-sweetened drinks; linked;” in “Sugar-sweetened
drinks linked to increased visceral fat”.

6 Conclusion
In this paper, we have created a benchmark data set, and used both manual and automated evaluation
methods to compare the performance of four information extractors on identifying the health claims in
health news headlines. Both methods reached consistent ﬁndings. Overall, 26% of health news head-
lines do not include health claims, and 74% do. The three general-purpose extractors (OPENIE-5.0, OL-
LIE, REVERB) performed better than the biomedicine-speciﬁc extractor (SemRep), probably because
SemRep was developed for documents in academic writing, not popular science writing. Among those
general-purpose extractors, OPENIE-5.0 has the best performance to extract “IV-relation-DV” triplets
with F-measure at 0.6 level. However, some characteristic linguistic structures in health news headlines
pose particular challenge to these extractors, especially on identifying non-verb relations and relations
in incomplete sentences. With F-measure at 0.4 level, further improvement is needed for identifying
multiple IVs or DVs in parallel phrases, or identifying actual verb relations when reporting verbs are
around. The extractors can identify verb relations and prepositional phrases in IVs and DVs relatively
well with F-measure at 0.6 level. In future work, we would like to develop new tools for identifying
headlines without claims and enrich the current rule-based systems with rules tailored to the linguistic
characteristics of health news headlines.

Acknowledgements
We would like to thank Dr. Jun Wang for providing the data. This research was partially supported by
China Scholarship Council and Syracuse University.

References
Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead and Oren Etzioni. 2007. Open informa-

tion extraction from the web. In IJCAI (Vol. 7, pp. 2670-2676).

Mollyann Brodie, Elizabeth C. Hamel, Drew E. Altman, Robert J. Blendon and John M. Benson. 2003. Health

news and the American public, 19962002. Journal of Health Politics, Policy and Law, 28(5), 927-950.

Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2011. An Analysis of Open Information Ex-
traction Based on Semantic Role Labeling. In Proceedings of the Sixth International Conference on Knowledge
Capture (pp. 113120). New York, NY, USA: ACM.

Lipika Dey, Muhammad Abulaish, Jahiruddin and Gaurav Sharma. 2007. Text Mining through Entity-Relationship
Based Information Extraction. In 2007 IEEE/WIC/ACM International Conferences on Web Intelligence and
Intelligent Agent Technology - Workshops (pp. 177180).

Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open Information Ex-

traction: The Second Generation. In IJCAI (Vol. 11, pp. 3-10)

Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extrac-
tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 15351545).
Stroudsburg, PA, USA: Association for Computational Linguistics.

42FDA. 2009. Guidance Documents & Regulatory Information by Topic - Guidance for Industry: Evidence-
Based Review System for the Scientiﬁc Evaluation of Health Claims. https://www.fda.gov/Food/
GuidanceRegulation/GuidanceDocumentsRegulatoryInformation/ucm073332.htm
(last access: May 10, 2018)

Boris Katz and Jimmy Lin. 2000. REXTOR: A System for Generating Relations from Natural Language. In ACL-
2000 Workshop on Recent Advances in Natural Language Processing and Information Retrieval (pp. 6777).
Hong Kong, China: Association for Computational Linguistics.

Halil Kilicoglu, Graciela Rosemblat, Marcelo Fiszman and Thomas C Rindﬂesch. 2011. Constructing a semantic

predication gold standard from the biomedical literature. BMC Bioinformatics, 12, 486.

Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Translation Quality Using Longest
Common Subsequence and Skip-bigram Statistics. In Proceedings of the 42Nd Annual Meeting on Association
for Computational Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics.

Mausam. 2016. Open information extraction systems and downstream applications. In Proceedings of the Twenty-

Fifth International Joint Conference on Artiﬁcial Intelligence (pp. 4074-4077). AAAI Press.

Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni. 2012. Open Language Learning
for Information Extraction. In Proceedings of the 2012 Joint Conference on Empirical Meth-ods in Natural
Language Processing and Computational Natural Language Learning (pp. 523534). Stroudsburg, PA, USA:
Association for Computational Linguistics.

NPR.

2018.

How

to

make

great

headlines.

http://training.npr.org/digital/

the-checklist-for-writing-good-headlines/ (last visited May 14th, 2018)

Harinder Pal and Mausam. 2016. Demonyms and Compound Relational Nouns in Nominal Open IE. In Pro-
ceedings of the 5th Workshop on Automated Knowledge Base Construction (pp. 3539). San Diego, CA: As-
sociation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational
Linguistics (pp. 311318). Stroudsburg, PA, USA: Association for Computational Linguistics.

Thomas C. Rindﬂesch and Marcelo Fiszman. 2003. The interaction of domain knowledge and linguistic structure
in natural language processing: interpreting hypernymic propositions in biomedical text. Journal of Biomedical
Informatics, 36(6), 462477.

Mary-Anne Saari, Candace Gibson and Andrew Osler. 1998. Endangered species: science writers in the Canadian

daily press. Public Understanding of Science, 7(1), 61-81.

Swarnadeep Saha, Harinder Pal and Mausam. 2017. Bootstrapping for Numerical Open IE. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 317323).
Vancouver, Canada: Association for Computational Linguistics.

Petroc Sumner, Solveiga Vivian-Grifﬁths, Jacky Boivin, Andy Williams, Christos A Venetis, Aime Davies, Jack
Ogden, Leanne Whelan, Bethan Hughes, Bethan Dalton, Fred Boy and Christopher D Chambers. 2014. The
association between exaggeration in health related science news and academic press releas-es: retrospective
observational study. BMJ, 349, g7015.

Melinda Voss. 2002. Checking the Pulse: Midwestern Reporters’ Opinions on Their Ability to Report Health Care

News. American Journal of Public Health, 92(7), 11581160.

Fei Wu and Daniel S. Weld. 2010. Open Information Extraction Using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational Linguistics (pp. 118127). Stroudsburg, PA, USA: Asso-
ciation for Computational Linguistics.

43
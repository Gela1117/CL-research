A Simple End-to-End Question Answering Model for Product Information

Tuan Manh Lai
Adobe Research

Trung Bui

Sheng Li

Adobe Research

Adobe Research

Nedim Lipka
Adobe Research

tlai@adobe.com

bui@adobe.com

sheli@adobe.com

lipka@adobe.com

Abstract

trates the overall process.

interest

When evaluating a potential product pur-
chase, customers may have many ques-
tions in mind. They want to get ade-
quate information to determine whether
the product of
is worth their
money. In this paper we present a simple
deep learning model for answering ques-
tions regarding product facts and speciﬁ-
cations. Given a question and a product
speciﬁcation, the model outputs a score in-
dicating their relevance. To train and eval-
uate our proposed model, we collected a
dataset of 7,119 questions that are related
to 153 different products. Experimental
results demonstrate that — despite its sim-
plicity — the performance of our model is
shown to be comparable to a more com-
plex state-of-the-art baseline.

Introduction

1
Customers ask many questions before buying
products. Developing a general question answer-
ing system to assist customers is challenging, due
to the diversity of questions. In this paper, we fo-
cus on the task of answering questions regarding
product facts and speciﬁcations. We formalize the
task as follows: Given a question Q about a prod-
uct P and the list of speciﬁcations (s1, s2, ..., sM )
of P , the goal is to identify the speciﬁcation that
is most relevant to Q. M is the number of spec-
iﬁcations of P , and si is the ith speciﬁcation of
P . In this formulation, the task is similar to the
answer selection problem (Rao et al., 2016; Bian
et al., 2017; Shen et al., 2017). ‘Answers’ shall be
individual product speciﬁcations in this case. Af-
ter identifying the most relevant speciﬁcation, the
ﬁnal response sentence is generated using prede-
ﬁned templates (Cui et al., 2017). Figure 1 illus-

In this paper, we present a simple deep learn-
ing model for selecting the product speciﬁcation
that is most relevant to a given question from a
set of candidate speciﬁcations. Given a question-
speciﬁcation pair, the model will output a score in-
dicating their relevance. To train and evaluate our
model, we collected a dataset of 7,119 questions,
covering 153 different products. Despite its sim-
plicity, the performance of our model is shown to
be comparable to a more complex state-of-the-art
baseline.

2 Related Work

2.1 Answer Selection

Answer selection is an active research ﬁeld and
has drawn a lot of attention. Given a question and
a set of candidate answers, the task is to identify
which of the candidates contains the correct an-
swer to the question. Two types of deep learn-
ing frameworks have been proposed for tackling
the answer selection problem. One is the Siamese
framework (Bromley et al., 1993) and the other is
the Compare-Aggregate framework (Wang et al.,
2017; Bian et al., 2017; Shen et al., 2017).
In
the Siamese framework, the same encoder (e.g.,
a CNN or a RNN) is used to map each input sen-
tence to a vector representation individually. Af-
ter that, the ﬁnal output is determined solely based
on the encoded vectors. There is no explicit in-
teraction between the sentences during the encod-
ing process. On the other hand, the Compare-
Aggregate framework aims to capture more in-
teractive features between sentences in consider-
ation, therefore typically has better performance
when evaluated on public datasets such as TrecQA
(Wang et al., 2007) and WikiQA (Yang et al.,
2015).

ProceedingsoftheFirstWorkshoponEconomicsandNaturalLanguageProcessing,pages38–43Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics38Figure 1: Answering questions regarding product facts and speciﬁcations

2.2 Customer Service Chatbot
The most closely related branches of work to
ours are probably customer service chatbots for
e-commerce websites. An example can be the
Shopbot 1 of eBay. Shopbot aims at helping con-
sumers narrow down the best deals from eBays
over a billion listings. The bot’s main focus is to
understand the user intent and then make person-
alized recommendations. Unlike Shopbot, here
we do not focus on making product recommen-
dations.
Instead we aim to develop a model for
answering questions about product speciﬁcations.
Another example is the SuperAgent (Cui et al.,
2017), a powerful chatbot designed to improve on-
line shopping experience. Given a speciﬁc product
page and a customer question, SuperAgent selects
the best answer from multiple data sources such
as in-page product information, existing customer
questions & answers, and customer reviews of the
product. Even though SuperAgent has a compo-
nent for answering questions about product spec-
iﬁcations, the novelties of our work are: 1) a new
simple deep learning model for answering ques-
tions about product facts and speciﬁcations 2) a
new method for collecting data to train and evalu-
ate our model.

3 Model Architecture

Given a question and a set of candidate speciﬁca-
tions, the goal is to identify the most relevant spec-
iﬁcation. We aim to train a classiﬁer that takes a
question and a speciﬁcation name as input and pre-
dicts whether the speciﬁcation is relevant to the
question. During inference, given a question, the
trained classiﬁer is used to assign a score to every
candidate speciﬁcation based on how relevant the
speciﬁcation is. After that, the top-ranked speciﬁ-
cation is selected.

A common trait of a number of recent state-of-
the-art methods for answer selection is the use of
the Compare-Aggregate architecture (Wang et al.,
2017; Bian et al., 2017; Shen et al., 2017). Under
this architecture, vector representations of smaller
units (such as words) of the input sentences are
compared. And then the comparison results are
aggregated (e.g., by a CNN or a RNN) to de-
termine the relationship of the input sentences.
Compared to Siamese models, most Compare-
Aggregate models are more complicated and can
capture more interactive features between input
sentences.

Our task of matching questions and product
speciﬁcations is similar to the answer selection
problem. “Answers” shall be individual product
speciﬁcations. However, in this case the name of
a speciﬁcation is relatively short. Therefore, our
hypothesis is that a well-designed Siamese model
would perform as well as a more complicated
Compare-Aggregate model. The added complex-
ity of comparing vector representations of smaller
units may not be needed as the speciﬁcation name
is already short and descriptive. To this end, we
propose a new Siamese model for tackling our
problem. We show the overall architecture of our
model in Figure 2. Given a question Q and a spec-
iﬁcation name S, the model calculates a score in-
dicating their relevance through the following lay-
ers.

Word Representation Layer. Using word
embeddings pre-trained with word2vec (Mikolov
et al., 2013) or GloVe (Pennington et al., 2014),
we transform Q and S into two sequences Qe =
[eQ
1 , eQ
n], where
eQ
is the embedding of the ith word of the ques-
i
tion and eS
j is the embedding of the jth word of
the speciﬁcation name. m and n are the lengths of
Q and S, respectively.

m] and Se = [eS

2 , ..., eQ

2 , ..., eS

1 , eS

1https://shopbot.ebay.com

BiLSTM Layer. We use a bi-directional

39Figure 2: Architecture of our model

LSTM (Hochreiter and Schmidhuber, 1997) to
obtain a context-aware vector representation for
each position of Q and S. We feed Qe and Se
individually into a parameter shared bi-directional
LSTM model. For the question Q:

−−−−→
LST M (qf
←−−−−
LST M (qb

i−1, eQ
i )
i+1, eQ
i )

qf
i =
qb
i =

i = 1, ..., m

i = m, ..., 1

feature vectors are calculated from the ﬁnal en-
codings oQ and oS: (1) the absolute difference of
the two vectors |oQ − oS|; (2) the element-wise
multiplication of the two vectors |oQ (cid:12) oS|. The
features are then concatenated and fed into a fully
connected layer and a softmax layer to produce the
ﬁnal score indicating the probability that speciﬁca-
tion S is relevant to question Q.

is a vector

representation of

where qf
the
i
ﬁrst i words in the question (i.e., [eQ
i ]),
qb
is a vector representing the context of the
i
last m − i + 1 words in the question (i.e.,
m, eQ
[eQ
i ]). Similarly, we use the same
bi-directional LSTM to encode S:

m−1, ..., eQ

2 , ..., eQ

1 , eQ

−−−−→
LST M (sf
←−−−−
LST M (sb

j−1, eS
j )
j+1, eS
j )

sf
j =
sb
j =

j = 1, ..., n

j = n, ..., 1

|| qb

The context-aware representation at each po-
sition of Q or S is obtained by concatenating the
two corresponding output sequences from both
j || sb
directions, i.e., qi = qf
j.
The ﬁnal representations of the question and
the speciﬁcation are generated by applying the
max-pooling operation on the context-aware
representations. We denote the ﬁnal representa-
tion of the question as oQ and denote the ﬁnal
representation of the answer as oS.

i and sj = sf

i

4 Data Collection

The dataset used for experiments is created using
Amazon Mechanical Turk (MTurk) 2, an online la-
bor market. MTurk connects requesters (people
who have works to be done) and workers (people
who work on tasks for money). Requesters can
post small tasks for workers to complete for a fee.
These small tasks are referred to as HITs or human
intelligence tasks. An example of a HIT is ﬁnd-
ing objects in an image or transcribing an audio
ﬁle. Requesters have several options for ensuring
their HITs are completed in a high-quality man-
ner. Requesters have the opportunity to determine
whether to approve completed HITs before having
to pay for them. In addition, requesters can also
limit which workers are eligible to complete their
tasks based on certain criteria.

We crawled the information of products listed
in the Home Depot website 3. For each product,
we create HITs where workers are asked to write

Comparison and Output Layers. Following
the approach mentioned in (Tai et al., 2015), two

2https://www.mturk.com
3https://www.homedepot.com

40questions regarding the speciﬁcations of the prod-
uct. Figure 3 shows a sample HIT, including the
instructions, which are shown to every participated
worker.
In this sample HIT, a question for the
speciﬁcation “Product Height (in.)” can be “How
tall is this shredder?” or “What is the height of
this shredder?”. To work on the HITs, workers
are required to have a 98% HIT approval rate, a
minimum of 800 HITs approved, and be located
in the United States or Canada. The constraints
ensure that the participated workers can provide
good questions in English. The ﬁnal dataset con-
sists of 7,119 question-speciﬁcation pairs in to-
tal, covering 369 kinds of speciﬁcations extracted
from 153 products. Even though in this work we
focus on products listed in the Home Depot web-
site, the data collection process is applicable to
other popular e-commerce websites such as Ama-
zon whose product pages typically have a section
for product facts and speciﬁcations.

5 Experiments and Results

5.1 Training and Evaluation
We set up two different experimental settings. The
only difference between the two settings is the way
in which we split up the collected HomeDepot
dataset into training set, development set, and test
set:

1. We divide the dataset so that the test set has
no products in common with the training set
or the development set.

2. We divide the dataset so that the test set has
no speciﬁcations in common with the train-
ing set or the development set. This is dif-
ferent from the ﬁrst setting, because two dif-
ferent products may have some speciﬁcations
in common. For example, a chair and a ta-
ble usually have a same speciﬁcation called
‘Product Weight’.

In both settings, the proportions of the training
set, development set, and test set are roughly 80%,
10%, and 10% of the total questions, respectively.
During training, the objective is to minimize the
cross entropy of all question-speciﬁcation pairs in
the training set:

loss(θ) = −log

pθ(y(i)|Q(i), S(i))

(cid:89)

i

where Q(i) and S(i)
represent a question-
speciﬁcation pair in the training set, y(i) indi-
cates whether speciﬁcation S(i) is relevant to ques-
tion Q(i), and pθ is the predicted probability with
model weights θ. We use all possible question-
speciﬁcation pairs for training. In other words, if
there are k questions about a product and the prod-
uct has h speciﬁcations, there are h × k question-
speciﬁcation examples related to the product, and
exactly k of them are positive examples. During
testing, for every question about a product, we sort
the speciﬁcations of the product in descending or-
der based on the predicted probability of being rel-
evant. After that, we calculate the precision at 1
(P@1), precision at 1 (P@2), and precision at 3
(P@3) of our model.

We compare the performance of our model with
the unigram model mentioned in (Yu et al., 2014)
and the IWAN model proposed in (Shen et al.,
2017). The unigram model is a simple bag-of-
words model. It ﬁrst generates a vector represen-
tation for each input sentence by summing over
the embeddings of all words in the sentence. The
ﬁnal output is then determined based on the gen-
erated vector representations. The unigram model
is less complicated than our model. On the other
hand, the IWAN model belongs to the Compare-
Aggregate framework, and it is more sophisti-
cated than our model.
In addition to comparing
between the ﬁne-grained word representations of
the input sentences, the IWAN model also has
an inter-weighted layer for evaluating the impor-
tance of each word in each sentence. The IWAN
model currently achieves state-of-the-art perfor-
mance on public datasets such as TrecQA (Wang
et al., 2007) and WikiQA (Yang et al., 2015).

We make use of the GloVe word embeddings
(Pennington et al., 2014) when training the mod-
els. We did try the word2vec word embeddings
(Mikolov et al., 2013), however they gave worse
performances than GloVe. We tune the hyper-
parameters of each model using the development
set.

5.2 Results

Table 1 shows the performances of all the mod-
els in the ﬁrst setting. Table 2 shows the perfor-
mances of all the models in the second setting.
The IWAN model and our model clearly outper-
form the unigram model. In addition, in both set-
tings, our model’s performance is comparable to

41Figure 3: An example of a HIT

Model
Unigram
IWAN

Our model

P@1
0.802
0.852
0.850

P@2
0.904
0.927
0.930

P@3
0.927
0.964
0.964

Model
Unigram
IWAN

Our model

P@1
0.399
0.525
0.563

P@2
0.529
0.661
0.640

P@3
0.627
0.789
0.759

Table 1: Test results in the setting where the
test set has no product in common with the
training set or the development set

Table 2: Test results in the setting where the
test set has no speciﬁcation in common with
the training set or the development set

the performance of the IWAN model despite be-
ing much simpler. We measured the speeds of our
model and the IWAN model. Our proposed model
is about 8% faster than the IWAN model. In ad-
dition, we see that all models perform worse in
the second setting than the ﬁrst setting. This may
due to the fact that in the ﬁrst setting two different
products in the train set and the test set may still
have many speciﬁcations in common (e.g., a LG
TV and a Samsung TV).

6 Conclusion

sults show that our model’s performance is compa-
rable to a state-of-the-art baseline despite having
less complexity. Our proposed model takes less
time for training and inference than the state-of-
the-art baseline.

Recently, researchers collected a large volume
of community question answering data and a large
volume of product reviews from the Amazon web-
site (McAuley and Yang, 2016). In the future, we
plan to investigate transfer learning techniques to
utilize this large dataset for improving the perfor-
mance of our proposed model.

In this work we explore the task of answering
questions related to product facts and speciﬁca-
tions. We propose a new, simple deep learning
model for tackling the task. To train and evalu-
ate the model, we collected a dataset of question-
speciﬁcation pairs using MTurk. Experimental re-

References

Weijie Bian, Si Li, Zhao Yang, Guang Chen, and
Zhiqing Lin. 2017. A compare-aggregate model
with dynamic-clip attention for answer selection. In
CIKM.

42Jane Bromley, James W. Bentz, L´eon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature ver-
iﬁcation using a ”siamese” time delay neural net-
work. IJPRAI, 7:669–688.

Lei Cui, Shaohan Huang, Furu Wei, Chuanqi Tan,
Chaoqun Duan, and Ming Zhou. 2017. Superagent:
A customer service chatbot for e-commerce web-
sites. In ACL.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9 8:1735–
80.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
customer reviews. In WWW.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119, USA. Curran
Associates Inc.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Jinfeng Rao, Hua He, and Jimmy J. Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In CIKM.

Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017.
Inter-weighted alignment network for sentence pair
modeling. In EMNLP.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. CoRR, abs/1503.00075.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP-CoNLL.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In IJCAI.

Yi Yang, Scott Wen-tau Yih, and Chris Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. ACL Association for Computa-
tional Linguistics.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen G. Pulman. 2014. Deep learning for answer
sentence selection. CoRR, abs/1412.1632.

43
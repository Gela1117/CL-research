Toward Cross-Domain Engagement Analysis in Medical Notes

Adam Faulkner ∗

Grammarly
NY, NY, USA

Sara Rosenthal
IBM Research

Yorktown Heights, NY, USA

adam.faulkner@grammarly.com

sjrosenthal@us.ibm.com

Abstract

We present a novel annotation task eval-
uating a patient’s engagement with their
health care regimen. The concept of en-
gagement supplements the traditional con-
cept of adherence with a focus on the pa-
tient’s affect, lifestyle choices, and health
goal status. We describe an engagement
annotation task across two patient note
domains:
traditional clinical notes and a
novel domain, care manager notes, where
we ﬁnd engagement to be more common.
The annotation task resulted in a κ of .53,
suggesting strong annotator intuitions re-
garding engagement-bearing language. In
addition, we report the results of a series of
preliminary engagement classiﬁcation ex-
periments using domain adaptation.
Introduction

1
The recent trend in medicine toward health pro-
motion, rather than disease management, has fore-
fronted the role of patient behavior and lifestyle
choices in positive health outcomes.
Social-
cognitive theories of health-promotion (Maes and
Karoly, 2005; Bandura, 2005) stress patient self-
monitoring of life-style choices, goal adoption,
and the enlistment of self-efﬁcacy beliefs as health
promotive. We call this cluster of behavioral
characteristics patient engagement. Traditional
strategies of patient follow-up have also been af-
fected by this trend: healthcare providers increas-
ingly employ “care managers” (CMs) to moni-
tor patient well-being and adherence to physician-
recommended changes in health behavior—i.e.,
engagement. In this paper, we present an annota-
tion schema for (lack of) engagement in CM notes
(CMNs) and generalize the schema to the related

∗ work completed at IBM

domain of electronic health records (EHRs). Our
high-level research questions are:

(1) Is the concept of engagement sufﬁciently well-
deﬁned that annotators can recognize the con-
cept across text domains with an acceptable
level of agreement?

(2) Can the annotations produced in (1) be used to
classify engagement-bearing language across
text domains?

In section 3, we report the results of our explo-
ration of (1), describing an annotation task involv-
ing ∼ 6500 CMN and EHR sentences that resulted
in an average κ of .53. In sections 4 and 5 we ad-
dress (2) and report the results of several classiﬁ-
cation experiments that ablate classes of features
and use domain adaptation to adapt these features
to the CM and EHR target domains.

2 Related Work

The notion of patient engagement explored here is
inspired by the self-regulation paradigm of (Ban-
dura, 2005; Leventhal et al., 2012; Mann et al.,
2013), where a patient’s successful completion of
health-related goals is predicated on their ability
to “self-regulate”, i.e., to plan and execute actions
that promote attaining those goals, and their abil-
ity to maintain a positive attitude toward self-care.
We are also aligned with the more recent work of
Higgins et al. (2017) whose deﬁnition of engage-
ment includes a “desire and capability to actively
choose to participate in care”.

NLP approaches assessing doctor compliance
include Hazelhurst et al. (2005) who evaluate
notes for doctor compliance to tobacco cessa-
tion guidelines and Mishra et al. (2012) who as-
sess ABCs protocol compliance in discharge sum-
maries.

ProceedingsoftheBioNLP2018workshop,pages189–193Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics189Label
Engagement
with care

Engagement
with CM
Lack of engage-
ment with care

Lack of engage-
ment with CM
CM Advice
Other

Description
The patient is engaged in their well-being by de-
scribing/exhibiting healthy behavior, positive out-
look, and social ties.
Adherence to a doctor or CM instruction or under-
standing of CM advice.
Lack of engagement by using language suggestive
of non-adherence to guidelines, health-adverse be-
havior, lack of social ties, or negative impression
of patient self-care.
Non-adherence to a prescribed instruction or a
negative response to interaction.
CM advice or suggestion
Default label to be chosen when no other label ﬁts.

Examples
“Patient disappointed by lack of weight loss but is just
beginning exercise regimen”; “Patient joined book
club.”
“Patient verbalized understanding”; “Patient conﬁded
that she has gaps in nitroglycerin use.”
“White female, disheveled appearance”; “Patient ad-
mits to ‘sedentary’ lifestyle.”

“Patient rude during call”; “Patient angrily refused
further outreach.”
“I suggested he watch his diet and increase exercise”
“Patient has a history of atrial ﬁbrillation on corticos-
teroids”; “Chest is clear with no crackles.”

Table 1: Annotation labels with descriptions and anecdotal examples. We use the term CM to describe
both the para-professionals interacting with patients in CM notes and the physicians in EHRs.

While there exists work dealing with sentiment
in clinical notes, such as positive or negative af-
fect (Ghassemi et al., 2015) and speculative lan-
guage (Cruz D´ıaz et al., 2012), (lack of) engage-
ment cannot be reduced to sentiment. Lack-of-
engagement-bearing language, for example, can
also contain positive sentiment, e.g., patient is
feeling better so she has stopped taking her medi-
cation. We include sentiment in our feature set, as
described in Section 4.

The most closely related work is Topaz et al.
(2017) who developed a document-level discharge
note classiﬁcation model that identiﬁes the adher-
ence of a patient in the discharge note. Their an-
notation task differs from ours, however, as they
focus only on lack of adherence, speciﬁcally, to-
wards medication, diet, exercise, and medical ap-
pointments. We also distinguish the targets of both
engagement and lack of engagement by allowing
annotators to identify either the CM or the care it-
self as the target.

3 Annotation Task and Data
The majority of our data consists of CMNs gener-
ated by a care manager service located in Florida,
USA. CMs typically contact patients via phone
to inquire into the patient’s status with respect to
health goals and enter the resulting information
into the structured sections of a reporting tool. In
addition, CMs note their impressions of the pa-
tient in a note as unstructured text, which we use
here. To expand the domain scope of the task,
we included EHR notes from the i2b2 Heart Dis-
ease Risk Factors Challenge Data Set (Stubbs and
Uzuner, 2015; Stubbs et al., 2015), which includes
notes dealing with diabetic patients at risk for
Coronary Artery Disease (CAD). All notes were

annotated in the same manner regardless of source.

3.1 Annotation Guidelines
Table 1 includes descriptions of the annotation la-
bels along with anecdotal examples of each label
type (original sentences are excluded due to pri-
vacy constraints1). Annotators were allowed to
choose more than one label for each sentence, or
no label at all (considered other). Our schema
captures three different label classes: engagement,
lack of engagement, and cm advice. We included
cm advice because it can provide an indication that
the next sentence should be classiﬁed as (lack of)
engagement. We initially explored “barrier” lan-
guage (e.g. patient could not get to his appt be-
cause he didn’t have a car) as this can be indica-
tive of lack of engagement, however, we found it
to be too rare to include in the annotation tasks.

3.2 Annotation Challenges
Our ﬁrst challenge was encoding a distinction be-
tween engagement and the more familiar con-
cept of patient “adherence” (Vermeire et al., 2001;
Topaz et al., 2017) in the annotation guidelines.
While engagement-bearing language can include
adherence-bearing language (e.g., is monitoring
blood sugar, made follow-up appointment), the re-
verse is often not the case: Engagement-bearing
language can include mentions of social ties (e.g.,
discusses struggles to lose weight with sister) and
positive or negative evaluations of health-related
goals (e.g., patient was irritable when asked about
efforts to reduce smoking), neither of which in-
volve adherence per se. By annotating such ex-
amples as engagement-bearing, we capture “self-

1All examples provided throughout the paper are anecto-

dal

190efﬁcacy beliefs,” which theories of patient self-
regulation (Bandura, 1998, 2005) have suggested
are predictive of health goal attainment.

An additional distinction that emerged during
the annotation process involved the target of the
engagement-bearing language: Is the patient (not)
engaged with the CM or with the care itself? This
distinction is evident in sentences that display a
lack of engagement with care but a level of en-
gagement with the CM. For example, in the sen-
tence He appeared cheerful in our interactions
and admitted that he has not been exercising daily,
the patient is conﬁding in their CM (engagement)
that they are not pursuing their health goals (lack
of engagement). By allowing annotators to an-
notate such sentences as both engaged with the
CM but unengaged with care we were able to ex-
clude sentences that contained internally inconsis-
tent engagement-bearing language from our data.
Another challenge involved the frequent use of
“canned language” in the CM data, or language
that does not report the CM’s interactions with
the patient but is used to meet some reporting cri-
terion recommended by the health-care provider.
For example, Patient is scheduled for follow up
appointment in two weeks, is a frequently occur-
ring canned language. Thus, we excluded com-
mon canned language sentences from the data.

3.3 Data Statistics

After several initial pilot rounds inter-annotator
agreement for our six annotators on a ﬁnal pi-
lot
round of 200 sentences (100 from each
source) ranged from .46 to .66 among the anno-
tators with an overall average of .53 (using Co-
hen’s κ), indicating moderate to substantial agree-
ment (McHugh, 2012).
4011 CMN sentences were annotated, extracted
from ∼ 10, 000 unique CMNs. In order to broaden
the range of language in our data, 2561 EHR sen-
tences were annotated, with an equal number of
sentences drawn from the three patient cohorts in-
cluded in the i2b2 data. For each EHR, we re-
stricted our annotation effort to sections that were
more likely to include engagement-bearing lan-
guage, speciﬁcally, the social history, family his-
tory, personal medical history, and history of the
present illness sections. Table 2 shows the label
distribution of the annotated data relative to note
source. Although we allowed the annotators to
differentiate between engagement/lack of engage-

114
395
509

No Engage Advice Other
56
2376
3304
172
228
5680

Source Engage
EHRs
CMNs
Total
Table 2: Label distribution relative to note type for
all annotated sentence data.

15
140
155

ment with care or the CM, we ultimately conﬂated
these two categories into one for our experiments.
4 Method
Given the small size of our data we elected to use
a feature-engineering-based approach along with
a discriminative classiﬁcation algorithm in our ex-
periments. Our features can be divided into ﬁve
lexico-syntactic, lexical-count, senti-
categories:
ment, medical, and embeddings.

Lexico-syntactic. Standard NLP features for
text-classiﬁcation such as n-grams and part-of-
speech (POS) tags, along with dependency tuples
(De Marneffe and Manning, 2008) with either the
governor or dependent generalized to its POS.

Lexical-count. Frequency-based features such
as sentence length, min and max word length, and
number of out of vocabulary words.

Sentiment. We ran two sentiment classiﬁers
over the data
(Socher et al., 2013; Hutto and
Gilbert, 2014) and included the resulting tags as
features.
In addition, we developed “comply
word” features by inducing a lexicon based on
WordNet-
(Fellbaum, 1998) and Uniﬁed Med-
ical Language System (UMLS)-based2 synonym
expansion of seed words such as “take” and “de-
cline.”

Medical. Using the MetaMap3 tool, we gen-
erated Concept-Unique Identiﬁers (CUIs) for any
medical concepts in the sentence. We also in-
cluded both the “preferred names” and semantic
types returned by UMLS for each concept

Embeddings. We extracted term-term, CUI-
CUI and term-CUI co-occurrences pairs from a
large medical corpus and used wordtovecf4
(Levy and Goldberg, 2014) to learn embeddings
from this co-occurrence dataset. We generated the
mean of the embeddings for all content-words and
CUIs in the sentence as a feature.

5 Experiments
All experiments were performed using an SVM
classiﬁer with a linear kernel basis function and
2http://www.nlm.nih.gov/research/umls/
3http://metamap.nlm.nih.gov/
4http://bitbucket.org/yoavgo/word2vecf

191CM

EHR

CM

ALL

experiment
eng lack other eng lack other
95.4
18.4 25.1 91.3 7.1
n-grams
3.3
95.4
18.0 24.9 91.3 7.2
+embeddings
3.3
94.6
+lexico-synt
22.7 20.7 90.2 7.0
3.2
93.1
+lexical counts 21.7 21.9 89.2 9.1
3.1
+sentiment
19.8 22.9 88.7 9.1
92.8
6.1
93.6
22.3 24.3 89.3 8.4
+medical
8.6
6.2
24.3 21.7 89.1 9.5
all
93.1
12.7 93.7
27.4 26.3 91.2 9.2
n-grams
12.7 93.7
27.5 26.8 91.3 8.5
+embeddings
+lexico-synt
27.9 25.2 88.7 7.4
9.8
91.7
12.1 89.6
+lexical counts 27.4 27.5 87.5 8.4
27.6 29.4 87.4 7.9
+sentiment
11.3 89.4
28.9 28.6 87.8 10.6 10.4 90.2
+medical
29.6 25.4 87.1 10.8 13.0 89.6
all
0.0
n-grams
9.4
96.3
96.3
9.4
+embeddings
0.0
96.4
+lexico-synt
12.7 1.1
96.3
+lexical counts 12.0 3.2
96.5
10.2 3.2
+sentiment
+medical
15.6 9.6
96.3
96.2
11.1 7.2
all
Table 3: F-score results of ablation experiments
for Engagement (eng) and Lack of Engagement
(lack). Row headers refer to training sets and col-
umn headers refer to test sets. The best F-score for
each test set is shown in bold.

0.0
92.1 0.0
0.0
92.1 0.0
6.5
91.9 5.5
6.3
91.9 5.6
91.9 6.9
6.3
91.7 17.0 6.1
91.7 14.4 6.3

EHR

one-vs-rest multiclass classiﬁcation strategy as
implemented in scikit-learn.5
To deal
with the skew in class distribution we experi-
mented with both over- and under-sampling but
got our best performance by simply adjusting class
weights to be inversely proportional to class fre-
quencies. Given the relatively small size of our
data we used 5-fold cross-validation throughout.
We also conﬂated cm advice with other to boost
performance. We show F-score results for all three
classes, but our analysis will focus on (lack of) en-
gagement since other is trivially high-performing
due to the massive data skew.

In our ﬁrst set of experiments we examined the
impact of training and testing on EHRs and CMNs
individually, as well as together, while ablating the
feature classes described in section 4. As shown in
Table 3, all feature classes seem to help the model,
but sentiment helps more for predicting lack of
engagement in the CMNs while medical features
help more for predicting lack of engagement in
the EHRs. These experiments show that a CMN-
trained model can perform well on EHRs. The
best result for lack of engagement occurs when
training on CM notes, with an F-score of 13.0.

The results in Table 3 encouraged us to apply
domain adaptation (DA) to improve the results of

5http://scikit-learn.org

experiment

CM

EHR

eng lack other eng lack other
95.8
21.0 25.7 91.6 9.2
n-grams
3.3
95.8
+embeddings 21.1 25.5 91.6 10.4 3.3
3.2
+lexico-synt
93.4
23.0 22.9 89.1 9.2
15.0 91.5
+lexical count 20.2 24.2 89.1 9.9
23.0 26.1 88.8 7.8
+sentiment
2.7
93.0
23.4 22.9 88.8 9.9
+medical
93.2
5.1
22.3 23.2 89.0 13.3 12.8 91.4
all

Table 4: F-score results of DA experiments. The
best F-score for each test set is shown in bold.

the smaller dataset (EHRs) while also taking ad-
vantage of the larger dataset (CMNs) by consid-
ering EHRs to be “in-domain” and CMNs to be
“out-of-domain”. As this is still preliminary work,
we started with a simple, yet effective DA strat-
egy: the feature representation transformation pro-
cedure described in Daum´e III (2007). Table 4
shows the results using DA. In the EHRs, where
there is less data, on average DA provided an im-
provement, particularly for lack of engagement.

6 Conclusion

In this paper we presented an annotation schema
that captures engagement in CMNs and EHRs. We
described the challenges of developing an annota-
tion schema for a subjective task and show that
annotators achieved moderate to high agreement
in our ﬁnal task. We annotated 6,572 sentences
for (lack of) engagement and show preliminary re-
sults of a classiﬁcation experiment on our dataset
using feature ablation and domain adaptation. Our
results are promising, showing that both features
and domain adaptation are useful. However, they
remain preliminary due to the rarity of (lack of)
engagement labels. In future work, we plan to ex-
plore transfer learning to increase the size of our
data, which in turn will allow use to explore deep
learning approaches to this task.

Acknowledgements

Deidentiﬁed clinical records used in this research
were provided by the i2b2 National Center for
Biomedical Computing funded by U54LM008748
and were originally prepared for the Shared Tasks
for Challenges in NLP for Clinical Data organized
by Dr. Ozlem Uzuner, i2b2 and SUNY. Research
is based on care manager data provided by Or-
lando Health, a Florida-based community of hos-
pitals.

192Stan Maes and Paul Karoly. 2005. Self-regulation as-
sessment and intervention in physical health and ill-
ness: A review. Applied Psychology, 54(2):267–
299.

Traci Mann, Denise De Ridder, and Kentaro Fujita.
2013. Self-regulation of health behavior: social psy-
chological approaches to goal setting and goal striv-
ing. Health Psychology, 32(5):487.

Mary L. McHugh. 2012.

Interrater reliability:

the

kappa statistic. In Biochemia medica.

Ninad K. Mishra, Roderick Y. Son, and James J.
Arnzen. 2012. Towards automatic diabetes case de-
tection and abcs protocol compliance assessment.
Clinical medicine & research, pages cmr–2012.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Amber Stubbs, Christopher Kotﬁla, Hua Xu, and
Azlem Uzuner. 2015.
Identifying risk factors for
heart disease over time: Overview of 2014 i2b2
uthealth shared task track 2. Journal of Biomedical
Informatics, 58:S67 – S77.

Amber Stubbs and ¨Ozlem Uzuner. 2015. Annotat-
ing risk factors for heart disease in clinical nar-
Journal of Biomedi-
ratives for diabetic patients.
cal Informatics, 58:S78 – S91. Proceedings of the
2014 i2b2/UTHealth Shared-Tasks and Workshop
on Challenges in Natural Language Processing for
Clinical Data.

Maxim Topaz, Kavita Radhakrishnan, Suzanne Black-
ley, Victor Lei, Kenneth Lai, and Li Zhou. 2017.
Studying associations between heart failure self-
management and rehospitalizations using natural
language processing. Western journal of nursing re-
search, 39(1):147–165.

Etienne Vermeire, Hilary Hearnshaw, Paul Van Royen,
and Joke Denekens. 2001. Patient adherence to
treatment: three decades of research. a comprehen-
sive review. Journal of clinical pharmacy and ther-
apeutics, 26(5):331–342.

References
Albert Bandura. 1998. Health promotion from the per-
spective of social cognitive theory. Psychology and
health, 13(4):623–649.

Albert Bandura. 2005.

self-
regulation in health promotion. Applied Psychology,
54(2):245–254.

The primacy of

Noa P Cruz D´ıaz, Manuel J Ma˜na L´opez, Jacinto Mata
V´azquez, and Victoria Pach´on ´Alvarez. 2012. A
machine-learning approach to negation and specu-
lation detection in clinical texts. Journal of the As-
sociation for Information Science and Technology,
63(7):1398–1410.

Hal Daum´e III. 2007. Frustratingly easy domain adap-

tation. ACL 2007, page 256.

Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Coling 2008: proceedings of the work-
shop on cross-framework and cross-domain parser
evaluation, pages 1–8. Association for Computa-
tional Linguistics.

Christiane Fellbaum. 1998. A semantic network of
english verbs. WordNet: An electronic lexical
database, 3:153–178.

Mohammad M Ghassemi, Roger G Mark, and Shamim
Nemati. 2015. A visualization of evolving clini-
cal sentiment using vector representations of clini-
cal notes. In Computing in Cardiology Conference
(CinC), 2015, pages 629–632. IEEE.

Brian Hazlehurst, H. Robert Frost, Dean F. Sittig, and
Victor J. Stevens. 2005. Mediclass: A system for
detecting and classifying encounter-based clinical
Journal
events in any electronic medical record.
of the American Medical Informatics Association,
12(5):517–529.

Tracy Higgins, Elaine Larson, and Rebecca Schnall.
2017. Unraveling the meaning of patient engage-
ment: A concept analysis. Patient Education and
Counseling, 100(1):30 – 36.

C.J. Hutto and Eric Gilbert. 2014. Vader: A parsi-
monious rule-based model for sentiment analysis of
In Eighth International Confer-
social media text.
ence on Weblogs and Social Media (ICWSM-14).
Available at (20/04/16) http://comp. social. gatech.
edu/papers/icwsm14. vader. hutto. pdf.

Howard Leventhal, Ian Brissette, and Elaine A. Lev-
enthal. 2012. The common-sense model of self-
In The self-
regulation of health and illness.
regulation of health and illness behaviour, pages
56–79. Routledge.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), vol-
ume 2, pages 302–308.

193
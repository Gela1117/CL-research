Improving Classiﬁcation of Twitter Behavior During Hurricane Events

Kevin Stowe, Jennings Anderson, Martha Palmer, Leysia Palen, Ken Anderson

University of Colorado, Boulder, CO 80309

[kest1439, jennings.anderson, mpalmer, palen, kena]@colorado.edu

Abstract

2 Task One: Improving Tweet

Classiﬁcation

A large amount of social media data is
generated during natural disasters, and
identifying the relevant portions of this
data is critical for researchers attempting
to understand human behavior, the effects
of information sources, and preparatory
actions undertaken during these events. In
order to classify human behavior during
hazard events, we employ machine learn-
ing for two tasks: identifying hurricane re-
lated tweets and classifying user evacua-
tion behavior during hurricanes. We show
that feature-based and deep learning meth-
ods provide different beneﬁts for tweet
classiﬁcation, and ensemble-based meth-
ods using linguistic, temporal, and geospa-
tial features can effectively classify user
behavior.

1

Introduction

Identifying relevant information for natural disas-
ter and other hazards is a difﬁcult task, particu-
larly in social media, which is often noisy. Under-
standing people’s behavior during events is an im-
portant task for both researchers studying human
responses to hazards after events and real-time
processing of disaster-related information. Key-
word searches can be an effective ﬁrst pass, but
are insufﬁcient to fully understand user behavior
and can generate large numbers of both false pos-
itives and false negatives. To improve our ability
to study behavior during crisis events, we employ
supervised machine learning for two tasks: iden-
tifying tweets that are relevant to hurricane events
and classifying Twitter users’ evacuation behavior.

Twitter data is often difﬁcult to understand due
to limited length of tweets and the noise inher-
ent in the medium. As a result, there is a vari-
ety of research in attempting to effectively iden-
tify and classify tweets. There are multiple stud-
ies in classiﬁcation of ﬂu-related tweets (Culotta,
2010; Aramaki et al., 2011). One relevance classi-
ﬁcation approach is Lamb et al. (2013), which ini-
tially classiﬁes tweets for relevance and then ap-
plies ﬁner-grained classiﬁers. They build classi-
ﬁers using syntactic and Twitter-speciﬁc features
to detect awareness versus infection, self versus
others, and whether tweets are relevant to the ﬂu
or not.

Sriram et al. (2010) propose a somewhat more
speciﬁc system, classifying tweets into general
categories like news, events, and opinions, achiev-
ing accuracies between .85 and .95 depending on
category. Sankaranarayanan et al. (2009) perform
a similar task, classifying tweets into either news
or non-news. Recently, the work of Volkova et al.
(2017) attempts to classify suspicious and trusted
tweets. They ﬁnd that deep learning models out-
perform feature-based models, but linguistics fea-
tures can be helpful. They report F1 scores of be-
tween .88 and .92 depending on the category clas-
siﬁed.

For our ﬁrst task of relevant tweet classiﬁca-
tion, we employ supervised machine learning to
predict whether individual tweets are relevant to
a hurricane. This study focuses on the Hurri-
cane Sandy event in October of 2012. This hur-
ricane made landfall on the eastern seaboard of
the United States on October 29, causing massive
damage to many areas including New York and
New Jersey. To collect data for this event, we ini-
tially performed a collection capturing all tweets

ProceedingsoftheSixthInternationalWorkshoponNaturalLanguageProcessingforSocialMedia,pages67–75,Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics67Tweet
For the love of that money.....
Lol the struggle for gas and Power
where u been hiding at through this storm
Smh I still don’t get to play Halo 4 yet...

Relevant

n
y
y
n

Table 1: Sample Tweet Classiﬁcation Stream

using the following keywords:

DSNY, cleanup, debris, frankenstorm, garbage,
hurricane, hurricanesandy, lbi, occupysandy, per-
fectstorm, sandy, sandycam, stormporn, super-
storm

This generated approximately 22.2 million
unique tweets from 8 million users. We then iden-
tiﬁed users who had geo-tagged tweets within ar-
eas that were heavily impacted by the event. This
allowed us capture users who were likely to be sig-
niﬁcantly impacted and local to the event. From
these we randomly selected 105 users, collecting
the tweets from a week before landfall to a week
after, resulting in 25,474 tweets. We annotated
these tweets for hurricane relevance (two anno-
tators, agreement approximately .9). Our task is
to classify for each user which tweets are relevant
(Table 1).

We developed a standard feature-based machine
learning classiﬁer and compare it to several deep
learning approaches. We split our data into train-
ing (60%), validation (20%), and test (20%) sets,
tuning each model on the validation set and evalu-
ating on the test data.

2.1 Feature-based
As a baseline for feature-based classiﬁcation, we
follow the setup and features of Stowe et al.
(2016), who employ support vector machines and
linguistic features to classify hurricane related
tweets. As a baseline, we re-implement this ap-
proach, leaving out features that appeared to have
negligible contribution. We used the following
features from their set:

• Bag of words based on Pointwise Mutual In-
formation (PMI) for unigrams, bigrams, and
trigrams. We chose the n terms with highest
PMI for positive and negative classes, with
n set to 200 as was determined in validation.
Selecting the bag of words lexicon based on
PMI signiﬁcantly improves results over using
the full set of words.

Model
Stowe et al (SVM Baseline)
Multi-layer Perceptron
Convolutional NN

F1
.769
.834
.815

Prec Recall
.886
.678
.788
.886
.874
.763

Table 2: Tweet Classiﬁcation Results

• The time of the target tweet, using a one-hot
vector representing the time bin of the target
tweet. Through validation we chose to use
384 bins, or one per hour.
• Average word embeddings for each tweet.
We experimented with using Google News
vectors generated using word2vec (Mikolov
et al., 2013) and Glove Twitter embeddings
(Pennington et al., 2014). We selected the
Google News vectors, as they had the best
performance.

For comparison, we employ two deep learn-
ing approaches: a multi-layered perceptron (MLP)
and a convolutional neural network (CNN).

2.2 Multi-layered Perceptron (MLP)
For our MLP, we started with inputing each tweet
as a collection of words, padded up to length 25.
We used an embedding layer of dimension 300 us-
ing the pretrained Google News vectors, and fed
this through a 50 node dense layer using a rectiﬁed
linear unit (relu) activation with a dropout rate of
.5. This was then fed into the output layer, using
sigmoid activation to predict either relevant or ir-
relevant. The model was trained using categorical
hinge loss, running 50 epochs.

2.3 Convolutional Neural Network (CNN)
Convolutional neural networks incorporate local
word context using convolutions of words within
a contextual window, and have proven effective
in a variety of sentence classiﬁcation tasks (Kim,
2014; Li et al., 2017). As tweets can be consid-
ered a sentence, we experiment with using CNNs
for relevance classiﬁcation.

We follow the approach of Kim (2014), using an
embedding layer (from the Google News vectors),
which is then fed into a convolutional layer. We
use kernel sizes of 2, 3, and 4, with 16 ﬁlters per
kernel size. We use max pooling to combine the
outputs, with a pool size of 4. Finally, we use a
fully connected layer to the binary output nodes,
using sigmoid activation to predict relevance.

Both deep learning models improve over the re-
implemented SVM baseline. However, the CNN

68doesn’t improve over the basic multi-layer percep-
tron.

2.4 Effects of Context
Sentence classiﬁcation is a common task, and it
has been applied effectively to tweets. However,
most classiﬁcation for Twitter data is done on in-
dividual tweets, without regard to their larger con-
text. This causes an impoverished information en-
vironment: knowing the context a tweet is present
in from a user’s perspective provides valuable in-
formation about the meaning of the tweet.

Because of this, we experimented with using
contextual models to predict tweet relevance. We
experimented with using the same SVM model
above, experimenting with expanding the feature
window to include more context, as well as adding
additional contextual tweets the MLP model. In
both cases, we used contextual windows from 1-
16 words before and after the tweets. We found
that performance decreased consistently as more
context was added, and using only the target tweet
yielded the best results

We also experimented with using sequence
taggers, speciﬁcally a long short-term memory
(LSTM) network. We input each user as a training
batch, treating the tweets they produced chrono-
logically as a sequence. Our results using the
LSTM model were much lower than the non-
sequence taggers (.65 compared to .83). Tuning
model size and dropout, as well as adding bidi-
rectional and attention layers failed to signiﬁcantly
improve performance.

From the data, it appears that context is vital
for determining tweet relevance, but our models
have not been able to capture the signiﬁcance. We
believe this is due to the irregular nature of help-
ful context. In tweet streams, it is often the case
that one particular tweet in the context is neces-
sary to understand the target, but the location in
context of the tweet is not consistent. Because of
this inconsistency, the model cannot reliable deter-
mine which element in context is contributing the
necessary information. As a future goal, we aim
to incorporate better methods of representing con-
text that can ﬁlter out contextual tweets that likely
don’t inﬂuence the target.

2.5 Effects of Data Size
As each event is unique and other kinds of natural
hazards are likely to pose completely new prob-
lems, we would ideally like to be able to generate

new classiﬁers with as little data as possible. We
experiment with varying the size of our training
data to assess how much is necessary to reach peak
performance. We held out 20% of our data as a
test set, and then trained classiﬁers incrementally,
adding 100 instances of training data at a time. We
also tested the effectiveness of combining models
by implementing a combined classiﬁer. This clas-
siﬁer uses the output of the MLP and the SVM as
features for training a logistic regression classiﬁer.
The results of these classiﬁers as training data is
added are shown in Figure 1.

The SVM achieves strong recall very quickly,
at over .8 with only 5,000 training instances. The
perceptron follows an opposite pattern, with preci-
sion over .9 at 5,000 but very low recall. The SVM
is consistently improving at around 2,500 training
instances, and shows only minimal improvement
after 7,5000. The perceptron is much more irreg-
ular, being ineffective until nearly 7,500 instances
and leveling off near 12,500.

We believe that precision is more important for
this task, as there are such a large number of tweets
available, it is more important to identify tweets
correctly than to capture all of them. However,
the perceptron takes more data to be consistent.
Combining classiﬁers in this case doesn’t improve
performance over either the SVM or MLP indi-
vidually, although the logistic regression approach
is comparable. The best approach for extending
classiﬁcation novel events is to assess whether pre-
cision or recall is more important, and select the
individual classiﬁer that ﬁts the goals of the re-
search.

Classiﬁcation of tweets can be improved by em-
ploying deep learning models, which signiﬁcantly
outperforms feature-based methods. Comparisons
to other work are difﬁcult to the differences in
tasks. We do not achieve the F1 scores of Volkova
et al. (2017) or Sriram et al. (2010), both between
.85 and .95, but the tasks are likely too different
for meaningful comparison.

3 Task 2: Evacuation Classiﬁcation

Tweet classiﬁcation provides information about
user behavior as users tweet about their experi-
ences and actions as the events are unfolding. At a
broader level, we can also use tweet streams from
a user to attempt to determine their evacuation be-
havior during an event. For this, we need to exam-
ine their entire stream and understand both their

69Figure 1: Classiﬁcation as Training Data is Added

language and their actions. In this section we de-
scribe our annotation of Twitter users’ evacuation
behavior, and show that linguistic and geospatial
data can used for classiﬁcation.

User level classiﬁcation for Twitter users is a
well known problem for many domains. One com-
mon task is identifying political afﬁliation. Both
linguistic and non-linguistic features have proven
effective in classifying political leanings of users
in Twitter data (Tatman et al., 2017). Preoiuc-
Pietro et al. (2017) provide a method for identify-
ing whether users are liberal or conservative, and
point to a variety of user level classiﬁcations that
can be predictive of political ideology. These user-
level attributes apply generally; we intend to clas-
sify users based on a particular behavior they en-
gage in (evacuation or sheltering in place).

More similar is Sanagavarapu et al. (2017), who
predict whether users participate in events that
they are tweeting about. They use linguistics fea-
tures coupled with support vector machines to pre-
dict users’ participation in speciﬁc events, which
parallels our task of predicting a user’s event-
related behavior.

In the domain of crisis informatics, recent work
by Mart´ın et al. (2017) identiﬁes evacuation pat-
terns, using aggregates of geo-located tweets as
well as particular user behaviors. However, they
don’t empirically validate their observations, and
thus don’t attempt statistic learning for classiﬁca-
tion. Another study from Yang et al. (2017) stud-
ies user behavior during crisis events, using lin-
guistic and spatial features to analyze shifting sen-
timent during Hurricane Sandy. While they focus
on keyword tweets clustered geographically, they
show that geospatial features are helpful for anal-

ysis of user attitudes during crises.

3.1 Data
Our analysis is focused on users that are poten-
tially at risk, but these users are difﬁcult to iden-
tify due to the noisiness of Twitter data. To alle-
viate this problem, we attempt to identify vulner-
able users using geospatial information. For our
data, location-enabled tweets include any tweet re-
turned by the Twitter API with a precise point-
location attribute. This is sometimes the precise
latitude and longitude of the user’s mobile device;
however, and more common in recent years, these
are more general locations that, while encoded in
the tweet as a single geographic coordinate, rep-
resent businesses or more general regional loca-
tions. These often include cross-posts from other
social media services that track location such as
FourSquare, Swarm, or Instagram. Examples of
these locations include: ”Starbucks” (as an exact
store) or ”South Beach” (as a region).

For Hurricane Sandy, we used bounding boxes
for Evacuation Zone A in New York City as
well as boundaries of the coastal counties of New
Jersey to deﬁne geographically vulnerable areas.
Each of these areas were under mandatory evacu-
ation orders and generally exhibited high levels of
geographic risk to the storm.

3.2 Spatial Clustering
To reduce the noise and identify the most impor-
tant locations for a user, we apply a clustering al-
gorithm to all of the tweets for a given user. We
use Density Based Spatial Clustering (DBScan) to
cluster each user’s tweets based on their coordi-
nates (Ester et al., 1996). We chose this algorithm

70for two reasons. First, it does not require that
we declare a particular number of clusters ahead
of time. Since we cannot make any assumptions
about a user’s consistent located-enabled Twitter
activity, we do not know how many clusters will
best represent the recurring locations for any given
user. Second, it does not require that all points be
classiﬁed. This allows for rigid, similar sized clus-
ters with separate unclassiﬁable points. Through
empirical analysis of our data, this is critical to un-
derstanding a user’s recurring tweet locations be-
cause users tend to tweet very irregularly (spatially
speaking): on a moving bus or train, for example.

Once these spatially outlying points are marked
as noise, we focus analysis on locations of consis-
tent, recurring Twitter behavior, such as one’s res-
idence or workplace. Our clustering parameters
require a user to have at least ﬁve tweets within
100 meters of one-another within the three-month
period of study. These parameters are stricter than
those used in Jurdak et al. (2015) and were decided
through empirical analysis of spatial tweet distri-
butions of a few users. Since the purpose of the
clusters is to identify areas of work or residence
that may be at risk of a coastal hazard, 100 meters
allows the clustering to account for some noise and
inaccuracies in the reported location over the en-
tire study period. We remove any users who do not
have at least one identiﬁable cluster.

3.3 Temporal Clustering

To learn about a user’s regular (non-storm) Twit-
ter behavior, we identify their temporal tweeting
patterns up to the time of the storm. To general-
ize this over the entire period of study, we look
speciﬁcally at times of tweets per week. Given the
regular diurnal Twitter activity among users, we
next cluster the tweets by time of day and day of
week to establish a weekly tweeting distribution
for each user. Krumm et al. (2013) use a similar
method of discerning home locations based on the
time one is active, based on the American Time
Use Survey. First, we distinguish days as week-
days or weekends and then split these days into
six four-hour periods. The resulting 12 time bins
distinguish between common home and working
hours. Of these times, weekday evenings gener-
ally see the most Twitter activity.

3.4 Spatio-Temporal Clustering: Home

Locations

Co-occurrences between the geo- and temporal-
clusters identify likely home clusters as distinct
from work or school clusters. For example, if
a user’s tweets from geo-cluster A occur primar-
ily during weekdays from 12-4pm while geo-
cluster B primarily includes tweets from week-
day evenings from 8pm-12am, then we may in-
fer that geo-cluster A could represent that user’s
school or workplace while cluster B could repre-
sent their home. To perform this identiﬁcation of a
user’s before-storm home location, we then iden-
tify geo-clusters that commonly co-occur with the
following speciﬁc time bins that represent home
times: Weekdays between 12-4am, 4-8am, and
8pm-12am. The geometric centroid of the clus-
ter with the most tweets during these times is said
to be the user’s home location.

Note that these home locations don’t necessar-
ily represent where the user lives. While we qual-
itatively observe that these home locations usu-
ally appear to be correct, they also can be gyms,
ofﬁces, and other places that the user typically
tweets from. We’ll see in section 3.6 that home
location information is a good predictor of evacu-
ation behavior, regardless of whether it represents
an actual ’home’ or merely a location of consis-
tent behavior for a particular user when daily life
is not interrupted by a major storm. If this location
lands within the geographically vulnerable areas
under mandatory evacuation described above, this
user is said to geographically vulnerable. Further-
more, the empirically observed accuracy of this
approach to determining a user’s home location in-
vites further research that optimizes the clustering
(both spatially and the temporal bins) to improve
detection of a user’s home-location based on their
geo-located social media activity.

The simplicity of our approach combined with
the observed accuracy suggests that users are
likely not aware of the extent and accuracy of the
public geo-trace they are producing through their
social media activity. All of the tweets used for
this work were posted to the user’s Twitter time-
line for public consumption. As a ﬁrst step to pro-
tecting user’s privacy, we do not publish the user’s
Twitter handle, against the formal guidelines for
republication of Twitter data. Further, we inten-
tionally do not show a larger-scale rendering of
their calculated home location. For these reasons,

71Figure 2: A screen shot of our annotation tool. The timeline across the top shows this user was most
active at time of landfall. Tweets are displayed in chronological order on the left and an interactive
map on the right shows tweets, colored by different clusters. The transparent blue circle indicates the
calculated home location. Just before landfall and throughout the duration of the storm, the user is
tweeting from a different location (see popup) further inland than their calculated home: a strong signal
of evacuation.

and in part because the data is now over ﬁve years
old, we choose to publish the data as-is, without
their identiﬁable Twitter handle. These are self-
imposed ethical responsibilities because as far as
the data providers are concerned, this is public
data. We hope this situation invites further con-
versation around social media privacy, information
sharing, and more formalized ethical standards in
social media research concerning these highly per-
sonalized data traces.

3.5 Annotation

Our perception of spatially derived evacuation pat-
terns is clear: geographically vulnerable users
tweet from their vulnerable locations before the
storm and then do not tweet from this location at
landfall. However, few users have such clear cut
movement proﬁles. Furthermore, programmati-
cally searching for this behavior yields a troubling
amount of false positives. Just because a user is
not tweeting from home does not mean they have
chosen to evacuate. These complex user behaviors
led us to develop a tool and annotation process for
determining individuals responses to the events.

Our annotation involves determining if a user
evacuated, sheltered in place, or their behavior was
unclear based on the available data. Each user
was given one of these categories based on both
their tweet content and movement patterns as in-
ferred by manual inspection. This involved devel-
oping a framework for displaying tweets on a map
over a sliding window of time, allowing annota-
tors to easily identify what users were saying at
which locations, thus giving the capability to de-
termine possible evacuation behavior quickly and
accurately. Using this tool, we tagged 200 users
with evacuation, sheltering in place, or unclear,
along with a conﬁdence score for evacuation and
sheltering in place.

Note that this annotation process has inherent
problems: we can only indicate whether we be-
lieve a user evacuated based on their tweets and
geo-location. We can not prove that any user evac-
uated based only on these limited resources. So
while annotators tend to agree on whether they be-
lieve a person took a particular action (κ=.705 for
tweets annotators were conﬁdent of the correct an-
swer), the analysis is not objectively veriﬁed.

72Tweets
Hurricane Party!
I had to evacuate this is bull
East NY 4ever!
Prediction

Coords
40.6,-73.9
40.8,-72.9
40.8,-73.4
Evacuated

Time

12/29 14:03
12/29 19:26
12/30 11:45

Table 3: Sample Evacuation Prediction

3.6 Classiﬁcation
We employ supervised machine learning to pre-
dict each user’s possible actions during each event.
This is done by employing word embeddings to
represent tweet semantics combined with tempo-
ral and spatial features generated from tweet meta-
data. We treat each user’s full contextual stream
(all the tweets they produced from a week before
to a week after landfall) as a document (see Table
3 for an example). As a baseline for classiﬁcation,
we start with the average embedding over all the
words in the contextual stream, providing a sim-
ple document-level embedding.

3.6.1 Temporal Information
We’ve seen from section 3.1 that users’ tweeting
behavior varies greatly based on time.
In order
to capture this, we split each user into a series of
time bins. For each bin, we generate the average
embedding over all the tweets in the time slice.
These embeddings are concatenated and supplied
as input to the classiﬁer. We experimented with a
variety of bin sizes from 4 hours to 4 days. Smaller
bins capture more speciﬁc data, but are often con-
tain too few tweets to be useful. Larger bins pro-
vide more consistent, general information.

3.6.2 Spatial Information
We combine information from geo-tags with word
embeddings to generate more accurate represen-
tations of user behavior. For each temporal bin
generated above, we calculate a handful of spatial
features. First, we calculate the average location
of the user during that bin, using the mean lati-
tude and longitude of each tweet in that bin that
contains a geo-tag. We then use this to determine
the geometric distance from the average location
in that bin to the calculated home location from
section 3.4. This is a simple scalar feature indicat-
ing their distance from their typical home location.
As a second spatial feature, we calculated the av-
erage distance of each tweet within a bin from the
starting location of that bin, which indicates the
average amount the user moved during that time.

3.6.3 Relevance Filtering
In most cases the majority of tweets a user pro-
duces are irrelevant to a particular event. This cre-
ates additional noise in each time bin, making it
hard to predict behavior. We employ the relevance
classiﬁer above, trained on the full dataset, and use
it to predict relevance for each user’s tweets. We
then restrict the features above to only tweets that
the classiﬁer deemed relevant.

We evaluate Logistic Regression, Support Vec-
tor Machines, and Naive Bayes algorithms for user
classiﬁcation. We experimented with deep learn-
ing methods, but they showed much lower re-
sults, perhaps due to the small size of the dataset.
Support vector machines provided the best perfor-
mance on the baseline, and was used to evaluate
additional features. We performed 10-fold cross
validation over users. Table 4 shows the results of
adding each feature type and bin size. Each col-
umn represents the size of the temporal bin used.
The ”All” column uses only one bin, with all the
user’s tweets averaged. In this case the Distance
from Home Location is the distance from their
overall average location to the location of their cal-
culated home location from section 3.4.

Bin sizes from 1 to 4 days are most effective,
with distance from home location being the best
feature. Note that we did not objectively verify
these home locations: the classiﬁer uses this fea-
ture effectively regardless of whether it represents
the user’s real home, or just a location they regu-
larly tweet from. Relevance ﬁltering does not pro-
vide consistent improvement, which may be due
to data sparsity. Any ﬁltering reduces the amount
of tweets available for each bin, making the clas-
siﬁcation task more difﬁcult.

These four features (word embeddings, tempo-
ral and spatial information, and relevance ﬁltering)
all provide different ways of understanding user
behavior. Because they represent the data in differ-
ent ways, they are capable of classifying different
sections of the data accurately. We leverage this
by employing ensemble classiﬁcation employing
these features.
3.6.4 Ensemble Classiﬁcation
To combine feature’s beneﬁts, we use each of the
48 classiﬁers generated for Table 4. We combine
these classiﬁers incrementally, starting with the
classiﬁers that had the best performance in cross
validation. We trained each classiﬁer on 50% of
the data and evaluated it on the remaining 50%.

73Feature
Word Embedding Average
+Relevance Filter
Distance from Home Location
+Relevance Filter
Average Movement per Time Bin
+Relevance Filter
All
+Relevance Filter

4 hours

.467
.481
.610
.657
.484
.541
.533
.484

8 hours

.491
.484
.692
.621
.506
.501
.504
.485

1 day
.529
.489
.695
.672
.504
.526
.550
.493

2 days
.505
.606
.686
.661
.587
.566
.524
.486

4 days All Days
.555
.526
.639
.608
.641
.574
.550
.534

.529
.533
.674
.664
.533
.508
.531
.534

Table 4: Classiﬁcation Results (F1) for Each Feature Type and Bin Size. Bold indicates the best result
for that feature, italics is our word embedding baseline.

We then weighted each classiﬁer’s classiﬁcation
by the F1 score it received in cross validation on
the training set. This allowed for more classiﬁers
to be added providing additional information, but
still favoring the classiﬁers that performed best in
training. Results of the incremental addition of
classiﬁers are shown in Figure 3.

Figure 3: System performance (F1) as classiﬁers
are added.

3.7 Analysis
Adding additional classiﬁers improves perfor-
mance initially but after a certain point the added
classiﬁers decrease performance. The best per-
formance is achieved using around 20 classiﬁers,
which include those trained on all three indi-
vidual features for a wide variety of time bins.
While the ﬁrst 16 classiﬁers are based on dis-
tance from home cluster, the addition of word
embedding- and movement-based classiﬁers can
yield improved performance. As more classi-
ﬁers are added, performance drops, likely because
when the less accurate classiﬁers are added they
degrade performance.

Performance on user

classiﬁcation varies

greatly depending on the classiﬁcation method
and windows used. The basic word embedding
baseline over all tweets performs poorly (.529).
Prediction based on distance from a user’s home
location using a bin size of 1 day is the best
single classiﬁer (.695), and distance from their
calculated home performs best across all bin sizes.
The best F1 achieved through ensemble methods
is .741, a considerable improvement over the
performance of the best individual classiﬁer .694.
While it is difﬁcult to compare these results
to previous work, as the task has not yet been
attempted, there are some relevant comparisons.
Sanagavarapu et al.
(2017) report classifying
users’ participation in events with F1 scores vary-
ing from .52 to .74. They show that different fea-
tures yield different results based on the time pe-
riod, which parallels our results.

4 Conclusions

Evacuation behavior is difﬁcult to predict, but can
be done by leveraging both linguistic and geospa-
tial features. More data and better representations
of movement could improve this classiﬁcation, but
the changing nature of Twitter use is making pre-
cise geospatial data increasingly rare and harder
to make us of for behavior classiﬁcation in this
medium.

Our relevance classiﬁer achieves an F1 score of
near .83, and needs reﬁnement to be effectively
employed in this domain. Further improvements
to classiﬁcation can be made be effectively incor-
porating tweet context. In order to make use of this
classiﬁcation, we intend to experiment with real-
time relevance classiﬁcation, which will allow us
to better understand user behavior live as events
unfold.

74References
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the ﬂu: Detecting inﬂuenza
In Conference on Em-
epidemics using Twitter.
pirical Methods in Natural Language Processing
(EMNLP), pages 1568–1576.

Aron Culotta. 2010. Towards detecting inﬂuenza epi-
demics by analyzing twitter messages. In Proceed-
ings of the First Workshop on Social Media Ana-
lytics, SOMA ’10, pages 115–122, New York, NY,
USA. ACM.

Martin Ester, Hans-Peter Kriegel, J¨org Sander, and
Xiaowei Xu. 1996. A Density-Based Algorithm
for Discovering Clusters in Large Spatial Databases
2nd International Conference on
with Noise.
Knowledge Discovery and Data Mining, pages 226–
231.

Raja Jurdak, Kun Zhao, Jiajun Liu, Maurice Abou-
Jaoude, Mark Cameron, and David Newth. 2015.
Understanding human mobility from Twitter. PLoS
ONE, 10(7):e0131469.

Yoon Kim. 2014.

Convolutional neural networks
In Proceedings of the
for sentence classiﬁcation.
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar. Association for Computational Lin-
guistics.

John Krumm, Rich Caruana, and Scott Counts. 2013.
Learning likely locations. In Lecture Notes in Com-
puter Science (including subseries Lecture Notes in
Artiﬁcial Intelligence and Lecture Notes in Bioinfor-
matics), volume 7899 LNCS, pages 64–76.

Alex Lamb, Michael J Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking ﬂu infections on
twitter. In HLT-NAACL, pages 789–795.

Shen Li, Zhe Zhao, Tao Liu, Renfen Hu, and Xiaoy-
ong Du. 2017. Initializing convolutional ﬁlters with
semantic features for text classiﬁcation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 1884–1889,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Yago Mart´ın, Zhenlong Li, and Susan L. Cutter. 2017.
Leveraging twitter to gauge evacuation compli-
ance: Spatiotemporal analysis of hurricane matthew.
PLOS ONE, 12(7):1–22.

Tomas Mikolov, Kai Chen, Greg Corrado, and Dean
Jeffrey. 2013. Efﬁcient estimation of word repre-
sentations in vector space.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Daniel Preoiuc-Pietro, Ye Liu, Daniel Hopkins, and
Lyle Ungar. 2017. Beyond binary labels: Political
In Proceed-
ideology prediction of twitter users.
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 729–740, Vancouver, Canada. Associa-
tion for Computational Linguistics.

Krishna Chaitanya Sanagavarapu, Alakananda Vem-
pala, and Eduardo Blanco. 2017. Determining
whether and when people participate in the events
they tweet about. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 641–646,
Vancouver, Canada. Association for Computational
Linguistics.

Jagan Sankaranarayanan, Hanan Samet, Benjamin E
Teitler, Michael D Lieberman, and Jon Sperling.
In Proceed-
2009. Twitterstand: news in tweets.
ings of the 17th acm sigspatial international con-
ference on advances in geographic information sys-
tems, pages 42–51. ACM.

Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-
hatosmanoglu, and Murat Demirbas. 2010. Short
text classiﬁcation in twitter to improve information
In Proceedings of the 33rd international
ﬁltering.
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 841–842. ACM.

Kevin Stowe, Michael J. Paul, Martha Palmer, Leysia
Identifying
Palen, and Kenneth Anderson. 2016.
In Pro-
and categorizing disaster-related tweets.
ceedings of The Fourth International Workshop on
Natural Language Processing for Social Media,
pages 1–6, Austin, TX, USA. Association for Com-
putational Linguistics.

Rachael Tatman, Leo Stewart, Amandalynne Paullada,
and Emma Spiro. 2017. Non-lexical features encode
political afﬁliation on twitter. In Proceedings of the
Second Workshop on NLP and Computational So-
cial Science, pages 63–67, Vancouver, Canada. As-
sociation for Computational Linguistics.

Svitlana Volkova, Kyle Shaffer, Jin Yea Jang, and
Nathan Hodas. 2017. Separating facts from ﬁction:
Linguistic models to classify suspicious and trusted
In Proceedings of the 55th
news posts on twitter.
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
647–653, Vancouver, Canada. Association for Com-
putational Linguistics.

Min Yang, Jincheng Mei, Heng Ji, zhao wei, Zhou
Zhao, and Xiaojun Chen. 2017.
Identifying and
tracking sentiments and topics from social media
texts during natural disasters. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 527–533, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

75
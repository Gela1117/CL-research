Where is your Evidence: Improving Fact-checking by Justiﬁcation

Modeling

Tariq Alhindi and Savvas Petridis

Department of Computer Science

Columbia University

tariq@cs.columbia.edu
sdp2137@columbia.edu

Abstract

Fact-checking is a journalistic practice that
compares a claim made publicly against
trusted sources of facts. Wang (2017) intro-
duced a large dataset of validated claims from
the POLITIFACT.com website (LIAR dataset),
enabling the development of machine learn-
ing approaches for fact-checking. However,
approaches based on this dataset have fo-
cused primarily on modeling the claim and
speaker-related metadata, without considering
the evidence used by humans in labeling the
claims. We extend the LIAR dataset by auto-
matically extracting the justiﬁcation from the
fact-checking article used by humans to la-
bel a given claim. We show that modeling
the extracted justiﬁcation in conjunction with
the claim (and metadata) provides a signiﬁ-
cant improvement regardless of the machine
learning model used (feature-based or deep
learning) both in a binary classiﬁcation task
(true, false) and in a six-way classiﬁcation task
(pants on ﬁre, false, mostly false, half true,
mostly true, true).

1

Introduction

Fact-checking is the process of assessing the ve-
racity of claims.
It requires identifying evi-
dence from trusted sources, understanding the
context, and reasoning about what can be inferred
from the evidence. Several organizations such as
FACTCHECK.org, POLITIFACT.com and FULL-
FACT.org are devoted to such activities, and the
ﬁnal verdict can reﬂect varying degrees of truth
(e.g., POLITIFACT labels claims as true, mostly
true, half true, mostly false, false and pants on
ﬁre).

Until recently, the bottleneck for developing au-
tomatic methods for fact-checking has been the
lack of large datasets for building machine learn-
ing models. Thorne and Vlachos (2018) provide

Smaranda Muresan

Department of Computer Science

Data Science Institute
Columbia University

smara@columbia.edu

a survey of current datasets and models for fact-
checking (e.g., (Wang, 2017; Rashkin et al., 2017;
Vlachos and Riedel, 2014; Thorne et al., 2018;
Long et al., 2017; Potthast et al., 2018; Wang
et al., 2018)). Wang (2017) has introduced a large
dataset (LIAR) of claims from POLITIFACT, the
associated metadata for each claim and the ver-
dict (6 class labels). Most work on the LIAR
dataset has focused on modeling the content of the
claim (including hedging, sentiment and emotion
analysis) and the speaker-related metadata (Wang,
2017; Rashkin et al., 2017; Long et al., 2017).

However, these approaches do not use the ev-
idence and the justiﬁcation provided by humans
to predict the label. Extracting evidence from
(trusted) sources for fact-checking or for argu-
ment mining is a difﬁcult task (Rinott et al., 2015;
Thorne et al., 2018; Baly et al., 2018). For the
purpose of our paper, we rely on the fact-checking
article associated with the claim. We extend the
original LIAR dataset by automatically extract-
ing the justiﬁcation given by humans for labeling
the claim, from the fact-checking article (Section
2). We release the extended LIAR dataset (LIAR-
PLUS) to the community1.

The main contribution of this paper is to show
that modeling the extracted justiﬁcation in con-
junction with the claim (and metadata) provides
a signiﬁcant improvement regardless of the ma-
chine learning model used (feature-based or deep
learning) both in a binary classiﬁcation task (true,
false) and in a six-way classiﬁcation task (pants
on ﬁre, false, mostly false, half-true, mostly true,
true) (Section 4). We provide a detailed error anal-
ysis and per-class results.

Our work complements the recent work on pro-
viding datasets and models that enable the de-
velopment of an end-to-end pipeline for fact-

1https://github.com/Tariq60/LIAR-PLUS

ProceedingsoftheFirstWorkshoponFactExtractionandVERiﬁcation(FEVER),pages85–90Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics85checking ((Thorne et al., 2018) for English and
(Baly et al., 2018) for Arabic). We are primarily
concerned on showing the impact of modeling the
human-provided justiﬁcation for predicting the ve-
racity of a claim. In addition, our task aims to cap-
ture the varying degrees of truth that some claims
might have and that are usually labeled as such by
professionals (rather than binary true vs. false la-
bels).

2 Dataset

The LIAR dataset introduced by (Wang, 2017)
consists of 12,836 short statements taken from
POLITIFACT and labeled by humans for truthful-
ness, subject, context/venue, speaker, state, party,
and prior history. For truthfulness,
the LIAR
dataset has six labels: pants-ﬁre, false, mostly-
false, half-true, mostly-true, and true. These six
label sets are relatively balanced in size. The
statements were collected from a variety of broad-
casting mediums, like TV interviews, speeches,
tweets, debates, and they cover a broad range of
topics such as the economy, health care, taxes and
election.

We extend the LIAR dataset to the LIAR-PLUS
dataset by automatically extracting for each claim
the justiﬁcation that humans have provided in the
fact-checking article associated with the claim.
Most of the articles end with a summary that has a
headline “our ruling” or “summing up”. This sum-
mary usually has several justiﬁcation sentences
that are related to the statement. We extract all
sentences in these summary sections, or the last
ﬁve sentences in the fact-checking article when no
summary exists. We ﬁlter out the sentence that
has the verdict and related words. These extracted
sentences can support or contradict the statement,
which is expected to enhance the accuracy of the
classiﬁcation approaches. Excerpt from the LIAR-
PLUS dataset is shown in Table 1.

3 Methods

The main goal of our paper is to show that mod-
eling the human-provided justiﬁcation — which
can be seen as a summary evidence — improves
the assessment of a claim’s truth when compared
to modeling the claim (and metadata) alone, re-
gardless of the machine learning models (feature
based vs. deep learning models). All our models

Statement:“Says Rick Scott cut education to pay for even
more tax breaks for big, powerful, well-connected corpo-
rations.”
Speaker: Florida Democratic Party
Context: TV Ad
Label: half-true
Extracted Justiﬁcation: A TV ad by the Florida Demo-
cratic Party says Scott ”cut education to pay for even more
tax breaks for big, powerful, well-connected corporations.”
However, the ad exaggerates when it focuses attention
on tax breaks for ”big, powerful, well-connected corpo-
rations.” Some such companies beneﬁted, but so did many
other types of businesses. And the question of whether the
tax cuts and the education cuts had any causal relationship
is murkier than the ad lets on.

Table 1: Excerpt from the LIAR-PLUS dataset

use 4 different conditions: basic claim/statement2
representation using just word representations (S
condition), enhanced claim/statement representa-
tion that captures additional information shown
to be useful such as hedging, sentiment strength
and emotion (Rashkin et al., 2017) as well as
metadata information (S+M condition), basic
claim/statement and the associated extracted jus-
tiﬁcation (SJ condition) and ﬁnally enhanced
claim/statement representation, metadata and jus-
tiﬁcation (S+MJ condition).

Feature-based Machine Learning. We experi-
ment with both Logistic Regression (LR) and Sup-
port Vector Machines (SVM) with linear kernel.
For the basic representation of the claim/statement
(S condition) we experimented with unigram fea-
tures, tf-idf weighted unigram features and Glove
word embeddings (Pennington et al., 2014). The
best representation proved to be unigrams. For
the enhanced statement representation (S+) we
modeled: sentiment strength using SentiStrength,
which measures the negativity and positivity of
a statement on a scale of 1-to-5 (Thelwall et al.,
2010); emotion using the NRC Emotion Lexi-
con (EmoLex), which associates each word with
eight basic emotions (Mohammad and Turney,
2010), and the Linguistic Inquiry and Word Count
(LIWC) lexicon (Pennebaker et al., 2001). In ad-
dition, we include metadata information such as
the number of claims each speaker makes for ev-
ery truth-label (history) (Wang, 2017; Long et al.,
2017). Finally for representing the justiﬁcation in
the SJ and S+MJ conditions, we just use unigram
features.

2In the rest of the paper we will refer to the claim as state-

ment.

86Cond.

Model

Binary

S

SJ

S+M

S+MJ

LR
SVM

LR
SVM

valid
0.58
0.56
BiLSTM 0.59
0.68
0.65
BiLSTM 0.70
P-BiLSTM 0.69
0.61
0.57
BiLSTM 0.62
0.69
0.66
BiLSTM 0.71
P-BiLSTM 0.70

LR
SVM

LR
SVM

test
0.61
0.59
0.60
0.67
0.66
0.68
0.67
0.61
0.60
0.62
0.67
0.66
0.68
0.70

Six-way
test
0.25
0.23
0.23
0.37
0.34
0.31
0.35
0.25
0.25
0.25
0.37
0.35
0.32
0.36

valid
0.23
0.25
0.26
0.37
0.34
0.34
0.36
0.26
0.26
0.27
0.38
0.35
0.34
0.37

Class

class size

S

SJ

pants-ﬁre

false

mostly-false

half-true
mostly-true

true

total/avg

116
263
237
248
251
169
1284

LR BiLSTM LR BiLSTM P-BiLSTM
0.18
0.28
0.21
0.22
0.23
0.22
0.23

0.19
0.34
0.13
0.28
0.33
0.18
0.26

0.37
0.33
0.35
0.39
0.40
0.37
0.37

0.34
0.3
0.31
0.31
0.39
0.42
0.34

0.37
0.33
0.32
0.37
0.39
0.39
0.36

Table 3: F1 Score Per Class on Validation Set

Class

class size

S

SJ

pants-ﬁre

false

mostly-false

half-true
mostly-true

true

total/avg

92
250
214
267
249
211
1283

LR BiLSTM LR BiLSTM P-BiLSTM
0.12
0.31
0.25
0.24
0.23
0.25
0.25

0.11
0.31
0.15
0.26
0.30
0.16
0.23

0.38
0.35
0.35
0.41
0.35
0.37
0.37

0.33
0.32
0.27
0.27
0.35
0.36
0.31

0.39
0.35
0.33
0.34
0.33
0.41
0.35

Table 2: Classiﬁcation Results

Table 4: F1 Score Per Class on Test Set

Deep Learning Models. We chose to use Bi-
Directional Long Short-term Memory (BiLSTM)
(Hochreiter and Schmidhuber, 1997) architectures
that have been shown to be successful for vari-
ous related NLP tasks such a textual entailment
and argument mining. For the S condition we use
just one BiLSTM to model the statement. We use
Glove pre-trained word embeddings (Pennington
et al., 2014), a 100 dimension embedding layer
that is followed by a BiLSTM layer of size 32. The
output of the BiLSTM layer is passed to a soft-
max layer.
In the S+M condition, a normalized
count vector of those features (described above)
is concatenated with the output of the BiLSTM
layer to form a merge layer before the softmax.
We used a categorical cross entropy loss func-
tion and ADAM optimizer (Kingma and Ba, 2014)
and trained the model for 10 epochs. For the SJ
and S+MJ conditions we experiment with two ar-
chitectures:
in the ﬁrst one we just concatenate
the justiﬁcation to the statement and pass it to a
single BiLSTM, and in the second one we use
a dual/parallel architecture where one BiLSTM
reads the statement and another one reads the justi-
ﬁcation (architecture denoted as P-BiLSTM). The
outputs of these BiLSTMs are concatenated and
passed to a softmax layer. This latter architec-
ture has been proven to be effective for tasks that
model two inputs such as textual entailment (Con-
neau et al., 2017) or sarcasm detection based on
conversation context (Ghosh et al., 2017; Ghosh
and Veale, 2017).

4 Results and Error Analysis
Table 2 shows the results both for the binary and
the six-way classiﬁcation tasks under all 4 con-
ditions (S, SJ, S+M and S+MJ) for our feature-
based machine learning models (LR and SVM)
and the deep learning models (BiLSTM and P-
BiLSTM). For the binary runs we grouped pants
on ﬁre, false and mostly false as FALSE and true,
mostly true and half true as TRUE. As reference,
Wang (2017 best models (text and metadata) ob-
tained 0.277 F1 on validation set and 0.274 F1
on test set in the six-way classiﬁcation, showing
relatively similar results with our equivalent S+M
condition.

It is clear from the results shown in Table 2 that
including the justiﬁcation (SJ and S+MJ condi-
tions) improves over the conditions that do not use
the justiﬁcation (S and S+M, respectively) for all
models, both in the binary and the six-way classi-
ﬁcation tasks. For example, for the six-way classi-
ﬁcation, we see that the BiLSTM model for the SJ
condition obtains 0.35 F1 compared to 0.23 F1 in
the S condition. LR model has a similar behaviour
with 0.37 F1 for the SJ condition compared to 0.25
F1 in S condition. For the S+MJ conditions the
best model (LR) shows an F1 of 0.38 compared to
0.26 F1 in the S+M condition (similar results for
the deep learning). The dual/parallel BiLSTM ar-
chitecture provides a small improvement over the
single BiLSTM only in the six-way classiﬁcation.
We also present the per-class results for the six-
way classiﬁcation for the S and SJ conditions. Ta-
ble 3 shows the results on validation set, while
Table 4 on the test set.
In the S condition, we

87ID Statement
1

We have the highest tax rate anywhere
in the world.

2

3

4

5

6

7

“Says Rick Scott cut education to pay
for even more tax breaks for big, pow-
erful, well-connected corporations.”

Says Donald Trump has given more
money to Democratic candidates than
Republican candidates.

Says out-of-state abortion clinics have
marketed their services to minors in
states with parental consent laws.

inspections

Obamacare provision will allow forced
home
government
agents.
In the month of January, Canada created
more new jobs than we did.

by

There has been $5 trillion in debt added
over the last four years.

Justiﬁcation
Trump, while lamenting the condition of the middle class, said the U.S. has
”the highest tax rate anywhere in the world.” All sets of data we examined
for individual and family taxes prove him wrong. Statutory income tax
rates in the U.S. fall around the end of the upper quarter of nations. More
exhaustive measures - which compute overall tax burden per person and as
a percentage of GDP - show the U.S. either is in the middle of the pack or
on the lighter end of taxation compared with other advanced industrialized
nations.
A TV ad by the Florida Democratic Party says Scott ”cut education to pay
for even more tax breaks for big, powerful, well-connected corporations.”
However, the ad exaggerates when it focuses attention on tax breaks for
”big, powerful, well-connected corporations.” Some such companies ben-
eﬁted, but so did many other types of businesses. And the question of
whether the tax cuts and the education cuts had any causal relationship is
murkier than the ad lets on.
but public records show that the real estate tycoon has actually contributed
around $350,000 more to Republicans at the state and federal level than
Democrats. That, however, is a recent development. Fergusons statement
contains an element of truth but ignores critical facts.
As Cousins clinic in New York told Yellow Page users in Pennsylvania, ”No
state consents.” This is information the clinics wanted patients or potential
patients to have, and paid money to help them have it. Whether it was to
help persuade them to come in or not, it provided pertinent facts that could
help them in their decision-making. It ﬁt the deﬁnition of marketing.
But the program they pointed to provides grants for voluntary help to at-risk
families from trained staff like nurses and social workers. What bloggers
describe would be an egregious abuse of the law not whats allowed by it.
In November 2010, the U.S. economy created 93,000 jobs, compared to
15,200 for Canada. And in December 2010, the U.S. created 121,000 jobs,
compared to 22,000 for Canada. ”But on a per capita basis, in recent months
U.S. job creation exceeded Canada’s only in October.” January happened to
be a month when U.S. job creation was especially low and Canadian job
creation was especially high, but it is the most recent month and it reﬂects
the general pattern when you account for population.
number is either slightly high or a little low, depending on the type of mea-
surement used, and thats actually for a period short of a full four years. His
implication that Obama and the Democrats are to blame has some merit, but
it ignores the role Republicans have had.

label
false

S
X

S+M SJ

S+MJ

half-true

X X

mostly-false X X

true

X X

pants-ﬁre

X X

true

X X

X

X

X

X

mostly-true

X X

X

X

Table 5: Error analysis of Six-way Classiﬁcation (Logistic Regression)

see a larger degree of variation in performance
among classes, with the worst being the pants-on-
ﬁre for all models, and for the deep learning model
also the mostly-false and true classes. In the SJ
condition, we notice a more uniform performance
on all classes for all the models. We notice the
biggest improvement for the pants-on-ﬁre class for
all models, half-true for LR and mostly-false and
true for the deep learning models. When compar-
ing the P-BiLSTM and BiLSTM we noticed that
the biggest improvement comes from the half-true
class and the pants-on-ﬁre class.

Error Analysis
In order to further understand
the cause of the errors made by the models, we
analyzed several examples by looking at the state-
ment, justiﬁcation and predictions by the logistic
regression model when using the S, S+M, SJ, and
S+MJ conditions (Table 5). Logistic regression
was selected since it has the best numbers for the
six-way classiﬁcation task.

The ﬁrst example in Table 5 was wrongly clas-
siﬁed in the S condition, but classiﬁed correctly
in the S+M, SJ and S+MJ conditions. The justi-
ﬁcation text has a sentence saying “Statutory in-
come tax rates in the U.S. fall around the end of
the upper quarter of nations.”, which contradicts
the statement and thus is classiﬁed correctly when

modeling the justiﬁcation.

The second and third examples in Table 5 were
correctly predicted only when the justiﬁcation was
modeled (SJ and S+MJ conditions). For statement
2, the justiﬁcation text has a sentence “However,
the ad exaggerates...” indicates that the statement
has some false and some true information. There-
fore, the model predicts the correct label “half-
true” when modeling the justiﬁcation text. Also,
the justiﬁcation for statement 3 was simple enough
for the model to predict the gold label “mostly-
false”. It has a phrase like “more to Republicans”
while the statement had “more to Democratic can-
didates” which indicates falsehood in the state-
ment as well as discourse markers indicating con-
cessive moves (“but” and “however”).

Sometimes justiﬁcation features alone were not
enough to get the correct prediction without us-
ing the enhanced statement and metadata features.
The justiﬁcation for statement 4 in Table 5 is com-
plex and no direct connection can be made to the
statement. Therefore, the model fails when using
SJ and S+M conditions and only succeed when
using all features (i.e., S+MJ condition).
In ad-
dition, consider the 5th statement in Table 5 about
Obamacare, it seems that metadata features, which
have the history of the speaker, might have helped

88in predicting its factuality to be “pants on ﬁre”,
while it was wrongly classiﬁed when modeling
only the statement and the justiﬁcation.

For around half of the instances in validation
set, all models had wrong predictions. This is
not surprising since the best model had an average
F1 score of less than 0.40. The last two example
in Table 5 are instances where the model makes
mistakes under all 4 conditions. The claim and
justiﬁcation refer to temporal information which
is harder to model by the rather simple and shal-
low approaches we used. Incorporating temporal
and numeric information when modeling the claim
and justiﬁcation would be essential for capturing
the correct context of a given statement. Another
source of error for justiﬁcation-based conditions
was the noise in the extraction of the justiﬁcation
particularly when the “our ruling” and “summing
up” headers were not included and we resorted to
extract the last 5 sentences from the fact-checking
articles. Improving the extraction methods will be
helpful to improving the justiﬁcation-based classi-
ﬁcation results.

5 Conclusion and Future Work

We presented a study that shows that modeling
the human-provided justiﬁcation form the fact-
checking article associated with a claim is im-
portant leading to signiﬁcant improvements when
compared to modeling just the claim/statement
and metadata for all the machine learning mod-
els both in a binary and a six-way classiﬁca-
tion task. We released LIAR-PLUS, the extended
LIAR dataset that contains the automatically ex-
tracted justiﬁcation. We also provided an error
analysis and discussion of per-class performance.
Our simple method for extracting the justiﬁ-
cation from the fact-checking article can lead to
slightly noisy text (for example it can contain a
repetition of the claim or it can fail to capture
the entire evidence). We plan to further reﬁne
the justiﬁcation extraction method so that it con-
tains just the summary evidence. In addition, we
plan to develop methods for evidence extraction
from the web (similar to the goals of the FEVER
shared task (Thorne et al., 2018)) and compare
the results of the automatically extracted evidence
with the human-provided justiﬁcations for fact-
checking the claims.

References
Ramy Baly, Mitra Mohtarami, James Glass, Llu´ıs
M`arquez, Alessandro Moschitti, and Preslav Nakov.
2018. Integrating stance detection and fact checking
in a uniﬁed corpus. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 2 (Short Papers), pages
21–27.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680. Associ-
ation for Computational Linguistics.

Aniruddha Ghosh and Tony Veale. 2017. Magnets for
sarcasm: Making sarcasm detection timely, contex-
tual and very personal. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 482–491.

Debanjan Ghosh, R. Alexander Fabbri, and Smaranda
Muresan. 2017. The role of conversation context
for sarcasm detection in online interactions. In Pro-
ceedings of the 18th Annual SIGdial Meeting on Dis-
course and Dialogue, pages 186–196. Association
for Computational Linguistics.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Yunfei Long, Qin Lu, Rong Xiang, Minglei Li,
and Chu-Ren Huang. 2017. Fake news detection
through multi-perspective speaker proﬁles. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers).

Saif M Mohammad and Peter D Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 workshop on
computational approaches to analysis and genera-
tion of emotion in text, pages 26–34. Association for
Computational Linguistics.

James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71(2001):2001.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

89Martin Potthast, Johannes Kiesel, Kevin Reinartz,
Janek Bevendorff, and Benno Stein. 2018. A stylo-
metric inquiry into hyperpartisan and fake news. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 231–240. Association for Com-
putational Linguistics.

Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana
Volkova, and Yejin Choi. 2017. Truth of varying
shades: Analyzing language in fake news and polit-
ical fact-checking. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2931–2937.

Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M. Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence - an auto-
matic method for context dependent evidence detec-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing,
pages 440–450.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the As-
sociation for Information Science and Technology,
61(12):2544–2558.

J Thorne and A Vlachos. 2018. Automated fact check-
ing: Task formulations, methods and future direc-
In Proceedings of the 27th International
tions.
Conference on Computational Linguistics (COLING
2018).

James

Thorne,

Andreas Vlachos,

Christos
Christodoulopoulos,
and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
veriﬁcation. In NAACL-HLT.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task deﬁnition and dataset construction.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18–22.

William Yang Wang. 2017. “liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2017),
Vancouver, BC, Canada. ACL.

Xuezhi Wang, Cong Yu, Simon Baumgartner, and Flip
Korn. 2018. Relevant document discovery for fact-
In Companion of the The Web
checking articles.
Conference 2018 on The Web Conference 2018,
pages 525–533. International World Wide Web Con-
ferences Steering Committee.

90
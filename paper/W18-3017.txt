Knowledge Graph Embedding with Numeric Attributes of Entities

Yanrong Wu, Zhichun Wang∗

College of Information Science and Technology

Beijing Normal University, Beijing 100875, PR. China

yrwu@mail.bnu.edu.cn, zcwang@bnu.edu.cn

Abstract

(KG)

Knowledge Graph
embedding
projects entities and relations into low
dimensional vector space, which has been
successfully applied in KG completion
task. The previous embedding approaches
only model entities and their relations,
ignoring a large number of entities’
numeric attributes in KGs. In this paper,
we propose a new KG embedding model
which jointly model entity relations and
numeric attributes. Our approach com-
bines an attribute embedding model with
a translation-based structure embedding
model, which learns the embeddings of
entities, relations, and attributes simulta-
neously. Experiments of link prediction
on YAGO and Freebase show that
the
performance is effectively improved by
adding entities’ numeric attributes in the
embedding model.
Introduction

KGs

encode

1
a number of Knowledge Graphs
Recently,
(KGs) have been created,
such as DBpe-
dia (Lehmann, 2015), YAGO (Mahdisoltani
et al., 2015), and Freebase (Bollacker et al.,
2008).
structured informa-
tion of entities in the form of
triplets (e.g.
(cid:104)M icrosof t, isLocatedIn, U nitedStates(cid:105)), and
have been successfully applied in many real-
world applications. Although KGs contain a huge
amount of triplets, most of them are incomplete.
In order to further expand KGs, much work on KG
completion has been done, which aims to predict
new triplets based on the existing ones in KGs. A
promising group of research for KG completion
is known as KG embedding. KG embedding

∗Corresponding Author

approaches project entities and relations into a
continuous vector space while preserving the orig-
inal knowledge in the KG. KG embedding models
achieve good performance in KG completion in
terms of efﬁciency and scalability. TransE is a
representative KG embedding approach (Bordes
et al., 2013), which projects both entities and
relations into the same vector space:
if a triplet
(cid:104)head entity, relation, tail entity(cid:105) (denoted as
(cid:104)h, r, t(cid:105)) holds, TransE wants that h + r ≈ t.
The embeddings are learned by minimizing a
margin-based ranking criterion over the training
set. TransE model is simple but powerful, and
it gets promising results on link prediction and
triple classiﬁcation problems. There are several
enhanced model of TransE, including TransR (Lin
et al., 2015), TransH (Wang et al., 2014) and
TransD (Ji et al., 2015) etc. By introducing new
representations of
later
approaches achieve better performance at
the
cost of increasing model complexity. Recent
surveys (Wang et al., 2017; Nickel et al., 2016)
give detailed introduction and comparison of
various KG embedding approaches.

translation,

relational

(i.e.

However, most of
approaches

the existing KG em-
bedding
only model
relational
triplets
triplets of entity relations),
while ignoring a large number of attributive
triplets (i.e.
triplets of entity attributes, e.g.
(cid:104)M icrosof t, wasF oundedOnDate, 1975(cid:105))
in KGs.
attributive triplets describe various
attributes of entities, such as ages of people or
areas of a city. There are a huge number of
attributive triplets in real KGs, and we believe
that information encoded in these triplets is also
useful for predicting entity relations. Having
the above motivation, we propose a new KG
embedding approach that
jointly model entity
relations and entities’ numeric attributes. Our
approach consists of two component models,

Proceedingsofthe3rdWorkshoponRepresentationLearningforNLP,pages132–136Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics132structure embedding model and attribute embed-
ding model.
The structure embedding model
is a translational distance model that preserves
the knowledge of entity relations;
the attribute
embedding model is a regression-based model
that preserves the knowledge of entity attributes.
Two component models are jointly optimized
to get the embeddings of entities, relations, and
attributes.
Experiments of link prediction on
YAGO and Freebase show that the performance is
effectively improved by adding entities’ numeric
attributes in the embedding model.

2 Our Approach

To effectively utilize numeric attributes of entities
in KG embedding, we propose TransEA, which
combine a new attribute embedding model with
the structure embedding model of TransE. Two
component models in TransEA share the embed-
dings of entities, and they are jointly optimized in
the training process.

2.1 Structure Embedding
The structure embedding directly adopts the
translation-based method in TransE to model the
relational triplets in KGs. Both Entities and rela-
tions in a KG are represented in the same vector
space Rd. In a triplet (cid:104)h, r, t(cid:105), the relation is con-
sidered as a translation vector r, which connects
the vector of entities h and t with low error, i.e.
h + r ≈ t. The score function of a given triplet
(cid:104)h, r, t(cid:105) is deﬁned as

fr(h, t) = −||h + r − t||1/2

(1)
||x||1/2 denotes either the L1 or L2 norm. For all
the relational triplets in the KG, the loss function
of the structure embedding is deﬁned as:

(cid:88)

(cid:88)

LR =

(cid:104)h,r,t(cid:105)∈S

(cid:104)h(cid:48),r,t(cid:48)(cid:105)∈S(cid:48)

[γ +fr(h, t)−fr(h(cid:48), t(cid:48))]+

(2)
where [x]+ = max{0, x}, S(cid:48) denotes the set
of negative triplets constructed by corrupting
(cid:104)h, r, t(cid:105), i.e.
replacing h or t with a randomly
chosen entity in KG; γ > 0 is a margin hyper-
parameter separating positive and negative triplets.

2.2 Attribute Embedding
Attribute embedding model takes all the attribu-
tive triplets in a KG as input, and learns embed-
dings of entities and attributes. Both entities and

attributes are represented as vectors in space Rd.
In an attributive triplet (cid:104)e, a, v(cid:105), e is an entity, a is
an attribute, and v is the value of the entity’s at-
tribute. In our approach, we only consider attribu-
tive triplets containing numeric values or values
can be easily converted into numeric ones. For a
triplet (cid:104)e, a, v(cid:105), we deﬁne a score function as

fa(e, v) = −||a(cid:62) · e + ba − v||1/2

(3)

where a and e are vectors of attribute a and entity
e, ba is a bias for attribute a. The idea of this score
function is to predict the attribute value by a linear
regression model of attribute a; the vector a and
bias ba are the parameters of the regression model.
For all the attributive triplets in the KG, the loss
function of the attribute embedding is deﬁned as:

LA =

fa(e, v)

(4)

(cid:88)

(cid:104)e,a,v(cid:105)∈T

where T is the set of all attributive triplets with
numeric values in the KG.

Joint Model

2.3
To combine the above two component models,
TransEA minimizes the following loss function:

L = (1 − α) · LR + α · LA

(5)

where α is a hyper-parameter that balances the im-
portance of structure and attribute embedding. In
the joint model, we let the embeddings of entities
shared by two component models. Entities, rela-
tions, and attributes are all represented by vectors
in Rd. We implement our approach by using Ten-
sorFlow1, and the loss function is minimized by
performing stochastic gradient descent.

3 Experiments
3.1 Datasets
The following two datasets are used in the experi-
ments, Table 1 shows their detail information.

YG58K. YG58K is a subset of YAGO3
(Mahdisoltani et al., 2015) which contains about
58K entities. YG58K is built by removing enti-
ties from YAGO3 that appear less than 25 times
or have no attributive triplets. All the remain-
ing triplets are then randomly split into train-
ing/validation/test sets.

1https://www.tensorﬂow.org

133FB15K. FB15K is a subset of triplets extracted
from Freebase2. This subset of Freebase was
originally used in (Bordes et al., 2013), and then
widely used for evaluating KB completion ap-
proaches. Since our approach consumes attribu-
tive triplets, we extract all the attributive triplets
of entities in FB15K from Freebase to build the
evaluation dataset.

Datasets
# Relational Triplets
# Attributive Triplets
# Entities
# Relations
# Attributes
# Train Sets
# Valid Sets
# Test Sets

YG58K FB15K
592213
497783
24034
130287
14951
58130
1345
336

32
24

399480
49171
49132

483142
59071
50000

Table 1: Statistics of datasets

3.2 Experimental setup
In the experiments, Mean Rank (the mean rank of
the original correct entity), Hits@k (the proportion
of the original correct entity to the top k entities),
and MRR (the mean reciprocal rank) are used as
evaluation metrics. Given a testing triplet (cid:104)h, r, t(cid:105),
we replace the head h by every entity in the KGs
and calculate dissimilarity measures according to
the score function fr. Ranking the scores in as-
cending order, then we get the rank of the original
correct triplet to compute the evaluation metrics.
And we repeat the procedure when removing the
tail t instead of the head h. We name the evalu-
ation setting as “Raw”. While corrupted triplets
that appear in the train/valid/test sets (except the
original correct one) may underestimate the met-
rics, we also ﬁlter out those corrupted triplets be-
fore getting the rank of each testing triplet and we
call this process “Filter”.

Because our approach is built based on TransE,
we compare our approach with TransE to see
whether adding attribute embedding in the model
improves the performance of link prediction. For
TransE and TransEA, we consider the learning
rate λ among {0.1, 0.01, 0.001}, the margin γ
among {1, 2, 4, 10}, the dimensions of embed-
ding d among {20, 50, 100, 150}, the types of
norm in two score functions among {L1, L2},
and α among {0.2, 0.3, 0.4, 0.5, 0.6}. Based
on the mean rank in validation set, we select
the best conﬁgurations for two approaches. On

2https://everest.hds.utc.fr/doku.php?id=en:transe

the YG58K dataset, the best parameter conﬁg-
uration for TransE is (λ = 0.1, γ = 4, d =
50, fr = L1, fa = L1), and for TransEA is
(λ = 0.001, γ = 4, d = 50, fr = L1, fa =
L1, α = 0.6). On the FB15K dataset, the best pa-
rameter conﬁguration for TransE is(λ = 0.01, γ =
1, d = 50, fr = L1, fa = L1), and for TransEA
is (λ = 0.001, γ = 2, d = 100, fr = L1, fa =
L1, α = 0.3).

3.3 Results

Table 2 shows the results of link prediction on
YG58K and FB15K datasets. The results of pre-
dicting head and tail entities are outlined sepa-
rately, and we also report the overall results by
considering prediction of both head and tail en-
tity. According to the overall results, TransEA out-
performs TransE on both two datasets in terms of
all the three metrics. TransEA gets lower Mean
Ranks by about 10 on YG58K dataset; the MRR
and Hits@k of two approaches are very close,
TransEA gets slightly better results, the improve-
ments of MRR and Hits@k are 0.1-0.2% and 0-
0.3%. On FB15K dataset, TransEA gets lower
Mean Ranks by 13, and it also gets better re-
sults than TransE according to MRR, Hits@10 and
Hits@3.

Table 3 shows the results of different relational
categories. In general, TransEA has superiority on
two datasets, except one-to-many relation for re-
placing head entity on YG58K. And the improve-
ments on FB15K are larger than YG58K.

In order to ﬁgure out which relations are
predicted more accurately by TransEA, Table
4 lists the top 5 improved relations in terms
of Hits@10 on YG58K. It shows the best im-
provement of Hits@10 is 25% for the relation
isInterestedIn. The second one is 12.5%
and the third
for hasAcademicAdvisor,
is 6.3% for worteMusicFor.
Entities of
these three relations have plenty of numeric
attributes
(wasBornOnDate, diedOnDate
) describing people, we believe they are helpful
to improving the embeddings of entity relations.
Entities in relational
triplets about livesIn,
(e.g. (cid:104)HankAzaria, livesIn, N ewY ork(cid:105)), also
have some numeric attributes (hasLatitude,
hasLongtude, hasNumberOfPeople,
etc), therefore TransEA gets a 5% improvement
of Hits@10.

On FB15K dataset, ﬁve relations have 100%

134Dataset

Entity

Model

Head

YG58K

Tail

All

Head

FB15K

Tail

All

TransE
TransEA
TransE
TransEA
TransE
TransEA
TransE
TransEA
TransE
TransEA
TransE
TransEA

Mean Rank
Raw Filter
731
950
723
944
234
240
229
223
482
595
473
586
115
240
225
100
87
168
76
157
101
204
191
88

MRR(%)

Raw Filter
5.2
3.1
5.4
3.1
10.2
8.4
8.5
10.5
7.7
5.7
7.9
5.8
25.2
14.5
15.1
28.1
28.2
17.6
30.9
18.2
26.7
16.0
16.7
29.5

Hits@10(%)
Raw Filter
15.4
9.1
16.0
9.4
31.9
27.0
27.6
32.7
23.7
18.0
24.3
18.5
68.7
47.0
49.5
74.0
75.1
54.8
80.5
57.5
71.9
50.9
53.5
77.3

Hits@3(%)
Raw Filter
4.1
8.4
8.5
4.1
17.0
12.2
12.4
17.6
12.7
8.2
13.0
8.2
52.4
26.2
28.0
60.1
58.9
32.8
66.6
34.5
55.7
29.5
31.3
63.3

Hits@1(%)
Raw Filter
3.2
1.0
3.4
1.1
6.5
4.5
4.7
6.8
4.8
2.8
5.1
2.9
30.6
11.8
11.8
34.8
16.4
35.7
40.0
16.3
14.1
33.1
37.4
14.0

DATASETS

YG58K

FB15K

TASK
REL.CAT
TransE
TransEA
TransE
TransEA

Table 2: Link prediction results

Predicting Head(Hits@10)

Prediction Tail(Hits@10)

1-to-1
61.4
63.9
78.1
84.3

1-to-N N-to-1 N-to-N 1-to-1
45.5
62.0
63.3
36.4
78.0
93.8
95.5
83.3

15.4
16.0
68.7
74.0

15.5
16.0
72.3
77.6

1-to-N N-to-1 N-to-N
31.1
18.2
31.9
22.7
75.6
42.1
52.4
81.1

31.9
32.7
75.1
80.5

Table 3: Hits@10(%) by relational category in the ﬁltered evaluation setting. (N. stand for MANY)

Relation
isInterestedIn
hasAcademicAdvisor
wroteMusicFor
livesIn
hasNeighbor

TransE
50.0
31.3
12.5
23.8
48.1

TransEA

75.0
43.8
18.8
28.8
52.8

Table 4: Top 5 relations of promoted Hits@10 and
their Hits@10(%) on YG58K

two

organization/dateFounded.
And the
relation
music/artists supported
has
triplets with
attributes
triplet
person/dateOfBirth
with person/heightMeters. Therefore, the
quality of predicted links can be improved as well
even with only a small number of entities numeric
attributes.

numeric
and

one

TransE

TransEA

4 Conclusion

Relation
business/brand/company
base/celebrity/restaurant
base/celebrity/product
music/artists supported
sports/competition/country

24
249
24
44
24

2
4
2
3
4

Table 5: Top 5 relations of promoted Hit@10 and
their Mean Rank on FB15K

because TransE
improvements of Hits@10,
does not correctly predict any correct
triplets
in the top 10 ranked ones. We ﬁnd that these
relations only have one single sample in the
test sets, so Table 5 lists the Mean Rank of
them. Obviously, TransEA improves their Mean
Rank a lot. Entities in triplets of the ﬁve rela-
tions have only a few attributes. For example,
the
business/brand/company
only has one numeric attributive triplet about

relation

In this paper, we propose TransEA, an embed-
ding approach which jointly models relational
and attributive triplets in KGs. TransEA com-
bines an attribute embedding model with the
translation-based embedding model
in TransE.
Experiments on YAGO and Freebase show that
TransEA achieves better performance than TransE
in link prediction task. In the future, we will study
how to predict missing attribute values in KGs
based on KG embedding.

Acknowledgments

The work is supported by the National Key Re-
search and Development Program of China (No.
2017YFC0804004) and the National Natural Sci-
ence Foundation of China (No. 61772079).

135References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. SIGMOD 08 Proceedings of the
2008 ACM SIGMOD international conference on
Management of data, pages 1247–1250.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
In Proceedings of Advances in
relational data.
neural information processing systems (NIPS2013),
pages 2787–2795.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and
Jun Zhao. 2015. Knowledge graph embedding via
In Proceedings of the
dynamic mapping matrix.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, vol-
ume 1, pages 687–696.

J. Lehmann. 2015. Dbpedia: A large-scale, multilin-
gual knowledge base extracted from wikipedia. Se-
mantic Web, 6(2):167–195.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015. Learning entity and relation
embeddings for knowledge graph completion.
In
Proceedings of the Twenty-Ninth AAAI Conference
on Artiﬁcial Intelligence (AAAI2015), volume 15,
pages 2181–2187.

Farzaneh Mahdisoltani,

Joanna Asia Biega,

and
Fabian M. Suchanek. 2015. YAGO3: A Knowledge
Base from Multilingual Wikipedias . 7th Biennial
Conference on Innovative Data Systems Research.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2016. A review of relational
machine learning for knowledge graphs. Proceed-
ings of the IEEE, 104(1):11–33.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
IEEE Transactions
approaches and applications.
on Knowledge and Data Engineering, 29(12):2724–
2743.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the Twenty-
eighth AAAI Conference on Artiﬁcial Intelligence
(AAAI2014), volume 14, pages 1112–1119.

136
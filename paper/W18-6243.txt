Emo2Vec: Learning Generalized Emotion Representation by Multi-task

Training

Peng Xu, Andrea Madotto, Chien-Sheng Wu, Ji Ho Park and Pascale Fung

Center for Artiﬁcial Intelligence Research (CAiRE)

The Hong Kong University of Science and Technology, Clear Water Bay
[pxuab,eeandreamad,cwuak,jhpark,pascale]@ust.hk

Abstract

In this paper, we propose Emo2Vec which en-
codes emotional semantics into vectors. We
train Emo2Vec by multi-task learning six dif-
ferent emotion-related tasks, including emo-
tion/sentiment analysis, sarcasm classiﬁcation,
stress detection, abusive language classiﬁca-
tion, insult detection, and personality recog-
nition. Our evaluation of Emo2Vec shows
that it outperforms existing affect-related rep-
resentations, such as Sentiment-Speciﬁc Word
Embedding and DeepMoji embeddings with
much smaller training corpora. When concate-
nated with GloVe, Emo2Vec achieves compet-
itive performances to state-of-the-art results on
several tasks using a simple logistic regression
classiﬁer.

Introduction

1
Recent work on word representation has been fo-
cusing on embedding syntactic and semantic in-
formation into ﬁxed-sized vectors (Mikolov et al.,
2013; Pennington et al., 2014) based on the distri-
butional hypothesis, and have proven to be useful
in many natural language tasks (Collobert et al.,
2011). However, despite the rising popularity re-
garding the use of word embeddings,
they of-
ten fail to capture the emotional semantics the
words convey. For example, the GloVe vector cap-
tures the semantic meaning of “headache”, as it is
closer to words of ill symptoms like “fever” and
“toothache”, but misses the emotional association
that the word carries. The word “headache” in the
sentence “You are giving me a headache” does not
really mean that the speaker will get a headache,
but instead implies the negative emotion of the
speaker.

To include affective information into the word
representation, Tang et al.
(2016) proposed
Sentiment-Speciﬁc Word Embeddings (SSWE)
which encodes both positive/negative sentiment

and syntactic contextual information in a vec-
tor space. This work demonstrates the effective-
ness of incorporating sentiment labels in a word-
level information for sentiment-related tasks com-
pared to other word embeddings. However, they
only focus on binary labels, which weakens their
generalization ability on other affect tasks. Yu
et al. (2017) instead uses emotion lexicons to tune
the vector space, which gives them better results.
Nevertheless, this method requires human-labeled
lexicons and cannot scale to large amounts of data.
Felbo et al. (2017) achieves good results on affect
tasks by training a two-layer bidirectional Long
Short-Term Memory (bi-LSTM) model, named
DeepMoji, to predict emoji of the input docu-
ment using a huge dataset of 1.2 billions of tweets.
However, collecting billions of tweets is expensive
and time consuming for researchers.

Furthermore, most works in sentiment and emo-
tion analysis have focused solely on a single task.
Nevertheless, as emotion is a complex concept, we
believe that all emotion involving situations such
as stress, hate speech, sarcasm, and insult, should
be included for a deeper understanding of emo-
tion. Thus, one way to achieve this is through a
multi-task training framework, as we present here.
Contributions: 1) We propose Emo2Vec 1 which
are word-level representations that encode emo-
tional semantics into ﬁxed-sized, real-valued vec-
tors. 2) We propose to learn Emo2Vec with a
multi-task learning framework by including six
different emotion-related tasks.
3) Compared
to existing affect-related embeddings, Emo2Vec
achieves better results on more than ten datasets
with much less training data (1.9M vs 1.2B doc-
uments). Furthermore, with a simple logistic re-
gression classiﬁer, Emo2Vec reaches competitive
performance to state-of-the-art results on several

1https://github.com/pxuab/emo2vec wassa paper

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages292–298Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P17292Figure 1: Multi-task learning diagram

datasets when combined with GloVe.

2 Methodology
We train Emo2Vec using an end-to-end multi-task
learning framework with one larger dataset and
several small task-speciﬁc datasets. The model is
divided into two parts: a shared embedding layer
(i.e. Emo2Vec), and task-speciﬁc classiﬁers. All
datasets share the same word-level representations
(i.e. Emo2Vec), thus forcing the model to encode
shared knowledge into a single matrix. For the
larger dataset, a Convolutional Neural Network
(CNN) (LeCun et al., 1998) model is used to cap-
ture complex linguistic features present in the cor-
pus. On the other hand, the classiﬁer of each small
dataset is a simple logistic regression.
Notation: We deﬁne D = fdL; d1; d2;(cid:1)(cid:1)(cid:1) ; dng
as the set of n + 1 datasets, where dL is the larger
dataset and the other di are the small datasets. We
denote a sentence X i with i 2 fL; 1; 2;(cid:1)(cid:1)(cid:1) ; ng as
[wi;1; wi;2;(cid:1)(cid:1)(cid:1) ; wi;Ni] where wi;j is the j-th word
in the i-th sample and Ni is the number of words.
All the models’ parameters are deﬁned as M(cid:8) =
fT; CNN; LRϕ1; : : : ; LRϕn
g, where T 2 RjV j(cid:2)k
is the Emo2Vec matrix, jV j is the vocabulary size
and k is the embedding dimension, CNN is a Con-
volutional Neural Network model and LRϕi for
i 2 [1; n] is a logistic regression classiﬁer param-
eterized by ϕi which is speciﬁc for the dataset di.
We denote the embedded representation of a word
wi;j with ewi;j .
2.1 CNN model
The CNN architecture used is illustrated in Fig-
ure 2. Firstly, 1-D convolution is used to extract n-

Figure 2: Structure of CNN model

gram features from the input embeddings. Specif-
ically, the j-th ﬁlter denoted by Fj, is convolved
with embeddings of words in a sliding window of
size kj, giving a feature value cj;t. J ﬁlters are
learned trough this process:

cj;t = Fj (cid:3) ewL;t:t+kj

(cid:0)1 + bj

where (cid:3) is the 1-D convolution operation. This is
followed by a layer of ReLU activation (Nair and
Hinton, 2010) for non-linearity. After that, we add
a max-pooling layer of pooling size M (cid:0) Fj +
1 along the time dimension to force the network
to ﬁnd the most relevant feature for predicting yL
correctly. The result of this series of operations is
a scalar output of f mj. All f mj for j 2 [1; J]
are then concatenated together to produce a vector
representation f m1:J of the whole input sentence.

f mj = Max Pooling (ReLU (cj;t))

To make the ﬁnal classiﬁcation,

the vector
f mi;1:J is projected to the target label space by
adding another fully connected layer (i.e. parame-
terized by W and b), with a softmax activation.

^yL = Softmax(W (cid:1) [f m1:J ] + b)

2.2 Multi-task learning
Since collecting a huge amount of labeled datasets
is expensive, we collect two types of corpora,
one larger dataset (millions of training samples)
and a set of small datasets (thousands of train-
ing samples each) with accurate labels. For small
datasets, sentiment analysis, emotion classiﬁca-
tion, sarcasm detection, abusive language classi-
ﬁcation, stress detection, insult classiﬁcation and

293personality recognition are included. The reason
why we include many datasets is to 1) leverage dif-
ferent aspects of words emotion knowledge, which
may not be present in single domain dataset; 2)
create a more general embedding emotional space
that can generalize well across different tasks and
domains. To avoid over-ﬁtting, L2 regularization
penalty is added from the weights of all logistic
regression classiﬁers ϕi for i 2 [1; n]. Hence, we
jointly optimize the following loss function:

n∑

n∑

L(M(cid:8)) =

1
n

Lj + (cid:21)

j=1

j=1

∥LRϕj

∥2

Where Lj is the negative log likelihood (NLL) be-
tween ^yj and yj, and (cid:21) an hyper-parameter for the
regularization terms.

3 Experimental Setup

3.1 Dataset
Larger dataset
We collect a larger dataset from Twitter with hash-
tags as distant supervision. Such distant super-
vision method using hashtags has already been
proved to provide reasonably relevant emotion la-
bels by previous works (Wang et al., 2012).We
construct our hashtag corpus from Wang et al.
(2012), and Sintsova et al. (2017) 2. More tweets
between January and October 2017 are addition-
ally added using the Twitter Firehose API by us-
ing the hashtags based on the hierarchy mentioned
in Shaver et al. (1987). The hashtags are trans-
formed into corresponding emotion labels of Joy,
Sadness, Anger, and Fear. When extending the
dataset, we only use documents with emotional
hashtags at the end and ﬁlter out any documents
with URLs, quotations, or less than ﬁve words as
Wang et al. (2012) did. The total number of doc-
uments is about 1.9 million with four classes: joy
(36.5%), sadness (33.8%), anger (23.5%), and fear
(6%). The dataset is randomly split into a train
(70%), validation (15%), and test set (15%) for ex-
periments.

Small datasets
For sentiment, we include 8 datasets. (1,2) SST-
ﬁne and SST-binary (Socher et al., 2013) (3)
OpeNER (Agerri et al., 2013) (4,5) tube auto

2http://hci.epﬂ.ch/sharing-emotion-lexicons-and-

data#emo-hash-data

and tube tablet
(Uryupina et al., 2014) (6) Se-
mEval (Hltcoe, 2013) (7,8) SS-Twitter and SS-
Youtube
(Thelwall et al., 2010). For emotion
tasks, we include 4 datasets, (1) ISEAR (Wallbott
and Scherer, 1986) (2) WASSA (Mohammad and
Bravo-Marquez, 2017) (3) Olympic Sintsova et al.
(2013) (4) SE0714 (Staiano and Guerini, 2014).
We further include 6 other affect-related datasets.
(1,2) SCv1-GEN and SCv2-GEN for sarcasm de-
tection, (3) Stress (Winata et al., 2018), (4) Abu-
sive (Waseem, 2016; Waseem and Hovy, 2016).
(5) Personality (Pennebaker and King, 1999) (6)
Insult. The detailed statistics can be found in Table
4 and Table 5 in Supplemental Material.

3.2 Pre-training Emo2Vec
Emo2Vec embedding matrix and the CNN model
are pre-trained using hashtag corpus alone. Pa-
rameters of T and CNN are randomly initialized
and Adam is used for optimization. Best param-
eter settings are tuned on the validation set. For
the best model, we use the batch size of 16, em-
bedding size of 100, 1024 ﬁlters and ﬁlter sizes
are 1,3,5 and 7 respectively. We keep the trained
embedding and rename it as CNN embedding for
comparison. 100-dim for Emo2Vec is used in all
experiments.

3.3 Multi-task training
We tune our parameters of learning rate, L2 reg-
ularization, whether to pre-train our model and
batch size with the average accuracy of the devel-
opment set of all datasets. We early stop our model
when the averaged dev accuracy stop increasing.
Our best model uses learning rate of 0.001, L2
regularization of 1.0, batch size of 32. We save
the best model and take the embedding layer as
Emo2Vec vectors.

3.4 Evaluation
Baselines: We use 50-dimension Sentiment-
speciﬁc Word Embedding (SSWE)
(Tang et al.,
2016) as our baseline, which is an embedding
model trained with 10 millions of tweets by en-
coding both semantic and sentiment information
into vectors. Also, lots of work about the detec-
tion/classiﬁcation in sentiment analysis implicitly
encodes emotion inside the word vectors. For ex-
ample, Felbo et al. (2017) trains a two-layer bidi-
rectional Long Short-Term Memory (bi-LSTM)
model, named DeepMoji, to predict emoji of the

294model
SSWE

DeepMoji embedding

CNN embedding

Emo2Vec

SS-T
0.815
0.788
0.803
0.801

SS-Y SS-binary SS-ﬁne OpeNER tube auto
0.835
0.841
0.862
0.859

0.701
0.754
0.713
0.744

0.698
0.751
0.734
0.812

0.620
0.628
0.605
0.629

0.365
0.369
0.369
0.416

tube tablet SemEval

0.654
0.675
0.667
0.688

0.629
0.676
0.622
0.638

average
0.665
0.685
0.672
0.698

Table 1: Comparison between different emotion representations on sentiment datasets, all results are reported with
accuracy. The best results are highlighted with bold fonts. Emo2Vec achieves best average score.

model
SSWE

DeepMoji embedding

CNN embedding

Emo2Vec

ISEAR WASSA SE0714 Olympic Stress SCv1-GEN SCv2-GEN Insult Abusive Personality
0.327
0.379
0.384
0.372

0.559
0.666
0.623
0.647

0.674
0.678
0.676
0.675

0.660
0.658
0.657
0.674

0.539
0.586
0.560
0.588

0.508
0.485
0.480
0.506

0.704
0.739
0.744
0.744

0.217
0.286
0.259
0.323

0.466
0.532
0.549
0.559

0.678
0.685
0.707
0.710

average
0.533
0.569
0.564
0.580

Table 2: Comparison between different representations on other affect related datasets. All results are reported
with f1 score. The best results are highlighted with bold fonts. On average, Emo2Vec achieves best f1 score.

input document using a huge dataset of 1.2 bil-
lion tweets. Their embedding layer is implicitly
encoded with emotion knowledge. Thus, we use
the DeepMoji embedding, the 256-dimension em-
bedding layer of DeepMoji as another baseline.
Evaluation method: To make a fair comparison
with other baseline representations, we ﬁrst take
one dataset di out from n small datasets as the test
set. The remaining n (cid:0) 1 small datasets and the
larger dataset are used to train Emo2Vec through
multi-task learning. We take the trained Emo2Vec
as the feature for di and train a logistic regression
on di to compare the performance with other base-
line representations. The procedure is repeated n
times to see the generalization ability on differ-
ent datasets. We release Emo2Vec trained on all
datasets. For sentiment tasks, accuracy score is
reported. For other tasks, if it is binary task, we
report f1 score for the positive class. If it is multi-
class classiﬁcation tasks, we make it binary clas-
siﬁcation problem for each class and report aver-
aged f1 score.
4 Results
We compare our Emo2Vec with SSWE, CNN em-
bedding, DeepMoji embedding and state-of-the-
art(SOTA) results on 18 different datasets. The
results can be found in Table 1 and Table 2.
Compared with CNN embedding: Emo2Vec
works better than CNN embedding on 14/18
datasets, giving 2.6% absolute accuracy improve-
ment for the sentiment task and 1.6% absolute f1-
score improvement on the other tasks.
It shows
multi-task training helps to create better general-
ized word emotion representations than just using
a single task.
Compared with SSWE: Emo2Vec works much
better on all datasets except SS-T datasets, which
gives 3.3% accuracy improvement and 4.7% f1

DeepMoji

score improvement respectively on sentiment and
other tasks. This is because SSWE is trained on
10M binary classiﬁcation task on twitter which
then over-ﬁts on dataset SS-T, and generalizes
poorly to other tasks.
Compared
embedding:
with
Emo2Vec outperforms DeepMoji on 13/18
datasets despite the much smaller size of our
training corpus (1.9M documents for us vs 1.2B
documents for DeepMoji). On average, it gives
1.3% improvement in accuracy for the sentiment
task and 1.1% improvement of f1-score on the
other tasks.
Compared with SOTA results: We further com-
pare the performance of Emo2Vec vectors with
SOTA results on 14 datasets where the same split
is shared. Since Emo2Vec is not trained by pre-
dicting contextual words, it is weak on capturing
synthetic and semantic meaning. Thus, we con-
catenate Emo2Vec with the pre-trained GloVe vec-
tors, which are trained on Twitter and Wikepedia
3. Then, the concatenated vector of GloVe and
Emo2Vec, the concatenated vector of GloVe and
DeepMoji embeddings and GloVe are included for
comparison with SOTA results. Note that SOTA
results require complex bi-LSTM model while all
these representations are trained and reported with
a logistic regression classiﬁer. Here, we want to
highlight that solely using a simple classiﬁer with
good word representation can achieve promising
results.

Table 3 shows that GloVe+Emo2Vec out-
Com-
performs GloVe on 13/14 datasets.
pared with GloVe+DeepMoji, GloVe+Emo2Vec
achieves same or better results on 11/14 datasets,
which on average gives 1.0% improvement.
GloVe+Emo2Vec achieves better performances on

3http://nlp.stanford.edu/data/glove.twitter.27B.zip

http://nlp.stanford.edu/data/glove.6B.zip

and

295dataset

SS-Twitter
SS-Youtube
SS-binary
SS-ﬁne
OpeNER
tube auto
tube tablet
SemEval
ISEAR
SE0714
Olympic

stress

SCv1-GEN
SCv2-GEN

Average

Previous SOTA results
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Yu et al., 2017)
bi-LSTM (Yu et al., 2017)

bi-LSTM (Barnes et al., 2017)

SVM (Barnes et al., 2017)
SVM (Barnes et al., 2017)

bi-LSTM (Barnes et al., 2017)
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Winata et al., 2018)
bi-LSTM (Felbo et al., 2017)
bi-LSTM (Felbo et al., 2017)

0.88
0.93
0.886
0.497
0.825
0.662
0.681
0.685
0.57
0.37
0.61
0.743
0.69
0.75

GloVe GloVe+DeepMoji GloVe+Emo2Vec
0.78
0.84
0.795
0.414
0.750
0.630
0.650
0.671
0.41
0.36
0.52
0.759
0.69
0.73
0.642

0.81
0.86
0.809
0.421
0.781
0.628
0.678
0.695
0.43
0.36
0.52
0.793
0.68
0.74
0.657

0.81
0.87
0.823
0.436
0.778
0.660
0.684
0.680
0.45
0.43
0.53
0.770
0.68
0.74
0.667

Table 3: Comparison between different word-level emotion representations with state-of-the-art results. The best
results are in bold. New state-of-the-art results Emo2Vec that achieves are highlighted with boxes.

SOTA results on three datasets (SE0714, stress
and tube tablet) and comparable result to SOTA on
another four datasets (tube auto, SemEval, SCv1-
GEN and SCv2-GEN). We believe the reason why
we achieve a much better performance than SOTA
on the SE0714 is that headlines are usually short
and emotional words exist more commonly in
headlines. Thus, to detect the corresponding emo-
tion, more attention needs to be paid to words.

5 Related work
For sentiment analysis, numerous classiﬁcation
models (Kalchbrenner et al.; Iyyer et al., 2015;
Dou, 2017) have been explored. Multi-modal sen-
timent analysis (Zadeh et al., 2017; Poria et al.,
2017) extends text-based model to the combi-
nation of visual, acoustic and language, which
achieves better results than the single modality.
Various methods are developed for automatic con-
structions of sentiment lexicons using both su-
pervised and unsupervised way (Wang and Xia,
2017). Aspect-based sentiment (Chen et al., 2017;
Wang et al., 2016) is also a hot topic where re-
searchers care more about the sentiment towards
a certain target. Transfer learning from the large
corpus is also investigated by Felbo et al. (2017)
to train a large model on a huge emoji tweet
corpus, which boosts the performance of affect-
related tasks. Multi-task training has achieved
great success in various natural language tasks,
such as machine translation (Dong et al., 2015;
Malaviya et al., 2017), multilingual tasks (Duong
et al., 2015; Gillick et al., 2016), semantic pars-

ing (Peng et al., 2017). Hashimoto et al. (2017)
jointly learns POS tagging, chunking, dependency
parsing, semantic relatedness, and textual en-
tailment by considering linguistic hierarchy and
achieves state-of-the-results on ﬁve datasets. For
sentiment analysis, Balikas et al. (2017) jointly
trains ternary and ﬁne-grained classiﬁcation with
a recurrent neural network and achieves new state-
of-the-art results.

6 Conclusion and Future Work

In this paper, we propose Emo2Vec to represent
emotion with vectors using a multi-task train-
ing framework. Six affect-related tasks are uti-
lized, including emotion/sentiment analysis, sar-
casm classiﬁcation, stress detection, abusive lan-
guage classiﬁcation,
insult detection, and per-
sonality recognition. We empirically show how
Emo2Vec leverages multi-task training to learn a
generalized emotion representation.
In addition,
Emo2Vec outperforms existing affect-related em-
beddings on more than ten different datasets. By
combining Emo2Vec with GloVe, logistic regres-
sion can achieve competitive performances on sev-
eral state-of-the-art results.

7 Acknowledgements

This work is partially funded by ITS/319/16FP
of Innovation Technology Commission, HKUST
16248016 of Hong Kong Research Grants Coun-
cil.

296References
Rodrigo Agerri, Montse Cuadros, Sean Gaines, and
German Rigau. 2013. Opener: Open polarity en-
hanced named entity recognition. Procesamiento
del Lenguaje Natural, (51).

Georgios Balikas, Simon Moura, and Massih-Reza
Amini. 2017. Multitask learning for ﬁne-grained
In Proceedings of the
twitter sentiment analysis.
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ’17, pages 1005–1008, New York, NY, USA.
ACM.

Jeremy Barnes, Roman Klinger, and Sabine Schulte
im Walde. 2017. Assessing state-of-the-art senti-
ment models on state-of-the-art sentiment datasets.
In Proceedings of the 8th Workshop on Computa-
tional Approaches to Subjectivity, Sentiment and So-
cial Media Analysis, pages 2–12.

Steven Bird and Edward Loper. 2004. Nltk: the nat-
In Proceedings of the ACL
ural language toolkit.
2004 on Interactive poster and demonstration ses-
sions, page 31. Association for Computational Lin-
guistics.

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 452–461.

Ronan Collobert, Jason Weston, Lon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
In Proceedings of the
tiple language translation.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 1723–1732.

Zi-Yi Dou. 2017. Capturing user and product informa-
tion for document level sentiment analysis with deep
memory network. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 521–526.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low resource dependency parsing:
Cross-lingual parameter sharing in a neural network
In Proceedings of the 53rd Annual Meet-
parser.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
volume 2, pages 845–850.

Bjarke Felbo, Alan Mislove, Anders Sgaard, Iyad Rah-
wan, and Sune Lehmann. 2017. Using millions of

emoji occurrences to learn any-domain representa-
tions for detecting sentiment, emotion and sarcasm.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1616–1626.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2016. Multilingual language process-
ing from bytes. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1296–1306.

Kazuma Hashimoto, Yoshimasa Tsuruoka, Richard
Socher, et al. 2017. A joint many-task model: Grow-
ing a neural network for multiple nlp tasks. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1923–
1933.

J Hltcoe. 2013. Semeval-2013 task 2: Sentiment anal-

ysis in twitter. Atlanta, Georgia, USA, 312.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daum´e III. 2015. Deep unordered com-
position rivals syntactic methods for text classiﬁca-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 1681–1691.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. A convolutional neural network for modelling
sentences.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.

Chaitanya Malaviya, Graham Neubig, and Patrick Lit-
tell. 2017. Learning language representations for ty-
pology prediction. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2529–2535.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Saif Mohammad and Felipe Bravo-Marquez. 2017.
Wassa-2017 shared task on emotion intensity.
In
Proceedings of the 8th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis, pages 34–49.

Vinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10), pages 807–814.

297Hao Peng, Sam Thomson, and Noah A Smith. 2017.
Deep multitask learning for semantic dependency
In Proceedings of the 55th Annual Meet-
parsing.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 2037–
2048.

James W Pennebaker and Laura A King. 1999. Lin-
guistic styles: Language use as an individual differ-
ence. Journal of personality and social psychology,
77(6):1296.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017. A review of affective computing:
From unimodal analysis to multimodal fusion. In-
formation Fusion, 37:98–125.

Phillip Shaver, Judith Schwartz, Donald Kirson, and
Cary O’connor. 1987. Emotion knowledge: Further
exploration of a prototype approach. Journal of per-
sonality and social psychology, 52(6):1061.

Valentina Sintsova, Margarita Bolvar Jimnez, and Pearl
Pu. 2017. Modeling the impact of modiﬁers on emo-
In Proceedings of the 18th Int.
tional statements.
Conference on Computational Linguistics and Intel-
ligent Text Processing (CICLing).

Valentina Sintsova, Claudiu-Cristian Musat, and Pearl
Pu. 2013.
Fine-grained emotion recognition in
olympic tweets based on human computation. In 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Jacopo Staiano and Marco Guerini. 2014. Depeche
mood: a lexicon for emotion analysis from crowd
annotated news. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), volume 2, pages
427–433.

Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting
Liu, and Ming Zhou. 2016.
Sentiment embed-
dings with applications to sentiment analysis. IEEE
Transactions on Knowledge and Data Engineering,
28(2):496–509.

Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the As-
sociation for Information Science and Technology,
61(12):2544–2558.

Olga Uryupina, Barbara Plank, Aliaksei Severyn,
Agata Rotondi, and Alessandro Moschitti. 2014.
Sentube: A corpus for sentiment analysis on youtube
social media. In LREC, pages 4244–4249. Citeseer.

Harald G. Wallbott and Klaus R. Scherer. 1986. How
universal and speciﬁc is emotional experience? ev-
In-
idence from 27 countries on ﬁve continents.
formation (International Social Science Council),
25(4):763–795.

Leyi Wang and Rui Xia. 2017. Sentiment lexicon con-
struction with representation learning based on hi-
erarchical sentiment supervision. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 502–510.

Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter”
big data” for automatic emotion identiﬁcation.
In
Privacy, Security, Risk and Trust (PASSAT), 2012
International Conference on and 2012 Interna-
tional Confernece on Social Computing (Social-
Com), pages 587–592. IEEE.

Yequan Wang, Minlie Huang, Li Zhao, et al. 2016.
Attention-based lstm for aspect-level sentiment clas-
siﬁcation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 606–615.

Zeerak Waseem. 2016. Are you a racist or am i seeing
things? annotator inﬂuence on hate speech detection
on twitter. In Proceedings of the ﬁrst workshop on
NLP and computational social science, pages 138–
142.

Zeerak Waseem and Dirk Hovy. 2016. Hateful sym-
bols or hateful people? predictive features for hate
In Proceedings of the
speech detection on twitter.
NAACL student research workshop, pages 88–93.

Genta Indra Winata, Onno Pepijn Kampman, and Pas-
cale Fung. 2018. Attention-based lstm for psycho-
logical stress detection from spoken language using
distant supervision. 2018 IEEE International Con-
ference on Acoustics, Speech and Signal Processing.

Liang-Chih Yu, Jin Wang, K Robert Lai, and Xuejie
Zhang. 2017. Reﬁning word embeddings for sen-
In Proceedings of the 2017 Con-
timent analysis.
ference on Empirical Methods in Natural Language
Processing, pages 534–539.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1103–1114.

298A Supplemental Material
A.1 Preprocessing
Numbers and user mentions are changed to spe-
cial tokens (e.g. <number>, <user>), and other
non-alphanumeric characters are removed to re-
duce noisy characters from the corpus. All words
are lowercased. The tweet tokenizer from NLTK
(Bird and Loper, 2004) library used to tokenize
each document. We use the vocabulary of GloVe
twitter for its huge size of 1193514 tokens. We
further add “UNK” to denote out-of-vocabulary
words and “PAD” to pad sentences.

datasets
Domain
#classes
#training
#validation

#testing

SS-T
tube auto
Tweets Coments Movie reviews Movie reviews Hotel reviews Coments

SS-binary

OpeNER

SS-ﬁne

SS-Y

2
800
100
1213

2
800
100
1242

2

6920
872
1821

5

8544
1102
2210

4

2780
186
743

2

3381
225
903

Table 4: statistics of 8 sentiment datasets

tube tablet SemEval
Coments
Tweets

2

4997
333
1334

3

6021
890
2376

datasets
Domain
#classes
#training
#validation

#testing

ISEAR

Experiences

WASSA
tweets

SE0714
Headlines

Olympic
Tweets

Stress

SCv1-GEN SCv2-GEN

Insult

Interviews

Debates

Debates

Comments

Abusive Personality
Tweets

Essays

7
900
100
6480

4

3613
347
3142

3
200
50
1000

4
200
50
762

2

1751
200
320

2
800
100
1095

2
800
100
2360

2

4450
495
1237

2

12016
3005
3756

5

1578
395
494

Table 5: statistics of emotion and other datasets


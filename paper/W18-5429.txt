Importance of Self-Attention for Sentiment Analysis

Ga¨el Letarte∗, Fr´ed´erik Paradis∗, Philippe Gigu`ere, Franc¸ois Laviolette

Department of Computer Science and Software Engineering

{gael.letarte, frederik.paradis}.1@ulaval.ca

Universit´e Laval, Qu´ebec, Canada

Abstract

Despite their superior performance, deep
learning models often lack interpretability. In
this paper, we explore the modeling of insight-
ful relations between words, in order to un-
derstand and enhance predictions. To this ef-
fect, we propose the Self-Attention Network
(SANet), a ﬂexible and interpretable archi-
tecture for text classiﬁcation. Experiments
indicate that gains obtained by self-attention
is task-dependent. For instance, experiments
on sentiment analysis tasks showed an im-
provement of around 2% when using self-
attention compared to a baseline without atten-
tion, while topic classiﬁcation showed no gain.
Interpretability brought forward by our archi-
tecture highlighted the importance of neigh-
boring word interactions to extract sentiment.

1

Introduction

Deep neural networks have achieved great suc-
cesses on numerous tasks. However, they are of-
ten seen as black boxes, lacking interpretability.
Research efforts in order to solve this issue have
steadily increased (Simonyan et al., 2013; Zeiler
and Fergus, 2014; Bach et al., 2015; Ribeiro et al.,
2016; Fong and Vedaldi, 2017). In language mod-
eling, interpretability often takes place via an at-
tention mechanism in the neural network (Bah-
danau et al., 2014; Xu et al., 2015; Sukhbaatar
et al., 2015; Choi et al., 2017).
In this context,
attention essentially allows a network to identify
which words in a sentence are more relevant. Be-
yond interpretability, this often results in improved
decision making by the network.

Recently, Vaswani et al. (2017) proposed the
Transformer architecture for machine translation.
It relies only on attention mechanisms, instead of
making use of either recurrent or convolutional

∗ Authors contributed equally to this work.

neural networks. This architecture contains lay-
ers called self-attention (or intra-attention) which
allow each word in the sequence to pay attention
to other words in the sequence, independently of
their positions. We modiﬁed this architecture, re-
sulting in the following contributions:
• A novel architecture for text classiﬁcation
called Self-Attention Network (SANet) that
models the interactions between all
input
It is sequence length-agnostic,
word pairs.
thanks to a global max pooling layer.
• A study on the impact of this self-attention
mechanism on large scale datasets. In partic-
ular, we empirically demonstrate the positive
impact of self-attention in terms of perfor-
mance and interpretability for sentiment anal-
ysis, compared to topic classiﬁcation. In the
study, we make use of two quantitative met-
rics (Gini coefﬁcient and diagonality) that ex-
hibit particular behaviors for attention mech-
anisms in sentiment analysis.

2 Related Work

The majority of text classiﬁcation techniques ei-
ther use convolutional or recurrent neural net-
works on the words or the characters of the sen-
tence (Zhang et al., 2015, 2017; Yang et al., 2016;
Conneau et al., 2017; Johnson and Zhang, 2016,
2017; Howard and Ruder, 2018). One notable ex-
ception is the fastText architecture (Joulin et al.,
2016) which essentially employs a bag-of-words
approach with word embeddings of the sentence.
Attention mechanisms are a way to add inter-
pretability in neural networks. They were in-
troduced by Bahdanau et al. (2014), where they
achieved state-of-the-art in machine translation.
Since then, attention mechanisms have been used
in other language modeling tasks such as image
captioning (Xu et al., 2015), question answer-

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages267–275Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics267ing (Sukhbaatar et al., 2015; Choi et al., 2017), and
text classiﬁcation (Yang et al., 2016). The con-
cept of self-attention (Cheng et al., 2016; Parikh
et al., 2016), central to our proposed approach,
has shown great promises in natural language pro-
cessing; It produced state-of-the-art results for ma-
chine translation (Vaswani et al., 2017).

In text classiﬁcation, the focus on interpretabil-
ity has thus far been limited. Lee et al. (2018) used
a convolutional neural network (CNN) with Class
Activation Mapping (CAM) (Oquab et al., 2015)
to do sentiment analysis. CAM basically uses
the weights of the classiﬁcation layer to derive a
heatmap on the input. Wang et al. (2018) used a
densely connected CNN (Huang et al., 2017) to
apply attention to n-grams. However, their ap-
proach limits the range and acuteness of the in-
teractions between the words in the text. Lin et al.
(2017) and Yang et al. (2016) both combined an
attention mechanism with a recurrent neural net-
work. The main difference with our work is, while
being interpretable, these approaches do not per-
form true word-on-word attention across a whole
sequence such as our self-attention layer.

3 SANet: Self-Attention Network

Figure 1: Our Self-Attention Network (SANet),
derived
architec-
ture (Vaswani et al., 2017). The self-attention
block is repeated N times.

Transformer

from the

Inspired by the Transformer architecture (Vaswani
et al., 2017) which performed machine translation
without recurrent or convolutional layers, we pro-
pose the Self-Attention Network (SANet) archi-
tecture targeting instead text classiﬁcation. One
key difference between our approach and Vaswani
et al. (2017)’s is that we only perform input-input
attention with self-attention, as we do not have se-
quences as output but a text classiﬁcation. More-
over, we employ global max pooling at the top,
which enables our architecture to process input se-
quences of arbitrary length.
1 ; xT

n ] be the con-
catenation of a sequence of n vectors giving a ma-
trix X ∈ Rn×d such that xi ∈ Rd. Vaswani et al.
(2017) deﬁned attention as a function with as input
a triplet containing queries Q, keys K with asso-
ciated values V .

Formally, let X = [xT

2 ; . . . ; xT

Att(Q, K, V ) = softmax(cid:0)QKT(cid:1) V

self-attention mechanism as follows.

Self-Att(X) = Att(XWQ, XWK, XWV )

= softmax(cid:0)XWQKX T(cid:1) XWV

Where WQ, WK, WV , WQK
WQK = WQW T
learned parameters.

∈ Rd×d and
K. Hence, WQK and WV are

Our network (depicted in Figure 1) ﬁrst encodes
each word to its embedding. Pre-trained embed-
dings, like GloVe (Pennington et al., 2014), may
be used and ﬁne-tuned during the learning pro-
cess. Next, to inject information about the order
of the words, the positional encoding layer adds
location information to each word. We use the
positional encoding vectors that were deﬁned by
Vaswani et al. (2017) as follows.

(cid:16)
(cid:16)

(cid:17)
(cid:17)

PEpos,2i = sin

PEpos,2i+1 = cos

pos

100002i/d

pos

100002i/d

In the case of self-attention, Q, K and V are linear
projections of X. Thus, we deﬁne the dot-product

Where pos is the position of the word in the se-
quence and 1 ≤ i ≤ d is the dimension in the
positional encoding vector.

268Self-AttentionAdd&NormAdd&NormFeedForward+InputEmbeddingPositionalEncodingInputsGlobalMaxPoolingFeed-ForwardSoftmaxLinearClassProbabilitiesNSelf-AttentionBlocksFigure 2: Visualization of sequences length distributions. For each dataset, the total number of exam-
ples is presented on the right and task semantics are identiﬁed on the left: Topic Classiﬁcation (TC) or
Sentiment Analysis (SA).

A linear layer then performs dimensionality re-
duction/augmentation of the embedding space to
a vector space of dimension d, which is kept con-
stant throughout the network. It is followed by one
or several “self-attention blocks” stacked one onto
another. These blocks are comprised of a self-
attention layer followed by a feed-forward net-
work, both with residual connections. Contrary
to Vaswani et al. (2017), we only use a single
attention head, with attention performed on the
complete sequence with constant d-dimensional
inputs.

The feed-forward network consists of a single

hidden layer with a ReLU.

FFN(x) = max(0, xW1 + b1)W2 + b2

Where W1, W2 ∈ Rd×d are learned parameters.
The “Add & Norm” layer is a residual connection
deﬁned by LayerNorm(x + SubLayer(x)), where
SubLayer(x) is the output of the previous layer
and LayerNorm is a layer normalization method
introduced by Ba et al. (2016). Let xi be the vec-
tor representation of an element in the input se-
quence. The normalization layer simply normal-
izes xi by the mean and the variance of its ele-
ments. Throughout this paper, dropout of 0.1 is
applied to the output of SubLayer(x)

Finally, since we restrict ourselves to classiﬁca-
tion, we need a ﬁxed-size representation of the se-
quence before the classiﬁcation layer. To achieve
this, we apply a global max pooling operation for

each dimension across all the n words of the se-
quence. That is, if X ∈ Rn×d, then the max pool-
ing on X outputs a vector in Rd. This technique
was inspired by global average pooling introduced
by Lin et al. (2013) for image classiﬁcation in
CNNs. Global max pooling allows us to handle se-
quences of any length (up to memory limitations).
Thus, our approach is length-agnostic contrary to
some approaches based on CNN, where sequences
are truncated or padded to obtain a ﬁxed-length
representation.

4 Experiments
We evaluated our model on seven large scale
text classiﬁcation datasets introduced by Zhang
et al. (2015), grouped into two kinds of tasks.
The ﬁrst one is topic classiﬁcation: AG’s News
with 4 classes of news articles, DBPedia with
14 classes of the Wikipedia ontology and Ya-
hoo! Answers containing 10 categories of ques-
tions/answers. Yelp and Amazon reviews involve
sentiment analysis with ratings from 1 to 5 stars.
Two versions are derived from those datasets: one
for predicting the number of stars, and the other
involving the polarity of the reviews (negative for
1-2 stars, positive for 4-5 stars).

Each text entry was split into sentences and tok-
enized using NLTK (Bird et al., 2009). Sequences
longer than 1000 tokens were truncated to accom-
modate GPU memory limitations, only affecting a
negligible portion of the texts. See Figure 2 for

26902004006008001000SequencelengthAGDBPYah.A.YelpP.YelpF.Amz.F.Amz.P.Datasets127.6KTC630KTC1460KTC598KSA700KSA3650KSA4000KSATable 1: Test error rates (%) for text classiﬁcation. In bold, the state-of-the-art and in italic, our best
model. Lin et al. (2017)’s results provided by Wang et al. (2018). Stars (*) indicate attention mechanisms.

-

Model

4.36
4.3
2.90

37.95
36.1
32.39

Sentiment Analysis

Topic Classiﬁcation
AG DBP. Yah. A. Yelp P. Yelp F. Amz. F. Amz. P.
7.64 1.31
ngrams/CNN (Zhang et al., 2015)
fastText (Joulin et al., 2016)
7.5
1.4
word-CNN (Johnson and Zhang, 2016) 6.57 0.84
HN-ATT* (Yang et al., 2016)
VDCNN (Conneau et al., 2017)
DCNN (Zhang et al., 2017)
DPCNN (Johnson and Zhang, 2017)
SA-Embedding* (Lin et al., 2017)
ULMFiT (Howard and Ruder, 2018)
DCCNN-ATT* (Wang et al., 2018)
Baseline (base model)
SANet* (base model)
Baseline (big)
SANet* (big)

8.67 1.29
1.17
6.87 0.88
8.5
1.7
5.01 0.80
0.8
6.4
7.34 1.30
7.86 1.27
7.20 1.25
7.42 1.28

30.58
36.6
29.98
34.0
39.98
38.16
38.92
36.03

4.28
3.96
2.64
5.1
2.16
3.5
6.39
6.26
6.42
4.77

28.26
27.7
24.85
24.2
26.57
25.82
23.90

37.0
41.80
40.08
40.58
38.67

40.43
39.8
36.24
36.4
37.00

26.87
26.99
25.90
25.88

6.38
5.55
5.82
4.52

4.98
5.4
3.79

34.81
40.2

35.28

3.32

4.28

-
-
-

-
-
-

-

-

-

-

-

-

-

-

-

a visualization of the resulting sequences length
distribution and the total number of examples per
dataset.

We used 20% of the training texts for vali-
dation. The vocabulary was built using every
word appearing in the training and validation
sets. The words embeddings were initialized using
pre-trained word vectors from GloVe (Pennington
et al., 2014) when available, or randomly initial-
ized otherwise.

We experimented with two conﬁgurations for
our proposed SANet. The base model used N = 1
self-attention blocks, an embedding size of 100
and a hidden size of d = 128. The big model
doubled these numbers, i.e. N = 2 self-attention
blocks, embedding size of 200 and hidden size
d = 256. For each conﬁguration, we also trained
a baseline network without any attention mecha-
nisms, replacing each self-attention layer with a
feed forward layer.

Training was performed using SGD with a mo-
mentum of 0.9, a learning rate of 0.01 and mini-
batches of size 128. For the embeddings, a learn-
ing rate of 0.001 was applied without momentum.
All learning rates were halved for the big model.
We trained for 40 epochs and selected the best
epoch, based on validation accuracy.

5 Results and Discussion

From a performance perspective, as shown in Ta-
ble 1, our model based entirely on attention is
competitive while offering high level interpretabil-
ity. There is a notable exception with Yelp Review
Polarity that will be discussed. Our results also
indicate that the increase in depth and representa-
tion size in the big model is beneﬁcial, compared
to the simpler base model. Most noteworthy, we
noticed considerably different behaviors of the at-
tention mechanism depending on the type of task.
We offer an analysis below.

5.1 Topic Classiﬁcation Tasks
On the topic classiﬁcation task, the self-attention
behavior can be described as looking for interac-
tions between important concepts, without consid-
ering relative distance. As such, it acts similarly
to a bag-of-word approach, while highlighting key
elements and their associations. Thus, the atten-
tion matrix takes shape of active columns, one per
concept. One such matrix is depicted in Figure
3a, where the attention is focused on distanced
pairs such as (microsoft, class-action)
or (settlement, billions) to help SANet
predict the Business category, while the baseline
wrongfully predicts Sci/Tech. We observed this
column-based structure for attention matrix for ev-
ery topic classiﬁcation dataset, see Figure 4 for

270(a) Topic Classiﬁcation.

(b) Sentiment Analysis. Label 4 means 5-star review

Figure 3: Self-attention different behavior for each text classiﬁcation task. The attention matrices were
extracted from the SANet base model applied on the testing set of each dataset. Words on the y-axis are
attending to the words on the x-axis. GT refers to the ground truth.

multiple examples. Although it adds interpretabil-
ity to the model, our results seem to indicate that
self-attention does not improve performances for
topic classiﬁcation, compared to the baseline.

5.2 Sentiment Analysis Tasks
For sentiment analysis tasks, self-attention im-
proves accuracy for every dataset and model con-
ﬁgurations that we tested. For Yelp Review Po-
larity, although attention helps, the overall perfor-
mances remain subpar.

Noticeably for the other datasets, SANet is able
to extract subtle interactions between words, with
a strong focus on neighboring relation. Hence, the
attention matrices are close to being band matri-
ces, with interest concentrated on very small re-
gions near the diagonal. This is observable in Fig-
ure 5 where multiple examples from all sentiment
analysis datasets are presented. Concentration of
the attention around the diagonal indicates that
the useful features learned by the attention mech-
anism consist essentially of skip-bigrams with rel-
atively small gaps. Of note, Wang and Manning
(2012) previously observed consistent gains when
including word bigram features to extract senti-
ment. Thus, our model corroborates this intu-
ition about sentiment analysis while yielding in-
terpretable insights on relevant word pairs across

all possible skip-bigrams.

Figure 3b is a typical example of such ma-
trix with a band diagonal structure, for a 5-star
Yelp review. A number of positive elements are
highlighted by the self-attention mechanism such
as i) the initial strong sentiment with the inter-
action between this with love and ! ii) the
favorable comparison with even and better
iii) the enticing openness to experiences with
try and something and iv) the positive com-
bination of two negative words with never and
disappointed.

Positional encoding helps the self-attention
mechanism when interpreting words repetitions,
in order to extract sentiment gradation. When re-
peating three times an adjective before the mod-
iﬁed noun, attention on the adjective increases
with their proximity to the noun: horrible
horrible horrible service. Punctu-
ation repetitions exhibit a similar behavior, as in
the sentence “love this place!!!”, where the words
love and all three exclamation points apply at-
tention to this with varying intensities: love
this place ! ! ! . This particular behav-
ior of the model reinforces our belief that it learns
intricate knowledge for the task of sentiment anal-
ysis. Entire attention heatmaps for complete se-
quences can be found in Figure 6.

271californiaslasheslegalfeesinsettlementofmicrosoftcase.californialawyerswhoreacheda$1.1billionclass-actionsettlementwithmicrosoftwillgetlessthanhalfthelegalfeestheyrequested.californiaslasheslegalfeesinsettlementofmicrosoftcase.californialawyerswhoreacheda$1.1billionclass-actionsettlementwithmicrosoftwillgetlessthanhalfthelegalfeestheyrequested.GT:Business,SANet:Business,Baseline:Sci/Techilovethisplace!greatpricesandevenbetterfood.itrysomethingdiﬀerenteverytimeandamneverdisappointed.ilovethisplace!greatpricesandevenbetterfood.itrysomethingdiﬀerenteverytimeandamneverdisappointed.GT:4,SANet:4,Baseline:4Figure 4: Randomly selected attention matrices for topic classiﬁcation task. Each row corresponds to a
different dataset in this order: AG’s News, DBPedia and Yahoo! Answers. The column-based pattern is
clearly present in the attention mechanism for topic classiﬁcation.

Figure 5: Randomly selected attention matrices for sentiment analysis task. Each row corresponds to a
different dataset in this order: Yelp Review Polarity, Yelp Review Full, Amazon Review Full and Amazon
Review Polarity. The diagonal band pattern of the matrices is clearly present in the attention mechanism
for sentiment analysis except for the Yelp Review Polarity dataset.

5.3 Quantitative Analysis

We now present a quantitative analysis of the at-
tention matrices to support the qualitative intuition
stated previously. Two metrics are used in order to
assess the properties of the matrices; the ﬁrst one
(Gini coefﬁcient) quantiﬁes the sparsity of the at-
tention, whereas the second one (diagonality) fo-
cuses on the diagonal concentration. These two

properties are relevant for interpretability issues.
The results are presented in Table 2.

The Gini coefﬁcient which measures the in-
equality in the attention weights distribution is ﬁrst
computed. For topic classiﬁcation datasets, the
mean of the Gini coefﬁcient is 63.57%, whereas,
for sentiment analysis datasets, it raises at 87.15%
without considering Yelp Review Polarity. Thus,
for topic classiﬁcation it reveals that every word

272(a) Adjective repetition.

(b) Punctuation emphasis.

Figure 6: Positional encoding impact for sentiment gradation through self-attention mechanism. Both
examples are extracted from the testing set of the Yelp Review Full dataset. Label 4 means 5-star review
and label 0 means 1-star review. Words on the y-axis are attending to the words on the x-axis. GT refers
to the ground truth.

Table 2: Quantitative statistics of the self-attention mechanism behavior for the two text classiﬁcation
tasks. Metrics are computed on the testing sets using the SANet base model.

Metric

Sentiment Analysis

Topic Classiﬁcation
AG DBP. Yah. A. Yelp P. Yelp F. Amz. F. Amz. P.
55.31 67.94
87.76
Gini coefﬁcient
40.01
Diagonality (bandwidth = 1) 7.44
8.49
60.34
Diagonality (bandwidth = 2) 11.86 13.80
71.43
Diagonality (bandwidth = 3) 16.21 18.88
Diagonality (bandwidth = 4) 20.42 23.74
77.21
80.56
Diagonality (bandwidth = 5) 24.48 28.25

67.45
6.34
9.83
13.28
16.59
19.65

65.16
5.02
7.89
10.62
13.19
15.62

84.18
23.54
36.89
45.49
50.90
54.54

89.50
41.77
62.35
73.53
79.49
83.09

interacts with multiple other words in the se-
quence. On the other hand, for sentiment analy-
sis, the attention is focused on a fewer number of
word pairs. The second metric will also point out
that the sentiment analysis attention is sparse and
speciﬁcally based on pair of words that are close
in the sentence. This structurally corresponds to
an attention matrix concentrated near the diagonal
and justiﬁes the introduction of the following met-
ric.

This new metric evaluates the resemblance with
a band matrix by computing the proportion of at-
tention weights which occur inside the band diago-
nal of a given bandwidth b, thus the band diagonal-

ity or diagonality for short. It expresses the inter-
actions of every element with itself, and the b ele-
ments before and after in the sequence. This met-
ric of diagonality was computed for a bandwidth
of b = 1, 2, . . . , 5 as presented in Table 2. Re-
sults clearly reveal that sentiment analysis atten-
tion matrices are structurally close to being band
matrices. Notably, with a bandwidth of b = 3 for
topic classiﬁcation, 16.12% of the weights occur
inside the band diagonal, as for sentiment analysis
without considering Yelp Review Polarity, 63.48%
is located inside the band diagonal.

In our opinion, the combination of these two
metrics supports our qualitative observations of

273horriblehorriblehorribleservice.wesatfor10minutesandwerenotgreetedsowegotuptoleaveandalltheydidwaswaveusgoodbyehorriblehorriblehorribleservice.wesatfor10minutesandwerenotgreetedsowegotuptoleaveandalltheydidwaswaveusgoodbyeGT:0,SANet:0,Baseline:0lovethisplace!!!excellentserviceevenwhenswamped!foodready&freshonrevolvingserver.:)lovethisplace!!!excellentserviceevenwhenswamped!foodready&freshonrevolvingserver.:)GT:4,SANet:4,Baseline:4the attention matrices.
It strengthens the differ-
ence in attention behavior between the topic clas-
siﬁcation and sentiment analysis task. Moreover,
this quantitative analysis clearly exposes SANet
inability to learn the appropriate attention behavior
for sentiment analysis with Yelp Review Polarity.
Its failure to adequately exploit the self-attention
mechanism coincide with its poor performance to
extract sentiment. Interestingly, Yelp Review Po-
larity examples are a subset of Yelp Review Full
with merged classes, for which SANet performs
well with the expected attention behavior. The
cause of this discrepancy with the Yelp datasets
is unknown and left for future work as is some lin-
guistic investigation of the impact of close inter-
acting words in sentiment analysis.

6 Conclusion

In this paper, we introduced the Self-Attention
Network (SANet), an attention-based length-
agnostic model architecture for text classiﬁcation.
Our experiments showed that self-attention is im-
portant for sentiment analysis. Moreover, the im-
proved interpretability of the model through atten-
tion visualization enabled us to discover consid-
erably different behaviors of our attention mech-
anism between the topic classiﬁcation and senti-
ment analysis tasks. The interpretable perspective
of this work gives insights on the importance of
modeling interaction between neighboring words
in order to accurately extract sentiment, as noted
by (Wang and Manning, 2012) for bigrams.
It
highlights how interpretability can help us under-
stand models behavior to guide future research. In
the future, we hope to apply our Self-Attention
Network to other datasets such as bullying detec-
tion on social network data and tasks from vari-
ous ﬁelds, such as genomic data in bioinformat-
ics. Finally, we wish to study the properties of the
introduced global max pooling layer as a comple-
mentary tool for interpretability in a similar way
that was done with CAM (Oquab et al., 2015) for
global average pooling. The outcome will be some
attention on individual words that can take into ac-
count the context given by the self-attention mech-
anism. This contrast with the approach of this
paper which focuses on interaction between ele-
ments as pairs. Thus we are allowed to expect that
these two mechanisms will act in a complementary
way to enrich interpretability.

Acknowledgments
We would like to thank Nicolas Garneau, Alexan-
dre Drouin and Jean-Samuel Leboeuf for their ad-
vice and insightful comments. This work is sup-
ported in part by NSERC. We gratefully acknowl-
edge the support of NVIDIA Corporation with the
donation of the Titan Xp GPU used for this re-
search.

This work was done using the PyTorch li-
brary (Paszke et al., 2017) with the PyToune
framework (Paradis and Garneau).

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
arXiv preprint

ton. 2016. Layer normalization.
arXiv:1607.06450.

Sebastian Bach, Alexander Binder, Gr´egoire Mon-
tavon, Frederick Klauschen, Klaus-Robert M¨uller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classiﬁer decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. ” O’Reilly
Media, Inc.”.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733.

Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia
Polosukhin, Alexandre Lacoste, and Jonathan Be-
rant. 2017. Coarse-to-ﬁne question answering for
long documents. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
209–220.

Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classiﬁcation. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 1107–1116.

Ruth C Fong and Andrea Vedaldi. 2017. Interpretable
explanations of black boxes by meaningful perturba-
tion. arXiv preprint arXiv:1704.03296.

Jeremy Howard and Sebastian Ruder. 2018.

Fine-
tuned language models for text classiﬁcation. arXiv
preprint arXiv:1801.06146.

274Gao Huang, Zhuang Liu, Kilian Q Weinberger, and
Laurens van der Maaten. 2017. Densely connected
convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, volume 1, page 3.

Rie Johnson and Tong Zhang. 2016. Convolutional
neural networks for text categorization: Shallow
word-level vs. deep character-level. arXiv preprint
arXiv:1609.00718.

Rie Johnson and Tong Zhang. 2017. Deep pyramid
convolutional neural networks for text categoriza-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 562–570.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. arXiv preprint arXiv:1607.01759.

Gichang Lee,

Jeong,

Jaeyun

Seungwan Seo,
CzangYeob Kim, and Pilsung Kang. 2018. Sen-
timent classiﬁcation with word localization based
on weakly supervised learning with a convolutional
neural network. Knowledge-Based Systems.

Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Net-
work in network. arXiv preprint arXiv:1312.4400.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Maxime Oquab, L´eon Bottou, Ivan Laptev, and Josef
Sivic. 2015. Is object localization for free?-weakly-
supervised learning with convolutional neural net-
In Proceedings of the IEEE Conference
works.
on Computer Vision and Pattern Recognition, pages
685–694.

Fr´ed´erik Paradis and Nicolas Garneau.

http://pytoune.org.

PyToune.

Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
In Empiri-
model for natural language inference.
cal Methods in Natural Language Processing, pages
2249–2255.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should i trust you?: Explain-
ing the predictions of any classiﬁer. In Proceedings
of the 22nd ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining, pages
1135–1144. ACM.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2013. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
maps. arXiv preprint arXiv:1312.6034.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Shiyao Wang, Minlie Huang, and Zhidong Deng. 2018.
Densely connected cnn with multi-scale feature at-
tention for text classiﬁcation. In Proceedings of the
Twenty-Seventh International Joint Conference on
Artiﬁcial Intelligence, IJCAI-18.

Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
In Proceedings of the 50th Annual
classiﬁcation.
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
In International Conference on Machine
tention.
Learning, pages 2048–2057.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
In Eu-
and understanding convolutional networks.
ropean conference on computer vision, pages 818–
833. Springer.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
In Advances in neural information pro-
siﬁcation.
cessing systems, pages 649–657.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan,
Ricardo Henao, and Lawrence Carin. 2017. Decon-
volutional paragraph representation learning. In Ad-
vances in Neural Information Processing Systems,
pages 4172–4182.

275
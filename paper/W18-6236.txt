Disney at IEST 2018: Predicting Emotions using an Ensemble

Wojciech Witon1∗

Pierre Colombo1∗

Ashutosh Modi1

Mubbasir Kapadia1,2

1Disney Research Los Angeles, 2Rutgers University

{wojtek.witon, pierre.colombo}@disneyresearch.com

{ashutosh.modi, mubbasir.kapadia}@disneyresearch.com

Abstract

This paper describes our participating system
in the WASSA 2018 shared task on emotion
prediction. The task focuses on implicit emo-
tion prediction in a tweet.
In this task, key-
words corresponding to the six emotion la-
bels used (anger, fear, disgust, joy, sad, and
surprise) have been removed from the tweet
text, making emotion prediction implicit and
the task challenging. We propose a model
based on an ensemble of classiﬁers for pre-
diction. Each classiﬁer uses a sequence of
Convolutional Neural Network (CNN) archi-
tecture blocks and uses ELMo (Embeddings
from Language Model) as an input. Our sys-
tem achieves a 66.2% F1 score on the test set.
The best performing system in the shared task
has reported a 71.4% F1 score.
Introduction

1
Besides understanding the language humans com-
municate in, AI systems that naturally interact with
humans should also understand implicit emotions
in language. To be consistent and meaningful, an
AI system conversing with humans should reply
while taking into account the emotion of the utter-
ance spoken by the human. If the user appears to
be unhappy, a subsequent joyful response from the
system would likely detract from the engagement
of the user in the conversation. In recent years,
several researchers have attempted to address this
problem by developing automated emotion predic-
tion for text (Medhat et al., 2014).

Predicting emotions implicit in natural language
is not trivial. A na¨ıve attempt to classify text based
on emotion keywords may not always work due
to the presence of various linguistic phenomena
(e.g., negation, ambiguities, etc.) in the text. More-
over, emotion may be triggered by a sequence of
words and not just a single keyword, requiring an

∗ indicates equal contribution.

automated system to understand the underlying se-
mantics of the text. In the WASSA shared task,
keywords describing the emotion label have been
removed, making the emotion implicit in the text.
This makes the task more challenging.

Typically, a system developed for implicit emo-
tion prediction must understand the meaning of the
entire text and not just predict using a few key-
words. We propose a model which uses a CNN
based architecture (Gehring et al., 2017) for emo-
tion prediction. The model stacks CNN blocks
on ELMo (Embeddings from Language Model),
as introduced by Peters et al. (2018). Addition-
ally, we include word level Valence, Arousal, and
Dominance (VAD) features for guiding our model
towards prediction. We describe our model in detail
in Section 4. As described in Section 6, our model
achieves 66% accuracy on the WASSA task. We
further investigate the generalizability of our model
by experimenting on the Cornell movie dataset as
shown in Section 7.

2 Related Work

Emotion prediction is related to the task of sen-
timent analysis. The best performance in senti-
ment analysis has been attained using supervised
techniques as outlined in a survey by Medhat
et al. (2014). Recent breakthroughs in deep learn-
ing have shown strong results in sentence clas-
siﬁcation (Joulin et al., 2016), language model-
ing (Dauphin et al., 2016) and sentence embed-
ding (Peters et al., 2018). Our emotion prediction
model is also based on deep learning techniques.
Recently, fastText (Joulin et al., 2016) has been pro-
posed for generating word representations which
have shown state-of-the-art performance on a num-
ber of text related tasks. Our model makes use of a
fastText model for emotion classiﬁcation.

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages248–253Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P17248Chen et al. (2018) introduce an emotion cor-
pus based on conversations taken from Friends TV
scripts and propose a similar emotion classiﬁcation
model using a CNN-BiLSTM. Our model is similar
to the model proposed by (Chen et al., 2018), but
we use a pre-trained ELMo instead of a BiLSTM.
Mohammad (2018) have proposed a VAD lexi-
con for emotion detection systems. We use VAD
features together with ELMo (Peters et al., 2018).
Recently, the ELMo model has been shown to boost
performance on a number of Natural Language Pro-
cessing (NLP) tasks. To the best of our knowledge,
we are the ﬁrst to make use of VAD features in a
deep learning setting for emotion prediction.

3 Task Description
The WASSA 2018 shared task∗ (Klinger et al.,
2018) is about predicting implicit emotion in a
given tweet. The task is challenging because the
keyword indicative of the emotion has been re-
moved from the tweet. The participating system is
required to predict the implicit emotion based on
the remaining context using world knowledge or
statistical techniques.

3.1 Emotion Corpus
The corpus provided for the competition has around
188,000 tweets (∼150,000 for training, ∼9,000
for validation, ∼28,000 for testing) annotated with
6 emotion labels (anger, surprise, joy, sad, fear,
disgust). The dataset has a balanced distribution of
examples for the six label classes (see Table 1).

Emotion

Joy

Disgust
Surprise
Anger
Fear
Sad
Total

Train
27762
25541
25449
25439
24435
22836
151462

Val
1719
1595
1595
1592
1520
1443
9464

Test
5246
4794
4794
4792
4791
4340
28757

Table 1: Label distribution of the provided corpus.

4 Emotion Prediction Model

Our model has two sets of classiﬁers at its disposal:
an ensemble of CNN-based classiﬁers and a fast-
Text classiﬁer (Joulin et al., 2016). A CNN-based

∗http://implicitemotions.wassa2018.

com

classiﬁer requires a ﬁxed length input. Since tweets
have a variable number of words, padding is typi-
cally added to the shorter word sequences in order
to have equal lengths across the mini-batch. In
practice, having long sequences may not work well
due to noise introduced by padding. Based on tweet
length distribution (see Figure 1) and our experi-
ments, we set the maximum length of a tweet to
40 words. These tweets were classiﬁed using CNN
based models. For longer tweets (> 40), we used
a fastText classiﬁer. FastText works by averaging
word representations into a text representation us-
ing bag of words (BoW) and bag of n-grams. The
text representation is then fed into a linear classiﬁer
with a hierarchical softmax on top. FastText was
chosen based on its simplicity and efﬁciency.

Figure 1: Raw tweet length distribution used for setting
a maximum length of input sequence for the classiﬁer.

4.1 Deep CNN Classiﬁer
We use an ensemble of CNN-based classiﬁers for
shorter (< 41 words) tweets. Each of the CNN-
based classiﬁers in the ensemble has a network
architecture as shown in Figure 2. The CNN classi-
ﬁer has two sub-modules:

• Text sub-module: At the lowest level, this
module captures the dependencies between
the words of the tweet using a Bi-Directional
LSTM model with sub-word information (ex-
tracted via character-level CNN) as introduced
in ELMo by Peters et al. (2018). The weights
of this recurrent network were initialized with
values provided by the authors (pre-trained on
a 1 billion word benchmark), and updated dur-
ing training. The next layers of the classiﬁer
are CNN blocks (see §4.2).

249Figure 2: Left: the proposed emotion classiﬁer architecture. Right: CNN block structure.

• Emotion sub-module: In this sub-module,
the model uses VAD emotion values (see
§4.3), this is followed by a CNN block layer.

Outputs from both networks are mapped to con-
stant size layers, concatenated, mapped to the out-
put (classiﬁcation) layer of size 6 and normalized
using a softmax function. An overview of the sys-
tem is presented in Figure 2.

4.2 Convolutional Block Structure
We base our network on Convolutional Blocks in-
troduced by Gehring et al. (2017). We make use of
a CNN encoder, which consists of several convo-
lutional layers (blocks), followed by Gated Linear
Units (GLU) layers introduced by Dauphin et al.
(2016), and residual connections. The architecture
of the CNN block is presented in Figure 2 (right).
Inputs to the ﬁrst convolutional block are ELMo
representations w = (w1, ..., wm), mapped to
size d, for a given input sequence of length m.
Each convolution kernel takes as input X ∈ Rm×d
and outputs a single element Y ∈ R2d. This output
is then mapped to Y (cid:48) ∈ Rd using GLU. We use m
different kernels, which are concatenated to form a
ﬁnal output matrix Z ∈ Rm×d that serves later as
an input to the next block. The output of the last
block is mapped to a one dimensional vector using
a linear layer.

4.3 VAD Lexicon
To model the emotions carried by each tweet at a
word level we use VAD features extracted from an

external lexicon introduced by Mohammad (2018).
Each of the 14,000 words in the lexicon is repre-
sented by a vector in the VAD space (v ∈ [0, 1]3)
and each sentence is associated with a matrix result-
ing from concatenation of VAD vectors for words
in the sentence (V ∈ Rm×3). To label out-of-
vocabulary (OOV) words, the closest word in the
dictionary is found using a difﬂib library† in python
(algorithm based on the Levenshtein distance). If
no word with more than 90% of similarity is found,
a default VAD value (v = [0.5, 0.5, 0.5]) is as-
signed. At the end of the process, around 50%
of words of the training set are labeled with VAD
values.

4.4 Classiﬁer Ensemble

The model ensemble consists of a 6-emotion (gen-
eral) CNN classiﬁer and six binary CNN classiﬁers
(e.g., “happy” vs all other emotions). The ﬁnal
prediction is made by looking for an agreement
between binary classiﬁers – 5 classiﬁers predict
the “negative” class and the other one predicts the
“positive” class with a conﬁdence score for the “pos-
itive” class that is over a certain threshold T . If the
conditions are not met, the tweet is classiﬁed using
the 6-emotion classiﬁer. The threshold T is tuned
based on validation accuracy.

†https://docs.python.org/3/library/

difflib.html

250V2V3V4V5Vpad V1Vpad AngerSurpriseJoySadFearDisgustEEEEE</s><s>IamhappyE<p>E<p>CNN Block 1CNN Block NWCNNVADVADVADVADVAD</s><s>IamhappyVAD<p>VAD<p>ELMoCNNVAD BlockWoutWdWdWdWdWdWdWd5 Experiments

In this section, we describe the procedure for train-
ing classiﬁers as part of the Ensemble Classiﬁer.
The parameters were tuned based on both valida-
tion loss and accuracy.

5.1 Preprocessing
Each tweet in the dataset is ﬁrst tokenized using
the Spacy tokenizer‡. Then, each of the 6 most
common emojis is mapped into a sequence of
ASCII characters (e.g.,
is mapped to “:d”). As
the last step, the start and end of sentence tokens
(<SOS>, <EOS>) are added, together with pad to-
kens (<PAD>) to match the maximum sequence
length.

5.2 Training Procedure
Our Deep emotion classiﬁer is composed of 2 CNN
blocks (N = 2) stacked on top of ELMo and 1
CNN block stacked on top of VAD features. We set
the window size of the Convolutional Block to 5,
ELMo size to 1024 (mapped to d = 256), initial
learning rate for ADAM optimizer (Kingma and
Ba, 2014) to 0.001, dropout rate to 0.5, batch size
to 128, and the threshold T to 0.86.

Each batch of samples used for training the bi-
nary classiﬁers is balanced by randomly sampling
half of the batch from positive labels and half of the
batch from negative labels (the number of negative
labels is 5 times larger). Sampling using this pro-
cess makes the training more robust to overﬁtting.
Additionally, noise is added to the training sam-
ples; a small amount of negative labels are sampled
and presented as positive labels to the classiﬁer
(Section 6.1).

6

Implicit Emotion Prediction Results

In this section we present the results on the Implicit
Emotion Prediction task. The six binary classiﬁers
and the 6-emotion classiﬁer used in the Ensemble
Classiﬁer were chosen based on validation accuracy
presented in Table 2. Our system achieved a macro
F1 Score of 66.2%, whereas the top 3 participating
systems have reported a score of 71.4%, 71% and
70.3%, respectively.

6.1 Analysis
Table 2 shows that some emotions (e.g., joy, fear)
are easier to predict. In some cases we see improve-

‡https://spacy.io

Emotion

Joy

Disgust
Surprise
Anger
Fear
Sad

6 emotions

No noise Noise 5% Noise 10%
91.4%
89.4%
87.7%
86.9%
90.9%
89.0%
64.8%

92.2%
89.6%
87.9%
86.5%
90.8%
87.7%
64.5%

91.0%
89.7%
87.1%
86.4%
90.9%
89.6%
65.5%

Table 2: Validation accuracy for each classiﬁer (note:
high accuracy scores for binary classiﬁers come from
unbalanced classes).

Figure 3: ROC curve for predicting surprise emotion
on the test set (predictions are made on a balanced
dataset).

ment in validation accuracy after adding noise. Sur-
prisingly, this does not lead to statistically signiﬁ-
cant improvement (Figure 3).

Results for the 6-emotion classiﬁer and the En-
semble Classiﬁer are presented in Table 3. Some
emotions are easier to predict than others, this is
corroborated by the confusion matrix in Figure 4.
Joy is easier to predict, whereas predicting anger
remains a difﬁcult task (also shown in Table 2).
Some emotions are harder to distinguish (surprise
with fear and disgust), whereas some emotions are
very unlikely to be confused with each other (e.g.,
joy with disgust). Our model probably commits
errors because ﬁrstly, emotions are not disjoint –
a sentence can express more than one emotion at
the same time (i.e., a sentence can be classiﬁed
as either “disgust” or “fear”), and secondly, sev-
eral emotion labels could be assigned to the same
sentence by changing only the trigger words (e.g.,
the sentence “I am #TRIGGERWORD to see you
here.” can be classiﬁed both by anger and surprise,

251Classiﬁer
fastText

6-emotion classiﬁer
Ensemble classiﬁer

F1 Score
50.0%
65.2%
66.2%

Table 3: F1 Score on test set.

Joy

Emotion F1 Score
52.1%
48.2%
52.0%
50.0%
50.1%
44.2%
49.4%

Disgust
Surprise
Anger
Fear
Sad
Total

Table 4: F1 Score on Cornell dataset.

7.2 Prediction on 6 emotions
In the ﬁrst experiment, we take the top 2 emotions
predicted by the ﬁnal system on D1 and check if at
least one of the predicted labels matches one of the
golden labels. F1 Scores are presented in Table 4.

7.3 Prediction on neutral emotion
In the second experiment, we determine that the
classiﬁer predicts a neutral emotion if each emotion
is predicted with low conﬁdence (conﬁdence lower
than 0.5). We evaluate our system on D2. The
ﬁnal system predicts a neutral emotion for 85% of
sentences, whereas fastText only reaches 4% of
accuracy. FastText misclassiﬁes those neutral lines
as joy with high conﬁdence (> 80%).

In conclusion, the results show that our model
generalizes well on the Cornell Movie corpus when
compared to a fastText classiﬁer, pre-trained simi-
larly on the task dataset. While we do not expect
to reproduce precisely the same performance on
the Cornell Movie Corpus, since the word distribu-
tion and writing style are very different, the system
generalizes reasonably well.

8 Conclusion

In this paper, we presented a system making use
of state-of-the-art techniques for Natural Language
Processing, such as ELMo and a CNN encoder
for emotion classiﬁcation. We have designed a
robust classiﬁer for sentences without any assump-
tions about the intrinsic properties of the task which
make it generalizable to other tasks (e.g., explicit
emotion prediction) on other datasets.

References
Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Ting-Hao Huang, and Lun-Wei Ku. 2018. Emotion-
lines: An emotion corpus of multi-party conversa-
tions. CoRR, abs/1802.08379.

Figure 4: Confusion Matrix for the Ensemble Classi-
ﬁer.

depending on whether the trigger word was “happy”
or “surprised”).

7 Model Generalization

In order to have a better understanding of the per-
formance of our system for real world applications,
we tested our system on an explicit emotion predic-
tion task.

7.1 Dataset and Task

For our experiments, we used the Cornell Movie
Corpus built by Danescu-Niculescu-Mizil and Lee
(2011), which is composed of around 300,000 ut-
terances extracted from 600 movies. A group of
internal annotators manually annotated a subset of
58,000 lines, with at most 2 of 7 emotion labels
(fear, surprise, anger, disgust, joy, sad, neutral).
We use this data for two experiments. In the ﬁrst
experiment, we measure how well the classiﬁer pre-
dictions correlate with human annotation for the 6
emotions. For this experiment we create the dataset
D1 by randomly sampling 4800 lines consisting of
800 samples for each emotion class (except for
the neutral class). In the second experiment, we
measure how well the classiﬁer is able to predict
a neutral emotion. We create the dataset D2 by
extracting a subset of 45,000 neutral lines.

252Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A
new approach to understanding coordination of lin-
guistic style in dialogs. In Proceedings of the Work-
shop on Cognitive Modeling and Computational Lin-
guistics, ACL 2011.

Yann N. Dauphin, Angela Fan, Michael Auli,
Language model-
CoRR,

and David Grangier. 2016.
ing with gated convolutional networks.
abs/1612.08083.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Con-
volutional sequence to sequence learning. CoRR,
abs/1705.03122.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efﬁcient text
classiﬁcation. CoRR, abs/1607.01759.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Roman Klinger, Orph´ee de Clercq, Saif M. Moham-
mad, and Alexandra Balahur. 2018.
Iest: Wassa-
2018 implicit emotions shared task. In Proceedings
of the 9th Workshop on Computational Approaches
to Subjectivity, Sentiment and Social Media Analysis,
Brussels, Belgium. Association for Computational
Linguistics.

Walaa Medhat, Ahmed Hassan, and Hoda Korashy.
2014. Sentiment analysis algorithms and applica-
tions: A survey. Ain Shams Engineering Journal,
5(4):1093–1113.

Saif M. Mohammad. 2018. Obtaining reliable hu-
man ratings of valence, arousal, and dominance for
In Proceedings of The An-
20,000 english words.
nual Conference of the Association for Computa-
tional Linguistics (ACL), Melbourne, Australia.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proc. of NAACL.

253
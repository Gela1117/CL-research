Ternary Twitter Sentiment Classiﬁcation with

Distant Supervision and Sentiment-Speciﬁc Word Embeddings

Mats Byrkjeland Frederik Gørvell de Lichtenberg Björn Gambäck

Department of Computer Science

Norwegian University of Science and Technology

NO—7491 Trondheim, Norway

{matsbyr,frederikgdl}@gmail.com, gamback@ntnu.no

Abstract

The paper proposes the Ternary Sentiment
Embedding Model, a new model for creat-
ing sentiment embeddings based on the Hy-
brid Ranking Model of Tang et al. (2016),
but
trained on ternary-labeled data instead
of binary-labeled, utilizing sentiment embed-
dings from datasets made with different distant
supervision methods. The model is used as
part of a complete Twitter Sentiment Analysis
system and empirically compared to existing
systems, showing that it outperforms Hybrid
Ranking and that the quality of the distant-
supervised dataset has a great impact on the
quality of the produced sentiment embeddings.

1

Introduction

Bengio et al. (2003) introduced word embed-
dings as a technique for representing words as
low-dimensional real-valued vectors capturing the
words’ semantic and lexical properties, based on
ideas dating back to the 1950s (Firth, 1957). Col-
lobert and Weston (2008) showed the utility of
using pre-trained word embeddings, and after the
introduction of word2vec (Mikolov et al., 2013),
which is much faster to train than its predecessors,
word embeddings have become ubiquitous. This
effectuated a dramatic shift in 2016 at the Inter-
national Workshop on Semantic Evaluation (Sem-
Eval), with eight of the top-10 Twitter Sentiment
Analysis systems using word embeddings.

Word embeddings learn the representation of a
word by looking at its contexts (word neighbours
in a text), making it difﬁcult to discriminate be-
tween words with opposite sentiments that appear
in similar contexts, such as “good” and “bad”.
Hence, Tang et al. (2014) presented Sentiment-
Speciﬁc Word Embeddings (or Sentiment Embed-
dings), a model employing both context and senti-
ment information in word embeddings.

Training sentiment embeddings requires large
amounts of sentiment annotated data. Manual an-
notation is too expensive for this purpose, so fast,
automatic annotation is used to set low-quality
(weak) labels on large corpora; a procedure re-
ferred to as distant supervision. The traditional ap-
proach is to use occurrences of emoticons to guess
binary sentiment (positive / negative). Motivated
by the possible performance gains of focusing on
the ternary task (where tweets can also be classi-
ﬁed as neutral), this paper compares distant super-
vision methods on a large corpus of tweets that
can be used to train sentiment embeddings. To this
end, a new model architecture was developed with
a new loss function trained on three-way classi-
ﬁed distant supervised data. Various lexicon-based
sentiment classiﬁers are compared, with their per-
formance as distant supervision methods tested as
part of a complete Twitter Sentiment Analysis sys-
tem, evaluating both prediction quality and speed.
The paper is laid out as follows: Section 2 in-
troduces related work on word and sentiment em-
beddings. Section 3 describes the proposed model
for training ternary sentiment embeddings. Sec-
tion 4 introduces a set of distant supervision meth-
ods and a comparison between them. Section 5
explores the optimal setup for the Ternary Senti-
ment Embedding Model through hyperparameter
searches and dataset comparisons, while Section 6
compares the model against baselines and other
methods to establish its performance. Section 7
concludes and suggests future improvements.

2 Related Work

Recent years have seen a vast number of Twit-
ter Sentiment Analysis (TSA) systems, mainly be-
cause SemEval since 2013 has featured a TSA
task, providing training data and a platform to
compare different systems. This data will be uti-

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages97–106Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P1797lized here and the results below will be compared
to those of SemEval in Section 6.4. First, however,
the models most directly related to the present
work will be introduced: the Collobert and Weston
model (Collobert et al., 2011), three Sentiment
Embeddings models by Tang et al. (2014), and
their Hybrid Ranking Model (Tang et al., 2016).
These can be viewed as sequential reﬁnements of
each other and as predecessors of the Ternary Sen-
timent Embedding Model described in Section 3.
The Collobert and Weston (C&W) Model:
Collobert et al. (2011) proposed a task-general
multilayer neural network language processing ar-
chitecture. It starts with a Lookup Layer, which
extracts features for each word, using a window
approach to tag one word at a time based on its
context. The input vector is then passed through
one or several Linear Layers that extract features
from a window of words, treated as a sequence
with local and global structure (i.e., not as a bag of
words). The following layers are standard network
layers: a HardTanh Layer adds some non-linearity
to the model (Collobert and Weston, 2008) and a
ﬁnal Linear Layer produces an output vector with
dimension equal to the number of classes.

When learning word embeddings from context
information, the output vector has size 1. For each
context used to train the model, a corrupted con-
text is created by replacing the focus word with a
random word from the vocabulary. Both the cor-
rect and the corrupted context windows are passed
through the model, with the training objective that
the original context window should obtain a higher
model score than the corrupted by a margin of 1.
This can be formulated as a hinge loss function:
losscw(t, tr) = max(0, 1 − f cw(t) + f cw(tr))
where t and tr are the original and corrupted con-
text windows, and f cw(·) the model score.
Sentiment Embeddings: To improve word em-
beddings for sentiment analysis, Tang et al. (2014)
introduced Sentiment-Speciﬁc Word Embeddings
(SSWE). They enhanced the C&W word embed-
ding model by employing massive amounts of
distant-supervised tweets, assigning positive la-
bels to tweets containing positive emoticons and
negative to those containing negative emoticons.
Tang et al. used three strategies to incorporate
sentiment information in embeddings: two basic
models that only look at sentence sentiment polar-
ity, and a Uniﬁed Model which adds word context
and C&W’s corrupted context window training.

0 and f r

0 (t) + δs(t)f r

Basic Model 1 uses C&W’s window-based ap-
proach, but with the top linear layer’s output vec-
tor elements deﬁning probabilities over labels. A
softmax activation layer is added to predict posi-
tive n-grams as [1, 0] and negative as [0, 1]. This
constraint is relaxed in Basic Model 2, which re-
moves the softmax layer to handle more fuzzy dis-
tributions and uses a ranking objective function:
lossr(t) = max{0, 1 − δs(t)f r
1 (t)}
where f r
1 are the predicted positive and
negative scores, while δs(t) reﬂects the gold senti-
ment polarity of the context window t, with
δs(t) = {1 : f g(t) = [1, 0]} ∧ {−1 : f g(t) = [0, 1]}
Uniﬁed Model uses corrupted context window
training with two objectives: the original context
should get a higher language model score and be
more consistent with the gold polarity annotation
than the corrupted one. The loss function com-
bines word contexts and sentence polarity:
lossu(t, tr) = α·losscw(t, tr)+(1−α)·lossus(t, tr)
where 0 ≤ α ≤ 1 weights the parts, losscw is the
C&W loss function, and with δs(t) as above:
lossus(t, tr) = max{0, 1−δs(t)f u
1 (tr)}
1 (t)+δs(t)f u
Hybrid Ranking Model (Tang et al., 2016)
splits the top linear layer of the Uniﬁed Model
into a context-aware layer that calculates a con-
text score f cw and a sentiment-aware layer calcu-
lating a sentiment score f r for the input context
window. The objective function only compares
the predicted positive and negative score for the
correct context window when calculating the loss:

losshy = α · lossr + (1−α) · losscw

3 Ternary Sentiment Embedding Model
A new neural network model for training word
embeddings called the Ternary Sentiment Embed-
ding Model is proposed. The model extends the
Hybrid Ranking Model by Tang et al. (2016) for
training Sentiment-Speciﬁc Word Embeddings by
also looking at tweets labeled as “neutral”, and
consists of three bottom (core) layers and two top
layers that work in parallel, as shown in Figure 1.
Core Layers: The ﬁrst layers identical to those
of the C&W model. As with that model, the ob-
jective of the context part of the Ternary Sentiment
Embedding Model is to assign a higher score to a
correct context window than a corrupted window:
lossc(t, tr) = max(0, m − f c(t) + f c(tr)) (1)

98where wt is the value of the parameter w at time t,
gt its gradient at time t, and lr the learning rate.

The model parameters are initialised as in Tang
et al. (2016). Lookup layer parameters are ini-
tialised with values from the uniform distribution
U (−0.01, 0.01), while hidden layer parameters
are initialised using fan-in (Collobert et al., 2011),
i.e., the number of inputs used by a layer, i. The
technique draws the initial parameters from a cen-
tred distribution with variance V = 1/
i. Fan-in
is also used for the learning rate, with lr for the
hidden layers in Eqn. 4 divided by the fan-in, i.

√

4 Distant Supervision of Tweets

The idea of distant supervision is to automatically
label data in order to be able to leverage large
amounts of it. These data are called distant su-
pervised or weakly annotated, as the quality is
not great, but the quantity is. To train sentiment
embeddings, large amounts of weakly annotated
tweets are needed. This section describes the ap-
proach of extracting weak labels from a corpus of
collected tweets (about 547 million), and explains
each of the sentiment analysis methods that are
compared for distant supervision use.

Figure 1: Ternary Sentiment Embedding Model.
At the top are the Context Linear Layer and the new Ternary
Sentiment Linear Layer; in the middle HardTanh and Linear
layers, with the word context Lookup Layer at the bottom.

where m is the margin (m = 1 ⇒ lossc = losscw),
t and tr the correct resp. corrupted context win-
dows, and f c(·) the context linear layer’s score.
Ternary Sentiment Linear Layer: A new top
linear layer is introduced to calculate sentiment
scores. It outputs a vector of size 3, representing
positive, negative and neutral scores for a given
context window. The objective is to give a higher
score to the value corresponding to the context’s
label than the other possible labels. A new margin
hinge loss function is used to train the model:

losss(t) = max(0, m − f s
+ max(0, m − f s

c (t) + f s
c (t) + f s

i1(t))
i2(t))

(2)

c (·)
where t is a context window, m the margin, f s
the sentiment score for the currently labelled sen-
i2(·)
i1(·) and f s
timent of the input context, and f s
the sentiment scores for the other two classes.
The model’s total loss function is a weighted
linear combination of the hinge losses for the Sen-
timent Linear and Context Linear layers:
loss(t, tr) = α·losss(t)+(1−α)·lossc(t, tr) (3)

Model Training: As in the C&W model, param-
eters of the neural network are trained by taking
the derivative of the loss through backpropagation.
Stochastic Gradient Descent (SGD) is used to up-
date the model parameters. This means that sam-
ples, in this case context windows created from
tweets, are randomly drawn from the training cor-
pus, and the parameters are updated for each sam-
ple passed through the model, according to:

wt = wt−1 − lr · gt

(4)

1

The outputs are ranked using SemEval’s mea-
(the average of the F1-scores for pos-
sures FP N
itive and negative samples) and AvgRec (the av-
erage of the recall scores for the three classes).
While FP N
and AvgRec have been used in Sem-
Eval for both binary and ternary classiﬁcation, it is
debatable how representative they are for the lat-
ter. Hence, the Macro F1 metric used by Tang et al.
(2016) will also be calculated. It extends FP N
by
averaging the F1-scores of all three classes.

1

1

Emoticons and Emojis: Go et al. (2009) auto-
matically classiﬁed tweet sentiment using distant
supervision based on a few positive (‘:)’, ‘:-)’, ‘: )’,
‘:D’, ‘=)’) and negative (‘:(’, ‘:-(’, ‘: (’) emoticons,
while removing tweets containing both a positive
and negative emoticon. This method was reim-
plemented in Python and adapted to the ternary
task by classifying tweets containing none of the
emoticons as neutral. Further, since the sets of
emoticons used by Go et al. are quite sparse com-
pared to the vast amount of emojis and emoticons
available today, extended sets (“Emojis+”) were
also created, as shown in the Appendix.

99Dataset

2013-dev
2013-test
2013-train
2014-sarcasm
2014-test
2015-test
2015-train
2016-dev
2016-devtest
2016-test
2016-train

Class.

1,228
2,695
7,109
56
1,460
1,865
352
1,657
1,645
16,771
4,893

Dist.

Pos.

Neg.

959
1,839
5,411
52
997
1,363
281
1,051
1,171
12,072
3,256

353
827
2,171
20
556
610
103
453
574
4,328
1,714

198
318
878
26
134
249
39
207
193
1,899
515

Neut.

408
694
2,362
6
307
504
139
391
404
5,845
1,027

2013-2016-all

39,731

28,452

11,709

4,656

12,087

Table 1: Sentiment distribution in the datasets

AFINN, TextBlob and VADER: These meth-
ods respectively use the AFINN (Nielsen, 2011),1
TextBlob,2 and VADER (Hutto and Gilbert, 2014)
libraries to count tweet sentiment scores. For
AFINN, tweets with a 0 sentiment score were clas-
siﬁed as neutral, while those with scores greater
and lower than 0 were classiﬁed as positive and
negative, respectively. For TextBlob, tweets with
subjectivity score less than a threshold θs were de-
ﬁned as neutral; a threshold θp was set to clas-
sify tweets with polarity < −θp as negative and
those with polarity > θp as positive. VADER re-
turns a 3D vector where each element represents a
score for each sentiment class. The vector is nor-
malized so that positiveScore+negativeScore+
neutralScore = 1. Setting a conﬁdence thres-
hold θc > 0.5 acertains that the other scores are be-
low 0.5. If no score is > θc, the tweet is skipped.
VADER also gives a compound score, a single
sentiment score from −1 to 1 (most positive).

The methods’ hyperparameters were tuned
through grid searches, testing each value in in-
creasing steps of 0.1. VADER struggled to classify
positive and negative tweets as the threshold in-
creased, and performed best at θc = 0.1. TextBlob
performed best with a low subjectivity threshold,
with θs = 0.1 and θp = 0.3 chosen for the ﬁnal
classiﬁer, as these values gave the best Macro F1.

Combo Average: An ensemble of the AFINN,
TextBlob, and VADER classiﬁers, with scores nor-
malised to be in the [−1, 1] range. For AFINN, its
score is divided by 5 · n (the number of words in
the tweet), since |5| is the highest score a word
can get. For VADER, the compound score is used,

1github.com/fnielsen/afinn
2github.com/sloria/textblob

Method

F1
.570
LC
Combo B .561
Combo A .557
.541
TextBlob
AFINN
.537
.532
VADER
Emoji+
.259
Emoticon
.251

FP N
1
.532
.532
.537
.502
.542
.524
.130
.061

FPOS
1
.593
.626
.628
.643
.620
.621
.101
.086

FNEG
1
.472
.437
.446
.361
.465
.428
.159
.036

FNEU
1
.646
.620
.598
.619
.526
.546
.517
.630

ms

0.93
2.27
2.26
0.48
1.21
0.63
0.11
0.09

Table 2: Distant supervision, SemEval 2013–2016

while TextBlob’s score is already normalised. The
scores are combined using a weighted average:
(a · aﬁnn + b · vader + c · textblob)/(a + b + c).
A threshold θ is set so that tweets with score > θ
are classiﬁed positive, those with score <−θ neg-
ative, and all others neutral. Running a grid search
as above to select the method’s four parameters,
the combination achieving the top FP N
score was
{a = 0.0, b = 0.4, c = 0.4, θ = 0.2} (this is called
Combo A below), while the Macro F1 winner was
{a = 0.3, b = 0.1, c = 0.1, θ = 0.1} (Combo B).
Lexicon Classiﬁer (LC): A Python port of the
Lexicon Classiﬁer of Fredriksen et al. (2018), us-
ing their best performing lexicon and parameters.3

1

Evaluation: All manually annotated SemEval
datasets from 2013 to 2016 were downloaded.
They contain IDs for 50,333 tweets, but 10,251
of those had been deleted, while duplicates were
removed,4 leaving 39,731 tweets for later classi-
ﬁer training (the second column of Table 1). For
the distant supervision, further ﬁltering removed
retweets (i.e., copies of original tweets; including
retweets might lead to over-representation of cer-
tain phrases), tweets containing ° symbols (mostly
weather data), tweets containing URLs, and tweets
ending with numbers (often spam). Then 28,452
tweets remained for evaluating the distant super-
vision methods, distributed as in Table 1 (note that
only 16% of the total tweets are negative).

Comparisons of the methods with tuned param-
eters on all SemEval datasets merged into one (the
2013-2016-all dataset of Table 1) are shown in Ta-
ble 2. We see that the top Macro F1 score is 0.570,
which does not seem very impressive. However,
to our knowledge no previous sentiment analysis

3github.com/draperunner/fjlc
4If duplicate tweets with the same sentiment label were
If duplicate tweets were found

found, only one was kept.
with different labels, both were deleted.

100research has been evaluated against the complete
set of SemEval datasets, making the results hard
to compare to other work. Evaluating each dataset
individually, a trend could be observed with de-
creasing scores for later data, with a top Macro F1
score on the 2013-test set of 0.628 compared to
0.578 on the 2016-test set. This is consistent with
Fredriksen et al. (2018) who noted signiﬁcantly
dropping scores for tests on 2016 data, attributing
this to those sets having more noise and annotation
errors than earlier datasets.

The runtimes (ms) in Table 2 were obtained on a
computer with four AMD Opteron 6128 CPUs and
125 GB RAM running Ubuntu 16.04 (note that the
given runtimes do not include saving to ﬁle). The
emoticon-based methods are very fast (0.09 and
0.11 ms/tweet), but their scores are substantially
worse than the others. The ensemble methods are
slow (2.26 and 2.27 ms/tweet), since they have to
calculate the score of each component method.

5 Optimising the Model
In order to ﬁnd the best performing conﬁguration
of the Ternary Sentiment Embedding Model, the
hyperparameters were tested one-by-one through
a search of manually selected values.

More than 500 million tweets had been col-
lected at the time of the start of the experiments,
with URLs, mentions, reserved words and num-
bers removed. The tweets were lower-cased and
elongated words reduced to contain a maximum
of three repeating characters. Using the distant
supervision methods described above,
the col-
lection was iterated through, and the resulting
datasets saved. To create even datasets for each
method and label, three sets of 1M tweets from
each sentiment class were extracted from the total
datasets for each method (except for the Emoti-
con method, which only annotated 151,538 tweets
as negative, so its datasets were limited to 150k
tweets for each label). The model hyperparameter
searches were performed using the dataset created
by the Lexicon Classiﬁer, since it was the top per-
former in Table 2. The following paragraphs give
the results for each of the hyperparameters.
Context Window Size: Testing with window
sizes in the range [1, 9] showed the best perform-
ing size to be 3 (Macro F1 = 0.6325). Tweets are
typically short texts with informal language. It is
possible that larger context windows will lead to
the model considering excerpts that are too long

for tweet lingo, and would ﬁt better for more for-
mal texts. However, the differences in the results
were too small to draw any conclusions.

Embedding Length is the dimension of each
word embedding vector. The larger the dimen-
sion, the more ﬁne-grained information the vec-
tors can hold. {50, 75, 100, 125, 150} dimensions
were tested, with 150 performing best (Macro F1
= 0.6249), indicating that larger embeddings re-
sult in better scores for the model. This is no sur-
prise, as word embeddings as GloVe and word2vec
are commonly trained with dimensions of 200 or
300. However, a length of 100 was selected, since
larger embeddings only gave minor improvements
but severely increased processing time.

Hidden Layer Size: For the Ternary Sentiment
Embedding Model, the hidden layers are the Lin-
ear Layer and the HardTanh Layer. Experiments
show a minimal impact of varying the hidden
layer size ({10, 20, 30, 50, 100} neurons), hav-
ing a range on the score values of only 0.0046.
The best performance (Macro F1 = 0.6201) was
achieved with size 100. These results correspond
well to the claim by Collobert et al. (2011) that
the size of the hidden layer, given it is of sufﬁcient
size, has limited impact on the generalisation per-
formance. However, the size of the hidden lay-
ers has a signiﬁcant impact on training runtime, so
since the difference in score values were small, a
hidden layer size of 50 was used in the ﬁnal model.

Alpha is the weighting between the sentiment
loss and the context loss in the combined loss
function (Eqn. 3) used when training the model.
α-values in the range [0.1, 1.0] were explored. The
best score (Macro F1 = 0.6310) was achieved for
α = 0.2. This indicates that the contexts of the
words are more important than the sentiment of the
tweets. However, leaving out sentiment informa-
tion altogether (α = 0) gave the by far worse score
(0.5400). Interestingly, leaving out context infor-
mation (α = 1) did not perform as badly (0.6069).

Learning Rate
states how fast the neural net-
work parameters are updated during backpropa-
gation. A small rate makes the network slowly
converge towards a possible optimal score, while
a large rate can make it overshoot the optimum.
Testing on values from 0.001 to 1.1, the best learn-
ing rate was 0.01 (Macro F1 = 0.6231), although
the total range of the scores was only 0.0412.

101Method

F1
Combo B
.609
Combo A .608
.604
LC
AFINN
.602
VADER
.596
.584
TextBlob
.548
Emoji+
Emoticon
.504

FP N
1
.595
.596
.587
.589
.583
.571
.525
.481

FPOS
1
.668
.667
.665
.660
.656
.657
.630
.595

FNEG
1
.522
.524
.509
.518
.511
.486
.419
.368

FNEU
1
.637
.633
.637
.628
.623
.608
.594
.550

Table 3: Distant supervision method comparison

prises the Ternary Sentiment Embedding Model
and a linear kernel Support Vector Machine
(SVM). The C parameter of the SVM classiﬁer
was set through a coarse search on values ranges
from 0.001 to 1000 with the word embeddings
produced by the Ternary Sentiment Embedding
Model using the LC distant supervision dataset
and trained for 20 epochs, followed by two ﬁner
searches around the best value of 0.01, cover-
ing value ranges of [0.001, 0.009] and [0.01, 0.09],
with a C value of 0.006 giving the best performing
classiﬁer. A small C means the classiﬁer favours
more misclassiﬁed samples over separating sam-
ples by a large margin, indicating that it is hard to
avoid misclassifying some samples. However, the
differences in scores were very low even for large
variations of the parameter, meaning the samples
to classify are not easily linearly separable.

6.1 Comparing Distant Supervision Methods
The Ternary Sentiment Embedding Model was
trained for 20 epochs using the different distant
supervision methods and the produced sentiment
embeddings tested using the SVM classiﬁer using
10-fold cross validation on the unﬁltered 39,731
tweets from the 2013-2016-all dataset (i.e., all the
combined 2013–2016 SemEval datasets), 15,713
of which were positive (39.5%), 5,945 negative
(15.0%), and 18,073 neutral (45.5%). Table 3
shows different metrics for the tests, sorted by de-
scending Macro F1 score.

The Combo methods perform best in this com-
parison. By averaging over three methods, they
can overcome weaknesses of their components.
While the combo methods were not the top per-
formers in the comparison of the distant super-
vision methods in Table 2, the ability to balance
other weak classiﬁers seems to be important when
used as distant supervision method for the pro-
posed model. The results show that the Ternary
Sentiment Embedding Model performs best when

Figure 2: Distance supervision scores / epochs
(Colour legend, top-to-bottom: Lexicon Classiﬁer, Combo B,
Combo A, VADER, AFINN, TextBlob, Emoji+, Emoticon.)

Margin deﬁnes how the scores should be sepa-
rated in the loss functions of Eqn. 1 and Eqn. 2.
Larger margins lead to similar scores for each sen-
timent class giving a larger total loss, with the
model parameters being updated by a larger value
during backpropagation. Experimenting with mar-
gins in the range [0.5, 10.0], a value of 2.0 ob-
tained the best Macro F1 (0.6188). It is hard to
predict the impact of higher margins, but since the
loss is greater when sentiment scores are close,
this appears to give a better separation of words
from tweets belonging to each sentiment class.

Distant Supervision Method: Using the above-
selected hyperparameter values, the Ternary Sen-
timent Embedding Model was trained on the 3M
tweet datasets created by using each distant super-
vision method (450k tweets for the Emoticon
method). Performance over 1–20 epochs is shown
in Figure 2. A top Macro F1 score of 0.6440 was
obtained for LC after 10 epochs, but the scores
vary notably for each epoch. For a more robust
comparison, the Macro F1 scores were averaged
over epochs 10 to 20, with LC again perform-
ing best (0.6383), but followed closely by the en-
semble methods (Combo B: 0.6361, Combo A:
0.6352), VADER (0.6339), and AFINN (0.6296).

6 Evaluating the Final System

To evaluate the performance of the Ternary Sen-
timent Embedding Model, it was compared to the
Hybrid Ranking Model by Tang et al. (2016) us-
ing different distant supervision methods, as well
as to a range of baselines, among them other pop-
ular word embeddings models. Finally, the per-
formance of the total Twitter Sentiment Analysis
system was evaluated against the state-of-the-art.
The Twitter Sentiment Analysis system com-

102is good at classifying all

trained on data from a distant supervision method
that
tweets into all
three sentiment classes. The emoticon methods
and TextBlob have weaknesses when classifying
tweets into one or more of the classes, hence yield-
ing the worst results for the total system.

6.2 Comparison to Baselines
In order to see how well the ﬁnal TSA system
performs, it was compared to some existing sen-
timent analysis methods. The systems were also
tested using 10-fold cross-validation on the unﬁl-
tered 2013-2016-all dataset (39,731 tweets). Ta-
ble 4 shows the results for each TSA system using
the Macro F1 metric.

‘Random Uniform’ and ‘Random Weighted’ are
two simple baselines, respectively created by pick-
ing a random label from a uniform probability dis-
tribution and by picking a random label from the
same distribution as in the training set. The distant
supervision classiﬁers are as above, except that the
Emoticons and Emoji+ methods add the variation
that tweets containing both negative and positive
emoticons are regarded as neutral.

The word embeddings for Ternary Sentiment
Embedding Model, GloVe and word2vec were all
trained on the same set of 3M tweets, with 1M
from each of the sentiment classes, assigned by
the Lexicon Classiﬁer distant supervision method.
The GloVe model (Pennington et al., 2014) was
used to train word embeddings of dimension 100.
The word2vec (Mikolov et al., 2013) embeddings
were trained using both the Continuous Bag-of-
Words (CBOW) and the Continuous Skip-gram
model, also with 100 dimensions. Word embed-
dings were also produced using the Hybrid Rank-
ing Model of Tang et al. (2016) trained on a set
of 3M tweets classiﬁed with the LC method, but
using only tweets labelled as positive or nega-
tive when training word embeddings, with 1.5M
tweets of each class. All the word embeddings
were fed to the SVM classiﬁer speciﬁed above.

As the results in Table 4 shows, the best Macro
F1 scores were achieved by the word embed-
ding systems. The word embeddings produced
by the Ternary Sentiment Embedding Model gave
slightly better results than the word embeddings
produced by the Continuous Bag-of-Words model,
however, the difference is small.

One of the strengths of the word2vec models
is that they require much less training time than

Model

Ternary Sentiment Embedding Model
word2vec (CBOW)
Hybrid Ranking Model (w/LC)
word2vec (Skip-gram)
LC
GloVe
Combo B
Combo A
AFINN
VADER
TextBlob
Random Weighted
Random Uniform
Emoji+
Emoticon

F1

.6036
.6015
.5919
.5886
.5706
.5662
.5621
.5579
.5381
.5286
.3826
.3315
.3174
.2542
.2462

Table 4: The ﬁnal Ternary Sentiment Embedding
Model compared to baselines (Macro F1-scores)

larger neural network models such as the Collobert
and Weston model and the Ternary Sentiment Em-
bedding Model. The word2vec models used ap-
proximately three minutes, while the Ternary Sen-
timent Embedding Model used 24 hours to train
on 3M tweets. This advantage of the word2vec
models means that they could be trained using a
much larger dataset, which would likely yield an
even better performance.

The word2vec models do not utilise sentiment
information of the tweets, which is necessary
to create sentiment embeddings with the Ternary
Sentiment Embedding Model. This is another ad-
vantage of the word2vec models, as they have no
need for a separate distant supervision method.
The word2vec models are, however, slightly out-
performed by the Ternary Sentiment Embedding
Model in terms of the ﬁnal score, and with further
optimisation the difference could increase.

6.3 Comparison to Hybrid Ranking Model
In order to compare the distant supervision per-
formance of the sentiment embeddings produced
by the Ternary Sentiment Embedding Model and
the Hybrid Ranking Model of Tang et al. (2016),
both architectures were trained for 20 epochs on
3M tweets weakly annotated using the different
distant supervision methods of Section 4. The
Ternary Sentiment Embedding Model was trained
on tweets labelled as positive, negative or neu-
tral, with 1M of each, with the hyperparameters
stated in Section 5. The Hybrid Ranking Model
only utilises tweets labelled as positive or neu-
tral, and was as a result trained on 1.5M tweets
of each sentiment class, using the hyperparameters

103Dataset

AFINN
Combo A
Combo B
Emoticon
Emoji+
LC
TextBlob
VADER

TSA system

Ternary Embedding Hybrid Ranking

.602
.608
.609
.504
.548
.604
.584
.596
.655

.578
.587
.592
.528
.536
.592
.575
.596

.634

Table 5: Ternary Sentiment Embedding Model vs.
Tang et al.’s Hybrid Ranking (Macro F1-scores)

given by Tang et al. (2016). The produced senti-
ment embeddings were fed to the SVM classiﬁer
and tested using 10-fold cross-validation over the
2013-2016-all SemEval dataset.

The results in Table 5 show that the Ternary
Sentiment Embedding Model outperforms the Hy-
brid Ranking Model using all but two of the eight
tested distant supervision methods. The Hybrid
Ranking Model only performs signiﬁcantly better
than the proposed model on the Emoticon dataset.
The Hybrid Ranking Model is trained using only
tweets labelled as positive or negative, while the
Ternary Sentiment Embedding Model also utilises
neutral tweets. The Emoticon method performs
well for classifying tweets as positive or negative,
but not for neutral, meaning that the quality of the
positive and negative tweets is likely higher than
for neutral tweets. This possibly explains why the
Hybrid Ranking Model performs better when us-
ing this method.

When using the more sophisticated distant
supervision methods, the Ternary Sentiment Em-
bedding Model outperforms the Hybrid Ranking
Model, with the exception of VADER where the
scores are identical. This indicates that the pro-
posed model is able to better take advantage of
sentiment information from a larger set of tweets,
increasing performance when used for the ternary
sentiment classiﬁcation task.

To compare the entire Twitter Sentiment Anal-
ysis system performance to that of Tang et al.
(2016),
the unﬁltered datasets from SemEval
2013 were chosen for the classiﬁer optimisation,
with training on the 7,109 tweet 2013-train set
(distributed 2,660-1,010-3,439 positive-negative-
neutral) and testing on the 2013-dev set (1,228
tweets distributed 430-245-553), as this was the
validation set of the 2013 workshop.

Year

2013
2016
2017

Top SemEval result

Ternary Embedding

.6902
.633
.685

.61789
.580512
.62919

Table 6: Comparison to top results from differ-
ent SemEvals (FP N
scores). Subscripts denote the
ranking the system would have achieved each year.

1

Tang et al. (2016) trained sentiment embed-
dings on 5M positive and 5M negative distant-
supervised tweets, publishing the results produced
by their model when tested with a SVM classi-
ﬁer on the SemEval 2013 test dataset, as presented
in the last line of Table 5.5 The results indi-
cate that the Ternary Sentiment Embedding Model
performs better on the ternary classiﬁcation task
than the Hybrid Ranking Model, even though Tang
et al.’s embeddings were trained on a much larger
dataset than those used in the present work.

6.4 Comparison to SemEval

To see how the ﬁnal Twitter Sentiment Analysis
system fares against the state-of-the-art, its perfor-
mance was compared to the published results of
SemEval 2013 Task 2B (Nakov et al., 2013), Sem-
Eval 2016 Task 4A (Nakov et al., 2016), and Sem-
Eval 2017 Task 4A (Rosenthal et al., 2017).

The system was trained using the training sets
provided by the respective workshop. For 2013,
the model was trained on 2013-train-A and tested
on 2013-test-A. SemEval 2016 and 2017 allowed
training on the training and development datasets
of previous years, so for 2016, the model was
trained on a combined 2013-2016-train-dev-A
dataset and tested on 2016-test-A, while for 2017,
the model was trained on all 2013-2016 datasets
and tested on 2017-test-A.

The results in Table 6 show that the Ternary
Sentiment Embedding Model does not match the
top systems of the different years. There are some
possible reasons to this: The SemEval systems
might have trained on other or more data than here.
As tweets have been deleted, not as many could be
downloaded as were available at the time of each
workshop. Also, the model is optimised for Macro
F1 score. Had it been optimised for FP N
, better
scores for this metric could have been obtained.

1

5Only the most similar systems are compared here; Tang
et al.’s results improved by using additional lexical features.

1047 Conclusion and Future Work

The paper has proposed the Ternary Senti-
ment Embedding Model, a model for training
sentiment-speciﬁc word embeddings using dis-
tance supervision. The model is based on the Hy-
brid Ranking Model of Tang et al. (2016), but
considers the three classes positive, negative and
neutral instead of just positive and negative. Ex-
periments show the Ternary Sentiment Embedding
Model to generally perform better than the Hybrid
Ranking Model, and that the quality of the distant-
supervised dataset greatly impacts the quality of
the produced sentiment embeddings, and transi-
tively the Twitter Sentiment Analysis system.6

The Hybrid Ranking Model only performed sig-
niﬁcantly better than the proposed model on the
Emoticon dataset. Tang et al. (2016) use a dis-
tant supervision method similar to the Emoticon
method, due to the high precision that method can
give. For a ternary model, however, it is not sufﬁ-
cient to only ﬁnd some tweets that are likely pos-
itive or negative, and a more sophisticated dis-
tant supervision method is essential. This also
means that a much larger and more varied corpus
of distant supervised tweets can be used for train-
ing, since no tweets are discarded. Consequently,
the Ternary Sentiment Embedding Model outper-
formed the Hybrid Ranking Model when using
more sophisticated distant supervision methods.

Both Hybrid Ranking and Ternary Sentiment
Embedding assume that all senses of a word are
synonyms and that all words in a tweet have the
same sentiment, ignoring their prior sentiment po-
larity. Ren et al. (2016) proposed a model for
training topic-enriched multi-prototype word em-
beddings that addresses the issue of polysemy,
signiﬁcantly improving upon the results of Sem-
Eval 2013 on the binary classiﬁcation task. Xiong
(2016) addressed the prior polarity problem by ex-
ploiting both a sentiment lexicon resource (Hu and
Liu, 2004) and distant supervised information in
a multi-level sentiment-enriched word embedding
learning method. Further work could look at ex-
tending the Ternary Sentiment Embedding Model
with the ability to discriminate sentiment of pol-
ysemous words in three classes, and to use word-
sense aware lexica in order to combine the works
of Ren et al. (2016) and Xiong (2016).

Acknowledgements
Thanks to Valerij Fredriksen and Brage Ekroll
Jahren for providing their classiﬁer code, to the or-
ganisers of the different SemEval sentiment analy-
sis tasks for collecting the data, to the anonymous
reviewers for comments that helped enrich the dis-
cussion of the results, and to Steven Loria, Finn
Årup Nielsen, Clayton J. Hutton and Eric Gilbert
for respectively providing the TextBlob, AFINN
and VADER libraries.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
Journal of Machine Learning Re-
guage model.
search, 3:1137–1155.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th International Conference on
Machine Learning, pages 160–167, Helsinki, Fin-
land. ACM.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12:2493–2537.

John Rupert Firth. 1957. A synopsis of linguistic the-
ory 1930–55. Studies in Linguistic Analysis, Special
Volume of the Philological Society.

Valerij Fredriksen, Brage Jahren, and Björn Gambäck.
2018. Utilizing large Twitter corpora to create sen-
In Proceedings of the 11th Interna-
timent lexica.
tional Conference on Language Resources and Eval-
uation, pages 2829–2836, Miyazaki, Japan. ELRA.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classiﬁcation using distant supervision.
CS224N project report, Stanford University, CA,
USA.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
New York, NY, USA. ACM.

Clayton J. Hutto and Eric Gilbert. 2014. VADER: A
parsimonious rule-based model for sentiment anal-
In Proceedings of the
ysis of social media text.
Eighth International Conference on Weblogs and
Social Media, pages 216–225, Ann Arbor, MI, USA.
The AAAI Press.

6To perform the experiments, several tools and programs
See:

were developed, most of these are open sourced.
github.com/draperunner

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

105Appendix:
Positive and negative emoticons and emojis
The character combinations and Unicode charac-
ters used in the ‘Emoticons’ and ‘Emojis’ distant
supervision methods described in Section 4.

Positive

Negative

Emoticons

Emojis

Emoticons

Emojis

:)
:-)
: )
:D
=D
:-]
:]
:-3
:3
:->
:>
8-)
8)
:-}
:}
:o)
:c)
:ˆ)
=]
=)
:-D 8-D
8D
x-D
xD X-D
XD
=D
=3 B-ˆD
:-))
:’-)
:-*
:’)
:×
:*
;)
;-)
*-)
*)
;]
;-]
:-,
;ˆ)
;D
<3

DX
:(
:-/
:-(
:/
: (
:-.
:’(
>:\
:-(
>:/
:(
:\
:-c
=/
:c
=\
:-<
:L
:<
=L
:-[
:S
:[
</3
:-||
<\3
>:[
:{
>.<
:@ v.v
>:(
D-’:
D:<
D:
D8
D;
D=

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. SemEval-
2016 Task 4: Sentiment analysis in Twitter.
In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016), pages 1–18,
San Diego, CA, USA. ACL.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013),
pages 312–320, Atlanta, GA, USA. ACL.

Finn Årup Nielsen. 2011. A new ANEW: Evalua-
tion of a word list for sentiment analysis in micro-
blogs. In Proceedings of the ESWC2011 Workshop
on ’Making Sense of Microposts’: Big things come
in small packages, volume 718 of CEUR Workshop
Proceedings, pages 93–98, Heraklion, Crete.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
In Proceedings of the Con-
word representation.
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543, Doha, Qatar. ACL.

Yafeng Ren, Ruimin Wang, and Donghong Ji. 2016. A
topic-enhanced word embedding for Twitter senti-
ment classiﬁcation. Information Sciences, 369:188–
198.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. SemEval-2017 Task 4: Sentiment analysis
in Twitter. In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 493–509, Vancouver, Canada. ACL.

Duyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting
Liu, and Ming Zhou. 2016.
Sentiment embed-
dings with applications to sentiment analysis. IEEE
Transactions on Knowledge and Data Engineering,
28(2):496–509.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014.
Learning sentiment-
speciﬁc word embedding for Twitter sentiment clas-
siﬁcation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1: Long Papers, pages 1555–1565, Bal-
timore, MD, USA. ACL.

Shufeng Xiong. 2016.

classiﬁcation via multi-level
word embeddings. CoRR, abs/1611.00126.

Improving Twitter sentiment
sentiment-enriched

106
Investigating the importance of linguistic complexity features across

different datasets related to language learning

Spr˚akbanken, University of Gothenburg

Spr˚akbanken, University of Gothenburg

Ildik´o Pil´an

Sweden

Elena Volodina

Sweden

ildiko.pilan@gu.se

elena.volodina@gu.se

Abstract

We present the results of our investigations aiming at identifying the most informative linguis-
tic complexity features for classifying language learning levels in three different datasets. The
datasets vary across two dimensions: the size of the instances (texts vs. sentences) and the lan-
guage learning skill they involve (reading comprehension texts vs. texts written by learners them-
selves). We present a subset of the most predictive features for each dataset, taking into consid-
eration signiﬁcant differences in their per-class mean values and show that these subsets lead not
only to simpler models, but also to an improved classiﬁcation performance. Furthermore, we pin-
point fourteen central features that are good predictors regardless of the size of the linguistic unit
analyzed or the skills involved, which include both morpho-syntactic and lexical dimensions.

1

Introduction

Linguistic complexity, especially in cross-linguistic studies, is often approached in absolute terms, de-
scribing complexity as a property of a linguistic system in terms of e.g. number of contrastive sounds. In
this paper, however, we investigate a relative type of linguistic complexity from a cognitive perspective,
our focus being the ability of L2 learners to process or produce certain linguistic elements in writing
at different stages of proﬁciency. We operationalize the term linguistic complexity as the set of lexico-
semantic, morphological and syntactic characteristics reﬂected in texts (or sentences) that determine the
magnitude of the language skills and competences required to process or produce them. In this work, we
use linguistic complexity analysis as a means to predict second language learning (L2) levels. The scale
of learning (proﬁciency) levels adopted here is the CEFR, the Common European Framework of Refer-
ence for Languages (Council of Europe, 2001) which proposes a six-point scale of proﬁciency levels:
from A1 (beginner) to C2 (advanced) level.

Large corpora in the language learning domain are rather scarce due to either copy-right issues, privacy
reasons or the need for digitizing them. For the Swedish language, a number of resources have become
available recently (Volodina et al., 2014; Volodina et al., 2016b), which, although somewhat small in
size, encompass texts involving different skills and CEFR levels. This allows for investigations about the
similarities and differences between linguistic complexity observable at different proﬁciency levels for
different skill types, namely receptive skills, required when learners process passages produced by others
and productive skills, when learners produce the texts themselves. We perform linguistic complexity
analyses across two different dimensions: the type of learner skills involved when dealing with the texts
and the size of the linguistic context investigated. In the latter case, we carry out experiments both at the
text and at the sentence level.

Throughout the years, a large number of linguistic features related to complexity has been proposed.
Typically, out of the features suggested for a speciﬁc task some are more useful than others. Eliminating
redundant features can result in simpler and improved models that are not only faster, but might also
generalize better on unseen data (Witten et al., 2011, 308). Such selection can also contribute to un-
derstand further the main factors playing role in linguistic complexity, which can be a useful means for

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/

ProceedingsoftheWorkshoponLinguisticComplexityandNaturalLanguageProcessing,pages49–58SantaFe,NewMexico,USA,August25,2018.49determining whether non-native speakers can understand or produce certain linguistic input at different
learning levels. In this paper, we investigate therefore the importance of individual linguistic complexity
features for predicting proﬁciency levels across different L2 datasets. The two main research questions
we investigate are: (i) Which linguistic complexity features are most useful for determining proﬁciency
levels for each L2 dataset? (ii) Are there features that are relevant regardless of the context size and the
type of skill considered? Our contributions include, on the one hand, a subset of the most informative
features for each dataset whose use leads to improved classiﬁcation results. On the other hand, we iden-
tify some lexical, morphological and syntactic features that are good indicators of complexity across all
three datasets, namely, reading comprehension texts, essays and sentences.

In Section 2, we provide an overview of previous work related to linguistic complexity analysis, fol-
lowed by the description of our datasets in Section 3. In Section 4, we present the set of features used
and highlight their relevance for modeling linguistic complexity in the L2 context. We then describe our
experiments and their results in Section 5, presenting the most informative features and their effect on
classiﬁcation performance. Finally, we conclude our results and outline future work in Section 6.

2 Previous literature on linguistic complexity for predicting L2 levels
Expert-written (receptive) texts
In the L2 context, speciﬁc scales reﬂecting progress in language pro-
ﬁciency have been proposed. One such scale is the CEFR, introduced in section 1. An alternative to the
CEFR is the 7-point scale of the Interagency Language Roundtable (ILR), common in the United States.
In Table 1, we provide an overview of studies targeting L2 receptive complexity and compare the target
language, the type and amount of training data and the methods used. The studies are ordered alpha-
betically based on the target language of the linguistic complexity analysis. We only include previous
work here that shares the following characteristics: (i) texts rather than single sentences are the unit of
analysis; (ii) receptive linguistic complexity is measured; and (iii) NLP tools are combined with machine
learning algorithms. Under dataset size, we report the number of texts used (except for Heilman et al.
(2007)), where whole books were employed), followed by the number of tokens in parenthesis when
available.

Study

Salesky and Shen (2014)

Sung et al. (2015)
Heilman et al. (2007)
Huang et al. (2011)
Xia et al. (2016)
Zhang et al. (2013)
Franc¸ois and Fairon (2012)
Branco et al. (2014)
Curto et al. (2015)
Karpov et al. (2014)
Reynolds (2016)
Pil´an et al. (2016)

Target
language
Arabic, Dari

English, Pashto

Chinese
English
English
English
English
French

Portuguese
Portuguese

Russian
Russian
Swedish

CEFR

No

Yes
No
No
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes

Dataset size
in # texts
4 × 1400

4 books (200,000)

1852 (510,543)
110 (12,673)
237 (25,888)

1578

187
331
15

219
4689
867

Text
type

# levels

Method

Non-L2

7

Non-L2

L2
L2
Both
L2

L2
L2
L2
Both
Both
L2

6
4
6

6

6

5 (A2-C2)

1-10

5 (A1-C1)
5 (A1-C1)

4 (A1-B1, C2)

5 (A1-C1)

Regr.

Classif.
Regr.
Regr.
Both
Regr.
Classif.
Regr.
Classif.
Classif.
Classif.
Both

Table 1: An overview of studies on L2 receptive complexity.

CEFR-based studies have been more commonly treated as a classiﬁcation problem, a popular choice
of classiﬁer being support vector machines (SVM). A particular aspect distinguishing Xia et al. (2016)
from the rest of the studies mentioned in Table 1 is the idea of using L1 data to improve the classiﬁcation
of L2 texts. For the sake of comparability, the information in Table 1 describes only the experiments
using the L2 data reported in this study. The state-of-the-art performance reported for the CEFR-based
classiﬁcation described in the studies included in Table 1 ranges between 75% and 80% accuracy (Curto
et al., 2015; Sung et al., 2015; Xia et al., 2016; Pil´an et al., 2016a).

A large number of features have been proposed and tested in this context. Count-based measures
(e.g. sentence and token length, type-token ratio) and syntactic features (e.g. dependency length) have

50been conﬁrmed to be inﬂuencing factors in L2 complexity (Curto et al., 2015; Reynolds, 2016). Lexical
information based on either n-gram models (Heilman et al., 2007) or frequency information from word
lists (Franc¸ois and Fairon, 2012; Reynolds, 2016; Salesky and Shen, 2014) and Google search results
(Huang et al., 2011) has proven to be, however, one of the most predictive dimensions. Heilman et al.
(2007) found that lexical features outperform grammatical ones, which, although more important for L2
than L1 complexity, still remain less predictive for L2 English complexity. Nevertheless, the authors
mention that this may depend on the morphological richness of a language. Reynolds (2016), in fact,
ﬁnds that morphological features are among the most inﬂuential ones for L2 Russian texts.
Learner-written (productive) texts Similarly to L2 texts targeting reading skills, also texts produced
by L2 learners manifest varying degrees of complexity at different stages of proﬁciency. Typically how-
ever, receptive linguistic complexity is somewhat higher than its productive counterpart for a learner at a
given CEFR level (Barrot, 2015). Previous studies aiming at classifying CEFR levels in learner-written
texts include Hancke and Meurers (2013) for L2 German and Vajjala and L˜oo (2014) for L2 Estonian.
The most predictive features for L2 German include lexical and morphological features. Morphological
features (e.g. amount of distinct cases used) are also among the most informative ones for L2 Estonian at
all L2 development stages. A fundamental difference between assessing receptive and productive texts is
that, while receptive texts are expected to be relatively error free, the latter ones typically contain a vary-
ing amount of L2 errors, which have also been used to inform features. Errors are usually counted based
on the output of a spell checker (Hancke and Meurers, 2013; Tack et al., 2017) or by using hand-crafted
rules (Tack et al., 2017).
Smaller linguistic units Besides the text-level analyses in Table 1, studies targeting smaller units also
appear in the literature. Linguistic complexity in single sentences from an L2 perspective has been
explored in Karpov et al. (2014) and in Pil´an et al. (2016a). Both studies are CEFR-related, but rather
than classifying sentences into individual CEFR levels, a binary distinction is made (below or at B1 level
vs. above B1). In Pil´an et al. (2016a), we report 63% accuracy for a 5-way CEFR level classiﬁcation
of Swedish coursebook sentences. As for productive complexity, research on the automatic assessment
of short answers to open-ended questions in terms of using CEFR has been investigated in Tack et al.
(2017) for L2 English. The authors proposed an ensemble method consisting of integrating the votes of
a number of traditional classiﬁcation methods into a single prediction. Sentence and word length, lexical
features and information about the age of acquisition of words were found especially predictive.

3 Datasets
3.1 Text-level datasets
We used two L2 Swedish corpora consisting of texts in our experiments: SweLL (Volodina et al., 2016b)
comprised of essays written by L2 learners and COCTAILL (Volodina et al., 2014) containing L2 course-
books authored or adapted by experts for L2 learners. The SweLL corpus consists of essays produced
by adult learners of L2 Swedish on a variety of topics (TEXT-E). From the coursebook corpus, we only
include whole texts meant for reading comprehension practice (TEXT-R) since the linguistic annotation
of other coursebook elements (e.g. gap-ﬁlling exercises) may be prone to automatic linguistic annotation
errors. These two corpora cover ﬁve CEFR levels (A1 to C1). Each SweLL essay has been assigned a
CEFR level by teachers. For reading texts, CEFR levels were derived from the level of the lesson (chap-
ter) they occur in. It is worth mentioning that these two corpora are independent from each other, i.e. the
essays written by the learners are not based on, or inspired by, the reading passages. The distribution
of texts per type and CEFR level in the datasets is shown in Table 2. The total number of tokens in the
coursebook-based dataset was 289,312, while in the learner essay data it was 43,033.

3.2 A teacher-evaluated dataset of sentences
At the sentence level, we use a small dataset1 (SENT) based on the user evaluation of a corpus example
selection system, HitEx, which we described in detail in Pil´an et al. (2016b). HitEx aims at identifying

1The dataset is available at https://github.com/IldikoPilan/sent_cefr.

51sentences from corpora suitable as exercise items. The sentences in this dataset have been automatically
assessed for their CEFR level and have been ﬁltered for their well-formedness, independence from the
rest of their textual context and some additional lexical and structural criteria (e.g. abbreviations, inter-
rogative form) using HitEx. Out of the original 330 sentences from the evaluation material, we only
included in this dataset the subset of sentences: (i) that were found overall suitable (with an evaluation
score >= 2.5 out of 4); and (ii) where a majority of teachers agreed with the CEFR level assigned au-
tomatically by HitEx. This subset was complemented with 90 sentences for the otherwise insufﬁciently
represented A1 level from the COCTAILL corpus. Only individually occurring sentences in lists and
non-gapped exercises were considered, thus these are not a subset of the text-level dataset described
above. The distribution of sentences per CEFR level in the dataset is presented in Table 2. The total
number of tokens in the dataset is 4,060.

Writer
Unit
Learner Texts
Texts
Expert
Expert
Sentences

A1 A2
83
16
157
49
98
82

B1
75
258
58

B2 C1 Total
336
74
867
288
375
92

88
115
45

Table 2: CEFR-level annotated Swedish datasets.

All three corpora are equipped also with automatic linguistic annotation which includes lemmatization,

part-of-speech (POS) tagging and dependency parsing based on the Sparv2 pipeline.

4 A ﬂexible feature set for linguistic complexity analysis

In this section, we provide a detailed description of the set of features used and relate them to cognitive
aspects of linguistic complexity. The feature set is “ﬂexible” in the sense that it can be applied to different
types of L2 data and units of analysis (e.g. texts or sentences) since it does not incorporate text-level
features (e.g. discourse-related aspects) or learner language speciﬁc ones (e.g. L2 error features). The
feature set is comprised of 61 features in total, which we have previously used for CEFR classiﬁcation
experiments also in Pil´an et al. (2016c). Table 3 shows the complete feature set divided into ﬁve sub-
categories based on the type of NLP tools and resources used: count-based, lexical, morphological,
syntactic and semantic.

4.1 Count-based features
The feature set includes seven indicators that are based on simple counts or traditional readability
measures. One such measure for Swedish is LIX (L¨asbarhetsindex ‘Readability index’) proposed in
Bj¨ornsson (1968). LIX combines the sum of the average number of words per sentence in the text and
the percentage of tokens longer than six characters. Sentence length is measured both as the number of
tokens and that of characters. Sentence length can indicate syntactic difﬁculty and it can be a sign of
e.g. multiple clauses or larger noun phrases. Average token (T) length is computed based on the number
of characters. Extra-long words, i.e. tokens longer than 13 characters, are also counted since compound-
ing, frequent in Swedish, can result in particularly long words (Heimann M¨uhlenbock, 2013). Type-token
ratio (TTR), the ratio of unique tokens to all tokens, is an indicator of lexical richness (Graesser et al.,
2004). A bi-logarithmic and a square root TTR are used which decrease the effect of text and sentence
length (Vajjala and Meurers, 2012).

4.2 Word-list based lexical features
Besides richness, the frequency of words also inﬂuences lexical complexity as repeated exposure facil-
itates their processing (Graesser et al., 2004). Frequency information is collected from the KELLY list
(Volodina and Kokkinakis, 2012), based on web texts.

2https://spraakbanken.gu.se/sparv/

52COUNT
Sentence length
Avg token length
Extra-long token
Nr characters
LIX
Bilog TTR
Square root TTR
LEXICAL

Avg KELLY log freq
A1 lemma INCSC
A2 lemma INCSC
B1 lemma INCSC
B2 lemma INCSC
C1 lemma INCSC
C2 lemma INCSC
Difﬁcult W INCSC
Difﬁcult N&V INCSC
OOV INCSC
No lemma INCSC
SEMANTIC

Avg senses per token
N senses per N

SYNTACTIC

MORPHOLOGICAL

Function W INCSC
Particle INCSC
3SG pronoun INCSC
Punctuation INCSC
Subjunction INCSC
PR to N
PR to PP
Relative structure INCSC
S-V INCSC
S-V to V

Avg. DepArc length
DepArc Len > 5
Max length DepArc
Right DepArc Ratio
Left DepArc Ratio
Modiﬁer variation
Pre-modiﬁer INCSC
Post-modiﬁer INCSC
Subordinate INCSC
Relative clause INCSC
PP complement INCSC ADJ INCSC
MORPHOLOGICAL
Neuter N INCSC
CJ + SJ INCSC
Past PC to V
Present PC to V
Past V to V
Supine V to V
Present V to V
Nominal ratio
N to V
Modal V to V

ADJ variation
ADV INCSC
ADV variation
N INCSC
N variation
V INCSC
V variation
Lex T to Nr T
Lex T to non-lex T

Table 3: Feature set for linguistic complexity assessment in L2 data.

Instead of n-grams, weakly lexicalized features are employed to increase the generalizability of the
models on unseen data. Each token is represented by its corresponding CEFR level. Unlike in Pil´an et
al. (2016c), where we employed KELLY, the per-token CEFR level information is retrieved here from
two word lists compiled based on the L2 corpora described in Section 3. To guarantee the independence
of the word lists from the datasets, we use SweLLex (Volodina et al., 2016a), a frequency list based on
the learner essays when classifying CEFR levels in coursebook texts and SVALex (Franc¸ois et al., 2016),
containing frequencies from coursebooks for making predictions on the essays. For sentences, SVALex
has been used since it is independent from the dataset, but both reﬂect receptive linguistic complexity.
Frequency distributions in these lists have been mapped to single CEFR levels based on the difference in
per-level normalized frequency between adjacent levels as described in Alfter et al. (2016).
× Nc) is used to
reduce the inﬂuence of sentence length, where Nt is the total number of tokens and Nc is the count of a
certain category of tokens in the text or sentence (Graesser et al., 2004). The INCSC of difﬁcult tokens
is also computed, that is, tokens above a certain reference CEFR level, which can be the level of an L2
learner writing a text or whom the text would be presented to as reading material. This value is also
computed separately for nouns and verbs, since these are crucial for conveying meaning. Moreover, the
INCSC of tokens not present in the L2 word lists, i.e. out-of-vocabulary words (OOV INCSC) is also
considered as well as the INCSC of non-lemmatized tokens (No lemma INCSC).

Instead of absolute counts, a normalized value, an incidence score (INCSC = 1000
Nt

4.3 Morphological features
Morphological features include not only INCSC of different morpho-syntactic categories, but also vari-
ational scores, i.e. the ratio of a category to the ratio of lexical tokens: nouns (N), verbs (V), adjectives
(ADJ) and adverbs (ADV). Some speciﬁc features for L2 Swedish are the ratio of different verb forms
to verbs which are typically introduced at varying stages of L2 learning. S-verbs (S-VB) are a group
of Swedish verbs ending in -s that are peculiar in terms of morphology and semantics. They indicate

53either reciprocity, a passive construction or are deponent verbs, i.e. verbs active in meaning, but passive
in form. Neuter gender nouns are also considered since they can indicate the abstractness of a concept
(Graesser et al., 2004). Among relative structures relative adverbs, determiners, pronouns and posses-
sives are counted. Nominal ratio (Hultman and Westman, 1977) corresponds to the ratio of nominal
categories, i.e. nouns, prepositions (PP) and participles to the ratio of verbal categories, namely pro-
nouns (PR), adverbs, and verbs. Its simpliﬁed version is the ratio of nouns to verbs, and it is meant to
measure the information load of a text or reveal its genre (e.g. spoken vs. news text). A higher value
corresponds to higher degrees of complexity and a more elaborate genre.

INCSC for punctuation marks as well as sub- and conjunctions (SJ, CJ) are also computed since their
presence in larger quantities can indicate a more complex syntactic structure. Particles can change the
meaning of verbs considerably, similarly to English phrasal verbs (Heimann M¨uhlenbock, 2013). The
INCSC of the third person singular (3SG) pronoun inspired by Zhang et al. (2013) is also included since
this is often used referentially, which can further increase the difﬁculty of processing.

4.4 Syntactic and semantic features
Syntactic aspects are related to readers’ working memory load when processing sentences which can
be increased by ambiguity or embedded constituents (Graesser et al., 2004). Here, the average length
(depth) of dependency arcs (DepArc) and their direction is considered. Relative clauses, pre- and post-
modiﬁers (e.g. adjectives and prepositional phrases), prepositional complements as well as subordinates,
commonly used in previous research on linguistic complexity (Heimann M¨uhlenbock, 2013; Schwarm
and Ostendorf, 2005), are also counted.

The two semantic features included quantify available word senses per lemma based on the SALDO
lexicon (Borin et al., 2013). Both the average number of senses per token and the average number of
noun senses per noun are considered. Polysemous words can be demanding for readers as they need to
be disambiguated for a full understanding of the sentence (Graesser et al., 2004).
5 Cross-dataset feature selection experiments
In this section, we describe the results of our feature selection experiments on the three datasets presented
in Section 3. These experiments differ from the ones we described previously in Pil´an et al. (2016a) and
Pil´an et al. (2016c) in a number of respects. In this work, the worth of individual features is evaluated
rather than that of the complete set of features or groups of features. Moreover, as mentioned in section
4, most lexical features are based on L2 word lists rather than KELLY.

5.1 Experimental setup
We use 85% of each dataset for identifying the most informative features (DEV). The reported classi-
ﬁcation results using this part of the data are based on a stratiﬁed 5-fold cross-validation setup, that is,
the original distribution of instances per CEFR level in the dataset has been preserved in all folds. We
evaluated the generalizability of the selected subset of features on the remaining 15% of the data (TEST).
As learning algorithm for these models, we used LinearSVC as implemented in scikit-learn (Pedregosa
et al., 2011), which has been successfully applied in recent years in a number of NLP areas.

5.2 Feature selection method
As a pre-processing step before training our classiﬁers, we used a univariate feature selection method,
also available in scikit-learn, to identify the most informative features scored with analysis of variance
(ANOVA). This feature selection method is suitable for multi-class problems, it is independent of the
learning method used and it has been previously adopted for NLP tasks, e.g. by Carbon et al. (2014).
ANOVA is a statistical test that can be used to measure how strong the relationship between each feature
and the output class is (CEFR levels in our case). It relies on F-tests, which can be employed to score
features based on signiﬁcant differences in their per-class mean values. To detect these differences
indicating dependencies, ﬁrst, the variance, i.e. the dispersion of the data in terms of its distance from
the mean, is measured both within and between classes for each feature. Then, the F-statistic can be
computed as the ratio of the variance between class means and the variance within a class.

545.3 Results
The results of the models with and without feature selection in terms of accuracy and F1 are presented
in Table 4.

Data

Features

SENT

Acc
0.62
0.73
0.81

F1
0.61
0.71
0.79

TEXT-R
Acc
0.68
0.70
0.73

F1
0.68
0.70
0.73

TEXT-E
Acc
0.73
0.81
0.84

F1
0.71
0.81
0.82

21

54

24

DEV ALL
DEV K-BEST
TEST K-BEST
Number of K-BEST

Table 4: Accuracy with feature selection across datasets.

Reducing the complete set of features to the subset of the most informative ones improved the classi-
ﬁcation results for all datasets. The most substantial boost (+0.11 accuracy) was obtained for sentences.
The models with selected features generalized well also on the held-out test sets. Moreover, while for
SENT and TEXT-E only about one third of the features have been selected, almost all features were in-
cluded in the k number of best ones for TEXT-R. The selected features ranked based on ANOVA are
presented in Table 5. For TEXT-R, features with low importance are not listed separately. These are only
indicated when they overlap with a feature selected by the other models (with a rank > 24).

Fourteen features were among the most informative ones across all three datasets, which are high-
lighted in bold in Table 5. One such feature was the count-based measure of square root TTR, thus it
seems that a varied way of expression, through e.g. the use of synonyms, is a good indicator of linguis-
tic complexity in the L2 context. Among the word-list based lexical features, besides the proportion of
difﬁcult lexica, the amount of tokens at the extremes of the CEFR scale, namely the lowest, A1 level and
the advanced, C1 level (the highest available in our L2 lists) were also useful predictors. Interestingly,
two out of the three strong indicators of L2 English essay quality identiﬁed in Crossley and McNamara
(2011) were lexical diversity, closely related to our Square root TTR feature, and lexical frequency, based
on the same type of information as our word-list features. Lexical variation in terms of TTR as well as
verb variation were also found highly predictive for L2 Estonian learner texts (Vajjala and L˜oo, 2014).
These ﬁndings indicate the predictive strength of these features across languages. Furthermore, syntactic
features relative to the length of dependency arcs and verb-related morphological features (e.g. INCSC
of participles and s-verbs) were among the k-best for all datasets. Such verb forms are, in fact, typically
introduced explicitly to L2 learners at higher CEFR levels (Fasth and Kannermark, 1997). The amount
of punctuation and particles was also indicative of complexity. The former can, for example, indicate
clause boundaries and hence more complex sentences. Particles, on the other hand, can be challenging
for language learners, since they alter the meaning of verbs.

For the two datasets related to receptive skills, SENT and TEXT-R, a number of count features were
strongly predictive. Unlike for TEXT-E, sentence length in terms of both the number of tokens and
the number of characters were highly informative for determining receptive complexity. Although the
proportion of lexical tokens to all tokens was not informative at the sentence level, it proved to be a
good indicator of linguistic complexity at the text level. The traditional readability measure, LIX was
informative only for TEXT-R, which could be explained by the fact that this dataset was the most similar
to the intended use of LIX, namely determining readability at the text level. On the other hand, the other
traditional formula, nominal ratio, was more useful across datasets, especially in its simpliﬁed version
(N to V). It would be useful to investigate further whether this also depends on a difference in text genre.
A limitation of our study is the relatively small size of our datasets, which is especially true in the
case of the A1 level learner essays. Considering the difﬁculties in having access to similar types of L2
data, and the extension of our experiments to cross-dataset observations, the results could still provide
valuable insights for teaching experts and members of the NLP community targeting similar tasks.

55Feature name

Rank
TEXT-R

SENT

TEXT-E

Nr characters
Square root TTR
A1 lemma INCSC
Punctuation INCSC
Sentence length
Relative clause
Difﬁcult N&V INCSC
Avg. DepArc length
Max length DepArc
Bilog TTR
DepArc Len > 5
S-V INCSC
Present PC to V
Past PC to V
Particle INCSC
V variation
Difﬁcult W INCSC
V INCSC
C1 lemma INCSC
3SG pronoun INCSC
N to V
OOV INCSC
LIX
Extra-long token
Lex T to Nr T
PR to PP
Past V to V
B1 lemma INCSC
Function W INCSC
Right DepArc Ratio
Avg token length
B2 lemma INCSC
N senses per N
PR to N
Nominal ratio
N INCSC

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

4
7
3
11
5

> 24

1
10
6
24
8

> 24
18
> 24
> 24
15
2
22
> 24
> 24
> 24

9
12
13
14
16
17
19
20
21
23
> 24
> 24
> 24
> 24
> 24

-
9
2
12
-
8
1
14
13
-
-
-
17
18
16
10
4
-
5
-
20
-
-
6
15
-
19
3
-
-
7
11
21
22
23
24

Table 5: K-best features and their rank across different datasets.

6 Conclusion and future work

In this work, we described the results of a feature selection method applied to different language learning
related datasets. We found a small number of features that proved useful across all datasets regardless
of the length of the linguistic input or the type of relevant language learning skill. We showed that
besides lexical frequency and variation, the length of dependencies and the amount and type of verbs
carry valuable information for predicting proﬁciency levels. To our knowledge, the usefulness of single
features across receptive and productive L2 data of different sizes has not been previously explored. We
aimed at ﬁnding the optimal number and types of features to use in order to boost performance for these
types of predictions. An improved CEFR level classiﬁcation is especially important for its integration
into NLP applications aiming at on-the-ﬂy assessment of texts or exercise generation.
In the future,
extending this investigation of feature importances to datasets in other languages could contribute to
a deeper understanding about which indicators are more universally useful. Furthermore, the selected
subset of features could be evaluated also with the help of teaching experts to conﬁrm their usefulness.

Acknowledgements

We would like to thank the Royal Swedish Academy of Letters, History and Antiquities for providing a
travel grant to the ﬁrst author via the Wallenberg Foundation.

56References
David Alfter, Yuri Bizzoni, Anders Agebj¨orn, Elena Volodina, and Ildik´o Pil´an. 2016. From distributions to
labels: A lexical proﬁciency analysis using learner corpora. Number 130 in Proceedings of the joint workshop
on NLP for Computer Assisted Language Learning and NLP for Language Acquisition, pages 1–7. Link¨oping
University Electronic Press.

Jessie Saraza Barrot. 2015. Comparing the linguistic complexity in receptive and productive modes. GEMA

Online R(cid:13) Journal of Language Studies, 15(2).
Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.

Lars Borin, Markus Forsberg, and Lennart L¨onngren. 2013. SALDO: a touch of yin to WordNet’s yang. Language

Resources and Evaluation, 47(4):1191–1211.

Ant´onio Branco, Jo˜ao Rodrigues, Francisco Costa, Jo˜ao Silva, and Rui Vaz. 2014. Rolling out text categoriza-
tion for language learning assessment supported by language technology. In Proceedings of the International
Conference on Computational Processing of the Portuguese Language, pages 256–261. Springer.

Kyle Carbon, Kacyn Fujii, and Prasanth Veerina. 2014. Applications of machine learning to predict Yelp ratings.

Stanford Univ., Stanford, CA.

Council of Europe. 2001. Common European Framework of Reference for Languages: Learning, Teaching,

Assessment. Press Syndicate of the University of Cambridge.

Scott A Crossley and Danielle S McNamara. 2011. Understanding expert ratings of essay quality: Coh-Metrix
analyses of ﬁrst and second language writing. International Journal of Continuing Engineering Education and
Life Long Learning, 21(2-3):170–191.

Pedro Curto, Nuno J Mamede, and Jorge Baptista. 2015. Automatic text difﬁculty classiﬁer – Assisting the
selection of adequate reading materials for European Portuguese teaching. In Proceedings of the International
Conference on Computer Supported Education, pages 36–44.

Cecilia Fasth and Anita Kannermark. 1997. Form i focus: ¨ovningsbok i svensk grammatik. Del B. Folkuniv.

F¨orlag, Lund.

Thomas Franc¸ois and C´edrick Fairon. 2012. An “AI readability” formula for French as a foreign language. In
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 466–477.

Thomas Franc¸ois, Elena Volodina, Ildik´o Pil´an, and Ana¨ıs Tack. 2016. SVALex: a CEFR-graded lexical resource
In Proceedings of the 10th International Conference on

for Swedish foreign and second language learners.
Language Resources and Evaluation.

Arthur C Graesser, Danielle S McNamara, Max M Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of

text on cohesion and language. Behavior Research Methods, 36(2):193–202.

Julia Hancke and Detmar Meurers. 2013. Exploring CEFR classiﬁcation for German based on rich linguistic

modeling. In Learner Corpus Research Conference, pages 54–56.

Michal J. Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2007. Combining lexical and
grammatical features to improve readability measures for ﬁrst and second language texts. In Proceedings of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 460–467.

Katarina Heimann M¨uhlenbock. 2013.

Data linguistica, (24).

I see what you mean—assessing readability for speciﬁc target groups.

Yi-Ting Huang, Hsiao-Pei Chang, Yeali Sun, and Meng Chang Chen. 2011. A robust estimation scheme of
reading difﬁculty for second language learners. In 11th IEEE International Conference on Advanced Learning
Technologies (ICALT), pages 58–62. IEEE.

Tor G Hultman and Margareta Westman. 1977. Gymnasistsvenska. Liber.

Nikolay Karpov, Julia Baranova, and Fedor Vitugin. 2014. Single-sentence readability prediction in Russian. In

International Conference on Analysis of Images, Social Networks and Texts, pages 91–100. Springer.

57Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12(Oct):2825–2830.

Ildik´o Pil´an, Sowmya Vajjala, and Elena Volodina. 2016a. A readable read: automatic assessment of language
International Journal of Computational Linguistics and

learning materials based on linguistic complexity.
Applications (IJCLA), 7(1):143–159.

Ildik´o Pil´an, Elena Volodina, and Lars Borin. 2016b. Candidate sentence selection for language learning exer-
cises: from a comprehensive framework to an empirical evaluation. Traitement Automatique des Langues (TAL)
Journal, Special issue on NLP for Learning and Teaching, 57(3):67–91.

Ildik´o Pil´an, Elena Volodina, and Torsten Zesch. 2016c. Predicting proﬁciency levels in learner writings by trans-
ferring a linguistic complexity model from expert-written coursebooks. In Proceedings of the 26th International
Conference on Computational Linguistics, pages 2101–2111.

Robert Reynolds. 2016. Insights from Russian second language readability classiﬁcation: complexity-dependent
training requirements, and feature evaluation of multiple categories. In Proceedings of the 11th Workshop on
Innovative Use of NLP for Building Educational Applications, pages 289–300.

Elizabeth Salesky and Wade Shen. 2014. Exploiting morphological, grammatical, and semantic correlates for
improved text difﬁculty assessment. In Proceedings of the 9th Workshop on Innovative Use of NLP for Building
Educational Applications, pages 155–162, June.

Sarah E Schwarm and Mari Ostendorf. 2005. Reading level assessment using support vector machines and
In Proceedings of the 43rd Annual Meeting on Association for Computational

statistical language models.
Linguistics, pages 523–530.

Yao-Ting Sung, Wei-Chun Lin, Scott Benjamin Dyson, Kuo-En Chang, and Yu-Chia Chen. 2015. Leveling
L2 texts through readability: combining multilevel linguistic features with the CEFR. The Modern Language
Journal, 99(2):371–391.

Ana¨ıs Tack, Thomas Franc¸ois, Sophie Roekhaut, and C´edrick Fairon. 2017. Human and automated CEFR-
based grading of short answers. In Proceedings of the 12th Workshop on Innovative Use of NLP for Building
Educational Applications, pages 169–179.

Sowmya Vajjala and Kaidi L˜oo. 2014. Automatic CEFR level prediction for Estonian learner text. NEALT

Proceedings Series Vol. 22, pages 113–127.

Sowmya Vajjala and Detmar Meurers. 2012. On improving the accuracy of readability classiﬁcation using insights
from second language acquisition. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building
Educational Applications, pages 163–173.

Elena Volodina and Soﬁe Johansson Kokkinakis. 2012.

Introducing the Swedish Kelly-list, a new lexical e-
resource for Swedish. In Proceedings of the International Conference on Language Resources and Evaluation,
pages 1040–1046.

Elena Volodina, Ildik´o Pil´an, Stian Rødven Eide, and Hannes Heidarsson. 2014. You get what you annotate: a
pedagogically annotated corpus of coursebooks for Swedish as a second language. In Proceedings of the 3rd
workshop on NLP for Computer Assisted Language Learning, pages 128–144.

Elena Volodina, Ildik´o Pil´an, Lorena Llozhi, Baptiste Degryse, and Thomas Franc¸ois. 2016a. SweLLex: second
In Proceedings of the joint workshop on NLP for Computer As-
language learners’ productive vocabulary.
sisted Language Learning and NLP for Language Acquisition, number 130, pages 76–84. Link¨oping University
Electronic Press.

Elena Volodina, Ildik´o Pil´an, Ingegerd Enstr¨om, Lorena Llozhi, Peter Lundkvist, Gunl¨og Sundberg, and Monica
Sandell. 2016b. SweLL on the rise: Swedish learner language corpus for European Reference Level studies. In
Proceedings of the Tenth International Conference on Language Resources and Evaluation.

Ian H Witten, Eibe Frank, Mark A Hall, and Christopher J Pal. 2011. Data Mining: Practical machine learning

tools and techniques. Morgan Kaufmann.

Menglin Xia, Ekaterina Kochmar, and Ted Briscoe. 2016. Text readability assessment for second language learn-
ers. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications,
pages 12–22.

Lixiao Zhang, Zaiying Liu, and Jun Ni. 2013. Feature-based assessment of text readability. In 7th International

Conference on Internet Computing for Engineering and Science (ICICSE), pages 51–54. IEEE.

58
Concept Transfer Learning for Adaptive Language Understanding

Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering

SpeechLab, Department of Computer Science and Engineering

Su Zhu and Kai Yu ∗

Brain Science and Technology Research Center
Shanghai Jiao Tong University, Shanghai, China

{paul2204,kai.yu}@sjtu.edu.cn

Abstract

in lan-
Concept deﬁnition is important
guage understanding (LU)
adaptation
since literal deﬁnition difference can eas-
ily lead to data sparsity even if different
data sets are actually semantically corre-
lated. To address this issue, in this paper,
a novel concept transfer learning approach
is proposed. Here, substructures within lit-
eral concept deﬁnition are investigated to
reveal the relationship between concepts.
A hierarchical semantic representation for
concepts is proposed, where a semantic
slot is represented as a composition of
atomic concepts. Based on this new hi-
erarchical representation, transfer learning
approaches are developed for adaptive LU.
The approaches are applied to two tasks:
value set mismatch and domain adapta-
tion, and evaluated on two LU bench-
marks: ATIS and DSTC 2&3. Thorough
empirical studies validate both the efﬁ-
ciency and effectiveness of the proposed
method. In particular, we achieve state-of-
the-art performance (F1-score 96.08%) on
ATIS by only using lexicon features.

1

Introduction

The language understanding (LU) module is a
key component of dialogue system (DS), pars-
ing user’s utterances into corresponding seman-
tic concepts (or semantic slots 1). For example,
the utterance “Show me ﬂights from Boston to
New York” can be parsed into (from city=Boston,
to city=New York) (Pieraccini et al., 1992). Typ-
ically, the LU is seen as a plain slot ﬁlling task.

∗The corresponding author is Kai Yu.
1Slot and concept are equal in LU. They will be mixed in

the rest of this paper to some extent.

Figure 1: An example of hierarchical structure
to represent semantic slot with atomic concepts.
There are three levels in this structure. The plain
slot SLOT (transfer airport of the inbound ﬂight)
can be represented as a tuple of atomic concepts
sequentially.

With sufﬁcient in-domain data and deep learning
models (e.g.
recurrent neural networks, bidirec-
tional long-short term memory network), statis-
tical methods have achieved satisfactory perfor-
mance in the slot ﬁlling task recently (Kurata et al.,
2016; Vu, 2016; Liu and Lane, 2016).

However, retrieving sufﬁcient in-domain data
for training LU model (Tur et al., 2010) is unre-
alistic, especially when the semantic slot extends
or dialogue domain changes. The ability of LU ap-
proaches to cope with changed domains and lim-
ited data is a key to the deployment of commercial
dialogue systems (e.g. Apple Siri, Amazon Alexa,
Google Home, Microsoft Cortana etc).

In this paper, we investigate substructure of
semantic slots to ﬁnd out slot relations and pro-
mote data reuse. We represent semantic slots with
a hierarchical structure based on atomic concept
tuple, as shown in Figure 1.
Each semantic
slot is composed of different atomic concepts,
slot “from city” can be deﬁned as a tu-
e.g.
[“from location”, “city name”],
ple of atoms

ProceedingsoftheSIGDIAL2018Conference,pages391–399,Melbourne,Australia,12-14July2018.c(cid:13)2018AssociationforComputationalLinguistics391Figure 2: An example of mismatched LU datasets
labelled with [value:
FC refers to
“from city”. TC refers to “to city”.

slot].

“date of birth”

and
[“date”, “birth”].

can

be

deﬁned

as

Unlike the traditional slot deﬁnition on a plain
level, modeling on the atomic concepts helps iden-
tify linguistic patterns of related slots by atom
sharing, and even decrease the required amount
of training data. For example, the training and
test sets are unmatched in Figure 2, whereas the
patterns of atomic concepts (e.g. “from”, “to”,
“city”) can be shared.

In this paper, we investigate the slot ﬁlling task
switching from plain slots to hierarchical struc-
tures by proposing the novel atomic concept tuples
which are constructed manually. For comparison,
we also introduce a competitive method which
automatically learns slot representation from the
word sequence of each slot name. Our meth-
ods are applied to value set mismatch and domain
adaptation problems on ATIS (Hemphill et al.,
1995) and DSTC 2&3 (Henderson et al., 2013) re-
spectively. As shown in the experimental results,
the slot-ﬁlling based on concept transfer learning
is effective in solving the value set mismatch and
domain adaptation problems. The concept transfer
learning method especially achieves state-of-the-
art performance (F1-score 96.08%) on the ATIS
task.

The rest of the paper is organized as follows.
The next section is about the relation to prior work.
The atomic concept tuple is introduced in section
3. The proposed concept transfer learning is then
described in section 4. Section 5 describes a com-
petitive method with slot embedding derived from
the literal descriptions of slot names. In section
6, the proposed approach is evaluated on the value
set mismatch and domain adaptation problems. Fi-
nally, our conclusions are presented in section 7.

2 Related Work

Slot Filling in LU Zettlemoyer and Collins (2007)
proposed a grammar induction method by learn-
ing a Probabilistic Combinatory Categorial Gram-
mar (PCCG) from logical-form annotations. As a

grammar-based method, PCCG is close to a hier-
archical concepts structure in grammar generation
and combination. But this grammar-based method
does not possess high generalization capability for
atomic concept sharing, and heavily depends on a
well-deﬁned lexicon set.

Recent research on statistical slot ﬁlling in LU
has been focused on the Recurrent Neural Net-
work (RNN) and its extensions. At ﬁrst, RNN out-
performed CRF (Conditional Random Field) on
the ATIS dataset (Yao et al., 2013; Mesnil et al.,
2013). Long-short term memory network (LSTM)
was introduced to obtain a marginal improvement
over RNN (Yao et al., 2014). After that, many
RNN variations were proposed: encoder-labeler
model (Kurata et al., 2016), attention model (Liu
and Lane, 2016; Zhu and Yu, 2017) etc. However,
these work only predicted the plain semantic slot,
not the structure of atomic concepts.

Domain Adaptation in LU For the domain
adaptation in LU, Zhu et al. (2014) proposed
generating spoken language surface forms by us-
ing patterns of the source domain and the ontol-
ogy of the target domain. With regard to the
unsupervised LU, Heck and Hakkani-Tur (2012)
exploited the structure of semantic knowledge
graphs from the web to create natural language
surface forms of entity-relation-entity portions of
knowledge graphs. For the zero-shot learning of
LU, Ferreira et al. (2015); Yazdani and Hender-
son (2015) proposed a model to calculate similar-
ity scores between an input sentence and semantic
items. In this paper, we focus on the extension of
slots with limited seed data.

3 Atomic Concept Tuples

Although concept deﬁnition is one of the most cru-
cial problems of LU, there is no uniﬁed surface
form for the domain ontology. Even for the same
semantic slot, names of this slot may be quite dif-
ferent. For example, the city where the ﬂight de-
parts may be called “from city”, “depart city” or
“from loc.city name”. Ontology deﬁnitions from
different groups may be similar but not consistent,
which is not convenient for data reuse. Mean-
while, semantic slots deﬁned in traditional LU sys-
tems are on a plain level, while there is no structure
to indicate their relation.

To solve this problem, we propose to use atomic
concepts to represent the semantic slots. Atomic
concepts are exploited to break down the slots. We

392represent the semantic slots as atomic concept tu-
ples (Figure 1 is an example). The semantic slot
composed of these atomic concepts can keep a uni-
ﬁed resource for concept deﬁnition and extend the
semantic knowledge ﬂexibly.

We propose a criteria to construct atomic con-
cept manually. For a given vocabulary C of the
atomic concepts, a semantic slot s can be repre-
sented by a tuple [c1, c2, ..., ck], where ci ∈ C is in
the i-th dimension and k is tuple length. In partic-
ular, a “null” atom is introduced for each dimen-
sion. Table 1 illustrates an example of slot rep-
resentation on the ATIS task. To avoid a scratch
concept branch, we make a constraint:

Ci ∩ Cj = {null}, 1 ≤ i (cid:54)= j ≤ k

where Ci (1 ≤ i ≤ k) denotes all possible atomic
concepts which exist in dimension i (i.e. ci ∈ Ci).
The concept tuple is ordered.

In general, atomic concepts can be classiﬁed
into two categories, one is value-aware and the
other is context-aware. The principle for deﬁn-
ing slot as a concept branch is: lower dimension
less context-aware. For example, “city name” and
“airport name” depend on rare context (value-
aware). They should be located in the ﬁrst dimen-
sion. “from location” depends on the context like
a pattern of “a ﬂight leaves [city name]”, which
should be in the second dimension. The atomic
concept tuple shows the inner relation between
different semantic slots explicitly.

slot
city
from city
depart city
arrive airport

atomic concept tuple
[city name, null]
[city name, from location]
[city name, from location]
[airport name, to location]

Table 1: An example of slot representation by
atomic concepts.

Therefore, the procedure of constructing atomic
concept tuples for slots can be divided into the fol-
lowing steps.

• Firstly, we build a vocabulary C of the atomic
concepts for all the slots. By analyzing the
conceptual intersection of different slots, we
can split the slots into smaller ones which
are called atomic concepts. After that, each
slot is represented as a set of atomic concepts
which are not ordered.

• Secondly, we gather the atoms into differ-
ent groups. Atomic concepts from the same
group should be mutually exclusive. There-
fore we can investigate the inner relation and
outer relation of these groups.

• Finally, each group is associated with one
dimension (Ci) of the atomic concept tu-
ple. The groups are ordered depending on
whether they are value-aware or context-
aware.

4 Concept Transfer Learning
The slot ﬁlling is typically considered as a se-
quence labelling problem. In this paper, we only
consider the sequence-labelling based slot ﬁlling
task. The input (word) sequence is denoted by
w = (w1, w2, ..., wN ), and the output (slot tag) se-
quence is denoted by s = (s1, s2, ..., sN ). Since a
slot may be mapped to several continuous words,
we follow the popular in/out/begin (IOB) repre-
sentation (e.g. an example in Figure 3).

Figure 3: An example of annotation for slot ﬁlling.

The typical slot ﬁlling task predicts a plain slot
sequence given a word sequence, dubbed as plain
slot-ﬁlling (PS).

←−−
hi+1, ewi);

hi , i ∈ {1, .., N}:
−→
hi = f (

In this paper, the popular bidirectional LSTM-
RNN (BLSTM) is used to model the sequence la-
beling problem (Graves, 2012).
It can be ex-
ploited to capture both past and future features
for a speciﬁc time frame. The BLSTM reads the
input sentence w and generates N hidden states
hi =

←−
hi ⊕ −→
←−
hi = b(
←−
hi is the hidden vector of the backward pass
where
−→
in BLSTM and
hi is the hidden vector of the for-
ward pass in BLSTM at time i, b and f are LSTM
units of the backward and forward passes respec-
tively, ew denotes the word embedding for each
word w, and ⊕ denotes the vector concatenation
operation. We write the entire operation as a map-
ping BLSTMΘw (Θw refers to the parameters):

−−→
hi−1, ewi)

(h1...hN ) = BLSTMΘw (w1...wN )

(1)

Therefore, the plain slot ﬁlling deﬁnes a distri-
bution over slot tag sequences given an input word

393Figure 4: The proposed method about the atomic-concepts based slot ﬁlling. A slot is considered as a
tuple of atomic concepts, e.g. “from city” is represented as [“city name”, “from loc”]. Multiple output
layers are utilized to predict different atoms (including IOB schema). We involve two architectures: a) the
AC assumes that the output layers are independent, b) while the ACD makes a dependence assumption.

sequence:

p(s|w) =

=

N(cid:89)
N(cid:89)

i=1

i=1

p(si|hi)

softmax(Wo · hi)T δsi

(2)

where the matrix Wo (output layer) consists of the
vector representations of each slot tag, the symbol
δd is a Kronecker delta with a dimension for each
slot tag, and the softmax function is used to esti-
mate the probability distribution over all possible
plain slots.

4.1 Atomic-Concepts Based Slot Filling
The slot is indicated as an atomic concept tuple
based on hierarchical concept structure. Slot ﬁll-
ing is considered as a concept-tuple labelling task.

(a) Atomic concept independent
Slot ﬁlling can be transferred to a multi-task se-
quence labelling problem, regarding these atomic
concepts independently (i.e. AC). Each task pre-
dicts one atomic concept by a respective output
layer. Thus, the slot ﬁlling problem can be for-
mulated as

N(cid:89)

k(cid:89)

p(s|w) =

[p(IOBi|hi)

p(cij|hi)]

i=1

j=1

where the semantic slot si is represented by an
atomic concept branch [ci1, ci2, ..., cik], and IOBi
is the IOB schema tag at time i. As illustrated
in Figure 4(a), the semantic slot “from city” can
be represented as [“city name”, “from loc”]. The

prediction of IOB is regarded as another task
speciﬁcally. All tasks share the same parameters
except for the output layers.

(b) Atomic concept dependent
Atomic concepts can also be regarded depen-
dently (i.e. ACD) so that atomic concept predic-
tion depends on the former predicted results. The
slot ﬁlling problem can be formulated as

p(s|w)

N(cid:89)

=

i=1

[p(IOBi|hi)p(ci1|hi)

k(cid:89)

j=2

p(cij|hi, ci,1:j−1)]

where ci,1:j−1 = (ci,1, ..., ci,j−1) is the predicted
result of former atomic concepts of slot tag si,
indicating a structured multi-task learning frame-
work.

In this paper, we make some simpliﬁcations on
concept dependence. We predict atomic concept
only based on the last atomic concept, as shown in
Figure 4(b).

4.2 Training and Decoding
Since our approach is a structured multi-task
learning problem, the model loss is summed over
each task during training. For the domain adapta-
tion, we ﬁrstly gather training data from the source
domain and seed data from the target domain to
be a union set. Subsequently, the union data is fed
into the slot ﬁlling model.

During the decoding stage, we combine pre-
dicted atomic concepts with probability multipli-
cation. The evaluation is made on the top-best hy-
pothesis. Although the atomic-concepts based slot

394ﬁlling may predict an unseen slot. We didn’t per-
form any post-processing but considered the un-
seen slot as a wrong prediction.
5 Literal Description of Slot Name
In the section, we introduce a competitive system
which uses the literal description of the slot as an
input of the slot ﬁlling model. The literal descrip-
tion of slot used in this paper is the word sequence
of each slot name, which can be obtained automat-
ically. As the names of relative slots may include
the same or similar word, the word sequence of
slot name can also help reveal the relation between
different slots. Therefore, it is very meaningful to
compare this method with the atomic concept tu-
ples involving human knowledge.

Figure 5: The proposed framework of slot ﬁlling
based on the literal description of the slot. The
literal description of a slot is the word sequence
of slot name which can be obtained automatically,
e.g. “from city” is represented as a word sequence
of “from city”. Another BLSTM in the orange dot-
ted circle is exploited to derive softmax embed-
dings from the slot names.

The architecture of this competitive system is il-
lustrated in Figure 5. First, it assumes that each
slot name is a meaningful natural language de-
scription so that the slot ﬁlling task is tractable
from the input word sequence and slot name. Sec-
ond, another BLSTM model is applied to derive
softmax embedding from the slot names. In this
method, we also split the slot ﬁlling task into IOB
tag prediction and slot name prediction. In other
words, the slot tag si is broken down into IOBi
and slot name SNi, e.g. the slot tag “B-from city”
is split into “B” and “from city”. The details are
indicated below.
With the BLSTM applied on the input sequence,
we have hidden vectors hi, i ∈ {1, .., N} as shown
in Eqn. (1). This model redeﬁnes the distribution

N(cid:89)

over slot tag sequences given an input word se-
quence, compared with Eqn. (2):

p(s|w) =

p(IOBi|hi)p(SNi|hi)

i=1

where p(IOBi|hi) predicts the IOB tag and
p(SNi|hi) makes a prediction for the slot name.
We deﬁne

p(SNi|hi) = softmax(W · hi)T δSNi

where W ∈ RA×B is a matrix, hi ∈ RB is a vec-
tor, A is the number of all different slot names.
The matrix W consists of the embedding of each
slot name (i.e. each row vector of W with length
B).

To capture the slot relation within different
slot names, we apply another BLSTM model (as
shown in the orange dotted circle of Figure 5) onto
the word sequence (literal description) of each slot
name. For the j-th slot name (j ∈ {1, .., A}) with
a word sequence xj = (xj
←−
vj
n = lstmb(

1, ..., xj
Nj
−→
vj
n = lstmf (

), we have
−−→
vj
n−1, exj

←−−
vj
n+1, exj

←−
vj
n is the hidden vector of the backward pass
where
−→
vj
and
n is the hidden vector of the forward pass at
time n (n ∈ {1, .., Nj}), ex denotes the word em-
bedding for each word x. We take the tails of both
backward and forward pass as the slot embedding,
i.e.

);

)

n

n

←−
1 ⊕
vj

−→
vj
Nj

Wj =

where Wj is the j-th row vector of matrix W .

The relative slots using the same or similar word
in slot naming will be close in the space of slot
embedding inherently. Therefore, this method is
a competitive system to the atomic concept tuples.
We will show the comparison in the following sec-
tion.

6 Experiments

We evaluate our atomic-concept methods on two
tasks: value set mismatch and domain adaptation.
Value set mismatch task evaluates the general-
ization capability of different slot ﬁlling models.
In a language understanding (LU) system, each
slot has a value set with all possible values which
can be assigned to it. Since the semantically anno-
tated data is always limited, only a part of values

395is seen in the training data. Will the slot ﬁlling
model perform well on the unseen values? To an-
swer this question, we synthesize a test set by the
values mismatched with the training set of ATIS
corpus. Our methods may take advantages of the
prior knowledge about slot relations based on the
atomic concepts and the literal descriptions of slot
names.

Domain adaptation task evaluates the adapta-
tion capability of our methods when they meet
new slots in the target domain. In this task, a seed
training set of the target domain is provided. How-
ever, it is very limited: 1) some new slots may
not be covered; 2) not all contexts are covered for
each new slot. The atomic-concepts based method
would alleviate this problem. Each slot is de-
ﬁned as a tuple of atomic concepts in our method.
Therefore, it is possible to learn an unseen slot of
the target domain if its atomic concepts exist in the
data of the source domain and the seed data of the
target domain. It is also possible to see more con-
texts for a new slot if its atomic concepts exist in
the source domain which has much more data.

6.1 Value Set Mismatch
ATIS corpus has been widely used as a benchmark
by the LU community. The training data consists
of 4978 sentences and the test data consists of 893
sentences.

In this task, we perform an adaptation for un-
matched training and test sets, in which there are
many unseen slot-value pairs in the test set (Figure
2 is an example). It is a common problem in the
development of commercial dialogue system since
it is impossible to collect data covering all possible
slot-value pairs. We simulate this problem on the
ATIS dataset (Hemphill et al., 1995) by creating
an unmatched test set (ATIS X test).

ATIS X test is synthesized from the standard
ATIS test set by randomly replacing the value of
each slot with an unseen one. The unseen value
sets are collected from the training set according
to bottom-level concepts (e.g. “city name”, “air-
port name”). For example, if the value set of
“from city” is {“New York”, “Boston”} and the
value set of “to city” is {“Boston”}, then the un-
seen value for “to city” is “New York”. The test
sentence “Flights to [xx:to city]” can be replaced
to “Flights to [New York:to city]”. Finally, the
ATIS X test gets the same sentence number to the
standard ATIS test set.

6.1.1 Experimental Settings
We randomly selected 80% of the training data for
model training and the remaining 20% for valida-
tion. We deal with unseen words in the test set
by marking any words with only one single occur-
rence in the training set as (cid:104)unk(cid:105). We also con-
verted sequences of numbers to the string DIGIT,
e.g. 1990 is converted to DIGIT*4 (Zhang and
Wang, 2016). Regarding BLSTM model, we set
the dimension of word embeddings to 100 and the
number of hidden units to 100. For training, the
network parameters are randomly initialized in ac-
cordance with the uniform distribution (-0.2, 0.2).
Stochastic gradient descent (SGD) is used for up-
dating parameters. The dropout with a probability
of 0.5 is applied to the non-recurrent connections
during the training stage.

We try different learning rates by grid-search in
range of [0.008, 0.04]. We keep the learning rate
for 100 epochs and save the parameters that give
the best performance on the validation set. Finally,
we report the F1-score of the semantic slots on the
test set with parameters that have achieved the best
F1-score on the validation set. The F1-score is cal-
culated using CoNLL evaluation script. 2

6.1.2 Experimental Results and Analysis
Table 2 summarizes the recently published results
on the ATIS slot ﬁlling task and compares them
with the results of our proposed methods on the
standard ATIS test set. We can see that RNN
outperforms CRF because of the ability to cap-
ture long-term dependencies. LSTM beats RNN
by solving the problem of vanishing or explod-
ing gradients. BLSTM further improves the re-
sult by considering both the past and future fea-
tures. Encoder-decoder achieves the state-of-the-
art performance by modeling the label dependen-
cies. Encoder-labeler is a similar method to the
Encoder-decoder. These systems are designed to
predict the plain semantic slots traditionally.

Compared with the published results, our
method outperforms the previously published F1-
score, illustrated in Table 2. AC gets a marginal
improvement (+0.15%) over PS by predicting the
atomic concepts independently instead of the plain
slots. Moreover, ACD predicts the atomic con-
cepts dependently, gains 0.50% (signiﬁcant level
95%) over the AC. Worth to mention that ACD
achieves a new state-of-the-art performance of the

2http://www.cnts.ua.ac.be/conll2000/chunking/output.html

396Model
CRF (Mesnil et al., 2013)
RNN (Mesnil et al., 2013)
LSTM (Yao et al., 2014)
BLSTM (Zhang
and
Wang, 2016)
Encoder-decoder (Liu and
Lane, 2016)
Encoder-labeler
et al., 2016)
Encoder-decoder-pointer
(Zhai et al., 2017)
Encoder-decoder∗
BLSTM∗ (PS)
PS + dict-feats
AC
ACD
Slot name embedding

(Kurata

ATIS
92.94
94.11
94.85
95.14

95.72

95.66

95.86

95.79
95.43
95.57
95.58
96.08
95.52

ATIS X test

–
–
–
–

–

–

–

79.84
79.59
80.74
80.90
86.16
81.49

Table 2: Comparison with the published results
on the standard ATIS task, and evaluation on
ATIS X test. (∗ denotes our implementation.)

standard slot-tagging task on the ATIS dataset,
with only the lexicon features 3.

Our methods are also tested on the ATIS X test
to measure the ability of generalization.
For
comparison, we also apply dictionary features (n-
gram indication) of value sets (e.g. some kind of
gazetteers) collected from training data into the PS
model (i.e. PS+dict-feats in Table 2). From Ta-
ble 2, we can see that: 1) The plain slot ﬁlling
models (PS, Encoder-decoder) are not on par with
other models. 2) The atomic-concepts based slot
ﬁlling gets a slight improvement over the PS with
dict-feats, considering the concepts independently
(AC). 3) The atomic-concepts based slot ﬁllings
(ACD gains a large margin over AC, considering
the concepts dependently. 4) The method based
on slot name embedding (described in Section 5)
achieves a slight improvement than AC, which im-
plies that it is possible to reveal the relationship
between slots automatically.

3There are other published results that achieved better per-
formance by using Name Entity features, e.g. Mesnil et al.
(2013) got 96.24% F1-score. The NE features are manually
annotated and strong information. So it would be more mean-
ingful to use only lexicon features. Meanwhile, several other
works can obtain competitive results by using the intent clas-
siﬁcation as another task for joint training, e.g. Liu and Lane
(2016) achieved 95.98% F1-score. In this paper, we consider
the slot ﬁlling task only.

Case study: As illustrated in Table 3,

the
plain slot ﬁlling (PS) predicts the label of “late”
wrongly, whereas the atomic-concepts based slot
ﬁllings (i.e. AC and ACD) get the accurate an-
notation. The word of “late” is never covered by
the slot “period of day” in the training set. It is
hard for the plain slot ﬁlling (PS) to predict an un-
seen mapping correctly. Luckily, the “late” is cov-
ered by the family of the slot “period of day” in
the training set, e.g. “arrive time.period of day”.
Therefore, AC and ACD can learn this by model-
ing the atomic concepts separately.

6.2 Domain Adaptation
Our methods are also evaluated on the DSTC 2&3
task (Henderson et al., 2013) which is considered
to be a realistic domain adaptation problem.

DSTC 2 (source domain) comprises of dia-
logues from the restaurant information domain in
Cambridge. We use the dstc2 train set (1612 di-
alogues) for training and the dstc2 dev (506 dia-
logues) for validation.

DSTC 3 (target domain) introduces the tourist
information domain about restaurant, pubs and
coffee shops in Cambridge, which is an extension
of DSTC 2. We use seed data dstc3 seed (only 11
dialogues) as the training set of the target domain.
DSTC3 S test: In this paper, we focus on three
new semantic slots: “has tv, has internet, chil-
dren allowed”. 4 They only exist in the DSTC 3
dataset and have few appearances in the seed data.
A test set is chosen for speciﬁc evaluation on these
new semantic slots, by gathering all the sentences
(688 sentences) whose annotation contains these
three slots and randomly selecting 1000 sentences
irrelevant to these three slots from the dstc3 test
set. This test set is named as DSTC3 S test (1688
sentences).

The union of a slot and action is taken as a plain
“conﬁrm.food=Chinese”),
semantic slot (e.g.
since each slot is tied with an action (e.g. “in-
form”, “deny” and “conﬁrm”) in DSTC 2&3. The
slot and action are taken as atomic concepts. For
the slot ﬁlling task, only the semantic annotation
with aligned information is kept, e.g. the semantic
tuple “request(phone)” is ignored. We use tran-
scripts as input, and make slot-value alignment by

4For

of “has tv,

each
dren allowed”,
is replaced with “conﬁrm(slot=True)”.
the slot-tagging format, e.g.
sion:conﬁrm.has tv]”.

chil-
slot
the semantic annotation “request(slot)”
Then we have
”does it have [televi-

has internet,

397Reference
PS
AC
ACD

... could get in [boston:city name] [late:period of day] [night:period of day]
... could get in [boston:city name] [late:airport name] [night:period of day]
... could get in [boston:city name] [late:period of day] [night:period of day]
... could get in [boston:city name] [late:period of day] [night:period of day]

Table 3: Examples show how concept transfer learning beneﬁts. We use [value:slot] for annotation.

string matching simply.

6.2.1 Experimental Results and Analysis
The experimental settings are similar
to the
ATIS’s, whereas the seed data in DSTC 3 is also
used for validation.

Model

Training set
dstc3 seed

dstc2 train + dstc3 seed

PS
PS
AC
dstc2 train + dstc3 seed
AC
ACD dstc2 train + dstc3 seed

dstc3 seed

F1-score
83.52
89.57
83.58
91.98
92.15

Table 4: The performance of our methods evalu-
ated on the DSTC3 S test.

The performance of our methods in the DSTC
2&3 task is illustrated in Table 4. We can see that:
1) By incorporating the data of the source domain
(dstc2 train), PS and AC achieve improvements
respectively. 2) AC gains more than PS by mod-
eling the plain semantic slot as atomic concepts.
The atomic concepts promote the associated slots
to share input features for the same atoms. 3) The
atomic-concepts based slot ﬁlling considering the
concepts dependently (ACD) gains little (0.17%)
over AC considering the concepts independently.
It may be due to the small size of dstc3 seed.

Case study: Several cases from these mod-
els (trained on the union set of dstc2 train and
dstc3 seed) are also chosen to explain why the
atomic-concepts based slot ﬁlling outperforms the
typical plain slot ﬁlling, as shown in Table 5. From
the above part of Table 5, we can see PS pre-
dicts a wrong slot. Because the grammar “does it
have [something]” is only for the plain slot “con-
ﬁrm.hastv” in the seed data. From the below part
of Table 5, we can see that only ACD which con-
siders the concepts dependently predicts the right
slot. Since “conﬁrm.childrenallowed” never ex-
ists in the seed data, PS can’t learn patterns about
it. Limited by the quantity of the seed data, AC
also doesn’t extract the semantics correctly.

Reference
PS
AC
ACD
Reference
PS
AC
ACD

does it have [internet:conﬁrm.hasinternet]
does it have [internet:conﬁrm.hastv]
does it have [internet:conﬁrm.hasinternet]
does it have [internet:conﬁrm.hasinternet]
do they allow [children:conﬁrm.CA]
do they allow [children:CA]
do they allow [children:CA]
do they allow [children:conﬁrm.CA]

Table 5: Examples show how concept transfer
learning beneﬁts. CA denotes childrenallowed.

7 Conclusion

To address data sparsity problem of language un-
derstanding (LU) task, we present a novel method
of concept deﬁnition based on well-deﬁned atomic
concepts. We present the concept transfer learn-
ing for slot ﬁlling on the atomic concept level to
solve the problem of adaptive LU. The experi-
ments on the ATIS and DSTC 2&3 datasets show
our method obtains promising results and outper-
forms the traditional slot ﬁlling, due to the knowl-
edge sharing of atomic concepts.

The atomic concepts are constructed manually
in this paper. In future work, we want to explore
more ﬂexible concept deﬁnition for concept trans-
fer learning of LU. Moreover, we also propose a
competitive method based on slot name embed-
ding which can be extracted from the literal de-
scription of the slot name automatically. The ex-
perimental result shows that it lays foundation for
ﬁnding a more ﬂexible concept deﬁnition method
for adaptive LU.

Acknowledgments

This work has been supported by the China NSFC
project (No. 61573241), Shanghai International
Science and Technology Cooperation Fund (No.
16550720300) and the JiangSu NSFC project
(BE2016078). Experiments have been carried out
on the PI supercomputer at Shanghai Jiao Tong
University. We also thank Tianfan Fu for com-
ments that greatly improved the manuscript.

398Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-
offrey Zweig, and Yangyang Shi. 2014. Spoken lan-
guage understanding using long short-term memory
In 2014 IEEE Spoken Language
neural networks.
Technology Workshop (SLT), pages 189–194. IEEE.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In INTER-
SPEECH, pages 2524–2528.

Majid Yazdani and James Henderson. 2015. A model
of zero-shot learning of spoken language under-
In Conference on Empirical Methods in
standing.
Natural Language Processing, pages 244–249.

Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL 2007, Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, June 28-30,
2007, Prague, Czech Republic, pages 678–687.

Feifei Zhai, Saloni Potdar, Bing Xiang, and Bowen
Zhou. 2017. Neural models for sequence chunking.
In AAAI, pages 3365–3371.

Xiaodong Zhang and Houfeng Wang. 2016. A joint
model of intent determination and slot ﬁlling for
spoken language understanding. In the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI-16).

Su Zhu, Lu Chen, Kai Sun, Da Zheng, and Kai
Yu. 2014. Semantic parser enhancement for dia-
logue domain extension with little data. In Spoken
Language Technology Workshop (SLT), 2014 IEEE,
pages 336–341. IEEE.

Su Zhu and Kai Yu. 2017. Encoder-decoder with
focus-mechanism for sequence labelling based spo-
ken language understanding. In IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing(ICASSP), pages 5675–5679.

References
Emmanuel Ferreira, Bassam Jabaian, and Fabrice
Lefvre. 2015. Zero-shot semantic parser for spoken
language understanding. In 16th Annual Conference
of the International Speech Communication Associ-
ation (InterSpeech).

Alex Graves. 2012. Supervised Sequence Labelling
with Recurrent Neural Networks. Springer Berlin
Heidelberg.

L Heck and D Hakkani-Tur. 2012. Exploiting the se-
mantic web for unsupervised spoken language un-
derstanding. In Spoken Language Technology Work-
shop, pages 228–233.

Charles T Hemphill, John J Godfrey, and George R
Doddington. 1995. The atis spoken language sys-
In Proceedings of the Darpa
tems pilot corpus.
Speech and Natural Language Workshop, pages 96–
101.

Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013. Dialog state tracking challenge 2 &
3. [online] Available: http://camdial.org/
mh521/dstc/.

Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu.
2016. Leveraging sentence-level information with
encoder lstm for semantic slot ﬁlling. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2077–2083,
Austin, Texas. Association for Computational Lin-
guistics.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot ﬁlling. In 17th Annual Conference of the In-
ternational Speech Communication Association (In-
terSpeech).

Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013.
Investigation of recurrent-neural-
network architectures and learning methods for spo-
In INTERSPEECH,
ken language understanding.
pages 3771–3775.

Roberto Pieraccini, Evelyne Tzoukermann, Zakhar
Gorelov, J-L Gauvain, Esther Levin, C-H Lee, and
Jay G Wilpon. 1992. A speech understanding sys-
tem based on statistical representation of semantics.
In Acoustics, Speech, and Signal Processing, 1992.
ICASSP-92., 1992 IEEE International Conference
on, volume 1, pages 193–196. IEEE.

Gokhan Tur, Dilek Hakkani-T¨ur, and Larry Heck.
2010. What is left to be understood in atis? In Spo-
ken Language Technology Workshop (SLT), 2010
IEEE, pages 19–24. IEEE.

Ngoc Thang Vu. 2016. Sequential convolutional neu-
ral networks for slot ﬁlling in spoken language un-
derstanding. In 17th Annual Conference of the In-
ternational Speech Communication Association (In-
terSpeech).

399
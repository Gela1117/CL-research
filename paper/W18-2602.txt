Systematic Error Analysis of the Stanford Question Answering Dataset

Marc-Antoine Rondeau

Microsoft Research

Montr´eal, Qu´ebec, Canada

marondea@microsoft.com

Abstract

We analyzed the outputs of multiple ques-
tion answering (QA) models applied to
the Stanford Question Answering Dataset
(SQuAD) to identify the core challenges
for QA systems on this data set. Through
an iterative process, challenging aspects
were hypothesized through qualitative
analysis of the common error cases. A
classiﬁer was then constructed to predict
whether SQuAD test examples were likely
to be difﬁcult for systems to answer based
on features associated with the hypothe-
sized aspects. The classiﬁer’s performance
was used to accept or reject each aspect
as an indicator of difﬁculty. With this ap-
proach, we ensured that our hypotheses
were systematically tested and not simply
accepted based on our pre-existing biases.
Our explanations are not accepted based
on human evaluation of individual exam-
ples. This process also enabled us to iden-
tify the primary QA strategy learned by the
models, i.e., systems determined the ac-
ceptable answer type for a question and
then selected the acceptable answer span
of that type containing the highest density
of words present in the question within its
local vicinity in the passage.

1

Introduction

Since the introduction of the Stanford Question
Answering Dataset (SQuAD, Rajpurkar et al.,
2016), research groups have directed signiﬁcant
efforts towards achieving a high position on the
SQuAD leaderboard.1 This competition has re-

1https://rajpurkar.github.io/

SQuAD-explorer/

Timothy J. Hazen
Microsoft Research

Cambridge, Massachusetts, USA
tj.hazen@microsoft.com

sulted in many new models for question answering
using machine reading comprehension.

Within SQuAD, a single test example consists
of three components: a question, a text passage
and an answer. The answer is a span extracted
from the passage answering the question. Ques-
tions were created by human annotators, who were
shown a passage and asked to produce question
and answer pairs. In performing the question an-
swering task, the best performing systems em-
ployed complex attention ﬂow mechanisms for
matching questions to substrings of the text pas-
sage. These models, while varied, all belong to
the same general family of neural network archi-
tectures.

In this work, we conducted a systematic error
analysis on the development set of SQuAD to ex-
plain the common failures and successes of some
of these models; the results can be expected to
generalize to the entire family. Our goal was to ex-
plain the models’ failures and successes using well
deﬁned features automatically extracted from ex-
amples. We wanted to use simple features, such as
word identity, over complex features. We wanted
to avoid explanations based on the human strat-
egy used to answer a question, or complex features
that cannot be extracted automatically, such as rea-
soning, common sense or external knowledge. Fi-
nally, we wanted to isolate a passages’ readability
from the strategy required to answer questions.

Our methodology used classiﬁers to predict the
difﬁculty of questions. The classiﬁer performance
was used to conﬁrm or refute the validity of a hy-
pothesized challenge using its true and false posi-
tive rates over the entire development set. System-
atic testing across all system failures and successes
can reduce the risk of conﬁrmation bias inherent to
random spot checks.

A key difference with previous error analysis on
SQuAD is that we looked for successes present-

ProceedingsoftheWorkshoponMachineReadingforQuestionAnswering,pages12–20Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics12ing the same challenges observed in failures. This
conﬁrms that the same explanations are not appli-
cable to the successes. While many system errors
could be explained in term of human challenges,
features related to those challenges were usually
independent of failures and successes. This can
easily be missed by random spot checks relying
on human evaluation.

From our evaluations, we identiﬁed a reading
strategy that matched the observed failures and
successes. We believe that this methodology is
more robust than the common ad-hoc approaches
purely based on human evaluations over a small
random sample. The reading strategy we iden-
tiﬁed indicates that SQuAD is surprisingly well
suited for neural network based models. While
it remains a valuable resource, this now limits its
suitability for further improvement of QA models.

1.1 Text Organization
In this paper, we will ﬁrst present some related
works, and explain how our methodology differs.
A description of our methodology will follow. We
will then present experimental results for three
groups of hypotheses (readability, Q-words, and
acceptability), and a combined model. Finally,
we will describe the human analog of the models’
strategy, followed by our conclusions.

2 Related Works

Sugawara et al. (2017) evaluated various datasets,
in particular SQuAD, to determine how many hu-
man reading skills were required to answer ques-
tions. They described SQuAD as “difﬁcult to
read but easy to answer” for humans, ﬁnding that
SQuAD requires only a few simple skills. In con-
trast, we are identifying skills used by machines.
FastQA (Weissenborn et al., 2017) added sim-
ple word matching features, indicating that a word
was in both the passage and question, to a sim-
ple MRC model. Those simple features improved
performance using this simple MRC model. We
observed that variations of this feature were ac-
ceptable predictors of failures and successes

Adversarial SQuAD (Jia and Liang, 2017)
added distractor sentences at the end of SQuAD
examples. Model speciﬁc distractors were cre-
ated by adding random words, guided by the tar-
get model’s output, until it predicted a wrong an-
swer. The resulting sentences are ungrammati-
cal and have no semantic signiﬁcance, but match

words present in the question. Similarly, a more
generic set of distractors was created using a sim-
ple set of rules to transform the question into a
statement, and replacing keywords. The result-
ing sentence is grammatical and meaningful, but
is irrelevant to the question. The signiﬁcant num-
ber of word matches between the question and the
distractor signiﬁcantly reduces performance.

Those related works indicates that word to word
matching, similar to the reserved engineered strat-
egy described in Section 8, is sufﬁcient to obtain
good performance on SQuAD.

In this work, we used systematic hypothesis
testing over both failures and successes to identify
the strategy used by machines to reach high per-
formance on SQuAD. Systematic testing based on
automatically extracted features prevent us from
relying on human explanation. It also limits con-
ﬁrmation bias, which is a concern for qualitative
analysis. Human investigators will tend to explain
errors in term of the human skills required, even
when a simpler explanation is possible. It is also
important to conﬁrm that the same explanation is
not applicable to the models’ successes. Previous
error analysis focused on errors, and ignored suc-
cesses.

3 Methodology

We want to explain models’ failures and successes
while avoiding explanations based on human read-
ing comprehension, and to test our explanations
systematically. We deﬁned empirical difﬁculty
classes (Section 3.1) and used the linear separabil-
ity of those classes using the extracted features to
accept or reject a hypothetical explanation. We it-
erated qualitative analysis, hypothesis generation
and the creation of corresponding feature extrac-
tors, and testing.

3.1 Difﬁculty Classes Used
We used the single and ensemble outputs of the
three models listed in Table 1, for a total of six
models. The models were chosen due to their per-
formance on SQuAD: all were near the top of the
leaderboard at the time this work began. While
these models are only a subset of the models on the
SQuAD leaderboard, they share similar features
with the others. We believe that that our ﬁndings
generalize to the others. Questions were divided
into 3 classes : easy, hard and other. EASY ques-
tions were those questions where all six models re-

13BiDAF
Reasonet
FusionNet

(Seo et al., 2016)
(Shen et al., 2017)
(Huang et al., 2017)

Table 1: Models used for error analysis. The sin-
gle and ensemble version of each was used

Class

EASY (6 EMs)

6 PMs
5 PMs
4 PMs
3 PMs
2 PMs
1 PM

HARD (0 PMs)

Count Frequency (%)
5,874
55.57
459
4.34
11.15
1,179
7.12
753
611
5.78
6.00
634
5.97
631
429
4.06

Table 2: Distribution of question as a function of
the number of models predicting a partial match
(PM). Also includes the two main classes, EASY
(all models predicted exact matches, EMs), and
HARD (no prediction is a match.)

turned an exact match (EM) with a human answer,
and HARD questions were those where none of the
answers was a partial match (PM). All other ques-
tions were placed in the OTHERS class. Table 2
shows the resulting distribution, with the OTHERS
class subdivided according to the number of par-
tial or exact matches.2

3.2 Classiﬁer and Hypothesis Testing
Classiﬁers trained and tested on the entire devel-
opment set were used to measure the linear sepa-
rability of the questions’ empirical difﬁculty class.
We focused on the EASY vs (HARD ∪ OTHERS)
case. Feature were accepted if the area under the
receiver operating characteristic curve (AUC) was
0.6 or more; This threshold was picked based on
the performance of text complexity features. Fea-
tures were also accepted if they improved the AUC
when combined with the existing features.

Our goal was to identify features with two prop-

erties:

1. Features are linearly correlated with failures

or successes, and

2. The intersection between the feature values
for question in the EASY and HARD sets is as
small as possible.

2The 6 PMs may include up to ﬁve EMs.

This is equivalent to linear separability, which we
can evaluate using classiﬁers.

This process allowed for hypothesis testing: a
hypothesized explanation was rejected when it
was not possible to create corresponding features
that would improved the classiﬁer, or be predictive
by themselves.

3.3 Receiver Operating Characteristic

Curves

Receiver operating characteristic (ROC) curves
are used to compare the performance of classi-
ﬁers. They illustrate the trade-offs between false
positives and false negatives. In practice, they can
measure the linear separability of the feature used
in a linear classiﬁer.

In a linear classiﬁer, one or more features are
projected down to one dimension. If the projected
value is greater than the threshold t, then the ex-
ample is classiﬁed as belonging to the class, oth-
erwise it is classiﬁed as outside the class.

The ROC curve contains all the points (P (X ≥
t|c = 0), P (X ≥ t|c = 1))∀t ∈ supp(X), where
X is a random variable corresponding to the pro-
jected value for a random example, and supp(X)
is its support. P (X ≥ t|c = 0) is the false posi-
tive rate, while P (X ≥ t|c = 1) is the true posi-
tive rate. The area under the curve (AUC) can be
used to summarize the performance of the corre-
sponding classiﬁer. It would be 0.5 for a perfectly
random classiﬁer, and 1.0 for a perfect classiﬁer.

3.4

Iterative Procedure

We used an iterative process where ques-
tion/passage pairs were selected randomly, mainly
from the HARD class described in Section 3.1. A
qualitative analysis of this sample was then used to
identify common features that would explain the
models’ failure or success. Corresponding feature
extractors were then created, and used in logistic
regression classiﬁers in order to assign questions
to one of the difﬁculty classes described below.
Good features would be correlated with either fail-
ures or successes. The explanation was then ac-
cepted if it was sufﬁcient to separate, at least par-
tially, the two classes, or if it improved the classi-
ﬁer performance when combined with previously
accepted features. This process was repeated until
no new hypotheses were generated.

14What is the name of an algebraic structure
in which addition, subtraction and multi-
plication are deﬁned?
Prime numbers give rise to two more gen-
eral concepts that apply to elements of any
commutative ring R, an algebraic struc-
ture where addition, subtraction and
multiplication are deﬁned: prime ele-
ments and irreducible elements. An ele-
ment p of R is called prime element if it is
neither zero nor a unit (i.e., does not have
a multiplicative inverse) and satisﬁes the
following requirement: given x and y in R
such that p divides the product xy, then p
divides x or y. An element is irreducible if
it is not a unit and cannot be written as a
product of two ring elements that are not
units.

Table 3: Example of hard to read passage associ-
ated with an easy to answer question

4 Reading difﬁculty

Features based on text complexity and human
readability were used as control. Those features
were used to conﬁrm that the hypothesized expla-
nations were not proxies for human text complex-
ity. They were also used to establish the perfor-
mance threshold required to accept hypotheses.

We used the grade level metric (Kincaid et al.,
1975), commonly used to evaluate the readabil-
ity of document for humans. The grade level is
a weighted sum of the average number of sylla-
bles per word and words per sentences, weighted
to match reading ability expected of a student in
that grade, in the US education system. Figure 1
shows the ROC curve when predicting the error
class of a question given the grade level of the pas-
sage and question. The AUC is 0.53 when classi-
fying EASY vs (HARD ∪ OTHERS), which is effec-
tively random. Table 3 shows an example of a hard
to read passage associated with an EASY question.
We also investigated other features measuring
text complexity. Those features and their individ-
ual performance are described in Appendix AUs-
ing a combination of those features, the AUC is
0.54 when classifying EASY vs (HARD ∪ OTH-
ERS), which is effectively random. This indicates
that those features are not predictors of failures
or successes.
In practice, the difﬁculty class of

e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T

1

0.8

0.6

0.4

0.2

0

Grade Level (AUC=0.53)

0

0.2

0.4

0.6

0.8

1

False Positive Rate

Figure 1: ROC curve using Grade level to detect
EASY questions

a question is not based on the human readability
of the associated passage. In particular, complex
internal dependencies and co-referral structures,
which would support complicated questions, are
not predictors of failures or successes. This sug-
gests that the failures are not caused by the com-
plexity of the human strategy required to solve a
question, assuming complicated questions tend to
be associated with complicated passages.

5 Density and Proximity to Q-Words

To shorten notation, we refer to words present
in the question as Q-words. Qualitative analysis
suggested that successes tended to have many Q-
words nearby. Similarly, failures tended to have
few Q-words in their vicinity. Table 4 shows an ex-
ample of this concept extracted from SQuAD. All
systems selected the same incorrect answer, which
is in a region of high Q-word density. The correct
human answer is in a region of lower density, and
unlike the systems’ answer it is not adjacent to Q-
words.

Figure 2 shows the corresponding ROC curves
for a classiﬁer using density and proximity fea-
tures. The number of Q-words within up to 10
words from a human answer was the best individ-
ual feature. This measures the density of Q-words
in the vicinity of human answers. The AUC was
0.60 when classifying EASY vs (HARD ∪ OTH-
ERS), 0.66 for HARD vs (EASY ∪ OTHERS), and
0.70 for EASY vs HARD.

The second best individual feature was the dis-

15What was the name of the ﬁrst Doctor
Who story released as an LP?
The earliest Doctor Who-related au-
dio release was a 21-minute narrated
abridgement of the First Doctor tele-
vision story The Chase released in
the ﬁrst origi-
1966. Ten years later,
nal Doctor Who audio was released on
LP record; Doctor Who and the Pescatons
featuring the Fourth Doctor. The ﬁrst
commercially available audiobook was an
abridged reading of the Fourth Doctor
story State of Decay in 1981.
In 1988,
during a hiatus in the television show, Slip-
back, the ﬁrst radio drama, was transmit-
ted.

Table 4: Example of Q-word density and prox-
imity. The systems’ answer, in italic, is closer to
Q-Words, in bold, than the underlined human an-
swer.

tance between human answers and the nearest
peak in the Q-word density. This measures the
proximity of Q-word clusters to human answers.
The AUC was 0.59 when classifying EASY vs
(HARD ∪ OTHERS), 0.66 for HARD vs (EASY ∪
OTHERS), and 0.69 for EASY vs HARD.
The classiﬁers were able to classify EASY vs
HARD more easily than EASY vs (HARD ∪ OTH-
ERS) and HARD vs (EASY ∪ OTHERS). Those re-
sults shows that the overlap between EASY and
HARD is smaller than the overlaps between EASY
and OTHERS, and between HARD and OTHERS.
This indicates that there is an approximate order-
ing HARD ≤ OTHERS ≤ EASY when going from
low to high values of those density and proximity
metrics.

The density and proximity features are accept-
able predictors of failures and successes. This sim-
ple mechanical explanation shows that similarity
between the question and the answer’s surround-
ings will contribute to the machine difﬁculty of the
question. The features used were based on direct,
exact matches between words. Capitalization was
ignored, but we did not perform any other form
of normalization, or accept any other differences
when matching, including trivial ones such as plu-
ralization. This very strict matching was sufﬁcient
to create an acceptable predictor.3

3Better matching (e.g.cosine distance between word vec-

e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T

1

0.8

0.6

0.4

0.2

0

Density (AUC=0.60)
Proximity (AUC=0.59)

0

0.2

0.4

0.6

0.8

1

False Positive Rate

Figure 2: ROC when using Q-words density and
proximity to detect EASY questions

6 Acceptability

Qualitative analysis of the false-negatives and of
randomly sampled false-positives, using the den-
sity and proximity classiﬁer, suggested that mod-
els were returning answers that were “acceptable”
to the question. Questions would correspond to
one or more answer types, and the models would
retrieve the span belonging to one of those types
with the highest Q-word density and proximity.
While this notion of acceptability is hard to deﬁne,
a signiﬁcant portion of the answers are named en-
tities (NEs) of various types. This can be used to
test this hypothesis. We used CoreNLP (Manning
et al., 2014) to identify which answers are NEs, as
well as their type and competing spans.

Table 5 shows an example of acceptability. All
systems select the only date presents in the pas-
sage. The correct answer is the proper name of an
event. Unlike dates, “Super Bowl LI” would not
usually be used to answer “when” questions, and
would not generally be considered acceptable.

6.1 Typed and Competition Features
We created a typed feature indicating that a least
one human answer overlapped with a NE. The
AUC was 0.63 when classifying EASY vs (HARD
∪ OTHERS), 0.54 for HARD vs (EASY ∪ OTHERS),
and 0.60 for EASY vs HARD. The typed feature is
binary; this result is caused by the fact 53.18% of

tors) should improve performance, but would be less explain-
able than direct matches. As our goal is to explain errors,
rather than predict them, we decided to use direct matching.

16When will Roman numerals be used again
to denote the Super Bowl number?
On June 4, 2014,
the NFL announced
that the practice of branding Super Bowl
games with Roman numerals, a practice
established at Super Bowl V, would be
temporarily suspended, and that the game
would be named using Arabic numerals as
Super Bowl 50 as opposed to Super Bowl
L. The use of Roman numerals will be re-
instated for Super Bowl LI . . .

Table 5: Example of acceptability. The systems’
answer, in italic, is the only date in the passage.
The human answer is underlined.

e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T

1

0.8

0.6

0.4

0.2

0

Typed (AUC=0.63)
Comp. (AUC=0.65)

0

0.2

0.4

0.6

0.8

1

False Positive Rate

Figure 3: ROC when using typed and competition
features to detect easy questions

EASY questions are typed, but only 26% of (HARD
∪ OTHERS) questions are typed. This will be in-
vestigated in more details below.

A feature indicating the acceptable spans count
was also created. This feature is equal to the num-
ber of NE of the same type when least one human
answer overlap with a NE, falling back to the num-
ber of words in the passage in the case where no
human answer is a NE. In the NE case, this should
correspond to the number of competing hypothe-
ses; otherwise, the number of words in the passage
was picked as a simple heuristic. The AUC was
0.65 when classifying EASY vs (HARD ∪ OTH-
ERS), 0.54 for HARD vs (EASY ∪ OTHERS), and
0.61 for EASY vs HARD.

Figure 3 shows the corresponding ROC curves.

Type

Non-NE
All NEs
DATE
PER
LOC

NUMBER

ORG
Others

Count Freq. (%) EASY (%)
6,217
44.44
72.11
4,353
83.37
968
956
69.67
68.99
632
73.46
618
63.18
573
606
68.27

58.82
41.18
9.16
9.04
5.98
5.85
5.42
5.73

Table 6: Distribution of questions by NE type and
difﬁculty class.

The overlap between the typed feature’s predic-
tions and the acceptable span count’s prediction is
clearly visible in this ﬁgure.
When used only for typed questions, the AUC
was 0.59 when classifying EASY vs (HARD ∪
OTHERS), 0.62 for HARD vs (EASY ∪ OTHERS),
and 0.64 for EASY vs HARD. This shows that suc-
cesses and failures are correlated with the number
of acceptable spans, when the answer is a NE. This
will be investigated in more details below.

6.2 Named Entity Answers
Table 6 shows the distribution of question by
named entity type and difﬁculty class. When the
answer is not a named entity, 44.44% of ques-
tions are in the EASY class. This proportion is
72.11% when the answer is a named entity. As
shown in Figure 4, the proportion of EASY ques-
tions decreases as the number of named entities of
the same type in the passage increases. Figure 5
shows that the rank of the human answer, rela-
tive to the number of competing named entities,
based on the combined density and proximity fea-
tures, is a predictor of failures and successes. The
AUC was 0.61 when classifying EASY vs (HARD
∪ OTHERS), 0.68 for HARD vs (EASY ∪ OTHERS),
and 0.70 for EASY vs HARD.

Those results indicate that the models are selec-
tive and can ignore high Q-words density regions
of the passage if those regions do not contain an
acceptable span. They also indicates that the den-
sity and proximity of Q-words is used to select
which acceptable span should be retrieved.

7 Combination of Features

Complementarity between density, proximity and
acceptability, as well as some rejected features,
was tested in a single combined logistic regres-

17)
n

|

Y
S
A
E
(
P

100

80

60

40

20

0

0

4

2
Named entity count (n)

6

8

10

Figure 4: Proportion of EASY questions vs number
of named entity per passage

e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T

1

0.8

0.6

0.4

0.2

0

Rank (NE) (AUC=0.61)

0

0.2

0.4

0.6

0.8

1

False Positive Rate

e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T

1

0.8

0.6

0.4

0.2

0

Dens. + Prox. + Acpt. (AUC=0.71)

0

0.2

0.4

0.6

0.8

1

False Positive Rate

Figure 6: ROC using combination of features to
detect EASY questions

EASY HARD
0.52
0.67

0.54
0.71

0.71

0.66

Features
All Readability
All Density
+ All Proximity
+ All Acceptability
All Readability
+ All Density
+ All Proximity
+ All Acceptability

Figure 5: ROC when using the rank of named-
entities human answers to detect EASY questions

Table 7: Area under the curve (AUC) per group of
features

sion classiﬁer. Those features and their individ-
ual performance are described in details in Ap-
pendix B.The results are shown in Figure 6. The
AUC was 0.71 when classifying EASY vs (HARD
∪ OTHERS), 0.67 for HARD vs (EASY ∪ OTHERS),
and 0.74 for EASY vs HARD. Adding the read-
ability features described in Appendix A did not
signiﬁcantly improve results, with AUCs of 0.71,
0.66, and 0.74 respectively. This conﬁrms that
readability is not a predictor of failures or suc-
cesses, while the Q-words density and proximity,
and acceptability features are.

8 Reverse Engineered Strategy
Based on the density, proximity and acceptability
results presented above, we conclude that the mod-
els’ QA strategy is analogous to:

1. Classify question to identify acceptable span

types,

2. Extract acceptable spans from passage,

3. Rank extracted spans by Q-word density and

proximity, and

4. Return best span.

This simple strategy is a human equivalent that
would reproduce the failures and successes ob-
it is doubtful that the models are im-
served;
plementing it literally.
It also hides signiﬁcant
complexity in the acceptability and density steps,
which were not properly modeled in our experi-
ments as we wanted to ensure interpretablity. A
more powerful model would be better at model-
ing those concepts, but such a model would be
very similar to the models we are trying to ana-
lyze. This is similar to the results of the related
works listed in Section 2.

189 Priming During Data Collection
We attribute the success of the simple strategy de-
scribed above to priming and biases during ques-
tion generation. While we cannot conﬁrm prim-
ing experimentally, as this would involve asking a
justiﬁcation for the question during the initial data
collection, we can extrapolate from our own at-
tempts at question generation.

Reading the passage will prime the question
creators towards questions based on interrogative
paraphrases of the passage. As noted by Sug-
awara et al. (2017), “SQuAD was difﬁcult to read,”
which should further magnify this effect: when the
passage is hard to read, it is easier and faster to
scan it for a sentence stating a fact and to refor-
mulate that sentence as a question. In particular,
since crowdworkers are not motivated by a gen-
uine need for information, we can expect them to
use the ﬁrst question that came to mind. Table 3
shows such an example, where the question is a
slight reformulation of part of the passage.

We ﬁnd this priming issue concerning, and sus-
pect that it affects many datasets. It should be pos-
sible to avoid it by using true questions, collected
from various sources. Those questions should
be the product of a genuine need for information
rather than created for the sake of creating a ques-
tion, and should be created before reading a poten-
tially answering passage. Those can be matched
to relevant documents, and answered by human
annotators. If keyword search is used to retrieve
relevant documents, there is of course a risk of
retrieving documents containing declarative para-
phrases of the questions, which would effectively
prime the passage on the question.

10 Conclusion
We presented a methodology used to systemati-
cally analyze the errors of six models on SQuAD.
This methodology relies on simple feature extrac-
tors and classiﬁers to ensure that any hypothesized
explanation does not also co-occur with correct
answers. By iteratively sampling falsely negative
and positive predictions of this classiﬁers, we were
able to reverse engineer a simple QA strategy that
would match the models’ failures and successes.
While labor intensive4, this methodology avoids
conﬁrmation bias during a qualitative analysis of
a random sample. In particular, human readability
4Approximately 3-4 weeks, mostly creating and testing

hypotheses.

might mask the true cause of an error, as human in-
vestigators will tend to explain errors by the chal-
lenges they faced when examining the question.
This methodology can be applied to large datasets,
and also ensures that the errors are attributed to
well deﬁned causes. We recommend its use when
the challenges remaining in a dataset need to be
identiﬁed.

We attribute the success of the simple strategy
we identiﬁed to priming by the passage during
question generation. This limits the challenges,
for machines, truly present in SQuAD, and in-
dicates that, while necessary, good performance
on SQuAD is not sufﬁcient to say that a ma-
chine reading question answering model would
have good performance in general. As such, we
recommend the use of datasets where question cre-
ation is independent of the passage, such as:

• MSMarco (Nguyen et al., 2016)
• NarrativeQA (Kocisk´y et al., 2017)
• NewsQA (Trischler et al., 2017)
• SearchQA (Dunn et al., 2017)
• TriviaQA (Joshi et al., 2017)

Acknowledgements
We would like to thank Eric Lin, Peter Potash,
Yadollah Yaghoobzadeh, and Kaheer Suleman for
their feedback and helpful comments. We also
thanks the anonymous reviewers for their com-
ments.

References
Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur
G¨uney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with con-
text from a search engine. CoRR, abs/1704.05179.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2017. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. arXiv preprint arXiv:1711.07341.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021–2031, Copenhagen, Denmark. Association for
Computational Linguistics.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly

19supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 1601–1611.

Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. 1975. Derivation of
new readability formulas (automated readability in-
dex, fog count and ﬂesch reading ease formula) for
navy enlisted personnel. Chief of Naval Technical
Training, Research Branch Report 8-75.

Tom´as Kocisk´y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G´abor Melis, and
Edward Grefenstette. 2017. The narrativeqa reading
comprehension challenge. CoRR, abs/1712.07040.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
arXiv preprint
reading comprehension dataset.
arXiv:1611.09268.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. Squad: 100, 000+ ques-
tions for machine comprehension of text. CoRR,
abs/1606.05250.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi,
and Hannaneh Hajishirzi. 2016. Bidirectional at-
tention ﬂow for machine comprehension. CoRR,
abs/1611.01603.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. Reasonet: Learning to stop
reading in machine comprehension. In Proceedings
of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1047–1055. ACM.

Saku Sugawara, Yusuke Kido, Hikaru Yokono, and
Akiko Aizawa. 2017. Evaluation metrics for ma-
chine reading comprehension: Prerequisite skills
and readability. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
806–817.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. Newsqa: A machine compre-
In Proceedings of the 2nd Work-
hension dataset.
shop on Representation Learning for NLP, pages
191–200.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Fastqa: A simple and efﬁcient neural ar-
chitecture for question answering. arXiv preprint
arXiv:1703.04816.

20
An Operation Sequence Model for Explainable Neural Machine

Translation

Felix Stahlberg and Danielle Saunders and Bill Byrne

Department of Engineering
University of Cambridge, UK

{fs439,ds636,wjb31}@cam.ac.uk

Abstract

We propose to achieve explainable neural ma-
chine translation (NMT) by changing the out-
put representation to explain itself. We present
a novel approach to NMT which generates
the target sentence by monotonically walking
through the source sentence. Word reorder-
ing is modeled by operations which allow set-
ting markers in the target sentence and move
a target-side write head between those mark-
ers. In contrast to many modern neural mod-
els, our system emits explicit word alignment
information which is often crucial to practi-
cal machine translation as it improves explain-
ability. Our technique can outperform a plain
text system in terms of BLEU score under the
recent Transformer architecture on Japanese-
English and Portuguese-English, and is within
0.5 BLEU difference on Spanish-English.

1

Introduction

translation

Neural machine
(NMT) mod-
els (Sutskever et al., 2014; Bahdanau et al.,
2015; Gehring et al., 2017; Vaswani et al., 2017)
are remarkably effective in modelling the distri-
bution over target sentences conditioned on the
source sentence, and yield superior translation
performance compared to traditional statistical
machine translation (SMT) on many language
pairs. However, it is often difﬁcult to extract a
comprehensible explanation for the predictions
of these models as information in the network
is represented by real-valued vectors or matri-
ces (Ding et al., 2017). In contrast, the translation
process in SMT is ‘transparent’ as it can identify
the source word which caused a target word
through word alignment. Most NMT models
do not use the concept of word alignment.
It is
tempting to interpret encoder-decoder attention
matrices (Bahdanau et al., 2015) in neural models
as (soft) alignments, but previous work has found

that
the attention weights in NMT are often
erratic (Cheng et al., 2016) and differ signiﬁcantly
from traditional word alignments (Koehn and
Knowles, 2017; Ghader and Monz, 2017). We
will discuss the difference between attention and
alignment in detail in Sec. 4. The goal of this
paper is explainable NMT by developing a trans-
parent translation process for neural models. Our
approach does not change the neural architecture,
but represents the translation together with its
alignment as a linear sequence of operations. The
neural model predicts this operation sequence,
and thus simultaneously generates a translation
and an explanation for it in terms of alignments
from the target words to the source words that
generate them. The operation sequence is “self-
explanatory”; it does not explain an underlying
NMT system but is rather a single representation
produced by the NMT system that can be used to
generate translations along with an accompanying
explanatory alignment
to the source sentence.
We report competitive results of our method
on Spanish-English, Portuguese-English,
and
Japanese-English, with the beneﬁt of producing
hard alignments for better interpretability. We
discuss the theoretical connection between our
approach and hierarchical SMT (Chiang, 2005)
by showing that an operation sequence can be
seen as a derivation in a formal grammar.

2 A Neural Operation Sequence Model

Our operation sequence neural machine transla-
tion (OSNMT) model is inspired by the operation
sequence model for SMT (Durrani et al., 2011),
but changes the set of operations to be more ap-
propriate for neural sequence models. OSNMT is
not restricted to a particular architecture, i.e. any
seq2seq model such as RNN-based, convolutional,
or self-attention-based models (Bahdanau et al.,

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages175–186Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics1752015; Vaswani et al., 2017; Gehring et al., 2017)
could be used.
In this paper, we use the recent
Transformer model architecture (Vaswani et al.,
2017) in all experiments.

In OSNMT, the neural seq2seq model learns
to produce a sequence of operations. An OS-
NMT operation sequence describes a translation
(the ‘compiled’ target sentence) and explains each
target token with a hard link into the source sen-
tence. OSNMT keeps track of the positions of a
source-side read head and a target-side write head.
The read head monotonically walks through the
source sentence, whereas the position of the write
head can be moved from marker to marker in the
target sentence. OSNMT deﬁnes the following op-
erations to control head positions and produce out-
put words.

• POP SRC: Move the read head right by one

token.

• SET MARKER: Insert a marker symbol into
the target sentence at the position of the write
head.

• JMP FWD: Move the write head to the nearest
marker right of the current head position in
the target sentence.

• JMP BWD: Move the write head to the nearest
marker left of the current head position in the
target sentence.

• INSERT(t): Insert a target token t into the
target sentence at the position of the write
head.

Tab. 1 illustrates the generation of a Japanese-
English translation in detail. The neural seq2seq
model is trained to produce the sequence of opera-
tions in the ﬁrst column of Tab. 1. The initial state
of the target sentence is a single marker symbol
X1. Generative operations like SET MARKER or
INSERT(t) insert a single symbol left of the cur-
rent marker (highlighted). The model begins with
a SET MARKER operation, which indicates that
the translation of the ﬁrst word in the source sen-
tence is not at the beginning of the target sentence.
Indeed, after “translating” the identities ‘2000’
and ‘hr’, in time step 6 the model jumps back
to the marker X2 and continues writing left of
‘2000’. The translation process terminates when
the read head is at the end of the source sentence.
The ﬁnal translation in plain text can be obtained

by removing all markers from the (compiled) tar-
get sentence.

2.1 OSNMT Represents Alignments
The word alignment can be derived from the
operation sequence by looking up the position
of the read head for each generated target to-
ken. The alignment for the example in Tab. 1 is
shown in Fig. 1. Note that similarly to the IBM
models (Brown et al., 1993) and the OSM for
SMT (Durrani et al., 2011), our OSNMT can only
represent 1:n alignments. Thus, each target token
is aligned to exactly one source token, but a source
token can generate any number of (possibly non-
consecutive) target tokens.

2.2 OSNMT Represents Hierarchical

Structure

We can also derive a tree structure from the op-
eration sequence in Tab. 1 (Fig. 2) in which each
marker is represented by a nonterminal node with
outgoing arcs to symbols inserted at that marker.
The target sentence can be read off the tree by
depth-ﬁrst search traversal (post-order).

More formally, synchronous context-free gram-
mars (SCFGs) generate pairs of strings by pairing
two context-free grammars. Phrase-based hierar-
chical SMT (Chiang, 2005) uses SCFGs to model
the relation between the source sentence and the
target sentence. Multitext grammars (MTGs) are a
generalization of SCFGs to more than two output
streams (Melamed, 2003; Melamed et al., 2004).
We ﬁnd that an OSNMT sequence can be inter-
preted as sequence of rules of a tertiary MTG G
which generates 1.) the source sentence, 2.) the
target sentence, and 3.) the position of the target
side write head. The start symbol of G is

[(S), (X1), (P1)]T

(1)

which initializes the source sentence stream with
a single nonterminal S, the target sentence with
the initial marker X1 and the position of the write
head with 1 (P1). Following Melamed et al. (2004)
we denote rules in G as

[(α1), (α2), (α3)]T → [(β1), (β2), (β3)]T

(2)

where α1, α2, α3 are single nonterminals or
empty, β1, β2, β3 are strings of terminals and non-
terminals, and αi → βi for all i ∈ {1, 2, 3} with
nonempty αi are the rewriting rules for each of

176Operation

SET MARKER
2000
POP SRC
hr
POP SRC

JMP BWD

SET MARKER
of
POP SRC

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17 was
18
19
20
21

JMP BWD
stable
POP SRC
operation
POP SRC

JMP FWD

JMP FWD

POP SRC

POP SRC
conﬁrmed
POP SRC

Target sentence (compiled)

Source sentence
2000 hr Ĳ  4 ఔ N ௱ ı X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 2000 X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 2000 X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X3 X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr was X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr was X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr was X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr was conﬁrmed X1
2000 hr Ĳ  4 ఔ N ௱ ı stable operation X3 of X2 2000 hr was conﬁrmed X1

Table 1: Generation of the target sentence “stable operation of 2000 hr was conﬁrmed” from the source
sentence “2000 hr Ĳ  4 ఔ N ௱ ı”. The neural model produces the linear sequence
of operations in the ﬁrst column. The positions of the source-side read head and the target-side write
head are highlighted. The marker in the target sentence produced by the i-th SET MARKER operation is
denoted with ‘Xi+1’; X1 is the initial marker. We denote INSERT(t) operations as t to simplify notation.

Figure 1: The translation and the alignment derived
from the operation sequence in Tab. 1.

the three individual components which need to
be applied simultaneously. POP SRC extends the
source sentence preﬁx in the ﬁrst stream by one
token.

POP SRC : ∀s ∈ Vsrc :

(S)
 →

(sS)

 (3)

()
()

()
()

where Vsrc is the source language vocabulary. A
jump from marker Xi to Xj is realized by replac-

Figure 2: Target-side tree representation of the op-
eration sequence in Tab. 1.

ing Pi with Pj in the third grammar component:

JMP : ∀i, j ∈ N : [(), (), Pi]T → [(), (), (iPj)]T
(4)
where N = {k ∈ N|k ≤ n} is the set of
the ﬁrst n natural numbers for a sufﬁciently large
n. The generative operations (SET MARKER and

177OSNMT

SET MARKER
2000
POP SRC

hr

POP SRC

JMP BWD

SET MARKER

Derivation
[(S), (X1), P1]T
Eq. 3→ [(2000 S), (X1), P1]T
Eq. 5→ [(2000 S), (X2X1), (P1)]T
Eq. 6→ [(2000 S), (X2 2000 X1), (P1)]T
Eq. 3→

(P1)

(P1)

(X2 2000 X1)

(X2 2000 hr X1)


 (2000 hr S)

 (2000 hr S)

 (2000 hr Ĳ S)

 (2000 hr Ĳ S)
 (2000 hr Ĳ S)


(X2 2000 hr X1)

(X2 2000 hr X1)

(2000 hr Ĳ S)

(1 P2)

(1 P2)

(P1)

(X3X2 2000 hr X1)

(1 P2)



(X3 of X2 2000 hr X1)

Eq. 6→

Eq. 3→

Eq. 4→

Eq. 5→

Eq. 6→



of

...

...
Table 2: Derivation in G for the example of Tab. 1.

INSERT(t)) insert symbols into the second com-
ponent.

SET MARKER : ∀i ∈ N :

 →

 ()
 →
 ()

(Xi)
(Pi)

(Xi)
(Pi)

()

(Xi+1Xi)

(5)

(Pi)

 ()

(tXi)
(Pi)




INSERT : ∀i ∈ N , t ∈ Vtrg :

(6)

where Vtrg is the target language vocabulary. The
identity mapping Pi → Pi in the third component
enforces that the write head is at marker Xi. We
note that G is not only context-free but also reg-
ular in the ﬁrst and third components (but not in
the second component due to Eq. 5). Rules of
the form in Eq. 6 are directly related to alignment
links (cf. Fig. 1) as they represent the fact that tar-
get token t is aligned to the last terminal symbol
in the ﬁrst stream. We formalize removing mark-
ers/nonterminals at the end by introducing a spe-
cial nonterminal T which is eventually mapped to
the end-of-sentence symbol EOS:

[(S), (), ()]T → [(T ), (), ()]T (7)
[(T ), (), ()]T → [(EOS), (), ()]T (8)
∀i ∈ N : [(T ), (Xi), ()]T → [(T ), (), ()]T (9)
∀i ∈ N : [(T ), (), (Pi)]T → [(T ), (), ()]T (10)
Tab. 2 illustrates that there is a 1:1 correspon-
dence between a derivation in G and an OSNMT
operation sequence. The target-side derivation
(the second component in G) is structurally sim-
ilar to a binarized version of the tree in Fig. 2.
However, we assign scores to the structure via the
corresponding OSNMT sequence which does not
need to obey the usual conditional independence
assumptions in hierarchical SMT. Therefore, even
though G is context-free in the second component,
our scoring model for G is more powerful as it con-
ditions on the OSNMT history which potentially
contains context information. Note that OSNMT
is deﬁcient (Brown et al., 1993) as it assigns non-
zero probability mass to any operation sequence,
not only those with derivation in G.

We further note that subword-based OSNMT
can potentially represent any alignment to any tar-
get sentence as long as the alignment does not
violate the 1:n restriction. This is in contrast to
phrase-based SMT where reference translations
often do not have a derivation in the SMT system
due to coverage problems (Auli et al., 2009).

JMP FWD,

JMP BWD,

2.3 Comparison to the OSM for SMT
(POP SRC,
Our OSNMT set of operations
SET MARKER,
and
INSERT(t)) is inspired by the original OSM
for SMT (Durrani et al., 2011) as it also repre-
sents the translation process as linear sequence
of operations. However,
there are signiﬁcant
differences which make OSNMT more suitable
for neural models. First, OSNMT is monotone on
the source side, and allows jumps on the target
side. SMT-OSM operations jump in the source
sentence. We argue that source side monotonicity
potentially mitigates coverage issues of neural
models (over- and under-translation (Tu et al.,
2016)) as the attention can learn to scan the
source sentence from left to right. Another major
difference is that we use markers rather than gaps,
and do not close a gap/marker after jumping to it.
This is an implication of OSNMT jumps being
deﬁned on the target side since the size of a span
is unknown at inference time.

178ops.extend(JMP BWD.repeat(−d))

ops.extend(JMP FWD.repeat(d))

end if
if d > 0 then

for all j ∈ {j|aj = i} do
hole idx ← holes.ﬁnd(j)
d ← hole idx − head
if d < 0 then

Algorithm 1 Align2OSNMT(a, x, y)
1: holes ← {(0,∞)}
2: ops ← (cid:104)(cid:105) {Initialize with empty list}
3: head ← 0
4: for i ← 1 to |x| do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end for
26: return ops

holes.append((s, j − 1))
head ← head + 1
ops.append(SET MARKER)

end if
head ← hole idx
(s, t) ← holes[head]
if s (cid:54)= j then

end if
ops.append(yj)
holes[head] ← (j + 1, t)

end for
ops.append(SRC POP)

3 Training

We train our Transformer model as usual by
minimising the negative log-likelihood of the tar-
get sequence. However, in contrast to plain text
NMT, the target sequence is not a plain sequence
of subword or word tokens but a sequence of op-
erations. Consequently, we need to map the target
sentences in the training corpus to OSNMT repre-
sentations. We ﬁrst run a statistical word aligner
like Giza++ (Och and Ney, 2003) to obtain an
aligned training corpus. We delete all alignment
links which violate the 1:n restriction of OSNMT
(cf. Sec. 2). The alignments together with the tar-
get sentences are then used to generate the refer-
ence operation sequences for training. The algo-
rithm for this conversion is shown in Alg. 1.1 Note
that an operation sequence represents one speciﬁc
alignment, which means that the only way for an
OSNMT sequence to be generated correctly is if

1A Python implementation is available at https:

//github.com/fstahlberg/ucam-scripts/
blob/master/t2t/align2osm.py.

Corpus Language pair
Scielo
Spanish-English
Portuguese-English
Scielo
WAT
Japanese-English

# Sentences
587K
513K
1M

Table 3: Training set sizes.

both the word alignment and the target sentence
are also correct. Thereby, the neural model learns
to align and translate at the same time. However,
there is spurious ambiguity as one alignment can
be represented by different OSNMT sequences.
For instance, simply adding a SET MARKER op-
eration at the end of an OSNMT sequence does
not change the alignment represented by it.

4 Results
We evaluate on three language pairs: Japanese-
English (ja-en), Spanish-English (es-en), and
Portuguese-English (pt-en). We use the ASPEC
corpus (Nakazawa et al., 2016) for ja-en and the
health science portion of the Scielo corpus (Neves
and N´ev´eol, 2016) for es-en and pt-en. Train-
ing set sizes are summarized in Tab. 3. We
use byte pair encoding (Sennrich et al., 2016)
with 32K merge operations for all systems (joint
encoding models for es-en and pt-en and sepa-
rate source/target models for ja-en). We trained
Transformer models (Vaswani et al., 2017)2 un-
til convergence (250K steps for plain text, 350K
steps for OSNMT) on a single GPU using Ten-
sor2Tensor (Vaswani et al., 2018) after removing
sentences with more than 250 tokens. Batches
contain around 4K source and 4K target tokens.
Transformer training is very sensitive to the batch
size and the number of GPUs (Popel and Bojar,
2018). Therefore, we delay SGD updates (Saun-
ders et al., 2018) to every 8 steps to simulate 8
GPU training as recommended by Vaswani et al.
(2017). Based on the performance on the ja-en dev
set we decode the plain text systems with a beam
size of 4 and OSNMT with a beam size of 8 using
our SGNMT decoder (Stahlberg et al., 2017). We
use length normalization for ja-en but not for es-
en or pt-en. We report cased multi-bleu.pl
BLEU scores on the tokenized text to be compara-
ble with the WAT evaluation campaign on ja-en.3.

2We follow the transformer base conﬁguration and
use 6 layers, 512 hidden units, and 8 attention heads in both
the encoder and decoder.

3http://lotus.kuee.kyoto-u.ac.jp/WAT/

evaluation/list.php?t=2&o=4

179Method
Align on subword level
Convert word level alignments

es-en
36.7
37.1

pt-en
38.1
38.4

BLEU

Representation
Plain
OSNMT

es-en

37.6
37.1

BLEU

pt-en

37.5
38.4

ja-en

dev
28.3
28.1

test
28.1
28.8

Table 4: Generating training alignments on the
subword level.

Type
Valid
Not enough SRC POP
Too many SRC POP
Write head out of range

Frequency
92.49%
7.28%
0.22%
0.06%

Table 5: Frequency of invalid OSNMT sequences
produced by an unconstrained decoder on the ja-
en test set.

Generating training alignments As outlined in
Sec. 3 we use Giza++ (Och and Ney, 2003) to gen-
erate alignments for training OSNMT. We experi-
mented with two different methods to obtain align-
ments on the subword level. First, Giza++ can di-
rectly align the source-side subword sequences to
target-side subword sequences. Alternatively, we
can run Giza++ on the word level, and convert the
word alignments to subword alignments in a post-
processing step by linking subwords if the words
they belong to are aligned with each other. Tab. 4
compares both methods and shows that converting
word alignments is marginally better. Thus, we
use this method in all other experiments.

Constrained beam search Unconstrained neu-
ral decoding can yield invalid OSNMT sequences.
For example, the JMP FWD and JMP BWD opera-
tions are undeﬁned if the write head is currently
at the position of the last or ﬁrst marker, respec-
tively. The number of SRC POP operations must
be equal to the number of source tokens in or-
der for the read head to scan the entire source
sentence. Therefore, we constrain these opera-
tions during decoding. We have implemented the
constraints in our publicly available SGNMT de-
coding platform (Stahlberg et al., 2017). How-
ever, these constraints are only needed for a small
fraction of the sentences. Tab. 5 shows that even
unconstrained decoding yields valid OSNMT se-
quences in 92.49% of the cases.

Comparison with plain text NMT Tab. 6 com-
pares our OSNMT systems with standard plain
text models on all three language pairs. OSNMT
performs better on the pt-en and ja-en test sets, but

Table 6:
Comparison between plain text and
OSNMT on Spanish-English (es-en), Portuguese-
English (pt-en), and Japanese-English (ja-en).

slightly worse on es-en. We think that more engi-
neering work such as optimizing the set of oper-
ations or improving the training alignments could
lead to more consistent gains from using OSNMT.
However, we leave this to future work since the
main motivation for this paper is explainable NMT
and not primarily improving translation quality.
Alignment quality Tab. 7 contains example
translations and subword-alignments generated
from our Portuguese-English OSNMT model.
Alignment links from source words consisting of
multiple subwords are mapped to the ﬁnal sub-
word, visible for the words ‘temperamento’ in
the ﬁrst example and ‘pennisetum’ in the second
one. The length of the operation sequences in-
creases with alignment complexity as operation
sequences for monotone alignments consist only
of INSERT(t) and SRC POP operations (example
1). However, even complex mappings are cap-
tured very well by OSNMT as demonstrated by the
third example. Note that OSNMT can represent
long-range reorderings very efﬁciently: the move-
ment from ‘para’ in the ﬁrst position to ‘to’ in the
tenth position is simply achieved by starting the
operation sequence with ‘SET MARKER to’ and a
JMP BWD operation later. The ﬁrst example in par-
ticular demonstrates the usefulness of such align-
ments as the wrong lexical choice (‘abroad’ rather
than ‘body shape’) can be traced back to the source
word ‘exterior‘.

For a qualitative assessment of the alignments
produced by OSNMT we ran Giza++ to align the
generated translations to the source sentences, en-
forced the 1:n restriction of OSNMT, and used
the resulting alignments as reference for comput-
ing the alignment error rate (Och and Ney, 2003,
AER). Fig. 3 shows that as training proceeds, OS-
NMT learns to both produce high quality transla-
tions (increasing BLEU score) and accurate align-
ments (decreasing AER).

As mentioned in the introduction, a light-weight
way to extract 1:n alignments from a vanilla atten-

180Operation sequence: SRC POP ab road SRC POP as SRC POP an indicator SRC POP of SRC POP performance
SRC POP and SRC POP SRC POP temper ament SRC POP
Reference: the body shape as an indicative of performance and temperament

Operation sequence: behavior SRC POP of SRC POP SET MARKER clones SRC POP SRC POP SRC POP
SRC POP SRC POP JMP BWD pen n is et um SRC POP JMP FWD subjected SRC POP to SRC POP SET MARKER
periods SRC POP SRC POP JMP BWD SET MARKER restriction SRC POP JMP BWD SET MARKER water
SRC POP JMP BWD controlled SRC POP
Reference: response of pennisetum clons to periods of controlled hidric restriction

Operation sequence: SET MARKER to SRC POP analyze SRC POP these SRC POP data SRC POP JMP BWD
SET MARKER should be SRC POP used SRC POP JMP BWD SET MARKER methodologies SRC POP¿ JMP BWD
appropriate SRC POP JMP FWD JMP FWD JMP FWD . SRC POP
Reference: to analyze these data suitable methods should be used .

Table 7: Examples of Portuguese-English translations together with their (subword-)alignments induced
by the operation sequence. Alignment links from source words consisting of multiple subwords were
mapped to the ﬁnal subword in the training data, visible for ‘temperamento’ and ‘pennisetum’.

Representation Alignment extraction
LSTM forced decoding
Plain
LSTM forced decoding with supervised attention (Liu et al., 2016, Cross Entropy loss)
Plain
OSNMT
OSNMT

AER (in %)
dev
test
63.7
63.9
54.7
54.9
24.2
21.5

Table 8: Comparison between OSNMT and using the attention matrix from forced decoding with a
recurrent model.

tional LSTM-based seq2seq model is to take the
maximum over attention weights for each target
token. This is possible because, unlike the Trans-
former, LSTM-based models usually only have
a single soft attention matrix. However, in our
experiments, LSTM-based NMT was more than
4.5 BLEU points worse than the Transformer on
Japanese-English. Therefore, to compare AERs
under comparable BLEU scores, we used the
LSTM-based models in forced decoding mode on
the output of our plain text Transformer model
from Tab. 6. We trained two different LSTM mod-
els: one standard model by optimizing the like-

Figure 3: AER and BLEU training curves for OS-
NMT on the Japanese-English dev set.

181  0  5 10 15 20 25 30 50K100K150K200K250K 20% 30% 40% 50% 60% 70% 80% 90%100%Dev set BLEUAlignment Error RateTraining iterations BLEUAER(a) Layer 4, head 1; attending to the source side read
head.

(b) Layer 2, head 3; attending to the right trigram context
of the read head.

Figure 4: Encoder-decoder attention weights.

lihood of the training set, and a second one with
supervised attention following Liu et al. (2016).
Tab. 8 shows that the supervised attention loss
of Liu et al. (2016) improves the AER of the
LSTM model. However, OSNMT is able to pro-
duce much better alignments since it generates the
alignment along with the translation in a single de-
coding run.

OSNMT sequences contain target words in
source sentence order An OSNMT sequence
can be seen as a sequence of target words in
source sentence order, interspersed with instruc-
tions on how to put them together to form a ﬂu-
ent target sentence.
if we strip
out all SRC POP, SET MARKER, JMP FWD, and
JMP BWD operations in the OSNMT sequence in
the second example of Tab. 7 we get:

For example,

behavior of clones pennisetum sub-
jected to periods restriction water con-
trolled

The word-by-word translation back to Por-

tugese is:

comportamento de clones pennisetum
submetidos a per´ıodos restric¸˜ao h´ıdrica
controlada

This restores the original source sentence (cf.
Tab. 7) up to unaligned source words. There-
fore, we can view the operations for control-
ling the write head (SET MARKER, JMP FWD, and
JMP BWD) as reordering instructions for the target
words which appear in source sentence word order
within the OSNMT sequence.

Role of multi-head attention In this paper, we
use a standard seq2seq model (the Transformer ar-
chitecture (Vaswani et al., 2017)) to generate OS-
NMT sequences from the source sentence. This
means that our neural model is representation-
agnostic: we do not explicitly incorporate the no-
tion of read and write heads into the neural archi-
tecture. In particular, neither in training nor in de-
coding do we explicitly bias the Transformer’s at-
tention layers towards consistency with the align-
ment represented by the OSNMT sequence. Our
Transformer model has 48 encoder-decoder atten-
tion matrices due to multi-head attention (8 heads
in each of the 6 layers). We have found that many
of these attention matrices have strong and in-
terpretable links to the translation process repre-
sented by the OSNMT sequence. For example,
Fig. 4a shows that the ﬁrst head in layer 4 follows
the source-side read head position very closely:
at each SRC POP operation the attention shifts by

182one to the next source token. Other attention heads
have learned to take other responsibilities. For in-
stance, head 3 in layer 2 (Fig. 4b) attends to the
trigram right of the source head.

5 Related Work

Explainable and interpretable machine learning is
attracting more and more attention in the research
community (Ribeiro et al., 2016; Doshi-Velez and
Kim, 2017), particularly in the context of natu-
ral language processing (Karpathy et al., 2015; Li
et al., 2016; Alvarez-Melis and Jaakkola, 2017;
Ding et al., 2017; Feng et al., 2018). These ap-
proaches aim to explain (the predictions of) an ex-
isting model.
In contrast, we change the target
representation such that the generated sequences
themselves convey important information about
the translation process such as the word align-
ments.

Despite considerable consensus about the im-
portance of word alignments in practice (Koehn
and Knowles, 2017), e.g. to enforce constraints
on the output (Hasler et al., 2018) or to preserve
text formatting, introducing explicit alignment in-
formation to NMT is still an open research prob-
lem. Word alignments have been used as supervi-
sion signal for the NMT attention model (Mi et al.,
2016; Chen et al., 2016; Liu et al., 2016; Alkhouli
and Ney, 2017). Cohn et al. (2016) showed how to
reintroduce concepts known from traditional sta-
tistical alignment models (Brown et al., 1993) like
fertility and agreement over translation direction
to NMT. Some approaches to simultaneous trans-
lation explicitly control for reading source tokens
and writing target tokens and thereby generate
monotonic alignments on the segment level (Yu
et al., 2016, 2017; Gu et al., 2017). Alkhouli et al.
(2016) used separate alignment and lexical models
and thus were able to hypothesize explicit align-
ment links during decoding. While our motivation
is very similar to Alkhouli et al. (2016), our ap-
proach is very different as we represent the align-
ment as operation sequence, and we do not use
separate models for reordering and lexical trans-
lation.

The operation sequence model for SMT (Dur-
rani et al., 2011, 2015) has been used in a number
of MT evaluation systems (Durrani et al., 2014;
Peter et al., 2016; Durrani et al., 2016) and for
post-editing (Pal et al., 2016), often in combina-
tion with a phrase-based model. The main differ-

ence to our OSNMT is that we have adapted the
set of operations for neural models and are able to
use it as stand-alone system, and not on top of a
phrase-based system.

Our operation sequence model has some simi-
larities with transition-based models used in other
areas of NLP (Stenetorp, 2013; Dyer et al., 2015;
Aharoni and Goldberg, 2017). In particular, our
POP SRC operation is very similar to the step ac-
tion of the hard alignment model of Aharoni and
Goldberg (2017). However, Aharoni and Gold-
berg (2017) investigated monotonic alignments for
morphological inﬂections whereas we use a larger
operation/action set to model complex word re-
orderings in machine translation.

6 Conclusion
We have presented a way to use standard seq2seq
models to generate a translation together with
an alignment as linear sequence of operations.
This greatly improves the interpretability of the
model output as it establishes explicit alignment
links between source and target tokens. However,
the neural architecture we used in this paper is
representation-agnostic, i.e. we did not explicitly
incorporate the alignments induced by an opera-
tion sequence into the neural model. For future
work we are planning to adapt the Transformer
model, for example by using positional embed-
dings of the source read head and the target write
head in the Transformer attention layers.

Acknowledgments
This work was supported in part by the U.K. En-
gineering and Physical Sciences Research Council
(EPSRC grant EP/L027623/1). We thank Joanna
Stadnik who produced the recurrent translation
and alignment models during her 4th year project.

References
Roee Aharoni and Yoav Goldberg. 2017. Morphologi-
cal inﬂection generation with hard monotonic atten-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 2004–2015, Vancouver,
Canada. Association for Computational Linguistics.

Tamer Alkhouli, Gabriel Bretschner, Jan-Thorsten Pe-
ter, Mohammed Hethnawi, Andreas Guta, and Her-
mann Ney. 2016. Alignment-based neural machine
translation. In Proceedings of the First Conference
on Machine Translation, pages 54–65, Berlin, Ger-
many. Association for Computational Linguistics.

183Tamer Alkhouli and Hermann Ney. 2017. Biasing
attention-based recurrent neural networks using ex-
ternal alignment information. In Proceedings of the
Second Conference on Machine Translation, pages
108–117, Copenhagen, Denmark. Association for
Computational Linguistics.

David Alvarez-Melis and Tommi Jaakkola. 2017. A
causal framework for explaining the predictions of
In Pro-
black-box sequence-to-sequence models.
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 412–
421. Association for Computational Linguistics.

Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
224–232. Association for Computational Linguis-
tics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In ICLR, Toulon,
learning to align and translate.
France.

Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
Computational Linguistics,
rameter estimation.
19(2).

Wenhu Chen, Evgeny Matusov, Shahram Khadivi,
and Jan-Thorsten Peter. 2016. Guided alignment
training for topic-aware neural machine translation.
arXiv preprint arXiv:1607.01628.

Yong Cheng, Shiqi Shen, Zhongjun He, Wei He,
Hua Wu, Maosong Sun, and Yang Liu. 2016.
Agreement-based joint
training for bidirectional
attention-based neural machine translation. In Pro-
the Twenty-Fifth International Joint
ceedings of
Conference on Artiﬁcial Intelligence,
IJCAI’16,
pages 2761–2767. AAAI Press.

David Chiang. 2005. A hierarchical phrase-based
In Pro-
model for statistical machine translation.
ceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
pages 263–270, Ann Arbor, Michigan. Association
for Computational Linguistics.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016.
Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 876–885, San Diego, California. Association
for Computational Linguistics.

Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Visualizing and understanding neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers), pages 1150–
1159. Association for Computational Linguistics.

Finale Doshi-Velez and Been Kim. 2017. Towards a
rigorous science of interpretable machine learning.

Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and
Stephan Vogel. 2016. QCRI machine translation
systems for IWSLT 16. In International Workshop
on Spoken Language Translation. Seattle, WA, USA.

Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heaﬁeld. 2014. Edinburgh’s phrase-based
machine translation systems for WMT-14. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, pages 97–104, Baltimore, Mary-
land, USA. Association for Computational Linguis-
tics.

Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045–1054, Portland, Oregon, USA. Association for
Computational Linguistics.

Nadir Durrani, Helmut Schmid, Alexander Fraser,
Philipp Koehn, and Hinrich Sch¨utze. 2015. The op-
eration sequence model—combining n-gram-based
and phrase-based statistical machine translation.
Computational Linguistics, 41(2):157–186.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
In Proceedings of the 53rd Annual
term memory.
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343, Beijing, China. Associa-
tion for Computational Linguistics.

Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,
Pedro Rodriguez, and Jordan Boyd-Graber. 2018.
Pathologies of neural models make interpretations
difﬁcult. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. ArXiv e-prints.

Hamidreza Ghader and Christof Monz. 2017. What
does attention in neural machine translation pay at-
In Proceedings of the Eighth Interna-
tention to?
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 30–39.
Asian Federation of Natural Language Processing.

Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O.K. Li. 2017. Learning to translate in real-time
with neural machine translation. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume

1841, Long Papers, pages 1053–1062. Association for
Computational Linguistics.

Eva Hasler, Adri`a de Gispert, Gonzalo Iglesias, and
Bill Byrne. 2018. Neural machine translation de-
In Proceed-
coding with terminology constraints.
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. Associa-
tion for Computational Linguistics.

Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.
Visualizing and understanding recurrent networks.
arXiv preprint arXiv:1506.02078.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
In Pro-
lenges for neural machine translation.
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39. Association for Compu-
tational Linguistics.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016. Visualizing and understanding neural models
in NLP. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 681–691. Association for Computa-
tional Linguistics.

Lemao Liu, Masao Utiyama, Andrew Finch, and Ei-
ichiro Sumita. 2016. Neural machine translation
with supervised attention. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers, pages
3093–3102, Osaka, Japan. The COLING 2016 Or-
ganizing Committee.

I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.

I. Dan Melamed, Giorgio Satta, and Benjamin Welling-
ton. 2004. Generalized multitext grammars. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-04).

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Supervised attentions for neural machine translation.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2283–2288, Austin, Texas. Association for Compu-
tational Linguistics.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. ASPEC:
In LREC,
Asian scientiﬁc paper excerpt corpus.
pages 2204–2208, Portoroz, Slovenia.

Mariana L Neves and Aur´elie N´ev´eol. 2016. The Sci-
elo corpus: a parallel corpus of scientiﬁc publica-
tions for biomedicine. In LREC, pages 2942–2948,
Portoroz, Slovenia.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.

Santanu Pal, Marcos Zampieri, and Josef van Genabith.
2016. USAAR: An operation sequential model for
automatic statistical post-editing. In Proceedings of
the First Conference on Machine Translation, pages
759–763, Berlin, Germany. Association for Compu-
tational Linguistics.

Jan-Thorsten Peter, Andreas Guta, Nick Rossenbach,
Miguel Grac¸a, and Hermann Ney. 2016. The RWTH
Aachen machine translation system for IWSLT
In International Workshop on Spoken Lan-
2016.
guage Translation. Seattle, WA, USA.

Martin Popel and Ondˇrej Bojar. 2018.

tips for the transformer model.
arXiv:1804.00247.

Training
arXiv preprint

Marco Ribeiro, Sameer Singh, and Carlos Guestrin.
2016. “Why should I trust you?”: Explaining the
In Proceedings of
predictions of any classiﬁer.
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Demonstrations, pages 97–101. Association for
Computational Linguistics.

Danielle Saunders, Felix Stahlberg, Adri`a de Gispert,
and Bill Byrne. 2018. Multi-representation en-
sembles and delayed SGD updates improve syntax-
In Proceedings of the 56th Annual
based NMT.
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725. Association for Computational Linguistics.

Felix Stahlberg, Eva Hasler, Danielle Saunders, and
Bill Byrne. 2017. SGNMT – A ﬂexible NMT de-
coding platform for quick prototyping of new mod-
In Proceedings of the
els and search strategies.
2017 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations,
pages 25–30. Association for Computational Lin-
guistics. Full documentation available at http:
//ucam-smt.github.io/sgnmt/html/.

Pontus Stenetorp. 2013. Transition-based dependency
In NIPS

parsing using recursive neural networks.
Workshop on Deep Learning. Citeseer.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works.
In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

185Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 76–85.
Association for Computational Linguistics.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan N Gomez, Stephan Gouws,
Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki
Parmar, et al. 2018. Tensor2tensor for neural ma-
chine translation. arXiv preprint arXiv:1803.07416.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 6000–6010. Curran Asso-
ciates, Inc.

L Yu, P Blunsom, C Dyer, E Grefenstette, and T Ko-
cisky. 2017. The neural noisy channel. In Proceed-
ings of the 5th International Conference on Learning
Representations (ICLR), Toulon, France. Computa-
tional and Biological Learning Society.

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-
In Proceed-
ment to segment neural transduction.
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1307–1316,
Austin, Texas. Association for Computational Lin-
guistics.

186
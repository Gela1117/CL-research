Data Augmentation for Neural Online Chat Response Selection

Wenchao Du

Language Technology Institute

Carnegie Mellon University

Pittsburgh, PA 15213

wenchaod@cs.cmu.edu

Alan W Black

Language Technology Institute

Carnegie Mellon University

Pittsburgh, PA 15213
awb@cs.cmu.edu

Abstract

Data augmentation seeks to manipulate the
available data for training to improve the gen-
eralization ability of models. We investigate
two data augmentation proxies, permutation
and ﬂipping, for neural dialog response se-
lection task on various models over multiple
datasets, including both Chinese and English
languages. Different from standard data aug-
mentation techniques, our method combines
the original and synthesized data for predic-
tion. Empirical results show that our approach
can gain 1 to 3 recall-at-1 points over baseline
models in both full-scale and small-scale set-
tings.
Introduction

1
Building machines that are capable of conversing
like humans is one of the primary goals of artiﬁcial
intelligence. Extensive manual labor is typically
required by traditional rule-based systems, limit-
ing the scalability of such systems across multiple
domains. With the success of machine learning,
the quest of building data-driven dialog systems
has come into focus over the past few years (Ritter
et al., 2011). Existing approaches in this area can
be categorized into generation-based methods and
retrieval-based methods. While generation-based
methods are still far from reliably generating in-
formative responses, retrieval-based methods have
the advantage of ﬂuency and groundedness, since
they select responses from existing data. We con-
centrate on retrieval-based methods in this paper,
though we believe the proposed techniques could
also improve generation-based models.

While current state-of-the-art results for dialog
models are achieved by deep learning approaches,
the performance of neural models largely depends
on the amount of training data. However, acquir-
ing conversational data can be difﬁcult at times.
On the other hand, even with thousands of data

points, it is unclear whether these models can op-
timally beneﬁt from them. Therefore, data aug-
mentation and its efﬁcient use becomes an im-
portant problem. Our main contribution is that
we investigated new ways to manipulate chat data
and neural model architectures to improve perfor-
mance. To our knowledge, we are the ﬁrst to eval-
uate data augmentation on different types of neural
conversation models over multiple domains and
languages.
2 Data Augmentation
Recent studies (Adi et al., 2016; Khandelwal et al.,
2018) have shown that recurrent neural networks
(RNN), especially long-short term memory net-
works (LSTM), are sensitive to word order when
encoding contextual information. However, for
the response selection task, it is so far unclear to
what extent word order is important. This problem
is perplexed by the following language phenom-
ena we observed from existing chat data:

1. Broken continuity. Simultaneous conversa-
tions happen in multi-party dialogs (Elsner
and Charniak, 2008) very often, resulting in
some utterances not responding to their im-
mediately preceding ones. Even in conver-
sations between only two people, continuity
may still break due to one person switch topic
before the other responds. See Table 1 for ex-
amples.

2. Mixed turn-taking behavior. People can give
multiple utterances before the other respond.
Usually,
these consecutive messages from
same person form arguments that are in par-
allel (by ’argument’ we mean text spans that
form discourse relations with each other),
and their orderings are not that important. We
found this to be very common in online live
chats. See Table 2 for examples.

Proceedingsofthe2018EMNLPWorkshopSCAI:The2ndInt’lWorkshoponSearch-OrientedConversationalAI,pages52–58Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticsISBN978-1-948087-75-952Example 1:

Old

Kuja
Taru
Burner
Kuja

I dont run graphical ubuntu,
I run ubuntu server.
Haha sucker.
?
you can use ”ps ax” and ”kill (PID#)”
Anyways, you made the changes
right?

Example 2:

Customer 在(there) 吗(?)
Customer 看看(look at) 此(this) 款(one)
Agent
Agent

在的(I’m here) 亲(dear)
亲(dear)，请(please) 发(send)
链接(link)

Table 1: Example chat snippets for broken continuity.
The ﬁrst example is from (Lowe et al., 2015). Burner’s
message is responding to Old, and Kuja’s last mes-
sage is replying to Taru. The second example is from
Taobao, where the third message is responding to the
ﬁrst message, and the fourth message to the second
message.

Example 1:

Customer A 这(this) 款(one) 我(I) 穿(wear)

什么(what) 码(size)

Customer A 160高(tall)，107 斤(0.5kg)

重(heavy)
亲(dear) 如果(if) 喜欢(like)
宽松(loose) 点的就(then)
可以(can) 选(choose) L 哦

Agent

Example 2:

Customer B 158cm
Customer B 63kg
Customer B 穿(wear) 什么(what) 码(size)

Agent

的合适(ﬁt)
亲(dear) 根据(based on)
亲的(your) 数据(data)，
建议(suggest) 穿(wear)
L 码(size)

Example:

Wizard

Sorry, I cannot ﬁnd any trips
leaving from Gotham City. Could
you suggest another nearby
departure city?

Customer Would any packages to Mos Eisley

Wizard

be available, if I increase my
budget to $2500?
There are no trips available to
Mos Eisley.

Table 3: Example chat snippets from Frames. The ﬁrst
message has two sentences. The second message is a
conditional complex sentence.

Example 2 of Table 1 after Permutation:

在的(I’m here) 亲(dear)

Customer 在(there) 吗(?)
Agent
Customer 看看(look at) 此(this) 款(one)
亲(dear)，请(please) 发(send)
Agent
链接(link)

Example 1 of Table 2 after Permutation:

Customer A 160高(tall)，107 斤(0.5kg)

重(heavy)

Customer A 这(this) 款(one) 我(I) 穿(wear)

Agent

什么(what) 码(size)
亲(dear) 如果(if) 喜欢(like)
宽松(loose) 点的就(then)
可以(can) 选(choose) L 哦

Example of Table 3 after Flipping:

Wizard

Customer

Wizard

Could you suggest another nearby
departure city? Sorry, I cannot ﬁnd
any trips leaving from Gotham City.
if I increase my budget to $2500,
Would any packages to Mos Eisley
be available?
There are no trips available to Mos
Eisley.

Table 2: Example chat snippets for mixed turn-taking
from Taobao. The question for recommendation and its
relevant information (height and weight) can be com-
municated through different number of utterances in
arbitrary order.

Table 4: Results of proposed transformations on pre-
vious examples. In the ﬁrst and second examples, the
two messages right before the last agent’s response are
permuted.
In the third example, the ﬁrst message is
ﬂipped, splitting at the period; the second messages is
separated at the comma and ﬂipped.

533. Long utterances. Some utterances contain
multiple sentences. Some are single com-
pound sentence with multiple clauses. See
Table 3 for examples.

To summarize, the critical information for re-
sponding, which can be either a single word,
phrase, or a full sentence, may have varying rel-
ative positions in the context. Therefore, we hy-
pothesize that there exist alternative orderings of
utterances and intra-utterance arguments in chat
data that can help selecting responses, given recur-
rent neural models’ sensitivity to word order. In
this paper, our main goal is to seek improvement
by creating variations in the ordering of utterances
and arguments. We aim for generic methods, by-
passing the need of discourse and syntactic parsing
as an intermediate step. With the fact that online
chats are typically noisy with spelling errors and
ungrammaticality, a relative lack of precision may
actually help. We therefore propose the following
ways to manipulate chat data:

Permutation is simply reversing the order of
any two messages in the context. This may help
recover the continuity or create alternative order-
ing of parallel arguments.

Flipping breaks an utterance into two parts, and
concatenate them in their reversed order. The
break point is the punctuation that is closest to the
middle of the utterance if there is any. Otherwise,
we break the utterance at the middle.

As illustrated in Table 4, the proposed transfor-
mations neither change the implication of the con-
texts nor the appropriateness of the responses.

3 Data
We describe four datasets that we will be using to
evaluate our proposed methods:

Taobao chat log was collected by a vendor of
pajamas between 2013 and 2015. The conver-
sations took place on Taobao, one of the largest
Chinese e-commerce websites. The website al-
lows two-way conversations between customers
and agents in individual sessions.

Ubuntu dialog corpus (Lowe et al., 2015) is the
ﬁrst large dataset of online chats made available.
It contains multi-party chat logs from Ubuntu chat
room where people help each other to solve tech-
nical problems related to Ubuntu.

Douban conversation corpus is a collection of
web forum post discussions from Douban, a Chi-
nese internet community (Wu et al., 2016). It cov-

ers a wide range of topics, hence open-domain in
nature.

Frames dataset was collected by (Asri et al.,
2017) in wizard-of-oz setting. The chats are about
booking ﬂight. The wizard has access to database
to answer domain-speciﬁc questions. Unlike the
datasets mentioned above, the conversations of
Frames are highly controlled so that the language
is perfect and the chats have perfect turn ex-
changes.

4 Methodology

4.1 Model Overview
We ﬁrst give a high level abstraction of the neu-
ral models we will be investigating. Given context
and candidate responses, the models score each
candidate and the one with the highest score is se-
lected. The models are trained by maximizing the
likelihood of labels. To build training data, one
negative example is sampled from the corpus for
each pair of context and true response. We group
the models into the following two categories:

Dual-Encoder Model (DE) As ﬁrst proposed
in (Lowe et al., 2015), DE models encode context
m and response r into v(m) ∈ Rl, v(r) ∈ Rm,
respectively. Then

P (r | m) = σ(v(m)T M v(r))

where σ is the sigmoid function, M ∈ Rl×m. In
this paper, response encoder is LSTM. We con-
sider two choices of context encoder: one is word-
level LSTM encoder only (LSTM-DE), which
takes concatenated messages as input. The other
one is hierarchical recurrent encoder (HRE-DE).
For HRE, we encode each message with an LSTM
word-level encoder, and then feed the last hidden
states from the word-level encoder to an utterance-
level encoder, which is also an LSTM. We con-
catenate the last hidden state of the utterance-level
encoder to that of word-level encoder on concate-
nated messages as ﬁnal context encoding. Note
that HRE-DE is a simpliﬁed version of the model
in (Zhou et al., 2016).

Sequential Matching Network (SMN) Unlike
DE models, SMN ﬁnds the afﬁnity between con-
text messages and responses as a ﬁrst step (Wu
et al., 2016). Given messages mk where k =
1, ..., n and response r, SMN ﬁrst extract feature
u(mk, r) ∈ Rp of how related the two utterances
are, and then accumulate these features with an

54Language Medium
Ubuntu
English
Taobao
Chinese
Douban Chinese
Frames
English

Domain Size (Train) Vocabulary
Style
Task
Chat Room Noisy
Task
Chat Room Noisy
Web Forum Noisy
Open
Chat Room Controlled Task

1M
0.9M
1M
11k

400k
90k
300k
9k

Table 5: Comparison of four dialog corpora

(cid:88)

i,j

(cid:88)

i,j

RNN:

v(m, r) = RN N (u(mk, r)), k = 1, ..., n

P (r | m) = σ(wT v(m, r))

where v(m, r), w ∈ Rq.
4.2 Combining Transformed Data
Let πi be the applicable transformations including
the identity. For context m and response r, let
mi = πi(m), rj = πj(r). For DE models, we
use the same encoder for m, r to encode mi, rj.
Then we combine the encodings and predict by

P (r | m) = σ(

v(mi)T Mijv(rj))

where Mij ∈ Rl×m. Similarly, for SMN, the pre-
dicted score is

P (r | m) = σ(

wT

i,jv(mi, rj))

where wi,j ∈ Rq. Please note that this score func-
tion allows augmentations to be done at test time
for prediction. Additionally, we inject squared dis-
tance between the encodings of the original data
and the transformed data in order to enforce mod-
els to learn similar representations for them. We
are assuming that the transformation should not
drastically change the meanings of contexts and
responses even though they are not exactly label-
preserving. Empirically we found adding this reg-
ularization term actually helps. The training loss
for DE models becomes

(− log P (r | m) + t(

(cid:107)v(mi) − v(m)(cid:107)2

(cid:88)

(cid:88)

(m,r)

+

i

(cid:107)v(rj) − v(r)(cid:107)2)

(cid:88)

j

(cid:88)

(cid:88)

and the one for SMN becomes

(− log P (r | m)+t(

(cid:107)v(mi, rj)−v(m, j)(cid:107)2)

(m,r)

i,j

where t is a hyper-parameter. We tuned it on the
validation set in [0.01, 0.1].

5 Experiments
5.1 Setup
We evaluate our method on the datasets mentioned
in Section 3. For the Ubuntu dataset, we use the
version shared by (Xu et al., 2016). For Douban,
we discard the test set provided by the authors
since the responses are not from the same domain,
and re-split training set. Negative responses are
randomly sampled. For Frames, we select nega-
tive responses from those that have different slot
types and values from true responses. We also
conduct an experiment with smaller amount of
training data on the three large datasets, Ubuntu,
Douban, and Taobao, in which 1% of the training
set are randomly selected for training. Following
(Lowe et al., 2015), we evaluate the model perfor-
mance with recall-at-1, following previous work.
We experiment with two types of permutation:
the ﬁrst one is permuting the last and the penulti-
mate message in contexts, and the second one is
permuting the penultimate with the third to last
message. We only do the ﬁrst type of permuta-
tion for SMN since SMN seems to be insensitive
to permutation. We ﬂip all messages in contexts
and responses for SMN, and only ﬂip context mes-
sages for DE models.

5.2 Training
We initialize word embeddings using the results
of word2vec (Mikolov et al., 2013) trained on
the whole corpus. The size of word embeddings
is 300 for LSTM-DE and HRE-DE, and 200 for
SMN. For LSTM-DE and HRE-DE, each LSTM
layer has hidden size of 300. We use the same
hyper-parameters for SMN as in (Wu et al., 2016).
All models are trained with Adam optimizer with
learning rate of 0.001. We use early stopping
to choose parameters. For experiments on small
training sets (including Frames), we additionally
apply dropout (Srivastava et al., 2014) with rate
0.5 to all recurrent layers. As a side note, we ﬁnd
that dropout does not affect the result in any sig-
niﬁcant way under full-scale setting.

55LSTM-DE

+ permutation 1
+ permutation 2

+ ﬂipping
HRE-DE

+ permutation 1
+ permutation 2

+ ﬂipping

SMN

+ permutation 1

+ ﬂipping

Ubuntu

Taobao

Douban

100%
0.6546
0.6773
0.6854
0.6853
0.6729
0.6817
0.6786
0.6920
0.7050
0.7066
0.7156

1%

0.3470
0.3723
0.3685
0.3778
0.3654
0.3650
0.3713
0.3688
0.4771
0.4749
0.4893

100%
0.8446
0.8685
0.8693
0.8669
0.8728
0.8732
0.8787
0.8828
0.8194
0.8171
0.8231

1%

0.4862
0.5037
0.5071
0.5201
0.5085
0.5053
0.5207
0.5147
0.5312
0.5302
0.5387

100%
0.6193
0.6402
0.6469
0.6430
0.6443
0.6401
0.6430
0.6542
0.6700
0.6747
0.6800

1%

0.3301
0.3503
0.3444
0.3369
0.3350
0.3423
0.3395
0.3523
0.4662
0.4669
0.4876

Frames
100%
0.3941
0.3973
0.4122
0.4209
0.4436
0.4339
0.4518
0.4564
0.4055
0.4023
0.4116

Table 6: Numbers on recall-at-1. Best results for each dataset and each model are highlighted.

5.3 Main Results

Table 6 shows the performance of LSTM-DE,
HRE-DE, and SMN on 4 different datasets under
different types of augmentation. For each full-
scale dataset, nearly all models gain around 1 to
3 points with one of the proposed data augmenta-
tion methods. Permutation works best for LSTM-
DE, less so for HRE-DE, and has almost no effect
on SMN. This is probably because HRE-DE and
SMN have an utterance-level recurrent component
which makes them better at capturing long range
dependencies. Permutation 1 does not improve on
Frames dataset for any model. This might be that
Frames has perfect turn-taking, and wizards’ re-
sponses are mostly addressing their immediately
preceding messages, so moving away the last mes-
sage in context does not help. In small-scale set-
ting, LSTM-DE with data augmentation outper-
forms HRE-DE on some of the datasets. SMN
gains even more with ﬂipping than in full-scale
setting.

6 Related Work

Data augmentation has been widely adopted
in computer vision and speech recognition
(Krizhevsky et al., 2012; Ko et al., 2015). In im-
age processing, label-preserving transformations
such as tilting and ﬂipping are used, but in NLP,
ﬁnding such transformations that exactly preserve
meanings is difﬁcult. Language data is discrete
in nature, and minor perturbation may change the
meaning. Most commonly used techniques in-
clude word substitution (Fadaee et al., 2017) and
paraphrasing (Dong et al., 2017). These methods

may require heavy external resources, which can
be difﬁcult to apply across multiple languages and
domains.

Recently, there has been a surging interest in ad-
versarial training (Goodfellow et al., 2014). For
text data, one class of methods generate adversar-
ial examples by moving word embeddings along
the opposite direction of the gradient of loss func-
tions (Wu et al., 2017; Yasunaga et al., 2017),
hence small perturbation in the continuous space
of word vectors. Another class of methods aim to
create genuinely new examples. (Li et al., 2017)
adds syntactic and semantic variations to training
data based on grammar rules and thesaurus. (Xie
et al., 2017) add noises to data by blanking out or
substituting words for language modeling. (Yang
et al., 2017) adopt a seq2seq model (Sutskever
et al., 2014) to generate questions based on para-
graphs and answers into their generative adversar-
ial framework. One main difference between these
methods and our approach is that, while adversar-
ial training only manipulates training data, we in
addition apply transformations to data at test time
to help prediction. This is closer to (Dong et al.,
2017) in spirit.

7 Conclusion

We proposed a general method to improve dia-
log response selection through manipulating ex-
isting data that can be applied to different models.
Our results show that for both open-domain and
task-oriented dialogues, and for both English and
Chinese languages, at least one of the proposed
augmentation methods is effective, and the chance
that they hurt is rare. We have deliberately chosen

56a diverse set of domains and models to test this
on to try to understand the contribution of data
augmentation. Thus even when working on new
datasets, and new models, it seems data augmen-
tation is still a valuable addition that will likely
improve results. Being more speciﬁc about when
augmentation works is harder. One future research
direction would be to apply data transformation
situationally based on the discourse structure of
dialogs. In our experiments, we tried combining
permutation and ﬂipping but found no advantage
over using only one type of transformation. We
believe a more sophisticated method of combina-
tion could further improve the results, and leave it
to future work.

References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer
Lavi, and Yoav Goldberg. 2016. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. arXiv preprint arXiv:1608.04207.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: A
corpus for adding memory to goal-oriented dialogue
systems. arXiv preprint arXiv:1704.00057.

Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella
Lapata. 2017. Learning to paraphrase for question
answering. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 875–886.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012.
Imagenet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Yitong Li, Trevor Cohn, and Timothy Baldwin. 2017.
In Pro-
Robust training under linguistic adversity.
ceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 2, Short Papers, volume 2, pages
21–27.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the conference on empirical methods
in natural language processing, pages 583–593. As-
sociation for Computational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Micha Elsner and Eugene Charniak. 2008. You talking
to me? a corpus and algorithm for conversation dis-
entanglement. Proceedings of ACL-08: HLT, pages
834–842.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 1778–1783.

Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), volume 2,
pages 567–573.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. arXiv preprint arXiv:1412.6572.

Urvashi Khandelwal, He He, Peng Qi, and Dan Ju-
rafsky. 2018. Sharp nearby, fuzzy far away: How
neural language models use context. arXiv preprint
arXiv:1805.04623.

Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-
jeev Khudanpur. 2015. Audio augmentation for
speech recognition. In Sixteenth Annual Conference
of the International Speech Communication Associ-
ation.

Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and
Zhoujun Li. 2016. Sequential matching network:
A new architecture for multi-turn response selec-
arXiv preprint
tion in retrieval-based chatbots.
arXiv:1612.01627.

Ziang Xie, Sida I Wang, Jiwei Li, Daniel L´evy, Aiming
Nie, Dan Jurafsky, and Andrew Y Ng. 2017. Data
noising as smoothing in neural network language
models. arXiv preprint arXiv:1703.02573.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie
Sun, and Xiaolong Wang. 2016.
Incorporating
loose-structured knowledge into lstm with recall
arXiv preprint
gate for conversation modeling.
arXiv:1605.05110.

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
arXiv preprint
generative domain-adaptive nets.
arXiv:1702.02206.

57Michihiro Yasunaga,

Jungo Kasai, and Dragomir
Radev. 2017. Robust multilingual part-of-speech
arXiv preprint
tagging via adversarial training.
arXiv:1711.04903.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan. 2016.
Multi-view response selection for human-computer
In Proceedings of the 2016 Confer-
conversation.
ence on Empirical Methods in Natural Language
Processing, pages 372–381.

58
Multi-task learning for Joint Language Understanding and Dialogue State

Tracking

Abhinav Rastogi

Google AI

Mountain View

Raghav Gupta

Google AI

Mountain View

Dilek Hakkani-Tur

Google AI

Mountain View

abhirast@google.com

raghavgupta@google.com

dilek@ieee.org

Abstract

This paper presents a novel approach for
multi-task learning of language under-
standing (LU) and dialogue state tracking
(DST) in task-oriented dialogue systems.
Multi-task training enables the sharing of
the neural network layers responsible for
encoding the user utterance for both LU
and DST and improves performance while
reducing the number of network parame-
ters. In our proposed framework, DST op-
erates on a set of candidate values for each
slot that has been mentioned so far. These
candidate sets are generated using LU slot
annotations for the current user utterance,
dialogue acts corresponding to the preced-
ing system utterance and the dialogue state
estimated for the previous turn, enabling
DST to handle slots with a large or un-
bounded set of possible values and deal
with slot values not seen during training.
Furthermore, to bridge the gap between
training and inference, we investigate the
use of scheduled sampling on LU output
for the current user utterance as well as the
DST output for the preceding turn.

Introduction

1
Task-oriented dialogue systems interact with users
in natural language to accomplish tasks they have
in mind, by providing a natural language interface
to a backend (API, database or service). State of
the art approaches to task-oriented dialogue sys-
tems typically consist of a language understand-
ing (LU) component, which estimates the seman-
tic parse of each user utterance and a dialogue
state tracking (DST) or belief tracking component,
which keeps track of the conversation context and
the dialogue state (DS). Typically, DST uses the

System: Hello! How can I help?
Acts:
User:
Intent:
Acts:
State:

greeting
Hello, book me a table for two at Cascal.
RESERVE RESTAURANT
greeting, inform(#people), inform(restaurant)
restaurant=Cascal,#people=two

System:

Acts:
User:
Acts:
State:

I found a table for two at Cascal at 6 pm.
Does that work?
offer(time=6 pm)
6 pm isn’t good for us. How about 7 pm?
negate(time), inform(time)
restaurant=Cascal,#people=two,
time=7 pm

Figure 1: A dialogue with user intent, user and
system dialogue acts, and dialogue state.

semantic parse generated by LU to update the DS
at every dialogue turn. The DS accumulates the
preferences speciﬁed by the user over the dialogue
and is used to make requests to a backend. The re-
sults from the backend and the dialogue state are
then used by a dialogue policy module to generate
the next system response.

Pipelining dialogue system components often
leads to error propagation, hence joint modeling
of these components has recently gained popular-
ity (Henderson et al., 2014; Mrkˇsi´c et al., 2017;
Liu and Lane, 2017), owing to computational ef-
ﬁciency as well as the potential ability to recover
from errors introduced by LU. However, combin-
ing joint modeling with the ability to scale to mul-
tiple domains and handle slots with a large set of
possible values, potentially containing entities not
seen during training, are active areas of research.
In this work, we propose a single, joint model
for LU and DST trained with multi-task learning.
Similar to Liu and Lane 2017, our model employs
a hierarchical recurrent neural network to encode
the dialogue context. Intermediate feature repre-
sentations from this network are used for identify-
ing the intent and dialogue acts, and tagging slots

ProceedingsoftheSIGDIAL2018Conference,pages376–384,Melbourne,Australia,12-14July2018.c(cid:13)2018AssociationforComputationalLinguistics376Utterance:

Slot Tags:

Table
↓
O

for
↓
O

at
↓

two
↓
B-# O B-rest

Olive
↓

Garden

↓
I-rest

Figure 2: IOB slot tags for a user utterance. Slot
values # = two and rest = Olive Garden are ob-
tained from corresponding B and I tags.

in the user utterance. Slot values obtained using
these slot tags (as shown in Figure 2) are then used
to update the set of candidate values for each slot.
Similar to Rastogi et al. 2017, these candidate val-
ues are then scored by a recurrent scoring network
which is shared across all slots, thus giving an ef-
ﬁcient model for DST which can handle new en-
tities that are not present in the training set - i.e.,
out-of-vocabulary (OOV) slot values.

During inference, the model uses its own pre-
dicted slot tags and previous turn dialogue state.
However, ground truth slot tags and dialogue state
are used for training to ensure stability. Aiming to
bridge this gap between training and inference, we
also propose a novel scheduled sampling (Ben-
gio et al., 2015) approach to joint language under-
standing and dialogue state tracking.

The paper is organized as follows: Section 2
presents related work, followed by Section 3 de-
scribing the architecture of the dialogue encoder,
which encodes the dialogue turns to be used as fea-
tures by different tasks in our framework. The sec-
tion also deﬁnes and outlines the implementation
of the LU and DST tasks. Section 4 describes our
setup for scheduled sampling. We then conclude
with experiments and discussion of results.

2 Related Work

The initial motivation for dialogue state tracking
came from the uncertainty in speech recognition
and other sources (Williams and Young, 2007),
as well as to provide a comprehensive input to
a downstream dialogue policy component decid-
ing the next system action. Proposed belief track-
ing models have ranged from rule-based (Wang
and Lemon, 2013), to generative (Thomson and
Young, 2010), discriminative (Henderson et al.,
2014), other maximum entropy models (Williams,
2013) and web-style ranking (Williams, 2014).

Language understanding has commonly been
modeled as a combination of intent and dia-
logue act classiﬁcation and slot tagging (Tur and
De Mori, 2011). Recently, recurrent neural net-
work (RNN) based approaches have shown good

results for LU. Hakkani-T¨ur et al. 2016 used a
joint RNN for intents, acts and slots to achieve
better overall frame accuracy. In addition, mod-
els such as Chen et al. 2016, Bapna et al. 2017
and Su et al. 2018 further improve LU results by
incorporating context from dialogue history.

Henderson et al. 2014 proposed a single joint
model for single-turn LU and multi-turn DST to
improve belief tracking performance. However,
it relied on manually constructed semantic dictio-
naries to identify alternative mentions of ontology
items that vary lexically or morphologically. Such
an approach is not scalable to more complex do-
mains (Mrkˇsi´c et al., 2017) as it is challenging to
construct semantic dictionaries that can cover all
possible entity mentions that occur naturally in a
variety of forms in natural language. Mrkˇsi´c et al.
2017 proposed the NBT model which eliminates
the LU step by directly operating on the user ut-
terance. However, their approach requires iterat-
ing through the set of all possible values for a slot,
which could be large or potentially unbounded
(e.g. date, time, usernames). Perez and Liu 2017
incorporated end-to-end memory networks, as in-
troduced in Sukhbaatar et al. 2015, into state track-
ing and Liu and Lane 2017 proposed an end-to-
end model for belief tracking. However, these two
approaches cannot accommodate OOV slot values
as they represent DS as a distribution over all pos-
sible slot values seen in the training set.

To handle large value sets and OOV slot values,
Rastogi et al. 2017 proposed an approach, where a
set of value candidates is formed at each turn using
dialogue context. The DST then operates on this
set of candidates. In this work, we adopt a similar
approach, but our focus is on joint modeling of LU
and DST, and sampling methods for training them
jointly.

3 Model Architecture

Let a dialogue be a sequence of T turns, each
turn containing a user utterance and the preced-
ing system dialogue acts output by the dialogue
manager. Figure 3 gives an overview of our model
architecture, which includes a user utterance en-
coder, a system act encoder, a state encoder, a
slot tagger and a candidate scorer. At each turn
t ∈ {1, ..., T}, the model takes a dialogue turn and
the previous dialogue state Dt−1 as input and out-
puts the predicted user intent, user dialogue acts,
slot values in the user utterance and the updated

377Figure 3: Architecture of our joint LU and DST model as described in Section 3. xt is the sequence
of user utterance token embeddings, at is the system act encoding and blue arrows indicate additional
features used by DST as detailed in Section 3.8.

dialogue state Dt.

As a new turn arrives, the system act encoder
(Section 3.1) encodes all system dialogue acts in
the turn to generate the system dialogue act vector
at. Similarly, the utterance encoder (Section 3.2)
e, and
encodes the user utterance into a vector ut
o for
also generates contextual token embeddings ut
each utterance token. The state encoder (Section
3.3) then uses at, ut
e and its previous turn hidden
state, dt−1
, to generate the dialogue context vec-
tor dt
o, which summarizes the entire observed dia-
st.
logue, and its updated hidden state dt

st

The dialogue context vector dt

o is then used by
the user intent classiﬁer (Section 3.4) and user di-
alogue act classiﬁer (Section 3.5). The slot tagger
(section 3.6) uses the dialogue context from previ-
ous turn dt−1
, the system act vector at and contex-
o to generate reﬁned con-
tual token embeddings ut
textual token embeddings st
o. These reﬁned token
embeddings are then used to predict the slot tag
for each token in the user utterance.

o

The system dialogue acts and predicted slot tags
are then used to update the set of candidate values
for each slot (Section 3.7). The candidate scorer
(Section 3.8) then uses the previous dialogue state
Dt−1, the dialogue context vector dt
o and other
features extracted from the current turn (indicated
by blue arrows in Figure 3) to update the scores for
all candidates in the candidate set and outputs the

updated dialogue state Dt. The following sections
describe these components in detail.
3.1 System Act Encoder
Previous turn system dialogue acts play an impor-
tant role in accurate semantic parsing of a user ut-
terance. Each system dialogue act contains an act
type and optional slot and value parameters. The
dialogue acts are ﬁrst encoded into binary vectors
denoting the presence of an act type. All dialogue
acts which don’t have any associated parameters
(e.g. greeting and negate) are encoded as a binary
utt. Dialogue acts with just a slot
indicator vector at
s as parameter (e.g. request(date)) are encoded as
slot(s), whereas acts having a candidate value c
at
for a slot s as parameter (e.g. offer(time=7pm)) are
encoded as at
cand(s, c). These binary vectors are
then combined using equations 1-4 to obtain the
combined system act representation at, which is
used by other units of dialogue encoder (as shown
in Figure 3). In these equations, es is a trainable
slot embedding deﬁned for each slot s.

slot(s) ⊕ es ⊕ Σcat
at
sc(s) = at
(cid:16) 1
(cid:88)
sc · at
a(cid:48)t
sc(s) = ReLU (W a
a(cid:48)t
at
usc =
sc(s)
s∈St
usc · at
at = ReLU (W a

|St|

cand(s, c)
sc(s) + ba
sc)

(cid:17) ⊕ at

utt

usc + ba

usc)

(1)
(2)

(3)

(4)

378o = {ut

3.2 Utterance Encoder
The user utterance takes the tokens corresponding
to the user utterance as input. Special tokens SOS
and EOS are added at the beginning and end of
the token list. Let xt = {xt
m ∈ Rud,∀ 0 ≤ m <
M t} denote the embedded representations of these
tokens, where M t is the number of tokens in the
user utterance for turn t (including SOS and EOS).
We use a single layer bi-directional GRU recur-
rent neural network (Cho et al., 2014) with state
size du and initial state set to 0, to encode the user
utterance. The ﬁrst output of the user utterance en-
e ∈ R2du, which is a compact represen-
coder is ut
tation of the entire user utterance, deﬁned as the
concatenation of the ﬁnal states of the two RNNs.
o,m ∈ R2du, 0 ≤
The second output is ut
m < M t, which is the embedded representation of
each token conditioned on the entire utterance, de-
ﬁned as the concatenation of outputs at each step
of the forward and backward RNNs.
3.3 State Encoder
The state encoder completes our hierarchical dia-
logue encoder. At turn t, the state encoder gen-
erates dt
o, which is an embedded representation of
the dialogue context until and including turn t. We
implement the state encoder using a unidirectional
GRU RNN with each timestep corresponding to a
dialogue turn. As shown in Figure 3, the dialogue
encoder takes at⊕ ut
e and its previous hidden state
dt−1
as input and outputs the updated hidden state
st
st and the encoded representation of the dialogue
dt
o (which are the same in case of GRU).
context dt
3.4 User Intent Classiﬁcation
The user intent is used to identify the backend with
which the dialogue system should interact. We
predict the intents at each turn to allow user to
switch intents during the dialogue. However, we
assume that a given user utterance can contain at-
most one intent and model intent prediction as a
multi-class classiﬁcation problem. At each turn,
the distribution over all intents is calculated as

i = sof tmax(Wi · dt
pt
(5)
i) = |I|, Wi ∈ Rd×|I| and bi ∈
where dim(pt
R|I|, I denoting the user intent vocabulary and
o). During inference, we predict
d = dim(dt
i) as the intent label for the utterance.
argmax(pt
3.5 User Dialogue Act Classiﬁcation
Dialogue acts are structured semantic representa-
tions of user utterances. User dialogue acts are

o + bi)

o + ba)

used by the dialogue manager in deciding the next
system action. We model user dialogue act clas-
siﬁcation as a multilabel classiﬁcation problem, to
allow for the presence of more than one dialogue
act in a turn (Tur and De Mori, 2011). At each
turn, the probability for act a is predicted as

o). For each act α, pt

a = sigmoid(Wa · dt
pt
(6)
a) = |Au|, Wa ∈ Rd×|Au|, ba ∈
where dim(pt
R|Au|, Au is the user dialogue act vocabulary and
a(α) is interpreted
d = dim(dt
as the probability of presence of α in turn t. Dur-
ing inference, all dialogue acts with a probability
greater than tu are predicted, where 0 < tu < 1.0
is a hyperparameter tuned using the dev set.
3.6 Slot Tagging
Slot tagging is the task of identifying the presence
of values of different slots in the user utterance.
We use the IOB tagging scheme (Tjong Kim Sang
and Buchholz 2000, see Figure 2) to assign a label
to each token. These labels are then used to extract
the values for different slots from the utterance.

The slot tagging network consists of a single-
layer bidirectional LSTM RNN (Hochreiter and
Schmidhuber, 1997), which takes the contextual
token embeddings ut
o generated by the utterance
encoder as input. It outputs reﬁned token embed-
o,m,∀ 0 ≤ m < M t} for each to-
dings st
ken, M t being the number of tokens in user utter-
ance at turn t.

o = {st

o

Models making use of dialogue context for LU
have been shown to achieve superior performance
(Chen et al., 2016). In our setup, the dialogue con-
text vector dt−1
encodes all the preceding turns
and the system act vector at encodes the system
dialogue acts preceding the user utterance. As
shown in Figure 3, dt−1
is used to initialize 1 the
hidden state (cell states are initialized to zero) for
the forward and backward LSTM recurrent units
in the slot tagger, while at is fed as input to the
tagger by concatenating with each element of ut
o
as shown below. We use an LSTM instead of a
GRU for this layer since that resulted in better per-
formance on the validation set.

o

in = {ut
o,m ⊕ at,∀ 0 ≤ m < M t}
st
st
e,bw, st
o,bw = LST Mbw(st
st
e,f w, st
o,f w = LST Mf w(st
o = st
st

o,bw ⊕ st

in)
in)

o,f w

(7)
(8)
(9)
(10)

1After projection to the appropriate dimension.

379Let S be the set of all slots in the dataset. We
deﬁne a set of 2|S| + 1 labels (one B- and I- label
for each slot and a single O label) for IOB tagging.
The reﬁned token embedding st
o,m is used to pre-
dict the distribution across all IOB labels for token
at index m as

o,m + bs)

s,m = sof tmax(Ws · st
pt

(11)
s,m) = 2|S| + 1, Ws ∈ Rds×2|S|+1
where dim(pt
and bs ∈ R2|S|+1, ds = dim(st
o,m) is the output
size of slot tagger LSTM. During inference, we
predict argmax(pt
s,m) as the slot label for the mth
token in the user utterance in turn t.
3.7 Updating Candidate Set
s is deﬁned as a set of values of
A candidate set Ct
a slot s which have been mentioned by either the
user or the system till turn t. Rastogi et al. 2017
proposed the use of candidate sets in DST for ef-
ﬁciently handling slots with a large set of values.
In their setup, the candidate set is updated at ev-
ery turn to include new values and discard old val-
ues when it reaches its maximum capacity. The
dialogue state is represented as a set of distribu-
s ∪ {δ, φ} for each
tions over value set V t
slot s ∈ St, where δ and φ are special values
dontcare (user is ok with any value for the slot)
and null (slot not speciﬁed yet) respectively, and
St is the set of all slots that have been mentioned
either by the user or the system till turn t.

s = Ct

Our model uses the same deﬁnition and update
rules for candidate sets. At each turn we use the
predictions of the slot tagger (Section 3.6) and sys-
tem acts which having slot and value parameters
to update the corresponding candidate sets. All
candidate sets are padded with dummy values for
batching computations for all slots together. We
keep track of valid candidates by deﬁning indi-
cator features mt
v(s, c) for each candidate, which
take the value 1.0 if candidate is valid or 0.0 if not.
3.8 Candidate Scorer
The candidate scorer predicts the dialogue state by
updating the distribution over the value set V t
s for
each slot s ∈ St. For this, we deﬁne three in-
termediate features rt
cand(s, c).
utt is shared across all value sets and is deﬁned
rt
by equation 12. rt
slot(s) is used to update scores
for V t
s and is deﬁned by equation 13. Further-
more, rt
cand(s, c) is deﬁned for each candidate
c ∈ Ct
s ⊂ V t
s using equation 14 and contains all
features that are associated to candidate c of slot s.

slot(s) and rt

utt, rt

o ⊕ at
rt
utt = dt
slot(s) ⊕ [pt−1

utt

rt
slot(s) = at

(s), pt−1
cand(s, c) ⊕ [pt−1
cand(s, c) = at
rt
c
[mt
v(s, c), mt
u(c)]

δ

φ (s)]
(s)]⊕

(12)
(13)

(14)

c

δ

utt, at

slot(s) and at

(s) and pt−1

In the above equations, dt

o is the dialogue con-
text at turn t output by the state encoder (Section
3.3), at
cand(s, c) are system act
encodings generated by the system act encoder
(Section 3.1), pt−1
φ (s) are the scores
associated with dontcare and null values for
slot s respectively. pt−1
(s) is the score associated
with candidate c of slot s in the previous turn and
is taken to be 0 if c (cid:54)∈ Ct
v(s, c) are variables
indicating whether a candidate is valid or padded
(Section 3.8). We deﬁne another indicator feature
u(c) which takes the value 1.0 if the candidate
mt
is a substring of the user utterance in turn t or
0.0 otherwise. This informs the candidate scorer
which candidates have been mentioned most re-
cently by the user.

s. mt

r(cid:48)t
slot(s) = rt
lt
s(δ) = F F 1

utt ⊕ rt
cs(r(cid:48)t
slot(s) ⊕ rt
cs(r(cid:48)t
s(c) = F F 2
lt
cand(s, c))
s = sof tmax(lt
pt
s)

slot(s)
slot(s))

(15)
(16)
(17)
(18)

s and lt

cs and F F 2

Features used in Equations 12-14 are then used
s using Equations
to obtain the distribution over V t
s(δ) denotes the
15-17. In the above equations, lt
s(c) denotes
logit for dontcare value for slot s, lt
the logit for a candidate c ∈ Ct
s(φ) is a
trainable parameter. These logits are obtained by
processing the corresponding features using feed-
forward neural networks F F 1
cs, each
having one hidden layer. The output dimension
of these networks is 1 and the dimension of the
hidden layer is taken to be half of the input dimen-
sion. The logits are then normalized using softmax
to get the distribution pt
4 Scheduled Sampling
DST is a recurrent model which uses predictions
from the previous turn. For stability during train-
ing, ground truth predictions from the previous
turn are used. This causes a mismatch between
training and inference behavior. We use sched-
uled sampling (Bengio et al., 2015) to bridge this

s over V t
s .

380Figure 4: Illustration of scheduled sampling for training the candidate scorer. The left ﬁgure shows the
two locations in our setup where we can perform scheduled sampling, while the plot on the right shows
the variation of sampling probabilities pc and pD with training step. See Section 4 for details.

mismatch. Scheduled sampling has been shown to
achieve improved slot tagging performance on sin-
gle turn datasets (Liu and Lane, 2016). Figure 4
shows our setup for scheduled sampling for DST,
which is carried out at two different locations - slot
tags and dialogue state.

The performance of slot tagger is critical to
DST because any slot value missed by the slot tag-
ger will not be added to the candidate set (unless
it is tagged in another utterance or present in any
system act). To account for this, during training,
u)
we sample between the ground truth slot tags (¯ct
u), training initially
and the predicted slot tags (ct
u (i.e. with keep probability pc = 1) but
with ¯ct
gradually reducing pc i.e.
increasingly replacing
u. Using predicted slot tags during train-
u with ct
¯ct
ing allows DST to train in presence of noisy can-
didate sets.

During inference, the candidate scorer only has
access to its own predicted scores in the previous
turn (Equations 13 and 14). To better mimic this
setup during training, we start with using ground
truth previous scores taken from ¯Dt−1 (i.e. with
keep probability pD = 1) and gradually switch to
Dt−1, the predicted previous scores, reducing pD.

Both pc and pD vary as a function of the training
step k, as shown in the right part of Figure 4; only
ground truth slot tags and dialogue state are used
for training i.e. pc and pD stay at 1.0 for the ﬁrst
kpre training steps, and then decrease linearly as
the ground truth slot tags and state are increasingly
replaced by model predictions during training.

5 Experiments
The major contributions of our work are two-fold.
First, we hypothesize that joint modeling of LU
and DST results in a computationally efﬁcient
model with fewer parameters without compromis-
ing performance. Second, we propose the use of
scheduled sampling to improve the robustness of
DST during inference. To this end, we conduct
experiments across the following two setups.

Separate vs Joint LU-DST - Figure 3 shows the
joint LU-DST setup where parameters in the utter-
ance encoder and state encoder are shared across
LU tasks (intent classiﬁcation, dialogue act clas-
siﬁcation and slot tagging) and DST (candidate
scoring). As baselines, we also conduct experi-
ments where LU and DST tasks use separate pa-
rameters for utterance and state encoders.

Scheduled Sampling - We conduct scheduled
sampling (as described in Section 4) experiments
in four different setups.
1. None - Ground truth slot tags (¯ct

u) and previous

dialogue state ( ¯Dt−1) are used for training.

u) and predicted (ct

2. Tags - Model samples between ground truth
u) slot tags, sticking to

(¯ct
ground truth previous state.
3. State - Model samples between ground truth
( ¯Dt−1) and predicted (Dt−1) previous state,
sticking to ground truth slot tags.
4. Both - Model samples between ¯Dt−1 and Dt−1

u and ct
u.

as well as between ¯ct
In the last three setups, we start sampling from
predictions only after kpre = 0.3 kmax training
steps, as shown in Figure 4.

3815.1 Evaluation Metrics
We report user intent classiﬁcation accuracy, F1
score for user dialogue act classiﬁcation, frame ac-
curacy for slot tagging and joint goal accuracy and
slot F1 score for DST. During DST evaluation, we
always use the predicted slot values and the dia-
logue state in the previous turn. Slot frame accu-
racy is deﬁned as the fraction of turns for which all
slot labels are predicted correctly. Similarly, joint
goal accuracy is the fraction of turns for which the
predicted and ground truth dialogue state match
for all slots. Since it is a stricter metric than DST
slot F1, we use it as the primary metric to identify
the best set of parameters on the validation set.

5.2 Datasets
We evaluate our approaches on two datasets:
• Simulated Dialogues2 - The dataset, described
in Shah et al. 2017, contains dialogues from
restaurant (Sim-R) and movie (Sim-M) do-
mains across three intents. A challenging as-
pect of this dataset is the prevalence of OOV
entities e.g. only 13% of the movie names in
the dev/test sets also occur in the training data.
• DSTC2 - We use the top ASR hypothesis and
system dialogue acts as inputs. Dialogue act
labels are obtained from top SLU hypothesis
and state labels for requestable slots. DS labels
are obtained from state labels for informable
slots. We use a semantic dictionary (Hender-
son et al., 2014) to obtain ground truth slot tags.
We also use the semantic dictionary to canoni-
calize the candidate values since the slot val-
ues in the dialogue state come from a ﬁxed set
in the DSTC2 dialogues and may be different
from those present in the user utterance.

5.3 Training
We use sigmoid cross entropy loss for dialogue
act classiﬁcation and softmax cross entropy loss
for all other tasks. During training, we minimize
the sum of all task losses using ADAM optimizer
(Kingma and Ba, 2014), for 100k training steps
with batches of 10 dialogues each. We used grid-
search to identify the best hyperparameter values
(sampled within speciﬁed range) for learning rate
(0.0001 - 0.005) and token embedding dimension
(50 - 200). For scheduled sampling experiments,
the minimum keep rate i.e. pmin is varied between

2Dataset available at http://github.com/google-research-

datasets/simulated-dialogue/

0.1 - 0.9 with linear decay. The layer sizes for the
utterance encoder and slot tagger are set equal to
the token embedding dimension, and that of the
state encoder to half this dimension.

Slot Value dropout - To make the model ro-
bust to OOV tokens arising from new entities not
present in the training set, we randomly replace
slot value tokens in the user utterance with a spe-
cial OOV token with a probability that linearly in-
creases from 0.0 to 0.4 during training.

6 Results and Discussion

Table 1 shows our results across the two setups
described in Section 5, for the Simulated Dia-
logues datasets. For Sim-R + Sim-M, we ob-
serve that the joint LU-DST model with sched-
uled sampling (SS) on both slot tags and dialogue
state performs the best, with a joint goal accuracy
of 73.8% overall, while the best separate model
gets a joint goal accuracy of 71.9%, using SS only
for slot tags. Even for the no-SS baselines, the
joint model performs comparably to the separate
model (joint goal accuracies of 68.6% and 68.7%
respectively), indicating that sharing results in a
more efﬁcient model with fewer parameters, with-
out compromising overall performance. For each
SS conﬁguration, our results comparing separate
and joint modeling are statistically signiﬁcant, as
determined by the McNemar’s test with p < 0.05.
On the Sim-R dataset, the best joint model ob-
tains a joint goal accuracy of 87.1%, while the best
separate model obtains 85.0%. However, we ob-
serve a signiﬁcant drop in joint goal accuracy for
the Sim-M dataset for both the joint model and
the separate model as compared to Sim-R. This
can partly be attributed to the Sim-M dataset being
much smaller than Sim-R (384 training dialogues
as opposed to 1116) and that the high OOV rate of
the movie slot in Sim-M makes slot tagging per-
formance more crucial for Sim-M. While SS does
gently bridge the gap between training and testing
conditions, its gains are obscured in this scenario
possibly since it is very hard for DST to recover
from a slot value being completely missed by LU,
even when aided by SS.

For the two datasets, we also observe a signif-
icant difference between the slot frame accuracy
and joint goal accuracy. This is because an LU er-
ror penalizes the slot frame accuracy for a single
turn, whereas an error in dialogue state propagates
through all the successive turns, thereby drasti-

382Table 1: Experiments and results on test set with variants of scheduled sampling on separate and joint
LU-DST models, when trained on Sim-M + Sim-R.

Eval Set SS

Setup

Sim-R

Sim-M

None
Tags
State
Both
None
Tags
State
Both
Sim-R + None
Tags
Sim-M
State
Both

Intent

Accuracy
Joint
Sep
0.997
0.999
0.998
0.998
0.998
0.999
0.998
0.994
0.991
0.993
0.994
0.993
0.970
0.996
0.996
0.989
0.996
0.996
0.997
0.996
0.990
0.996
0.993
0.997

Dialogue Act

F1 Score
Sep
0.956
0.936
0.931
0.948
0.966
0.970
0.964
0.970
0.959
0.946
0.940
0.954

Joint
0.935
0.957
0.939
0.919
0.966
0.967
0.955
0.959
0.944
0.960
0.943
0.931

cally reducing the joint goal accuracy. This gap
is even more pronounced for Sim-M because of
the poor performace of slot tagger on movie slot,
which is often mentioned by the user in the begin-
ning of the dialogue. The relatively high values of
overall DST slot F1 for Sim-M for all experiments
also corroborates this observation.

Table 2: Reported joint goal accuracy of model
variants on the DSTC2 test set.

Model
No SS
Tags only SS
State only SS
Tags + State SS
Liu and Lane 2017
Mrkˇsi´c et al. 2017

Separate

0.661
0.655
0.661
0.656

-
-

Joint
0.650
0.670
0.660
0.658
0.73
0.734

Table 2 shows our results on the DSTC2 dataset,
which contains dialogues in the restaurant domain.
The joint model gets a joint goal accuracy of
65.0% on the test set, which goes up to 67.0% with
SS on slot tags. Approaches like NBT (Mrkˇsi´c
et al., 2017) or Hierarchical RNN (Liu and Lane,
2017) are better suited for such datasets, where the
set of all slot values is already known, thus elimi-
nating the need for slot tagging. On the other hand,
our setup uses slot tagging for candidate genera-
tion, which allows it to scale to OOV entities and
scalably handle slots with a large or unbounded set

Slot Frame
Accuracy
Joint
Sep
0.919
0.924
0.917
0.922
0.920
0.919
0.916
0.917
0.801
0.800
0.801
0.895
0.799
0.848
0.860
0.887
0.890
0.885
0.888
0.910
0.886
0.899
0.909
0.900

Joint Goal
Accuracy
Joint
Sep
0.846
0.850
0.805
0.871
0.852
0.829
0.849
0.829
0.276
0.283
0.262
0.504
0.266
0.384
0.460
0.438
0.687
0.686
0.698
0.719
0.683
0.702
0.738
0.717

DST Slot
F1 Score
Sep
0.951
0.936
0.935
0.942
0.806
0.839
0.803
0.805
0.902
0.902
0.897
0.894

Joint
0.952
0.962
0.951
0.953
0.817
0.805
0.797
0.845
0.906
0.905
0.899
0.915

of possible values, at the cost of performance.

Analyzing results for scheduled sampling, we
observe that for almost all combinations of met-
rics, datasets and joint/separate model conﬁgura-
tions, the best result is obtained using a model
trained with some SS variant. For instance, for
Sim-M, SS over slot tags and state increases joint
goal accuracy signiﬁcantly from 28.3% to 46.0%
for joint model. SS on slot tags helps the most
with Sim-R and DSTC2: our two datasets with
the most data, and low OOV rates, while SS on
both slot tags and dialogue state helps more on the
smaller Sim-M. In addition, we also found that slot
value dropout (Section 5.3), improves LU as well
as DST results consistently. We omit the results
without this technique for brevity.

7 Conclusions
In this work, we present a joint model for language
understanding (LU) and dialogue state tracking
(DST), which is computationally efﬁcient by way
of sharing feature extraction layers between LU
and DST, while achieving an accuracy compa-
rable to modeling them separately across multi-
ple tasks. We also demonstrate the effectiveness
of scheduled sampling on LU outputs and previ-
ous dialogue state as an effective way to simulate
inference-time conditions during training for DST,
and make the model more robust to errors.

383References
Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and
Sequential dialogue context
Larry Heck. 2017.
modeling for spoken language understanding.
In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 103–114.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 1171–1179.

Yun-Nung Chen, Dilek Hakkani-T¨ur, Jianfeng Gao,
and Li Deng. 2016. End-to-end memory networks
with knowledge carryover for multi-turn spoken lan-
guage understanding.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Dilek Hakkani-T¨ur, Asli Celikyilmaz, Yun-Nung Chen,
Jianfeng Gao, Li Deng, and Ye-Yi Wang. 2016.
Multi-domain joint semantic frame parsing using bi-
directional rnn-lstm.

M. Henderson, B. Thomson, and S. Young. 2014.
Word-based dialog state tracking with recurrent neu-
In Proceedings of the 15th Annual
ral networks.
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 292–299.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Bing Liu and Ian Lane. 2016. Joint online spoken lan-
guage understanding and language modeling with
recurrent neural networks. In 17th Annual Meeting
of the Special Interest Group on Discourse and Dia-
logue, page 22.

Bing Liu and Ian Lane. 2017. An end-to-end trainable
neural network model with belief tracking for task-
oriented dialog. In Proceedings of Interspeech.

Nikola Mrkˇsi´c, Diarmuid ´O S´eaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1777–
1788.

Julien Perez and Fei Liu. 2017. Dialog state track-
ing, a machine reading approach using memory net-
work. In Proceedings of the 15th Conference of the

European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 305–314.

A. Rastogi, D. Hakkani-T¨ur, and L. Heck. 2017. Scal-
able multi-domain dialogue state tracking. In IEEE
Workshop on Automatic Speech Recognition and
Understanding (ASRU).

P. Shah, D. Hakkani-T¨ur, G. Tur, A. Rastogi, A. Bapna,
N. Nayak, and L. Heck. 2017. Building a conversa-
tional agent overnight with dialogue self-play. arXiv
preprint arXiv:1801.04871.

Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen.
2018. How time matters: Learning time-decay at-
tention for contextual spoken language understand-
ing in dialogues. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A pomdp framework for
spoken dialogue systems. Computer Speech & Lan-
guage, 24(4):562–588.

Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of the 2nd workshop on Learn-
ing language in logic and the 4th conference on
Computational natural language learning-Volume 7,
pages 127–132. Association for Computational Lin-
guistics.

Gokhan Tur and Renato De Mori. 2011. Spoken lan-
guage understanding: Systems for extracting seman-
tic information from speech. John Wiley & Sons.

Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
In Proceedings of the SIG-
observed information.
DIAL 2013 Conference, pages 423–432.

Jason Williams. 2013. Multi-domain learning and gen-
eralization in dialog state tracking. In Proceedings
of the SIGDIAL 2013 Conference, pages 433–441.

Jason D Williams. 2014. Web-style ranking and slu
combination for dialog state tracking. In Proceed-
ings of the 15th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue (SIGDIAL),
pages 282–291.

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393–422.

384
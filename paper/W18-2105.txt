Are we experiencing the

Golden Age of Automatic Post-Editing?

Marcin Junczys-Dowmunt
Microsoft AI and Research

Translation Quality Estimation and Automatic Post-Editing

AMTA 2018

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 144Why automatic post-editing?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 145Why automatic post-editing?

Can’t we just retrain the original system?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 146Why automatic post-editing?

Can’t we just retrain the original system?

Not always:

(cid:2) black-box scenario
(cid:2) specialized system make better use of PE data (?)
(cid:2) synergy eﬀects (RB-MT + SMT, SMT + NMT)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 147Popular metrics:
TER (Translation Error Rate) and BLEU

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 148Historic APE systems:

Simard et. al (2007). Statistical Phrase-based
Post-editing. NAACL.

(cid:2) Automatic Post-editing of a rule-based system with a

phrase-based SMT system;

(cid:2) About 30,000 paragraphs of triples per language pair

(En-Fr/Fr-En);

(cid:2) Train PB-SMT system on RB-MT output and PE data;
(cid:2) Chain systems together;
(cid:2) Impressive gains over the baselines.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 149Historic APE systems:

Bechara et. al (2011). Statistical Post-Editing for a
Statistical MT System. MT-Summit.

(cid:2) Automatic Post-editing of a phrase-based SMT with another

phrase-based SMT system.

(cid:2) Barely any gains over the baselines.
(cid:2) But interesting idea: Contextual Statistical APE

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 150Contextual Statistical APE

le#the

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 151Contextual Statistical APE

le#the programme#programme

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 152Contextual Statistical APE

le#the programme#programme a#has

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 153Contextual Statistical APE

le#the programme#programme a#has ´et´e#been

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 154Contextual Statistical APE

le#the programme#programme a#has ´et´e#been
mis#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 155Contextual Statistical APE

le#the programme#programme a#has ´et´e#been
mis#implemented en#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 156Contextual Statistical APE

le#the programme#programme a#has ´et´e#been
mis#implemented en#implemented application#implemented

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 157Contextual Statistical APE

le#the programme#programme a#has ´et´e#been
mis#implemented en#implemented application#implemented

Problems?

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 158WMT 2015 Shared Task on Automatic post-editing
(The Stone Age of Automatic post-editing)

ID

Avg. TER

Baseline
FBK Primary
LIMSI Primary
USAAR-SAPE
LIMSI Contrastive
Abu-MaTran Primary
FBK Contrastive
(Simard et al., 2007)
Abu-MaTran Contrastive

22.91
23.23
23.33
23.43
23.57
23.64
23.65
23.84
24.72

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 159WMT 2015 Shared Task on Automatic post-editing
(The Stone Age of Automatic post-editing)

ID

Avg. TER

Baseline
FBK Primary
LIMSI Primary
USAAR-SAPE
LIMSI Contrastive
Abu-MaTran Primary
FBK Contrastive
(Simard et al., 2007)
Abu-MaTran Contrastive

WMT2016-best
WMT2017-best

22.91
23.23
23.33
23.43
23.57
23.64
23.65
23.84
24.72

23.29
??

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 160WMT 2016 Shared Task on Automatic post-editing

Create an APE system that returns automatic post-edition of an
English-German black-box MT system. 10,000 training triplets of
the following form were provided:

SRC These ﬁles are encoded as UTF-8 or ASCII , which is a subset of

UTF-8 .

MT Diese Dateien werden als UTF-8 oder ASCII , bei der es sich um eine

Untergruppe von UTF-8 kodiert .

PE Diese Dateien werden als UTF-8 oder ASCII , eine Teilmenge von

UTF-8 , kodiert .

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 161Problem: very little publicly available PE data

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 162Source: Koehn and Knowles (2017). Six Challenges for Neural Machine
Translation. 1st Neural Machine Translation Workshop. Vancouver.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 163Solution: create your own PE data using:

(cid:2) Oﬃcial APE training and develsopment data sets.
(cid:2) EN-DE bilingual data from the WMT-16 shared tasks on IT

and news translation.

(cid:2) German monolingual Common Crawl (CC) corpus.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 164Round-trip translation

gibt die Prozesskennung des aktuellen Prozesses zur¨uck . (= PE)

DE-EN Moses

the process ID of the current process . (= SRC)

EN-DE Moses

die Prozess-ID des aktuellen Prozesses . (= MT)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 165Selecting in-domain data

(cid:2) Cross-entropy ﬁltering of German CC corpus based on

in-domain post-editing and IT-domain data.

(cid:2) We keep 10M sentences with the best cross-entropy scores.

Filtering for TER statistics:

Data set

Sent. NumWd WdSh NumEr TER

training set
development set

round-trip.full
round-trip.n10
round-trip.n1

12K
1K

9,960K
4,335K
531K

17.89
19.76

13.50
15.86
20.92

0.72
0.71

0.58
0.66
0.55

4.69
4.90

5,72
5.93
5.20

26.22
24.81

42.02
36.63
25.28

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 166Experiments with neural models

(cid:2) Attentional encoded-decoder models trained with Nematus:

https://github.com/rsennrich/nematus

(cid:2) C++/CUDA AmuNMT decoder:

https://github.com/emjotde/amunmt

MT-PE and SRC-PE systems

(cid:2) Trained on round-trip.n10 data (4M triplets).
(cid:2) Fine-tuned on round-trip.n1 and 20x oversampled oﬃcial

training data (700K triplets).

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 16770.0

60.0

50.0

40.0

30.0

20.0

0

5

10

mt-pe
src-pe

30

35

40

20

15
n × 10000 iterations

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 168Log-linear combination

(cid:2) Log-linear combination of two models with diﬀerent input

languages.

(cid:2) Weights determined by MERT for two models: ca. 0.8 for

mt-pe and 0.2 for src-pe model.

(cid:2) Post-Editing Penalty (PEP) to control the faithfulness of the

APE results.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 169Progress on the dev set

System

TER

BLEU

25.14

62.92

Baseline (mt)
mt→pe
mt→pe×4
src→pe
src→pe×4
mt→pe×4 / src→pe×4
68.07
mt→pe×4 / src→pe×4 / pep 21.46 68.94

66.71
66.88

23.37
23.23

53.89
55.41

32.31
31.42

22.38

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 170Automatic evaluation on unseen test set

(cid:2) AMU (primary) = mt→pe×4 / src→pe×4 / pe
(cid:2) AMU (contrastive) = mt→pe×4

System

TER

BLEU

AMU (primary)
21.52 67.65
AMU (contrastive) 23.06 66.09
64.75
FBK
64.10
USAAR
CUNI
63.32
63.47
Baseline (Moses)
Baseline (mt)
62.11
58.60
DCU
JUSAAR
59.44

23.92
24.14
24.31
24.64
24.76
26.79
26.92

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 171Results of human evaluation

# Score Range System

1

2

3

4

1.967

0.033

-0.108
-0.191
-0.211

-0.712
-0.778

1

2

3-4
3-5
3-5

6-7
6-7

AMU (primary)

FBK

CUNI
USSAR
Baseline (mt)

JUSAAR
DCU

Table: With post-edited sentence shown as reference

Source: WMT2016 overview paper.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 172Results of human evaluation

# Score Range System

1

2

3

2.058

0.867

-0.213
-0.348
-0.374
-0.499
-0.675
-0.816

1

2

3-4
3-6
3-6
5-7
6-8
7-8

Human

AMU (primary)

CUNI
FBK
USSAR
Baseline (mt)
JUSAAR
DCU

Table: With post-edited sentence included as system

Source: WMT2016 overview paper.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 173Source: WMT 2016 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 174Some conclusions

(cid:2) One of the ﬁrst successful applications of NMT models to

APE

(cid:2) Artiﬁcial APE triplets allow training of NMT models with

little original training data and help against overﬁtting.

(cid:2) Positive eﬀects of log-linear combinations of NMT models

with multiple input languages.

(cid:2) Tuning with MERT to assign model component weights

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 175WMT 2017 Shared Task on Automatic post-editing

(cid:2) The same setting;
(cid:2) Additional 12,500 sentences of PE data;
(cid:2) Still no post-editing of NMT system

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 176Our submission to the WMT 2017 Shared Task on
Automatic Post-editing

(cid:2) We explore the interaction of hard-attention and

multi-encoder models.

(cid:2) All models trained and available in Marian

(http://marian-nmt.github.io)

(cid:2) We use the same data as last year.
(cid:2) This time proper regularization and no need for ﬁne-tuning.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 177Soft vs. hard monotonic attention

Tastatur-
einen
fehl-

be-

ssatz
im

festlegen
M en¨u
E O S

.

W ¨ahlen
Sie

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
aus
.

EOS

cgru

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 178Soft vs. hard monotonic attention

Tastatur-
einen
fehl-

be-

ssatz
im

festlegen
M en¨u
E O S

.

W ¨ahlen
Sie

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
aus
.

EOS

gru-hard

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 179Soft vs. hard monotonic attention

Tastatur-
einen
fehl-

be-

ssatz
im

festlegen
M en¨u
E O S

.

W ¨ahlen
Sie

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
aus
.

EOS

cgru-hard

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 180Reminder: Gated Recurrent Unit

GRU (s, x) =(1 − z) (cid:3) s + z (cid:3) s,
s = tanh (Wx + r (cid:3) Us) ,
r = σ (Wr x + Ur s) ,
z = σ (Wz x + Uz s) ,

(1)

where x is the cell input; s is the previous recurrent state; W, U,
Wr , Ur , Wz , Uz are trained model parameters1; σ is the logistic
sigmoid activation function.

1Biases have been omitted.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 181Conditional GRU (cgru)

C = {h1, . . . , hn}

sj = cGRUatt (sj−1, E[yj−1], C)

(2)

=GRU1 (sj−1, E[yj−1])

(cid:2)

sj
cj =ATT

sj =GRU2

(cid:2)

(cid:3)

(cid:3)

(cid:2)

(cid:2)
C, s
j
(cid:2)
j , cj
s

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 182Hard monotonic attention (gru-hard)

(cid:2) Aharoni and Goldberg (2016) introduce a simple model for

monolingual morphological re-inﬂection with hard monotonic
attention.
symbol (cid:4)step(cid:5)

(cid:2) The target word vocabulary Vy is extended with a special step
(cid:2) Whenever (cid:4)step(cid:5) is predicted as the output symbol, the hard

attention is moved to the next encoder state.

(cid:2) We calculate the hard attention indices as follows:

a1 = 1,

(cid:4)

aj =

aj−1 + 1 if yj−1 = (cid:4)step(cid:5)
aj−1
(cid:6)(cid:3)

otherwise.

(cid:2)

(cid:5)

sj−1,

E[yj−1]; haj

,

sj = GRU

(3)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 183Mixing hard and soft attention (cgru-hard)

(cid:2)

(cid:5)

(cid:6)

(cid:3)

sj = cGRUatt

sj−1,

E[yj−1]; haj

, C

(4)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 184Example sentence and corrections

mt
src

W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u festlegen .
Select a shortcut set in the Set menu .

cgru
gru-hard
cgru-hard
m-cgru
m-cgru-hard W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u ” Satz . ”

W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u aus .
W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u aus .
W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u aus .
W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u ” Satz ”aus .

pe

W¨ahlen Sie einen Tastaturbefehlssatz im Men¨u ” Satz . ”

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 185Dual attention

Tastatur-
einen
fehl-

be-

ssatz
im

festlegen
M en¨u
E O S

.

Select

a

W ¨ahlen
Sie

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
“
Satz
”
aus
.

EOS

shortcut

set

in the

Set m enu

.

E O S

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
“
Satz
”
aus
.

EOS

mt

src

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 186Dual attention

Tastatur-
einen
fehl-

be-

ssatz
im

festlegen
M en¨u
E O S

.

Select

a

W ¨ahlen
Sie

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
“
Satz
”
aus
.

EOS

shortcut

set

in the

Set m enu

.

E O S

W¨ahlen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Men¨u
“
Satz
”
aus
.

EOS

mt

src

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 187Dual soft attention (m-cgru)

1 , . . . , hmt
Tmt
1 , . . . , hsrc
Tsrc

}
}
(cid:9)Tsrc

i=1 hmt
Tmt

i

;

i=1 hsrc
Tsrc

i

C mt = {hmt
C src = {hsrc
(cid:7)
(cid:8)(cid:9)Tmt
(cid:2)

Winit

sj = cGRU2-att

sj−1, E[yj−1], Cmt, Csrc

s0 = tanh

(cid:10)(cid:11)
(cid:3)

.

.

(5)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 188Dual soft attention (m-cgru)

(cid:2)

(cid:3)

.

(6)

sj = cGRU2-att

sj−1, E[yj−1], Cmt, Csrc

(cid:2)
j =GRU1 (sj−1, E[yj−1]) ,

s

(cid:2)
(cid:2)

(cid:6)

(cid:2)
Cmt, s
j
(cid:2)
Csrc , s
j
; csrc
,
(cid:2)
j , cj

(cid:2)

s

j

,

,

(cid:3)
(cid:3)
(cid:3)

.

cmt
j =ATT
csrc
j =ATT
cmt
cj =
j

(cid:5)

sj =GRU2

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 189Dual soft attention with hard attention (m-cgru-hard)

(cid:12)

(cid:13)

(cid:14)

(cid:15)

sj = cGRU2-att

sj−1,

E[yj−1]; hmt
aj

, Cmt, Csrc

.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 190Results

WMT2016 best

21.52

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 191Results

WMT2016 best

cgru

21.52

22.27

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 192Results

WMT2016 best

21.52

cgru

gru-hard

22.27

22.72

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 193Results

WMT2016 best

21.52

cgru

gru-hard

cgru-hard

22.27

22.72

22.10

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 194Results

WMT2016 best

21.52

cgru

gru-hard

cgru-hard

m-cgru

22.27

22.72

22.10

20.69

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 195Results

WMT2016 best

21.52

cgru

gru-hard

cgru-hard

m-cgru

m-cgru-hard

22.27

22.72

22.10

20.69

20.87

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 196Results

WMT2016 best

21.52

cgru

gru-hard

cgru-hard

22.27

22.72

22.10

m-cgru
m-cgru ×4
m-cgru-hard
m-cgru-hard ×4

20.69

19.92

20.87

20.34

15

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 197Faithfulness

cgru
cgru

12.01

gru-hard
gru-hard

9.48

cgru-hard
cgru-hard

11.57

22.27

22.72

22.10

5

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 198Faithfulness

cgru
cgru

gru-hard
gru-hard

9.48

cgru-hard
cgru-hard

m-cgru
m-cgru

12.01

11.57

m-cgru-hard
m-cgru-hard

13.62

22.27

22.72

22.10

15.98

20.69

20.87

5

← TER (WMT16)

25

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 199Results for the WMT2017 shared task on APE

Systems

TER

BLEU

FBK EnsembleRerank Primary
19.60
AMU.multi-transducer-composed PRIMARY 19.77
20.11
DCU FRANKENAPE-TUNED PRIMARY
USAAR NMT-OSM PRIMARY
23.05
23.22
LIG chained syn PRIMARY
JXNU JXNU EDITFreq PRIMARY
23.31
24.03
CUNI char conv rnn beam PRIMARY
Oﬃcial Baseline (MT)
24.48
24.69
Baseline 2 (Statistical phrase-based APE)

70.07
69.50
69.19
65.01
65.12
65.66
64.28
62.49
62.97

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 200Results for the WMT2017 shared task on APE

# Ave % Ave z System

–

1

4

5

–

84.8

0.520 Human post edit

78.2
77.9
76.8

73.8

71.9
71.1
70.2

0.261 AMU
0.261 FBK
0.221 DCU

0.115

JXNU

0.038 USAAR
0.014 CUNI
-0.020

LIG

68.6

-0.083 No post edit

Source: WMT 2017 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 201Results for the WMT2017 shared task on APE

Systems

Modiﬁed

Improved Deteriorated

FBK Primary
AMU Primary
DCU Primary
...

1,607
1,583
1,592

1,035
1,040
1,014

334
322
361

Source: WMT 2017 overview paper

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 202WMT 2018 Shared Task on Automatic post-editing

(cid:2) First shared tasks to post-edit NMT output (exciting!)
(cid:2) Still en-de and IT (not ideal!)
(cid:2) Domain mis-match between artiﬁcal data and NMT (bad!)
(cid:2) More artiﬁcal data (good!)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 203More guesses

General MT is eating your lunch!

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 204Justiﬁcation

(cid:2) General QE and APE will be gone before translators even

need start worrying;

(cid:2) QE and APE are bug-ﬁxes that operate within very narrow

error margin (too bad to exploit full error margin);

(cid:2) This error margin might already be gone in many real-word

applications.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 205But ... but... it works, right?

Maybe, maybe not. I think we are mostly seeing:

(cid:2) Favorably chosen test sets, domains and language pairs;
(cid:2) Synergy eﬀects (diﬀerent approaches): SMT+NMT
(cid:2) System combination eﬀects (similar approches);
(cid:2) Two-pass decoding eﬀects (see MS results);
(cid:2) Domain-adaptation or style-transfer eﬀects (the last hope!)

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 206
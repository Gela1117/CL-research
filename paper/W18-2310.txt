A Neural Autoencoder Approach for Document Ranking and Query

Reﬁnement in Pharmacogenomic Information Retrieval

Jonas Pfeiffer1,2

Samuel Broscheit1 Rainer Gemulla1 Mathias G¨oschl2

1University of Mannheim, Mannheim, Germany
2Molecular Health GmbH, Heidelberg, Germany

{lastname}@informatik.uni-mannheim.de
Mathias.Goeschl@molecularhealth.com

Jonas.Pfeiffer.90@Gmail.com

Abstract

In this study, we investigate learning-to-
rank and query reﬁnement approaches for
information retrieval in the pharmacoge-
nomic domain. The goal is to improve the
information retrieval process of biomedi-
cal curators, who manually build knowl-
edge bases for personalized medicine. We
study how to exploit the relationships be-
tween genes, variants, drugs, diseases and
outcomes as features for document rank-
ing and query reﬁnement.
For a su-
pervised approach, we are faced with a
small amount of annotated data and a large
amount of unannotated data. Therefore,
we explore ways to use a neural document
auto-encoder
in a semi-supervised ap-
proach. We show that a combination of es-
tablished algorithms, feature-engineering
and a neural auto-encoder model yield
promising results in this setting.

Introduction

1
Personalized medicine strives to relate genomic
detail to patient phenotypic conditions (such as
disease, adverse reactions to treatment) and to as-
sess the effectiveness of available treatment op-
tions (Brunicardi et al., 2011). For computer-
assisted decision making, knowledge bases need
to be compiled from published scientiﬁc evidence.
They describe biomarker relationships between
key entity types: Disease, Protein/Gene, Vari-
ant/Mutation, Drug, and Patient Outcome (Out-
come) (Manolio, 2010). While automated infor-
mation extraction has been applied to simple re-
lationships — such as Drug-Drug (Asada et al.,
2017) or Protein-Protein (Peng and Lu, 2017);
(Peng et al., 2015); (Li et al., 2017) interaction —
with adequate precision and recall, clinically ac-

tionable biomarkers need to satisfy rigorous qual-
ity criteria set by physicians and therefore call
upon manual data curation by domain experts.

To ascertain the timeliness of information, cu-
rators are faced with the labor-intensive task to
identify relevant articles in a steadily growing ﬂow
of publications (Lee et al., 2018).
In our sce-
nario, curators iteratively reﬁne search queries in
an electronic library, such as PubMed.1 The in-
formation the curators search for, are biomarker-
facts in the form of {Gene(s) - Variant(s) - Drug(s)
- Disease(s) - Outcome}. For example, a cu-
rator starts with a query consisting of a single
gene, e.g. q1 = {PIK3CA}, and receives a set
of documents D1. After examining D1, the cu-
rator identiﬁes the variants H1047R and E545K,
which yields queries q2 = {PIK3CA, H1047R}
and q3 = {PIK3CA, E545K} that lead to D2 and
D3. As soon as studies are found that contain
the entities in a biomarker relationship, the enti-
ties and the studies are entered into the knowledge
base. This process is then repeated until, theoret-
ically, all published literature regarding the gene
PIK3CA has been screened.

Our goal is to reduce the amount of docu-
ments which domain experts need to examine.
To achieve this, an information retrieval system
should rank documents high that are relevant to
the query and should facilitate the identiﬁcation of
relevant entities to reﬁne the query.

Classic approaches for document ranking, like
tf-idf (Luhn, 1957); (Sp¨arck Jones, 1972), or bm25
(Robertson and Zaragoza, 2009), and, for exam-
ple, the Relevance Model (Lavrenko and Croft,
2001) for query reﬁnement are established tech-
niques in this setting. They are known to be robust
and do not require data for training. However, as
they are based on a bag-of-words model (BOW),

1https://www.ncbi.nlm.nih.gov/pubmed

ProceedingsoftheBioNLP2018workshop,pages87–97Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics87they cannot represent a semantic relationship of
entities in a document. This, for example, yields
search results with highly ranked review articles
that only list query terms, without the desired re-
lationship between them. Therefore, we investi-
gate approaches that model the semantic relation-
ships between biomarker entities. This can either
be addressed by combining BOW with rule-based
ﬁltering, or by supervised learning, i.e. learning-
to-rank (LTR).

Our goal is, to tailor document ranking and
query reﬁnement to the task of the curator. This
means that a document ranking model should as-
sign a high rank to a document that contains the
query entities in a biomarker relationship. A query
reﬁnement model should suggest additional query
terms, i.e. biomarker entities, to the curator that
are relevant to the current query. Given the com-
plexity of entity relationships and the high vari-
ety of textual realizations this requires either effec-
tive feature engineering, or large amounts of train-
ing data for a supervised approach. The in-house
data set of Molecular Health consists of 5833 la-
beled biomarker-facts, and 24 million unlabeled
text documents from PubMed. Therefore, a good
solution is to exploit the large amount of unla-
beled data in a semi-supervised approach. Li et al.
(2015) have shown that a neural auto-encoder with
LSTMs (Hochreiter and Schmidhuber, 1997) can
encode the syntactics and semantics of a text in
a dense vector representation. We show that this
representation can be effectively used as a feature
for semi-supervised learning-to-rank and query re-
ﬁnement.

In this paper, we describe a feature engineer-
ing approach and a semi-supervised approach. In
our experiments we show that the two approaches
are, in comparison, almost on par in terms of per-
formance and even improve in a joint model. In
Section 2 we describe the neural auto-encoder, and
then proceed in Section 3 to describe our models
for document ranking and in Section 4 the models
for query reﬁnement.

2 Neural Auto-Encoder

In this study, we use an unsupervised method to
encode text into a dense vector representation. Our
goal is to investigate if we can use this repre-
sentation as an encoding of the relations between
biomarker entities.

Following Sutskever et al. (2014) Cho et al.

Figure 1: Document Ranking

(2014), Dai and Le (2015), and Li et al. (2015) we
implemented a text auto-encoder with a Sequence-
to-Sequence approach. In this model an encoder
Enc produces a vector representation v = Enc(d)
of an input document d = [w1, w2, . . . , wn],
with wi being word embedding representations
(Mikolov et al., 2013). This dense representation
v is then fed to a decoder Dec, that tries to recon-
struct the original input, i.e. ˆd = Dec(v). During
training we minimize error( ˆd, d). After training
we only use the Enc(d) to encode the text. We
want to explore if we can use Enc to encode the
documents and the query. We will use the out-
put of the document encoder Enc as features for
a document ranking model and for a query reﬁne-
ment model.

3 Document Ranking
Information retrieval systems rank documents in
the order that is estimated to be most useful to a
user query by assigning a numeric score to each
document. Our pipeline for document ranking is
depicted in Figure 1: Given a query q, we ﬁrst re-
trieve a set of documents Dq that contain all of
the query terms. Then, we compute a representa-
tion repq(q) for the query q, and a representation
repd(d) for each document d ∈ Dq. Finally, we
compute the score with a ranker model scorerank.
For repd we need to ﬁnd a representation for
an arbitrary number of entity-type combinations,
because a fact can consist of e.g. 3 Genes, 4 Vari-
ants, 1 Drug, 0 Diseases and 0 Outcomes. In the
following, we describe several of the settings for
repq(q), repd(d) and the ranker model.

883.1 Bag-of-Words Models
We have implemented two commonly used BOW
models tf-idf and bm25. For these models the text
representations repq(q) and repd(d) is the vector
space model.

3.2 Learning-to-Rank Models
For the learning-to-rank models, we chose a mul-
tilayer perceptron (MLP) as the scoring function
scorerank.
In the following we explain how
repq(q) and repd(d) are computed.
Feature Engineering Model We created a set of
basic features: encoding the frequency of entity
types, distance features between entity types, and
context words of entities. In this model, features
are query dependent and are computed on-demand
by a feature function f (q, d).
The algorithm to compute the distance feature
is as follows: Given query q with entities e ∈ q
and document d = [w1, w2, . . . , wn], with w being
words in the document. Let type(e) be the func-
tion that yields the entity type, f.ex. type(e) =
Gene. Then, if ei, ej ∈ q and there exists a
wk = ei, wl = ej then we add |l − k| to the
bucket of {type(ei), type(ej)}. To summarize the
collected distances we compute min(), max(),
mean() and std() over all collected distances for
each bucket separately.

For the context words feature, we collected in
a prior step a list of the top 20 words for each
{type(ei), type(ej)} bucket, i.e. we collect words
that are between wk = ei and wl = ej if |k − l| <
10. We remove stop words, special characters
and numbers from this list and also manually re-
move words using domain knowledge. The top 20
of remaining words for each {type(ei), type(ej)}
bucket are used as boolean indicator features.
Auto-Encoder Model
In this model we use the
auto-encoder from Section 2 to encode the query
and the document. The input to the score function
is the element-wise product, denoted by (cid:12), of the
query encoding repq(q) = Enc(q) = q and the
document encoding repd(d) = Enc(d) = d:

scorerank(d, q) = M LP (d (cid:12) q)

(1)

To encode the queries we compose a pseudo text
using the query terms. The input to the auto-
encoder Enc are the word embeddings of the
pseudo text for repq(q) and the word embeddings
of the document terms for repd(d).

Figure 2: Query Reﬁnement

4 Query Reﬁnement

Query reﬁnement ﬁnds additional terms for the
initial query q that better describe the information
need of the user (Nallapati and Shah, 2006).
In
our approach we follow Cao et al. (2008), in which
the ranked documents Dq are used as pseudo rel-
evance feedback. Our goal is to suggest relevant
entities e that are contained in Dq and that are in a
biomarker relationship to q. Therefore, we deﬁne
a scoring function scoreref for ranking of candi-
date entities e to the query q with respect to the re-
trieved document set Dq. See Figure 2 for a sketch
of the query reﬁnement pipeline. In the following,
we describe several of the scoring functions.

4.1 Bag-of-Words Models
We implemented the two classic query reﬁne-
ment models the Rocchio algorithm (Rocchio and
Salton, 1965) and Relevance Model.

4.2 Auto-Encoder Model
In this model, we also try to exploit the auto-
encoding of the query. The idea is as follows: (i)
Given a list of documents and their scores Dq =
[(d1, s1), (d2, s2), . . . , (dn, sn)] for a query q from
ument repc(D, ˆs) = (cid:80)n
the previous step, we use the ranking score si as
pseudo-relevance feedback to create a pseudo doc-
i diˆsi = ˆd. The scores s
(cid:80)
are normalized so that they are non-negative and
i ˆsi = 1, see Appendix A.1. (ii) From all enti-
ties ei ∈ Dq\q we generate new query encoding
repq(q ∪ ei) = Enc(q ∪ ei) = ˆqei (iii) We rank
the entities based on the pseudo document using
the scoring function

scoreref (ˆqei, ˆd) = M LP (ˆd (cid:12) ˆqei)

(2)

89i.e. we propose those entities as a query reﬁnement
that agree with the most relevant documents.

5 Experiments

In this section, we ﬁrst explain our evaluation
strategy to assess the performance of the respec-
tive models for document ranking and query re-
ﬁnement. Subsequently, we describe the settings
for the data and the results of the experiments that
we have conducted.

5.1 Evaluation Protocol
Document ranking models are evaluated by their
ability to rank relevant documents higher than ir-
relevant documents. Query reﬁnement models are
evaluated both, by their ability to rank relevant
query terms high, and by the recall of retrieved
relevant documents when the query automatically
is reﬁned by the 1st, 2nd and 3rd proposed query
term. We evaluate our models using mean aver-
age precision (MAP) (Manning et al., 2008, Chap-
ter 11) and normalized discounted cumulative gain
(nDCG) (J¨arvelin and Kek¨al¨ainen, 2000).

For both the document ranking and query re-
ﬁnement approach we interpret a biomarker-fact
as a perfect query and the corresponding papers
as the true-positive (or relevant) papers associ-
ated with this query. In this way, we use the cu-
rated facts as document level annotation for our
approach. Because we want to assist the itera-
tive approach of curators in which they reﬁne an
initially broad query to ever narrower searches,
we need to create valid partial queries and asso-
ciated relevant documents to mimic this proce-
dure. Therefore, we generate sub-queries, which
are partials of the facts. We generated two data
sets: one for document ranking and one for query
reﬁnement. For document ranking, we generated
all distinct subsets of the facts. For query reﬁne-
ment, we deﬁned the eliminated entities (of the
sub-query generation process) as true-positive re-
ﬁnement terms. For both data sets, we use all asso-
ciated documents, of the original biomarker-fact,
as true-positive relevant documents.

5.2 Data
Unlabeled Data As unlabeled data we use ∼24
Million abstracts of PubMed. To automatically
annotate PubMed abstracts with disambiguated
biomarker entities, we use a tool set that has been
developed together with biomedical curators.
It

employs ”ProMiner”2 (Hanisch et al., 2005) for
Protein/Gene and Disease entity recognition and
regular expression based text scanning using syn-
onyms from ”DrugBank”3 and ”PubChem”4 for
the identiﬁcation of Drugs and manually edited
regular expressions, relating to ”HGVS”5 stan-
to retrieve Variants. We restricted the
dards,
PubMed documents to include at least one entity
of type Gene, Drug and Disease leaving us with
2.7 Million documents. Additionally we replaced
the text of every disambiguated entity with its id.

Labeled Data As labeled data we use a knowl-
edge base that contains a set of 5833 hand curated
{Gene(s) - Variant(s) - Drug(s) - Disease(s) - Out-
come} biomarker-facts and their associated papers
that domain experts extracted from ∼1200 full-
text documents. We only keep facts in which the
disambiguated entities are fully represented in the
available abstracts. This restricted our data set to a
set of 1181 distinct facts. The 4 top curated genes
are EGFR (29%), BRAF (13%), KRAS (8%), and
PIK3CA (5%).

Cross Validation To exploit all of our labeled
data for training and testing we do 4-fold cross-
validation. Because in our scenario a curator starts
with an initial entity of type Gene we have gener-
ated our validation and test sets based on Genes,
instead of randomly sampling facts. This also
guarantees us to never have the same sub-query
included in the training, validation and test set. In
total we have built 12 different splits of our data
set basing the validation and test set each on a dif-
ferent gene. The respective training sets are built
with all remaining facts that do not include the val-
idation and test gene. Statistics can be found in
Table 1.

5.3 Training of Embeddings and

Auto-Encoder

The training data for the embeddings and the auto-
encoder are the PubMed abstracts described in the
previous Section 5.2. We trained the embeddings
with Skip-Gram. For the vocabulary, we keep the
top 100k most frequent words, while making sure
all known entities are included. We use a win-
dow size of 10 and train the embeddings for 17

2https://www.scai.fraunhofer.de/en/business-research-

areas/bioinformatics/products/prominer.html

3https://www.drugbank.ca
4https://pubchem.ncbi.nlm.nih.gov
5http://www.hgvs.org

90Testing

Validation
#Data
#Data Gene
Gene
1822
EGFR
1135
BRAF
KRAS
1075
968
BRAF
PIK3CA 549
1088
BRAF
1241
BRAF
1700
EGFR
KRAS
1475
968
EGFR
PIK3CA 549
1605
EGFR
573
EGFR
1822
KRAS
BRAF
804
1241
KRAS
PIK3CA 549
KRAS
778
1822
EGFR
PIK3CA 298
PIK3CA 382
BRAF
1241
968
KRAS
PIK3CA 367

Train
#Data
4386
6310
6944
4386
5536
5957
5536
6310
7754
5957
6944
7754

Table 1: Statistics about the train/validation/test
splits

epochs. We normalize digits to ”0” and lower-
case all words. Tokenization is done by splitting
on white space and before and after special char-
acters.

For both, the encoder and the decoder, we use
two LSTM cells per block with hidden size 400
each. We skip unknown tokens and feed the
words in reverse order for the decoder following
Sutskever et al. (2014). The auto-encoder was
trained for 15 epochs using early stopping.

5.4 Document Ranking
In this section, we describe the document ranking
models, their training and then discuss the results
of their evaluation.

Models We evaluate the BOW models (tf-idf,
bm25) (Section 3.1) and the two LTR models using
feature engineering (feat) and the auto-encoded
features (auto-rank) (Section 3.2). We also eval-
uate an additional set of models to investigate if
maybe simpler solutions can be competitive. See
Table 2 for an overview over all ranking models.
(a.) A simpler solution than learning a M LP
for a score function is to compute the similarity
between q = Enc(q) and the document encod-
ing d = Enc(d). Therefore, we use the cosine
similarity as scoring function between the vector
representations q and d (auto/cosine).

(b.)

Instead of encoding the documents and
queries with the auto-encoder, we encode the doc-
uments and queries based on their tf-idf weighted

embeddings, i.e. q =(cid:80) tf idf (qi)∗wqi. Similarly

to the auto-rank model, the input to the classiﬁer
M LP is the element-wise product of the query en-
coding and the document encoding (emb).

(c.) Due to promising results of the auto-rank
model, bm25, and the feat model, we also tested
combinations of them. We tested the concatena-
tion of the the bm25 score with the auto-rank fea-
tures (auto-rank + bm25) as well as the concatena-
tion of feat with the auto-rank features (auto-rank
+ feat).

Training We train our models with Adam
(Kingma and Ba, 2014) and tune the initial learn-
ing rate, the other parameters are kept default of
TensorFlow6. We use a pairwise hinge loss (Chen
et al., 2009) and compare relevant documents with
irrelevant documents.

The ranking score function is parameterized by
a MLP for which the number of layers is a hyper-
parameter which is tuned using grid-search. The
input layer size is based on the number of input
features. To limit the total number of parameters,
we decrease the layer size while going deeper, i.e.
|u|, with b being the
layer i has size li = b−i+1
depth of the network, |u| the number of input fea-
tures. For activation functions between layers we
use ReLU (Glorot et al., 2011).

b

We employ grid search over

the hyper-
[0.3, 0.2, 0.1, 0.0], number
parameters: dropout:
[1, 2, 3, 4], learning rates for Adam:
of layers:
[40, 60], max 400
[0.0005, 0.001], batch size:
epochs. We conducted hyper-parameter tuning
for each model and validation/test split separately.
The best parameters for the models using the auto-
encoded features were: 1 layer, dropout p ∈
[0.3, 0.2] with a batch size of 60 and learning rate
0.0005. The feat models were best with 1-2 layers,
dropout 0.0 and a learning rate at 0.001.

The BOW models as well as auto/cosine were
only computed for the respective validation and
test sets.

Results
In Table 3 we have listed the average
MAP and nDCG scores of the test sets. The tf-idf
model is outperformed by most of the other mod-
els. However, bm25, which additionally takes the
length of a document into account, performs very
tf-idf and bm25 have the major beneﬁt of
well.
fast computation.

The feat model slightly outperforms the auto-

6Tensorﬂow V 1.3 https://www.tensorﬂow.org/api docs/

python/tf/train/AdamOptimizer

91Model
tf-idf
bm25
emb
feat
auto/cosine
auto
auto + bm25
auto + feat

repq(q)
BOW
BOW
q = tf-idf BOW ·w

score
dot product
bm25
M LP (q (cid:12) d)
M LP (f (q, d))
q = Enc(q)
cos(Enc(q), Enc(d))
M LP (q (cid:12) d)
q = Enc(q)
q = Enc(q) , BOW d = Enc(d), BOW + doc length M LP (q (cid:12) d, bm25)
M LP (q (cid:12) d, f (q, d))

repd(d)
BOW
BOW + doc length
q = tf-idf BOW ·w
f (q, d)
d = Enc(d)
d = Enc(d)

d = Enc(d)

q = Enc(q)

f (q, d)

Table 2: Query and document representation for ranking models

Test

Metric

tf-idf

bm25

emb

feat

EGFR

KRAS

BRAF

PIK3CA

MAP
nDCG
MAP
nDCG
MAP
nDCG
MAP
nDCG

0.289
0.424
0.327
0.456
0.342
0.480
0.341
0.473

0.632
0.728
0.610
0.723
0.656
0.751
0.633
0.729

0.310
0.460
0.466
0.592
0.427
0.572
0.486
0.617

0.575
0.695
0.609
0.712
0.704
0.802
0.625
0.718

auto/
cosine

auto-
rank

0.054
0.129
0.058
0.145
0.063
0.163.
0.079
0.171

0.545
0.653
0.575
0.688
0.563
0.671
0.541
0.656

auto-
rank
+ bm25
0.588
0.716
0.774
0.867
0.702
0.820
0.779
0.859

auto-
rank
+ feat
0.699
0.810
0.820
0.914
0.812
0.901
0.810
0.895

Table 3: Test Scores Document Ranking

auto-rank + feat model is slightly better than the
auto-rank + bm25 model, both of which have the
overall best performance. This shows, that the
auto-encoder learns something orthogonal to term
frequency and document length. The best model
with respect to document ranking is the auto-rank
+ feat model.

In Figure 3 we show the correlation between
the different models. Interestingly, the bm25 and
the feat strongly correlate. However, the scores
of bm25 do not correlate with the scores of the
combination of auto-rank and bm25. This indi-
cates, that the model does not primarily learn to
use the bm25 score but also focuses on the the
auto-encoded representation. This underlines the
hypothesis that the auto-encoder is able to repre-
sent latent features of the relationship of the query
terms in the document.

Inﬂuence of the Data
It is interesting to see that
the learned models do not perform well for the
EGFR set. The reason for this might be that testing
on it reduces the amount of training data substan-
tially, as EGFR is the best curated gene and thus
the largest split of the data set.

In a manual error analysis we compared the

Figure 3: Correlation of Document Ranking Mod-
els

rank model. The distance features are a strong in-
dicator for the semantic dependency between enti-
ties. These relationships need to be learned in the
auto-rank model.

The cosine similarity of a query and a docu-
ment (auto/cos) does not yield a good result. This
shows that the auto-encoder has learned many fea-
tures, most of which do not correlate with our task.
We also ﬁnd that emb does not yield an equal per-
formance to auto-rank. The combination of the

92rankings of four of the best models (bm25, feat,
auto-rank, and auto-rank + bm25). We observe
cases where the auto-rank model is unable to de-
tect, when similar entities are used, i.e. entities
like Neoplasm and Colorectal Neoplasm. In these
cases the bm25 helps, as it treats different words
as different features. However, both bm25 and
the feat models rank reviews high, that only list
query terms. For example, when executing the
query {BRAF, PIK3CA, H1047R}, these models
rank survey articles high (i.e.
(Li et al., 2016);
(Janku et al., 2012); (Chen et al., 2014)).

The auto-rank model on the other hand ranks
those document high, for which each entities are
listed in a semantic relationship (i.e.
(Falchook
et al., 2013)).

5.5 Query Reﬁnement

In this section we describe our training approaches
for query reﬁnement and discuss our ﬁndings. The
pseudo relevance feedback for the query reﬁne-
ment is based on the ranked documents from the
previous query. For our experiments we chose the
second best document ranker (auto-rank + bm25)
from the previous experiments, because our pro-
totype implementation for auto-rank + feat was
computationally too expensive.

Models We combined both the auto-encoder
features with the candidate terms of the respective
BOW models (auto-ref + rocch + relev). In order
to identify if the good results of this combination
are due to the BOW models, or if the auto-encoded
features have an effect, we trained a MLP with the
same amount of parameters, but only use the fea-
tures of the two BOW models as input (rocchio +
relev).

Training For training, we use the same settings
for query reﬁnement as for document ranking and
again use a pairwise hinge loss. Here we com-
pare entities that occur in the facts with randomly
sampled entities which occur in the retrieved doc-
uments.

Due to limitations in time we were only able to
test our query reﬁnement models on one valida-
tion/test split. We chose to use the split data set of
genes KRAS and PIK3CA for validation and test-
ing respectively. We have restricted our models to
only regard the top 50 ranked documents for re-
ﬁnement.

Results To evaluate the ranking of entity terms
we have computed nDCG@10, nDCG@100 and
MAP, see Table 4 for the results. We also compute
Recall@k of relevant documents for automatically
reﬁned queries using the 1st, 2nd and 3rd ranked
entities. The scores can be found in Table 5.

Tables 4 and 5 show that the Relevance Model
outperforms the Rocchio algorithm in every as-
pect. Both models outperform the auto-encoder
approach (auto-ref ). We suspect that summing
over the encodings distorts the individual features
too much for a correct extraction of relevant enti-
ties to be possible.

The combination of all three models (auto-ref +
rocchio + relevance) outperforms the other mod-
els in most cases. Especially the performance for
ranking of entity terms is increased using the auto-
encoded features. However, it is interesting to see
that the rocchio + relevance model outperforms
the recall for second and third best terms. This
indicates that for user-evaluated term suggestions,
the inclusion of the auto-encoded features is ad-
visable. For automatic query reﬁnement however,
in average, this is not the case.

Diseases
Color. Neop.
Liposarcoma
Adenocarcin.
Glioblastoma
Stomach Neop.

Variants
H1047R
V600E
T790M
E545K
E542K
Table 6: Reﬁnement Terms for Query {PIK3CA}

Drugs
Lapatinib
Mitomycin
Linsitinib
Dactolisib
Pictrelisib

Query Reﬁnement Example
In Table 6 we
show the top ranked entities of type Variants, Dis-
eases and Drugs for the query {PIK3CA}. While
the diseases and the drugs are all relevant, V600E
and T790M are in fact not variants of the gene
PIK3CA.

However, when reﬁning the query {PIK3CA,
V600E, BRAF, H1047R, Dabrafenib},
the top
ranked diseases are [Melanoma, Neoplasms, Car-
cinoma Non Small Cell Lung (CNSCL), Thy-
roid Neoplasms, Colorectal Neoplasms]. Using
Melanoma for reﬁnement, retrieves the top ranked
paper (Falchook et al., 2013) which perfectly in-
cludes all these entities in a biomarker relation-
ship.

93Metrics

rocchio

nDCG@10
nDCG@100
MAP

0.232
0.360
0.182

relevance
model

auto-ref

rocchio
+ relevance

0.274
0.397
0.223

0.195
0.329
0.156

0.341
0.439
0.270

auto-ref
+ rocchio
+ relevance

0.464
0.536
0.386

Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing

Metrics

Recall@10

Recall@40

Top
n-th
Entity

1
2
3
1
2
3

rocchio

relevance
model

auto-ref

rocchio
+ relevance

0.594
0.535
0.533
0.683
0.603
0.610

0.603
0.561
0.544
0.691
0.633
0.623

0.272
0.339
0.366
0.307
0.374
0.402

0.574
0.580
0.555
0.680
0.649
0.626

auto-ref
+ rocchio
+ relevance

0.696
0.522
0.458
0.779
0.586
0.522

Table 5: Reﬁnement Recall Scores for KRAS Validation and PIK3CA Testing

6 Related Work

The focus of research in this domain has primar-
ily targeted the extraction of entity relations. Peng
and Lu (2017), Peng et al. (2015), and Li et al.
(2017) try to extract Protein-Protein relationships.
Asada et al. (2017) try to extract Drug-Drug inter-
action and Lee et al. (2018) target the extraction
of Mutation-Gene and Mutation-Drug relations.
Jameson (2017) have derived a document ranking
approach for PubMed documents using word em-
beddings trained on all PubMed documents. Xu
et al. (2017) propose using auto-encoders on the
vector-space model in a supervised setting for in-
formation retrieval and show that it improves per-
formance. The quality of biomedical word em-
beddings was investigated by Th et al. (2015) and
Chiu et al. (2016). Dogan et al. (2017) have devel-
oped an open source data set to which we would
like to adapt our approach. Sheikhshab et al.
(2016) have developed a novel approach for tag-
ging genes which we would like to explore.

Palangi et al. (2016) use LSTMs to encode the
query and document and use the cosine similar-
ity together with the click-through data as features
for ranking in a supervised approach. Cao et al.
(2008) deﬁne a distance based feature-engineered
supervised learning approach to identify good ex-
pansion terms. They try to elaborate if the se-
lected terms for expansion are useful for informa-
tion retrieval by identifying if the terms are ac-

tually related to the initial query. Nogueira and
Cho (2017) have introduced a reinforcement learn-
ing approach for query reﬁnement using logging
data. They learn a representation of the text and
the query using RNNs and CNNs and reinforce
the end result based on recall of a recurrently
expanded query. Sordoni et al. (2015) have de-
veloped a query reformulation model based on
sequences of user queries. They have used a
Sequence-to-Sequence model using RNNs to en-
code and decode queries of a user.

7 Conclusion
We have considered several approaches for docu-
ment ranking and query reﬁnement by investigat-
ing classic models, feature engineering and, due
to the large amount of unlabeled data, a semi-
supervised approach using a neural auto-encoder.
Leveraging the large amounts of unlabeled data
to learn an auto-encoder on text documents yields
semantically descriptive features that make sub-
sequent document ranking and query reﬁnement
feasible. The combination with BOW features in-
creases the performance substantially, which for
our experiments, outputs the best results, for both
document ranking and query reﬁnement.

We were able to achieve promising results,
however, there is a wide range of Sequence-to-
Sequence architectures and text encoding strate-
gies, therefore, we expect that there is room for
improvement.

94References
Masaki Asada, Makoto Miwa, and Yutaka Sasaki.
2017. Extracting drug-drug interactions with atten-
tion cnns. In BioNLP 2017, Vancouver, Canada, Au-
gust 4, 2017, pages 9–18.

Francis Charles Brunicardi, Richard A. Gibbs,
David A. Wheeler,
John Nemunaitis, William
Fisher, John Goss, and Changyi Johnny Chen. 2011.
Overview of the development of personalized ge-
nomic medicine and surgery. World Journal of
Surgery, 35:1693–1699.

Guihong Cao,

Jian-Yun Nie,

Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expansion
In Proceed-
terms for pseudo-relevance feedback.
ings of the 31st Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR 2008, Singapore, July 20-
24, 2008, pages 243–250.

Jing Chen, Fang Guo, Xin Shi, Lihua Zhang, Aifeng
Zhang, Hui Jin, and Youji He. 2014. BRAF V600E
mutation and KRAS codon 13 mutations predict
poor survival in Chinese colorectal cancer patients.
BMC Cancer, 14(1):802.

Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhiming Ma,
and Hang Li. 2009. Ranking measures and loss
functions in learning to rank. In Advances in Neu-
ral Information Processing Systems 22: 23rd An-
nual Conference on Neural Information Processing
Systems 2009. Proceedings of a meeting held 7-
10 December 2009, Vancouver, British Columbia,
Canada., pages 315–323.

Billy Chiu, Gamal K. O. Crichton, Anna Korhonen,
and Sampo Pyysalo. 2016. How to train good word
embeddings for biomedical NLP. In Proceedings of
the 15th Workshop on Biomedical Natural Language
Processing, BioNLP@ACL 2016, Berlin, Germany,
August 12, 2016, pages 166–174.

Kyunghyun Cho, Bart van Merrienboer, C¸ aglar
G¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2014, October
25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
a Special Interest Group of the ACL, pages 1724–
1734.

Andrew M. Dai and Quoc V. Le. 2015.

Semi-
supervised sequence learning. In Advances in Neu-
ral Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Que-
bec, Canada, pages 3079–3087.

Rezarta Islamaj Dogan, Andrew Chatr-aryamontri, Sun
Kim, Chih-Hsuan Wei, Yifan Peng, Donald C.
Comeau, and Zhiyong Lu. 2017. Biocreative VI pre-
cision medicine track: creating a training corpus for

mining protein-protein interactions affected by mu-
tations. In BioNLP 2017, Vancouver, Canada, Au-
gust 4, 2017, pages 171–175.

Gerald S. Falchook, Jonathan C. Trent, Michael C.
Heinrich, Carol Beadling, Janice Patterson, Chris-
tel C. Bastida, Samuel C. Blackman, and Razelle
Kurzrock. 2013.
BRAF Mutant Gastrointesti-
nal Stromal Tumor: First report of regression
with BRAF inhibitor dabrafenib (GSK2118436) and
whole exomic sequencing for analysis of acquired
resistance. Oncotarget, 4(2).

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
In
2011. Deep sparse rectiﬁer neural networks.
Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011, pages 315–323.

Daniel Hanisch, Katrin Fundel, Heinz-Theodor Mevis-
sen, Ralf Zimmer,
and Juliane Fluck. 2005.
Prominer: rule-based protein and gene entity recog-
nition. BMC Bioinformatics, 6(S-1).

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Anthony Jameson. 2017. A tool that supports the psy-
chologically based design of health-related interven-
tions. In Proceedings of the 2nd International Work-
shop on Health Recommender Systems co-located
with the 11th International Conference on Recom-
mender Systems (RecSys 2017), Como, Italy, August
31, 2017., pages 39–42.

Filip Janku, Jennifer J. Wheler, Aung Naing, Vanda
M. T. Stepanek, Gerald S. Falchook, Siqing Fu, Ig-
nacio Garrido-Laguna, Apostolia M. Tsimberidou,
Sarina A. Piha-Paul, Stacy L. Moulder, J. Jack Lee,
Rajyalakshmi Luthra, David S. Hong, and Razelle
Kurzrock. 2012. PIK3CA Mutations in Advanced
Cancers: Characteristics and Outcomes. Oncotar-
get, 3(12).

Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2000. IR eval-
uation methods for retrieving highly relevant docu-
ments. In SIGIR 2000: Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
July 24-28, 2000, Athens, Greece, pages 41–48.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Victor Lavrenko and W. Bruce Croft. 2001. Relevance-
based language models. In SIGIR 2001: Proceed-
ings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, September 9-13, 2001, New Or-
leans, Louisiana, USA, pages 120–127.

95Kyubum Lee, Byounggun Kim, Yonghwa Choi,
Sunkyu Kim, Won-Ho Shin, Sunwon Lee, Sungjoon
Park, Seongsoon Kim, Aik Choon Tan, and Jaewoo
Kang. 2018. Deep learning of mutation-gene-drug
relations from the literature. BMC Bioinformatics,
19(1):21:1–21:13.

Yifan Peng, Samir Gupta, Cathy H. Wu, and K. Vijay-
Shanker. 2015. An extended dependency graph for
relation extraction in biomedical texts. In Proceed-
ings of the Workshop on Biomedical Natural Lan-
guage Processing, BioNLP@IJCNLP 2015, Beijing,
China, July 30, 2015, pages 21–30.

Gang Li, Cathy H. Wu, and K. Vijay-Shanker. 2017.
Noise reduction methods for distantly supervised
In BioNLP 2017,
biomedical relation extraction.
Vancouver, Canada, August 4, 2017, pages 184–193.

Yifan Peng and Zhiyong Lu. 2017. Deep learning for
extracting protein-protein interactions from biomed-
ical literature. In BioNLP 2017, Vancouver, Canada,
August 4, 2017, pages 29–38.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A Hierarchical Neural Autoencoder for Paragraphs
and Documents.

Wan-Ming Li, Ting-Ting Hu, Lin-Lin Zhou, Yi-Ming
Feng, Yun-Yi Wang, and Jin Fang. 2016. Highly
sensitive detection of the PIK3CA H1047R muta-
tion in colorectal cancer using a novel PCR-RFLP
method. BMC Cancer, 16(1):454.

Hans Peter Luhn. 1957. A statistical approach to mech-
anized encoding and searching of literary informa-
IBM Journal of Research and Development,
tion.
1(4):309–317.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval. Cambridge University Press.

Teri A. Manolio. 2010. Genomewide Association
Studies and Assessment of the Risk of Disease. New
England Journal of Medicine, 363(2):166–176.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
In Advances in Neural Information
positionality.
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–
3119.

Ramesh Nallapati and Chirag Shah. 2006. Evaluating
the quality of query reﬁnement suggestions in infor-
mation retrieval.

Rodrigo Nogueira and Kyunghyun Cho. 2017. Task-
oriented query reformulation with reinforcement
learning. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2017, Copenhagen, Denmark, Septem-
ber 9-11, 2017, pages 574–583.

Hamid Palangi, Li Deng, Yelong Shen, Jianfeng
Gao, Xiaodong He, Jianshu Chen, Xinying Song,
and Rabab K. Ward. 2016. Deep sentence em-
bedding using long short-term memory networks:
Analysis and application to information retrieval.
IEEE/ACM Trans. Audio, Speech & Language Pro-
cessing, 24(4):694–707.

Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3(4):333–389.

Joseph Rocchio and Gerard Salton. 1965. Information
search optimization and interactive retrieval tech-
niques. In AFIPS ’65 (Fall, part I).

Golnar Sheikhshab, Elizabeth Starks, Aly Karsan,
Anoop Sarkar, and Inanc¸ Birol. 2016. Graph-based
semi-supervised gene mention tagging. In Proceed-
ings of the 15th Workshop on Biomedical Natural
Language Processing, BioNLP@ACL 2016, Berlin,
Germany, August 12, 2016, pages 27–35.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In Proceedings of the 24th ACM International
Conference on Information and Knowledge Man-
agement, CIKM 2015, Melbourne, VIC, Australia,
October 19 - 23, 2015, pages 553–562.

Karen Sp¨arck Jones. 1972. A Statistical Interpreta-
tion of Term Speciﬁcity and its Retrieval. Journal
of Documentation, 28(1):11–21.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Muneeb Th, Sunil Kumar Sahu, and Ashish Anand.
2015. Evaluating distributed word representations
for capturing semantics of biomedical concepts. In
Proceedings of the Workshop on Biomedical Natu-
ral Language Processing, BioNLP@IJCNLP 2015,
Beijing, China, July 30, 2015, pages 158–163.

Bo Xu, Hongfei Lin, Yuan Lin, and Kan Xu. 2017.
Learning to rank with query-level semi-supervised
autoencoders. In Proceedings of the 2017 ACM on
Conference on Information and Knowledge Man-
agement, CIKM 2017, Singapore, November 06 - 10,
2017, pages 2395–2398.

96A Supplemental Material
A.1 Normalizing Document Scores for Query

Reﬁnement

Because we used a hinge loss instead of cross en-
tropy loss in the ranking model, we cannot inter-
pret the scores s of the ranker as logits. While we
do not know the magnitude of the ranker score,
we do, however, expect the scores to be positive
for relevant documents.
If however many docu-
ments have scores below zero, this should also be
regarded. Based on this, we have deﬁned a nor-
malization setting of the document scores:

smin = min(min(s), 0.0)

ˆs =

(cid:80)|s|
s − smin
i (si − smin)

(3)

(4)

97
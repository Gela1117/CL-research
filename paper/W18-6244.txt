Learning representations for sentiment classiﬁcation using Multi-task

framework

Hardik Meisheri

TCS Research
Mumbai, India

Harshad Khadilkar

TCS Research
Mumbai, India

hardik.meisheri@tcs.com

harshad.khadilkar@tcs.com

Abstract

Most of the existing state of the art senti-
ment classiﬁcation techniques involve the use
of pre-trained embeddings. This paper postu-
lates a generalized representation that collates
training on multiple datasets using a Multi-
task learning framework. We incorporate pub-
licly available, pre-trained embeddings with
Bidirectional LSTM’s to develop the multi-
task model. We validate the representations
on an independent test Irony dataset that can
contain several sentiments within each sample,
with an arbitrary distribution. Our experiments
show a signiﬁcant improvement in results as
compared to the available baselines for indi-
vidual datasets on which independent models
are trained. Results also suggest superior per-
formance of the representations generated over
Irony dataset.

1

Introduction

Sentiment analysis has attracted substantial re-
search interest, especially in the ﬁeld of social me-
dia, owing to the growing number of data and ac-
tive users.
In addition, the research community
has gravitated towards a pragmatic characteriza-
tion of language with the division into (and quan-
tiﬁcation of) speciﬁc emotions for sentiment anal-
ysis. This approach has come to prominence in
recent times as a large number of enterprises (not
just social media corporations) now rely on under-
standing customer sentiments for deﬁning prod-
uct and marketing strategies (Pang and Lee, 2004;
Socher et al., 2012).

Beyond strategic inputs, sentiment analysis also
performs a tactical role in the age of rapid (viral)
increases and decreases in the visibility of spe-
ciﬁc events, with magniﬁed consequences for cor-
porations and communities at large. For exam-
ple, United Airlines faced signiﬁcant business im-
pact due to a single (possibly isolated) passenger-

related incident, due to its spread over Twitter1. It
is conceivable that an automated system quickly
alerting the management about the rate and depth
of negative sentiments due to the incident, would
have enabled them to produce a more amelioratory
response from the outset.

Complementary to such motivating incidents is
the recent availability of large datasets from social
media sources. Twitter has become a go-to choice
for scraping data due to its large user base and the
easy accessibility of tweets through its API. The
result is a large corpus of complex sentiments for
identiﬁcation and analysis. Tweets (the messages
posted on Twitter) are limited to 140 characters,
which creates a plethora of challenges as the users
ﬁnd new and innovative ways of condensing the
messages using slang, hashtags, and emojis, often
defying traditional grammatical rules of the lan-
guage. This is further complicated by the fast, lo-
calized rise and decay of popular memes, slang,
and hashtags.

Traditional sentiment analysis using dictionary-
based methods has failed to capture these nuances,
as the methods rely on grammatically correct, in-
tact syntactic and semantic structures which are
not followed in this space. Traditional senti-
ment analyzers such as (Akkaya et al., 2009; Po-
ria et al., 2014; Sharma and Bhattacharyya, 2013)
that worked well with well-written texts, face chal-
lenges at lexical, syntactic and semantic levels
when dealing with tweets as analyzed in (Liu,
2012). Bag-of-words models and naive Bayes
models are sequence-agnostic, and have therefore
failed to generalize over a diverse distribution of
sentiments, especially when multiple ﬁne-grained
emotions are compressed into a 140 character
message. Word vectors trained on a large corpus
to represent the word in dense representations have

1https://twitter.com/i/moments/851423833160634368

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages299–308Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P17299proved to be efﬁcient in handling sentiment anal-
ysis and effective emotions. Deep learning and
speciﬁcally Recurrent Neural Networks have been
extensively used with word vectors to achieve state
of the art results on various sentiment analysis
tasks. Although there are large datasets available
on social media space, deep learning models re-
quire annotated data for supervised training. An-
notation for such a large dataset is expensive, since
multiple human annotators are required per sam-
ple for stable convergence.

A useful research question is how to leverage
resources available on social media sites to im-
prove sentiment classiﬁcation across datasets by
leveraging the generic representations and han-
dling the noise present in the space. These chal-
lenges have led people to use transfer learning and
multi-task learning approaches to transfer knowl-
edge across different datasets and languages. Re-
cently, neural-network-based models for multi-
task learning have become very popular, ranging
from computer vision (Misra et al., 2016; Nam and
Han, 2016) to natural language processing (Col-
lobert et al., 2011a; Luong et al., 2015), since they
provide a convenient way of combining informa-
tion from multiple tasks.

We propose a dual Attention based deep learn-
ing model which creates representations using
Bidirectional LSTM. In particular, given an in-
put tweet, our model ﬁrst uses a pair of bidirec-
tional LSTMs to learn a general representation.
This portion of the model is trained in a multi-
task framework. The general sentence representa-
tion is then mapped into a task-speciﬁc represen-
tation through an attention mechanism, so that the
most salient parts of the input are selected for each
task. We achieve signiﬁcant improvement over the
baselines and obtain comparable results with the
state of the art methods without any feature engi-
neering.

We have selected datasets which classify a text
into 3 classes, along with affect dataset. Affec-
tive dimensions provide much more granular anal-
ysis over emotions that are being conveyed. Af-
fective emotions are classiﬁed along the valence,
arousal and dominance axis according to circum-
plex model of affect, a well-established system for
describing emotional states (Russell, 1980; Posner
et al., 2005). Of these states, valence can directly
be mapped to sentiment classiﬁcation. These
scales represent valence (or sentiment) and arousal

(or intensity), which deﬁnes each posts position on
the circumplex of the 3 dimension

The major contributions of this paper are:
• Generating robust representation of a tweet
from three different set of pre-trained embed-
dings which can handle emoji/smileys and
out-of-vocabulary words in the dataset.
• Multi Task learning frame work using Bidi-
rectional Long short Memory Networks
(BiLSTM) and attention mechanism to ef-
fectively learn the representations across
datasets.

We evaluate the effectiveness of the model with
respect to both internal and external distribution.
The former refers to the setting where distribution
of the test data falls in one of the m training tasks,
and the latter refers to the setting where task and
data are different and we use just the representa-
tion to train the task-speciﬁc layers.

Rest of the paper is organized as follows, sec-
tion 2 discusses works related to multi-task learn-
ing along the lines of sentiment analysis. We
present our proposed approach in section 3, which
details the system architecture and its key compo-
nents. Two sets of experiments and results shown
in section 4 and 5 respectively. Finally section 6
concludes the paper with future direction.

2 Related Work

The current state of the art models for classifying
the sentiments over social media text speciﬁcally
tweets use a mixture of handcrafted features and
pre-trained embeddings. Lexicon-based features
along with neural network models to predict inten-
sity of emotions have been proposed (Mohammad
et al., 2013; Wilson et al., 2005; Ding et al., 2008;
Bravo-Marquez et al., 2016; Esuli and Sebastiani,
2007) which have proved successful. However,
these representations do not generalize well when
there is a change in the vocabulary and the distri-
bution. In addition, reﬁning and generating hand-
crafted features is an expensive and tedious pro-
cess. Our model do not require any hand-crafted
features and can work with raw text and hence it
can generalize well.

Two most popular embeddings that are being
used are word2vec (Mikolov et al., 2013) and
Glove (Pennington et al., 2014). Although these
embeddings have improved the baselines from the

300traditional bag-of-words model, they have been
trained over large corpus in an unsupervised man-
ner, they do not encode any sentiment information
in them. The words like good and bad, due to
their similar usage in the text appear to be close in
the embedding space. To better represent the sen-
timent in the embeddings, several approaches to
reﬁne and learn embeddings have been reported.
Learning of sentiment speciﬁc word embedding
(SSWE) is presented in (Tang et al., 2014) where,
embeddings were learned from a large corpus by
incorporating the sentiment signal in the loss func-
tion. These embeddings are then used with dif-
ferent classiﬁer such as convolutional Neural net-
works (CNN) followed by max-pooling (Collobert
et al., 2011b; Socher et al., 2011; Mitchell and La-
pata, 2010). We have considered this as one of our
baselines.

Enriching of embedding using the distant-
supervised method to learn set of embeddings us-
ing standard word2vec (Mikolov et al., 2013) and
GLove (Pennington et al., 2014) is shown in (De-
riu et al., 2016). Although this enrichment of em-
beddings is done using a large corpus of tweets,
the basic assumption is that positive emoticons
and emoji relate to the overall positive sentiment
of the tweets create a lot of instability in the em-
bedding space (Kunneman et al., 2014). This is
due to the fact that, emoticons and emoji are used
in various context and quite often in a polar oppo-
site way to express sarcasm and irony (Poria et al.,
2014). In addition, these methods are inefﬁcient
for more granular and ﬁne-grained sentiment anal-
ysis.

3 Proposed Framework
In Figure 1 we present our generalized system di-
agram. Raw text is ﬁrst preprocessed to normalize
noise using standard text processing techniques.

3.1 Pre-processing
Tweets are essentially short text messages that are
generated by humans to express their sentiments
and reviews, and are known to be inherently noisy
due to their condensed nature. This poses a chal-
lenge when trying to understand sentiment and af-
fect. We have used standard text processing tech-
niques with some modiﬁcation to better suit the
sentiment and affect domain:

• All the letters are converted to lower case
form

• Signiﬁcant amount of words are elongated
with repeated number of characters such as
”ANGRYYYYYYYYYYY”, we have lim-
ited these consequent characters to maximum
of 2
• All the hyperlinks are removed as they do not
serve the sentiment that is conveyed by the
text itself and might relate to the sentiment
pointed out by that links
• For words represented in hastags we remove
”#” symbol, and if the word is not found
in the vocabulary we try to segment it us-
ing Viterbi algorithm (Segaran and Hammer-
bacher, 2009)
• Usernames are replaced with ”mention” to-
ken
• Compacted versions of word phrases such as
”wasn’t”, ”when’s”, etc., are replaced with
corresponding expanded words

3.2 Embedding Generation
Processed text is then used to generate two sets
of embeddings. First set of embeddings are gen-
erated by using three different pre-trained embed-
dings.

• Pre-trained embeddings which are generated
from common crawl corpus have 6 Billion to-
kens which help in a better encoding of the
syntactic and semantic structure of the lan-
guage.
• Pre-trained emoji (Eisner et al., 2016) em-
beddings are used to represent the emojis and
emoticons in the text. Emojis and emoticons
are essential part of text which strongly con-
vey the sentiments.
• To handle out of vocabulary words after the
segmentations and the spelling corrections,
we use character embedding2 to generate a
representations by summing all the character
embeddings in that word. This helps in cap-
turing sentiment related signals better than
assigning it to the random tokens.

Pseudo code 1 details the process of generating
ﬁrst set of embedding which uses Glove embed-
dings trained over common crawl corpus of vector

2https://github.com/minimaxir/char-embeddings

301News Embedding

Shared layers

 

t
x
e
T
d
e
s
s
e
c
o
r
p
-
e
r
P

…

…

…

…

Twitter Embedding

…

…

…

…

Attention

Attention

Task 1

…

…

Task 2

Attention

Attention

…

…

Task n

Attention

Attention

…

…

Figure 1: System Architecture

Fully 

connected 

layers

Fully 

connected 

layers

.
.
.

Fully 

connected 

layers

Output

Output

Output

for each word. Second set of embeddings are ex-
tracted from pre-trained embeddings over Twitter
corpus to get vectors that represents the nuances
of the Twitter platform and in general of short
and noisy text. These embeddings provide varied
vector sizes, we use 300 and 200 dimensions of
embedding for common crawl and twitter respec-
tively.

word token = Tokenize tweet
for each word in word token do

if word is in EmojiEmbb then

word vector =
get vector(EmojiEmbb,
word vector)

else if word is in Glove then

word vector = get vector(Glove,
word vector)

else if word is in CharEmbb then

word vector = get vector(charEmbb,
word vector)

chars = tokeinze word token into
character
n = length(chars) word vector =
(cid:2)

n
1 get vector(charEmbb, chars)

else

end

end
Algorithm 1: Embedding Matrix generation

Embeddings are then zero padded to match the
sequence length across the datasets of different
task. We have used 90 words as the maximum se-
quence length to account for any variations in val-
idation datasets. For generalization, single sam-
ple of processed text can be represented in form of
two sets of matrix as (cid:2)nw × dg(cid:3) and (cid:2)nw × dt(cid:3),
where nw is maximum sequence length or maxi-
mum number of words present in the text and dg,
dt are the dimension of each embeddings. In this
paper nw = 90, dg = 300 and dt = 200.

These embeddings are then fed into 2 separate

BiLSTM layers for each set of embeddings.

3.3 Model Description
We use LSTM architecture that was proposed in
(Graves, 2013), which is governed by following
equations,

it = tanh(Wxixt + Whiht−1 + bi)
jt = sigm(Wxjxt + Whjht−1 + bj)
ft = sigm(Wxf xt + Whf ht−1 + bf )
ot = tanh(Wxoxt + Whoht−1 + bo)

ct = ct−1 ⊕ ft + it ⊕ jt
ht = tanh(ct) ⊕ ot

In these equations, the W∗ are the weight matri-
ces and b∗ are biases. The operation ⊕ denotes
the element-wise vector product. The variable ct
denotes memory of LSTM at time step t.

302Bidirectional Long Short Term memory (BiL-
STMs) are improvement over LSTM networks
where, two LSTM layers are stacked over each
other. One of the layer processes the sequence in
the forward pass and another process the seqeunce
in backward fashion. Equations for the LSTM lay-
ers remain same and training can be done using
stochastic gradient descent. So at each time step
t, we receive set of ht, one from forward pass and
one from backward pass in BiLSTM, we concate-
nate and term it as a single output ht.

As can be seen from above equation, forget
gate bias can prove to be inefﬁcient if it is initial-
ized to random value and might introduce prob-
lem of vanishing gradient problem by a factor of
0.5 (Hochreiter and Schmidhuber, 1997; Martens
and Sutskever, 2011). This can adversely affect
the long term dependencies, to address this prob-
lem we initialize forget gate bias bf to value just
above 1 to facilitate the gradient ﬂow as suggested
in (Gers et al., 2000; Jozefowicz et al., 2015). To
further regularize and avoid over-ﬁtting dropout is
used.

The output of both BiLSTM layers is then fed
into the task-speciﬁc layers. Figure 1 shows the
task-speciﬁc layers, where attention is used over
the output of each BiLSTM layer. Attention was
initially proposed for Neural Machine Translation
(NMT) for encoder-decoder architecture to pro-
vide a context in terms of weights to important
words (Bahdanau et al., 2014).
In our problem
where the ﬁnal goal is to classify or to predict the
intensity, attention is only required at the encod-
ing level. Context vector can be computed using
for the output of RNNs are follows,

(cid:2)

ct =

T
j=1 αtjhj

et = a(ht), at =

(cid:2)T

exp(et)
k=1 exp(ek)

where T is the total number of time steps in the
sequence(in our case maximum sequence length)
and αtj is the weight computed for hidden state hj
at each time step t. Context vector ct are then used
to compute new sequence using previous state in
the sequence and the context vectors. This ensures
the new sequences have direct access to the entire
sequence h.

Output of the attention layer is then fed to the
fully connected layers, and the size and activation
of the ﬁnal layer depends on the task at hand.

4 Experiments

In order to validate our approach we perform two
experiments. In experiment-1 we train our model
on mixture of regression and classiﬁcation tasks
and access its performance over the same task by
ﬁne tuning it for the same task. In experiment-2
we accesses the representations that are obtained
during experiment-1 on a different task.

4.1 Experiment-1: Multi-task training
We train and evaluate our model on sentiment
classiﬁcation SemEval dataset obtained through
shared task and affect emotion dataset
from
SemEval-2018. These tasks are based on Twitter
text and align to our objective of classifying short
and noisy text present in the social media space.
Although sentiment and affect task require a vary-
ing degree of representation where sentiment clas-
siﬁcation in positive, negative and neutral space
can be relatively easier, representations required
for this tasks are not present in the pre-trained em-
beddings.

4.1.1 Datasets
For
sentiment classiﬁcation dataset we use
SemEval-2017 Task 4 Subtask A dataset. It con-
tains a tweet and its respective label from pos-
itive, negative and neutral in english language.
From here on we refer to this dataset as Sem-3.
The classes presented are imbalance and negative
tweets are around 15% in training set and 32% in
test set (Rosenthal et al., 2017).

For Affective emotion, we use dataset which
was presented as in SemEval-2018 task 1 (Mo-
hammad et al., 2018) subtask EI-reg, EI-oc con-
tains tweets speciﬁc to 4 emotions namely, Anger,
fear, Joy and Sadness for english language. Sub-
task V-reg, V-oc contains the tweets for valence de-
noting range of positive to negative of sentiment.
In subtasks EI-reg and V-reg, Given a tweet and its
corresponding emotion, predict the intensity score
of that emotion between 0 to 1, 0 being lowest
and 1 being highest. Whereas, for subtasks EI-
oc and V-oc we need to classify them into prede-
ﬁned classes, where oc means ordinal classiﬁca-
tion. In this dataset, emotions are classiﬁed in 4
distinct labels from mildly felt emotion to strongly
felt emotion, while valence is classiﬁed into 7 dis-
tinct classes. Distribution of the datasets into the
train development and test set is presented in ta-
ble 1.

303Table 1: Data Distribution.

Anger
Fear
Joy
Sadness
Valence
Sem-3

Train
1701
2252
1616
1533
1181
50334

Dev
388
389
290
397
449
20632

Test
1002
986
1105
975
937
12284

Predicting intensity for emotions and valence
are considered as regression task, while classify-
ing into one of the classes is considered as clas-
siﬁcation task. We have 5 regression tasks and 6
classiﬁcation tasks across these two datasets.
4.1.2 Training Procedure
The sem-3 dataset have approximately 15 time
more training samples on an average when com-
pared to all the rest of tasks assuming regression
and classiﬁcation are different tasks for each emo-
tions. We deﬁne a training algorithm mentioned in
algorithm 2. We train for sem-3 task for 1 epochs
while others are trained for 15 epochs to account
for the sample imbalances. We have chosen to
keep the validation and test dataset as it is to better
compare over the baselines.

for episode in episodes do
train sem-3 for 1 epoch
list = random order of task rest of 10
classes
for task in list do

train task for 15 epochs

end

end

For classiﬁcation tasks we have used categorical
crossentropy as loss function, while for regression
task we have deﬁned a custom loss function as fol-
lows,
Loss = 0.7× (1− pearson) + 0.3× M SE (1)
where pearson is the pearson correlation and MSE
is the mean squared error. As pearson correlation
was the ofﬁcial metric for the regression task and
has proven to be better representative than mean
squared error. We have taken mean squared error
into account to decrease the bias than creeps in due
to batch size while training.

For classiﬁcation tasks, class weights were ap-
plied in the loss function to handle class imbal-

ances. Weights were set according to the inverse
of their frequency.

Model hyper-parameters are shown in table 2.
In addition, 0.5 and 0.35 dropout was used for
fully connected layer and BiLSTM respectively.
These parameters were chosen using grid search
over validation dataset. We have used Tanhyper-
bolic as for BiLSTM and Scaled Exponential Lin-
ear Units (selu) (Klambauer et al., 2017) for fully
connected layers as activation function. Fine tun-
ing for each task is by freezing the shared layer
weights after training to generate results for indi-
vidual tasks.

Table 2: Details of layers

Layers

BiLSTM Layer 1
BiLSTM Layer 2

Fully connected layer 1
Fully connected layer 2
Fully connected layer 3

Classif ication Regression

70
70
100
50

3/5/7

70
70
100
50
1

4.2 Experiment 2: Validating on external

distribution

Irony detection in the social media is one such
ﬁeld which is correlated with the sentiment analy-
sis. Although it requires different set of features,
sentiment and affective emotions enhances the de-
tection accuracies as reported in (Far´ıas et al.,
2016; Wallace, 2015). In this experiment, we ap-
ply representations generated earlier to irony clas-
siﬁcation to access its robustness. We have used
irony detection dataset introduced in SemEval-
2018 task 3 (Van Hee et al., 2018). Dataset was
augmented and hashtags used to mine the tweets
such as ”#irony”, ”#sarcasm”, etc., were omitted
for testing. We have removed this hashtags during
training as well to keep the dataset consistent.

This task contained two subtask, namely Sub-
task A and Subtask B. Objective of Subtask A
was to classify whether a tweet contains irony or
not, while of Subtask B was to classify into verbal
irony (V-irony), situational irony (S-irony), other
types of irony (O-irony) and non-ironic. Distribu-
tion of the dataset along the training and testing is
presented in table 3.

Table 3: Distribution of Irony Dataset across train and
test

Subtask A

Subtask B

Ironic Non-Ironic V-irony
1390
1911
311
164

1923
473

Train
Test

S-irony O-irony Non-ironic

316
85

205
62

1923
473

304For this we extract the representations from the
model trained in experiment 1, speciﬁcally we take
output of 2 BiLSTM layers. So for each sam-
ple in this dataset we have a 2D matrix of shape
(cid:2)nw × b1(cid:3), nw is the maximum sequence length
and b1 is the number of hidden units in the BiL-
STM layer 1. Similarly we obtain the representa-
tion from BiLSTM layer 2. We concatenate this
representation and pass it on to classiﬁcation net-
work consisting of single BiLSTM layer and two
fully connected layers.

5 Results and Discussions

We ran our multi-task experiment for 10 episodes
which translates to 100 training rounds. Figure 2
shows loss vs timesteps graph. Graphs are plotted
differently to account for the different loss func-
tion scales. We can clearly observe the conver-
gence over the time steps across tasks.

0.6

0.5

0.4

0.3

0.2

3.0
2.5
2.0
1.5
1.0

s
s
o
L

s
s
o
L

0

0

Emotions

anger_reg
fear_reg
sadness_reg
joy_reg
valence_reg

sem−3

25

50

75

100

25

50

Epochs

75

100

Figure 2: Upper graph shows the plot of loss vs
episodes for regression task, while lower graph shows
the corresponding plot for classiﬁcation task for sem-3
task

For experiment 1, we use standard baselines as
reported for the respective subtasks. In addition,
we train a simple CNN classiﬁer and LSTM clas-
siﬁer as our baselines. For EI-reg, EI-oc, V-reg
and V-oc, baseline system was developed using
wordvectors along with lexicons and support vec-
tor as ﬁnal classiﬁcation/regressor. We also com-
pare the results with sentiment speciﬁc word em-
beddings (Tang et al., 2014), where we use Fully
connected layers along with attention as the down-
stream model. For Sem-3 dataset we compare our
results with RCNN (Yin et al., 2017) and Siamese
network (Baziotis et al., 2017), which were top
performing teams in the task. In addition, we sepa-
rately train each task with same model parameters

0.8

Truth Value: 0.26

0.6

y
t
i
s
n
e
t
n

I

0.4

0.2

Models

Multitask
Separately trained
SSWE

n
o

i
t

n
e
m

n u r
a
m

t
s
u

j

r
e

t
t
i

b

t

u
o
b
a

u
n
a
m

i

g
n
k
a
m

t

a
e
r
g

s .
e
d
i
r
t
s

words

Figure 3: Plot of how intensity of the tweet changes
with the words, signifying the importance of the se-
quence

without multi-task frame work, to observe the im-
provement due to multi-task and also to access the
ability of the model architecture proposed. Results
are shown in table 4, where for emotions and va-
lence pearson correlation is reported and for sem-
3 task accuracy and F1 score(F1 score is averaged
for positive and negative class) is reported. Our
model clearly out performs the baselines, and also
provides signiﬁcant increase over the recently pro-
posed model architecture. Results also shows that
separately trained model is able to beat the base-
lines, while adding the multi-task framework is
able to boost the results further.

For experiment 2, baseline is unigram tf-idf fea-
tures with Support vector classiﬁers. We com-
pare it with the standard CNN and BiLSTM ar-
chitectures. In addition, we also compare against
recently proposed generalized representation for
language modeling (Peters et al., 2018), which has
been a state of the art for Yelp and IMDB dataset.
These representations are available in two sets; a
weighted sum of three layers of BiLSTMs (sam-
ples size, max length, 1024) refereed as ELMO-
3D and ﬁxed mean-pooling of all contextualized
word representations (samples size, 1024) refer-
eed as ELMO-2D. For ELMO-3D embeddings we
have used attention and fully connected layer as
the classiﬁer and for 2 dimension embeddings, we
have used fully connected layers as classiﬁers. Re-
sults are shown in table 5, where F1-score is re-
ported over the classes, for multiclass subtask av-
erage of F1 score for each class has been reported,
this was the ofﬁcial metric of this task. We ﬁnd
that our framework out-performs all the baselines

305Figure 4: Attention weights comparison of Multi-task and SSWE

Table 4: Results of Experiment 1

anger

fear

joy

sadness

valence

sem-3

reg
0.526
0.556
0.627
0.641

oc

0.382
0.445
0.511
0.498

reg
0.525
0.579
0.635
0.637

oc

0.355
0.462
0.497
0.483

reg
0.575
0.601
0.612
0.655

oc

0.469
0.534
0.556
0.60

reg
0.453
0.573
0.613
0.623

oc

0.370
0.459
0.507
0.539

reg
0.585
0.714
0.73
0.784

oc

0.509
0.591
0.621
0.634

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

0.697
0.732

0.551
0.622

0.696
0.736

0.527
0.575

0.689
0.728

0.629
0.664

0.685
0.722

0.597
0.628

0.804
0.832

0.667
0.703

Acc.
0.333
0.545
0.637
0.639

0.664

0.651

0.642
0.672

F 1P N
0.162
0.556
0.646
0.645

0.658

0.677

0.64
0.670

Baseline
CNN
BiLSTM
SSWE
RCNN Ensemble
et al., 2017)
Siamese LSTM (Baziotis
et al., 2017)
Separately trained
Multitask

(Yin

reported. Results are averaged across 10 runs to
reduce the variance.

Table 5: Irony Detection F1 score

Subtask A Subtask B

Baseline
CNN
BiLSTM
ELMO-2Dim
ELMO-3Dim
SSWE
Multitask Representation

0.585
0.535
0.592
0.591
0.604
0.557
0.629

0.327
0.329
0.396
0.406
0.412
0.361
0.425

A possible reason for the low performance of
pre-trained SSWE might be narrow vocabulary.
We have around 95K words in our vocabulary
whereas, SSWE had 137052 words in its pre-
trained vocabulary out of which only 33473 words
were in overlap with the dataset vocabulary. Al-
though the embeddings are reﬁned for sentiment
words, social media space often contains words
which are not present in the formal dictionaries,
here as our model was able to generate embed-
dings of out of vocabulary words using charac-
ter embeddings. Figure 3 shows an example from
fear emotion, where plot of how the ﬁnal intensity
of the sentence is changed over different model
is shown. SSWE jumps on the word ”bitter” as
the word contains highly negative sentiment as-
sociated with it, whereas the true value is low
for fear. Proposed model is able to normalize

over the sequence as the jump is not that drastic.
Figure 4 shows comparison of how our proposed
model learns to put weights to the words as com-
pared to SSWE model. We believe that adding the
sentiment context over in the embedding through
multi-task training aided in the Irony classiﬁcation
dataset.

6 Conclusion

In this paper, we present an approach for gener-
ating representations using sentiment and affect
dataset in the multi-task framework. We present
our deep learning based model with a dual atten-
tion over two sets of embedding space to capture
more rich nuances of Twitter while still keeping
the semantic and syntactic structure of language.
In addition, we use emoji and character embed-
dings to help in getting better sentiment speciﬁc
signals and to mitigate the effect of out of vocabu-
lary problem. Our experiments over both internal
and external distribution of data show the effec-
tiveness of the representation. We observe that our
model perform signiﬁcantly better as compared to
the baselines and the current state of the art meth-
ods for the tasks. Going further, it would be ef-
fective to devise an algorithm to modify these rep-
resentations with minimum computation and still
adapt to a different domain.

306References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation.
In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1-
Volume 1, pages 190–199. Association for Compu-
tational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate.
arXiv preprint
arXiv:1409.0473.

Christos Baziotis, Nikos Pelekis, and Christos Doulk-
eridis. 2017. Datastories at semeval-2017 task 6:
Siamese lstm with attention for humorous text com-
parison.
In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 390–395, Vancouver, Canada. Association for
Computational Linguistics.

Felipe Bravo-Marquez, Eibe Frank, Saif M Moham-
mad, and Bernhard Pfahringer. 2016. Determining
word–emotion associations from tweets by multi-
label classiﬁcation. In WI’16, pages 536–539. IEEE
Computer Society.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011a. Natural language processing (almost) from
scratch.
Journal of Machine Learning Research,
12(Aug):2493–2537.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch.
Journal of Machine Learning Research,
12(Aug):2493–2537.

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. Swisscheese at semeval-2016 task 4: Senti-
ment classiﬁcation using an ensemble of convolu-
tional neural networks with distant supervision. In
SemEval@ NAACL-HLT, pages 1124–1128.

Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the 2008 international conference
on web search and data mining, pages 231–240.
ACM.

Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko Bosnjak,
and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description.

Andrea Esuli and Fabrizio Sebastiani. 2007. Senti-
wordnet: A high-coverage lexical resource for opin-
ion mining. Evaluation, pages 1–26.

Felix A. Gers, J¨urgen A. Schmidhuber, and Fred A.
Cummins. 2000. Learning to forget: Continual pre-
diction with lstm. Neural Comput., 12(10):2451–
2471.

A. Graves. 2013. Generating Sequences With Recur-

rent Neural Networks. ArXiv e-prints.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures.
In Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML-15), pages 2342–2350.

G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochre-
Self-Normalizing Neural Networks.

iter. 2017.
ArXiv e-prints.

FA Kunneman, CC Liebrecht, and APJ van den Bosch.
2014. The (un) predictability of emotional hashtags
in twitter.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language tech-
nologies, 5(1):1–167.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
sequence to sequence learning.
arXiv preprint
arXiv:1511.06114.

James Martens and Ilya Sutskever. 2011. Learning
recurrent neural networks with hessian-free opti-
mization. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1033–1040.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity.
In Advances in neural information processing
systems, pages 3111–3119.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-stitch networks for
multi-task learning. CoRR, abs/1604.03539.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.

Saif Mohammad, Felipe Bravo-Marquez, Mohammad
Salameh, and Svetlana Kiritchenko. 2018. Semeval-
2018 task 1: Affect in tweets.
In Proceedings of
The 12th International Workshop on Semantic Eval-
uation, pages 1–17. Association for Computational
Linguistics.

Delia Iraz´u Herna´ndez Far´ıas, Viviana Patti, and Paolo
Rosso. 2016.
Irony detection in twitter: The role
of affective content. ACM Transactions on Internet
Technology (TOIT), 16(3):19.

Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets.
arXiv
preprint arXiv:1308.6242.

307Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014.
Learning sentiment-
speciﬁc word embedding for twitter sentiment clas-
siﬁcation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1555–
1565.

Cynthia Van Hee, Els Lefever, and Veronique Hoste.
2018. Semeval-2018 task 3: Irony detection in en-
glish tweets.
In Proceedings of The 12th Interna-
tional Workshop on Semantic Evaluation, pages 39–
50. Association for Computational Linguistics.

Byron C Wallace. 2015. Computational irony: A sur-
vey and new perspectives. Artiﬁcial Intelligence Re-
view, 43(4):467–483.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347–354. Association for Computational Linguis-
tics.

Yichun Yin, Yangqiu Song, and Ming Zhang. 2017.
Nnembs at semeval-2017 task 4: Neural twitter sen-
timent classiﬁcation: a simple ensemble method
with different embeddings.
In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017), pages 621–625, Vancouver,
Canada. Association for Computational Linguistics.

Hyeonseob Nam and Bohyung Han. 2016. Learning
multi-domain convolutional neural networks for vi-
sual tracking. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 4293–4302.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. CoRR, abs/1802.05365.

Soujanya Poria, Erik Cambria, Gregoire Winterstein,
and Guang-Bin Huang. 2014.
Sentic patterns:
Dependency-based rules for concept-level sentiment
analysis. Knowledge-Based Systems, 69:45–63.

Jonathan Posner, James A Russell, and Bradley S Pe-
terson. 2005. The circumplex model of affect: An
integrative approach to affective neuroscience, cog-
nitive development, and psychopathology. Develop-
ment and psychopathology, 17(3):715–734.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. Semeval-2017 task 4: Sentiment analysis in
twitter.
In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 502–518, Vancouver, Canada. Association for
Computational Linguistics.

James A Russell. 1980. A circumplex model of af-
fect. Journal of personality and social psychology,
39(6):1161.

Toby Segaran and Jeff Hammerbacher. 2009. Beautiful
data: the stories behind elegant data solutions. ”
O’Reilly Media, Inc.”.

Raksha Sharma and Pushpak Bhattacharyya. 2013.
In IJC-

Detecting domain dedicated polar words.
NLP, pages 661–666.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 joint conference on empirical
methods in natural language processing and com-
putational natural language learning, pages 1201–
1211. Association for Computational Linguistics.

308
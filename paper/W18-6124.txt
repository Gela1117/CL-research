Using Author Embeddings to Improve Tweet Stance Classiﬁcation

Adrian Benton∗† and Mark Dredze∗

∗Center for Language and Speech Processing, Johns Hopkins University

Baltimore, MD 21218 USA

†Bloomberg LP, New York, NY 10022
{adrian,mdredze}@cs.jhu.edu

Abstract

Many social media classiﬁcation tasks ana-
lyze the content of a message, but do not con-
sider the context of the message. For ex-
ample, in tweet stance classiﬁcation – where
a tweet is categorized according to a view-
point it espouses – the expressed viewpoint
depends on latent beliefs held by the user.
In this paper we investigate whether incor-
porating knowledge about the author can im-
prove tweet stance classiﬁcation.
Further-
more, since author information and embed-
dings are often unavailable for labeled training
examples, we propose a semi-supervised pre-
training method to predict user embeddings.
Although the neural stance classiﬁers we learn
are often outperformed by a baseline SVM, au-
thor embedding pre-training yields improve-
ments over a non-pre-trained neural network
on four out of ﬁve domains in the SemEval
2016 6A tweet stance classiﬁcation task. In a
tweet gun control stance classiﬁcation dataset,
improvements from pre-training are only ap-
parent when training data is limited.
Introduction

1
Social media analyses often rely on a tweet classi-
ﬁcation step to produce structured data for analy-
sis, including tasks such as sentiment (Jiang et al.,
2011) and stance (Mohammad et al., 2016) clas-
siﬁcation. Common approaches feed the text of
each message to a classiﬁer which predicts a la-
bel based on the content of the tweet. However,
many of these tasks beneﬁt from knowledge about
the context of the message, especially since short
messages can be difﬁcult to understand (Aramaki
et al., 2011; Collier and Doan, 2011; Kwok and
Wang, 2013). One of the best sources of context
is the message author herself. Consider the task of
stance classiﬁcation, where a system must identify
the stance towards a topic expressed in a tweet.
Having access to the latent beliefs of the tweet’s

author would provide a strong prior as to their ex-
pressed stance, e.g. general political leanings pro-
vide a prior for their statement on a divisive polit-
ical issue. Therefore, we propose providing user
level information to classiﬁcation systems to im-
prove classiﬁcation accuracy.

One of the challenges with accessing this type
of information on social media users, and Twitter
users in particular, is that it is not provided by the
platform. While political leanings may be helpful,
they are not directly contained in metadata or user
provided information. Furthermore, it is unclear
which categories of information will best inform
each classiﬁcation task. While information about
the user may be helpful in general, what informa-
tion is relevant to each task may be unknown.

We propose to represent users based on their on-
line activity as low-dimensional embeddings, and
provide these embeddings to the classiﬁer as con-
text for a tweet. Since a deployed classiﬁer will
likely encounter many new users for which we do
not have embeddings, we use the user embeddings
as a mechanism for pre-training the classiﬁcation
model. By pre-training the model to be predic-
tive of user information, the classiﬁer can better
generalize to new tweets. This pre-training can be
performed on a separate, unlabeled set of tweets
and user embeddings, creating ﬂexibility in which
tasks can be improved by using this method. Ad-
ditionally, we ﬁnd that this training scheme is most
beneﬁcial in low-data settings, further reducing
the resource requirement for training new classi-
ﬁers. Although semi-supervised approaches to so-
cial media stance classiﬁcation are not new, they
have only been performed at the message-level –
predicting held-out hashtags from a tweet for ex-
ample (Zarrella and Marsh, 2016). Our approach
leverages additional user information that may not
be contained in a single message.

We evaluate our approach on two stance clas-

Proceedingsofthe2018EMNLPWorkshopW-NUT:The4thWorkshoponNoisyUser-generatedText,pages184–194Brussels,Belgium,Nov1,2018.c(cid:13)2018AssociationforComputationalLinguistics184siﬁcation datasets: 1) the SemEval 2016 task of
stance classiﬁcation (Mohammad et al., 2016) and
2) a new gun related Twitter data set that con-
tains messages about gun control and gun rights.
On both datasets, we compare the beneﬁt of pre-
training a neural stance classiﬁer to predict user
embeddings derived from different types of online
user activity: recent user messages, their friend
network, and a multiview embedding of both of
these views.

2 Stance Classiﬁcation

The popularity of sentiment classiﬁcation is mo-
tivated in part by the utility of understanding the
opinions expressed by a large population (Pang
et al., 2008). Sentiment analysis of movie reviews
(Pang et al., 2002) can produce overall ratings for
a ﬁlm; analysis of product reviews allow for better
recommendations (Blitzer et al., 2007); analysis of
opinions on important issues can serve as a form
of public opinion polling (Tumasjan et al., 2010;
Bermingham and Smeaton, 2011).

Although similar to sentiment classiﬁcation,
stance classiﬁcation concerns the identiﬁcation of
an author’s position with respect to a given tar-
get (Anand et al., 2011; Murakami and Raymond,
2010). This is related to the task of targeted sen-
timent classiﬁcation, in which both the sentiment
and its target must be identiﬁed (Somasundaran
and Wiebe, 2009). In the case of stance classiﬁ-
cation, we are given a ﬁxed target, e.g. a political
issue, and seek to measure opinion of a piece of
text towards that issue. While stance classiﬁcation
can be expressed as a complex set of opinions and
attitudes (Rosenthal et al., 2017), we conﬁne our-
selves to the task of binary stance classiﬁcation,
in which we seek to determine if a single message
expresses support for or opposition to the given
target (or neither). This deﬁnition was used in the
SemEval 2016 stance classiﬁcation task (Moham-
mad et al., 2016).

In stance classiﬁcation,

the system seeks to
identify the position held by the author of the mes-
sage. While most work in this area infers the au-
thor’s position based only on the given message,
other information about the author may be avail-
able to aid in message analysis. Consider a user
who frequently expresses liberal positions on a
range of political topics. Even without observing
any messages from the user about a speciﬁc liberal
political candidate, we can reasonably infer that

the author would support the candidate. There-
fore, when given a message from this author with
the target being that speciﬁc candidate, our model
should have a strong prior to predict a positive la-
bel.

This type of information is readily available on
social media platforms where we can observe mul-
tiple behaviors from a user, such as sharing, liking
or promoting content, as well as the social net-
work around the user. This contextual informa-
tion is most needed in a social media setting. Un-
like long form text, common in sentiment analy-
sis of articles or reviews, analysis of social media
messages necessitates understanding short, infor-
mal text. Context becomes even more important
in a setting that is challenging for NLP algorithms
in general.

How can we best make use of contextual in-
formation about the author? Several challenges
present themselves:

What contextual information is valuable to so-
cial media stance classiﬁers? We may have pre-
vious messages from the user, social network in-
formation, and a variety of other types of online
behaviors. How can we best summarize a wide ar-
ray of user behavior in an online platform into a
single, concise representation?

We answer this question by exploring several
representations of context encoded as a user em-
bedding: a low-dimensional representation of the
user that can be used as features by the classiﬁca-
tion system. We include a multiview user embed-
ding method that is designed to summarize multi-
ple types of user information into a single vector
(Benton et al., 2016).

How can we best use contextual information
about the author in the learning process? Ideally,
we would be provided a learned user representa-
tion along with every message we were asked to
classify. This is unrealistic. Learning user repre-
sentations requires data to be collected for each
user and computation time to process that data.
Neither of these are available in many production
settings, where millions of messages are streamed
on a given topic. It is impractical to insist that ad-
ditional information be collected for each user and
new representations inferred, for each tweets that
the classiﬁer must label.

Instead, we consider how user context can be
used in a semi-supervised setting. We augment
neural models with a pre-training step that up-

185dates model weights according to an auxiliary ob-
jective function based on available user represen-
tations. This pre-training step initializes the hid-
den layer weights of the stance classiﬁcation neu-
ral network, so that the ﬁnal resulting model im-
proves even when observing only a single message
at classiﬁcation time.

Finally, while our focus is stance classiﬁcation,
this approach is applicable to a variety of docu-
ment classiﬁcation tasks in which author informa-
tion can provide important insights in solving the
classiﬁcation problem.

3 Models

The stance classiﬁcation tasks we consider focus
on tweets: short snippets of informal text. We rely
on recurrent neural networks as a base classiﬁca-
tion model, as they have been effective classiﬁers
for this type of data (Tang et al., 2015; Vosoughi
et al., 2016; Limsopatham and Collier, 2016; Yang
et al., 2017; Augenstein et al., 2016).

Our base classiﬁcation model is a gated recur-
rent unit (GRU) recurrent neural network classi-
ﬁer (Cho et al., 2014). The GRU consumes the
input text as a sequence of tokens and produces a
sequence of ﬁnal hidden state activations.
Input
layer word embeddings are initialized with GloVe
embeddings pre-trained on Twitter text (Penning-
ton et al., 2014). The update equations for the
gated recurrent unit at position i in a sentence are:

zi = σg(Wzxi + Uzhi−1 + bz)
ri = σg(Wrxi + Urhi−1 + br)
ni = σh(Whxi + Uh(ri ◦ hi−1) + bh)
hi = zi ◦ hi−1 + (1 − zi)ni

where σg and σh are elementwise sigmoid and
hyperbolic tangent activation functions respec-
tively. W∗ and U∗ are weight matrices acting over
input embeddings and previous hidden states, and
b∗ are bias weights. zi is the update gate (a soft
mask over the previous hidden state activations),
ri is the reset gate (soft mask selecting which val-
ues to preserve from the previous hidden state), ni
is the new gate, and hi are the hidden state activa-
tions computed for position i.

Models predict stance based on a convex combi-
nation of these hidden layer activations, where the
combination weights are determined by a global
dot-product attention using the ﬁnal hidden state

as the query vector (Luong et al., 2015). The equa-
tion for determining attention on the ith position
for a sentence of length n is:

(cid:80)n

exp(hT
i hn)
j=1 exp(hT

j hn)

ai =

where hj is the ﬁnal hidden layer activations at
position j, and ai is the attention placed on the hid-
den layer at position i. For bi-directional models,
the hidden layer states are the concatenation of ac-
tivations from the forward and backward pass. A
ﬁnal softmax output layer predicts the stance class
labels based on a convex combination of hidden
states.

For this baseline model, the RNN is ﬁt directly
to the training set, without any pre-training, i.e.
training maximizes the likelihood of class labels
given the input tweet.

We now consider an enhancement to our base

model that incorporates user embeddings.
RNN Classiﬁer with User Embedding Pre-
training We augment the base RNN classiﬁer
with an additional ﬁnal (output) layer to predict
an auxiliary user embedding for the tweet author.
The objective function used for training this out-
put layer depends on the type of user embedding
(described below). A single epoch is made over
the pre-training set before ﬁtting to train.

In this case, the RNN must predict informa-
tion about the tweet author in the form of an d-
dimensional user embedding based on the input
tweet text. If certain dimensions of the user em-
bedding correlate with different stances towards
the given topic, the RNN will learn representations
of the input that predict these dimensions, thereby
encouraging the RNN to build representations in-
formative for determining stance.

The primary advantage of this pre-training set-
ting is that it decouples the stance classiﬁcation
annotated training set from a set of user embed-
dings. It is not always possible to have a dataset
with stance labeled tweets as well as user embed-
dings for each tweet’s author (as is the case for our
datasets). Instead, this setting allows us to utilize a
stance annotated corpus, and separately create rep-
resentations for a disjoint set of pre-training users,
even without knowing the identity of the authors
of the annotated stance tweets. This is different
than work presented by Amir et al. (2016) to im-
prove sarcasm detection, since we are not provid-

186ing user embeddings as features to directly pre-
dict stance. Instead, predicting user embeddings
constitutes an auxiliary task which helps pre-train
model weights, and therefore are not expected at
prediction time.

Figure 1 depicts a 2-layer bi-directional version

of this model applied to a climate-related tweet.

3.1 User Embedding Models
We explore several methods for creating user em-
beddings. These methods capture both informa-
tion from previous tweets by the user as well as
social network features.

Keyphrases
In some settings, we may have a set
of important keyphrases that we believe to be cor-
related with the stance we are trying to predict.
Knowing which phrases are most commonly used
by an author may indicate the likely stance of that
author to the given issue. We consider how an
author has used keyphrases in previous tweets by
computing a distribution over keyphrase mentions
and treat this distribution as their user representa-
tion.

Author Text When a pre-speciﬁed list of
keyphrases is unknown, we include all words in
the user representation. Rather than construct a
high dimensional embedding – one dimension for
each type in the vocabulary – we reduce the di-
mensionality by using principal component analy-
sis (PCA). We compute a TF-IDF-weighted user-
word matrix based on tweets from the author (la-
tent semantic analysis) (Deerwester et al., 1990).
We use the 30,000 most frequent token types after
stopword removal.

Social Network On social media platforms,
people friend other users who share common be-
liefs (Bakshy et al., 2015). These beliefs may
extend to the target issue in stance classiﬁcation.
Therefore, a friend relationship can inform our pri-
ors about the stance held by a user. We construct
an embedding based on the social network by cre-
ating an adjacency matrix of the 100,000 most fre-
quent Twitter friends in our dataset (users whom
the ego user follows). We construct a PCA em-
bedding of the local friend network of the author.

Multiview Representations Finally, we con-
sider an embedding that combines both the content
of the user’s messages as well as the social net-
work. We perform a canonical correlation analysis

(CCA) of the text and friend network PCA embed-
ding described above, and take the mean projec-
tion of both views as a user’s embedding. Previous
work suggests that this embedding is predictive of
future author hashtag usage, a proxy for topic en-
gagement (Benton et al., 2016).

We use a mean squared error loss to pre-train
the RNN on these embeddings since they are all
real-valued vectors. When pre-training on a user’s
keyphrase distribution, we instead use a ﬁnal soft-
max layer and minimize cross-entropy loss.

For embeddings that rely on content from the
author, we collected the most recent 200 tweets
posted by these users using the Twitter REST
API1 (if the user posted fewer than 200 public
tweets, then we collected all of their tweets). We
constructed the social network by collecting the
friends of users as well2. We collected user tweets
and networks between May 5 and May 11, 2018.
We considered user embedding widths between
10 and 100 dimensions, but selected dimensional-
ity 50 based on an initial grid search to maximize
cross validation (CV) performance for the author
text PCA embedding.

3.2 Baseline Models
We compare our approach against two baseline
models.

As part of the SemEval 2016 task 6 stance
classiﬁcation in tweets task, Zarrella and Marsh
(2016) submitted an RNN-LSTM classiﬁer that
used an auxiliary task of predicting the hashtag
distribution within a tweet to pre-train their model.
There are a few key differences between our pro-
posed method and this work. Their approach
is restricted to predicting message-level features
(presence of hashtag), whereas we consider pre-
dicting user-level features, a more general form
of context. Additionally, their method predicts
a task-speciﬁc set of hashtags, whereas user fea-
tures/embeddings offer more ﬂexibility, because
they are not as strongly tied to a speciﬁc task.
However, we select this as a baseline for compari-
son because of how they utilize hashtags within a
tweet for pre-training.

We evaluate a similar approach by identifying
the 200 most frequent hashtags in the SemEval-
hashtag pre-training set (dataset described below).

1https://api.twitter.com/1.1/statuses/

user_timeline.json

2https://api.twitter.com/1.1/friends/

list.json

187Figure 1: Diagram of a 2-layer bi-directional GRU model acting over an example Climate change is a real concern
tweet. Included in green is both the stance classiﬁcation target which all models are trained to predict, as well as
the User embedding vector target which is used for pre-training a subset of models. Backward pass hidden state
activations are denoted by bi and forward pass activations by fi. Predictions are made from a convex combination
of second-hidden-layer activations (in red), where the attention query vector is determined by the ﬁnal hidden
states (forward and backward activations concatenated). All weights are shared between pre-training and trainining
except for Wstance and Wembedding.

After removing non-topic hashtags (e.g. #aww,
#pic), we were left with 189 unique hashtags,
with 32,792 tweets containing at least one of
these hashtags. Example hashtags include: #athe-
ist, #fracking, #nuclear, #parisattacks, and #usa.
Pre-training was implemented by using a 189-
dimensional softmax output layer to predict held-
out hashtags. RNNs were trained by cross-entropy
loss where the target distribution placed a weight
of 1 on the most frequent hashtag, with all other
hashtags having weight of 0. This is the identi-
cal training protocol used in Zarrella and Marsh
(2016). We call this model RNN-MSG-HASHTAG.

Our second baseline is a linear support vec-
tor machine that uses word and character n-gram
features ( SVM ). This was the best performing
method on average in the 2016 SemEval Task 6
shared task (Mohammad et al., 2016). We swept
over the slack variable penalty coefﬁcient to max-

imize macro-averaged F1-score on held-out CV
folds.

4 Data

4.1 Stance Classiﬁcation Datasets
We consider two different tweet stance classiﬁca-
tion datasets, which provide six domains of En-
glish language Twitter data in total.

SemEval 2016 Task 6A (Tweet Stance Classi-
ﬁcation) This is a collection of 2,814 training
and 1,249 test set tweets that are about one of
ﬁve politically-charged targets: Atheism, the Fem-
inist Movement, Climate Change is a Real Con-
cern, Legalization of Abortion, or Hillary Clinton.
Given the text of a tweet and a target, models must
classify the tweet as either FAVOR or AGAINST, or
NEITHER if the tweet does not express support or
opposition to the target topic. Participants strug-

188children’shealthuniquelyaﬀectedbychangeclimatef1f2f3f4f5f6f7Stance (Climate change is a real concern)b7b6b5b4b3b2b1f1f2f3f4f5f6f7b7b6b5b4b3b2b1User embeddingb7f7Querya1a2a3a4a5a6a7hWstanceWembeddinggled with this shared task, as it was especially dif-
ﬁcult due to imbalanced class sizes, small train-
ing sets, short examples, and tweets where the tar-
get was not explicitly mentioned. See Mohammad
et al. (2016) for a thorough description of this data.
We report model performance on the provided test
set for each topic and perform four-fold CV on the
training set for model selection3.
Guns Our second stance dataset is a collection
of tweets related to guns. Tweets were collected
from the Twitter keyword streaming API starting
in December 2012 and throughout 20134. The col-
lection includes all tweets containing guns-related
keyphrases, subject to rate limits. We labeled
tweets based on their stance towards gun control:
FAVOR was supportive of gun control, AGAINST
was supportive of gun rights. We automatically
identiﬁed the stance to create labels based on com-
monly occurring hashtags that were clearly associ-
ated with one of these positions (see Table 4.1 for
a list of keywords and hashtags). Tweets which
contained hashtags from both sets or contained
no stance-bearing hashtags were excluded from
our data. We constructed stratiﬁed samples from
26,608 labeled tweets in total. Of these, we sam-
pled 50, 100, 500, and 1,000 examples from each
class, ﬁve times, to construct ﬁve small, balanced
training sets. We then divided the remaining ex-
amples equally between development and test sets
in each case. Model performance for each num-
ber of examples was macro-averaged over the ﬁve
training sets. The hashtags used to assign class la-
bels were removed from the training examples as
a preprocessing step.

We constructed this dataset for two reasons.
First, it allows us to compare model performance
as a function of training set size. Second, we are
able to pre-train on user embeddings for the same
set of users that are annotated with stance. The
SemEval-released dataset does not provide status
or user IDs from which we could use to collect and
build user embeddings.

4.2 User Embedding Datasets
We considered two unlabeled datasets as a source
for constructing user embeddings for model pre-
training. Due to data limitations, we were unable

3CV folds were not released with these data. Since our
folds are different than other submissions to the shared task,
there are likely differences in model selection.

4https://stream.twitter.com/1.1/

statuses/filter.json

Set Name
About Guns
(General)

Control

Rights

guns,

Keyphrases/Hashtags
second amend-
gun,
ment, 2nd amendment, ﬁrearm,
ﬁrearms
#gunsensepatriot,
#gunsense,
#votegunsense,
#guncon-
trolnow, #momsdemandaction,
#momsdemand, #demandaplan,
#nowaynra,
#gunskillpeople,
#gunviolence, #endgunviolence
#protect2a,
#gunrights,
#molonlabe,
#molon-
#pro-
lab,
gun,#nogunregistry,
#vote-
gunrights,
#ﬁrearmrights,
#gungrab, #gunfriendly

#noguncontrol,

Table 1: Keyphrases used to identify gun-related tweets
along with hashtag sets used to label a tweet as support-
ing gun Control or gun Rights.

to create all of our embedding models for all avail-
able datasets. We describe below which embed-
dings were created for which datasets.

SemEval 2016 Related Users The SemEval
stance classiﬁcation dataset does not contain tweet
IDs or user IDs, so we are unable to determine au-
thors for these messages.
Instead, we sought to
create a collection of users whose tweets and on-
line behavior would be relevant to the ﬁve topics
discussed in the SemEval corpus.

in a similar

This ensured that

We selected query hashtags used in the shared
task (Mohammad et al., 2016) and searched for
tweets that included these hashtags in a large
sample of the Twitter 1% streaming API sam-
ple from 20155.
tweets
were related to one of the targets in the stance
evaluation task, and were from authors dis-
cussing these topics
time pe-
riod.
The hashtags we searched for were:
#nomorereligions, #godswill, #atheism, #glob-
alwarmingisahoax, #climatechange, #ineedfemi-
nismbecaus, #feminismisawful, #feminism, #go-
hillary, #whyiamnovotingforhillary, #hillary2016,
#prochoice, #praytoendabortion, and #planned-
parenthood. We queried the Twitter API to pull the
200 most recent tweets and local friend networks
for these speciﬁc tweet authors. We omitted tweets
made by deleted and banned users as well as those
who had fewer than 50 tweets total returned by
the API. In total, we obtained 79,367 tweets for
49,361 unique users, and pulled network informa-
tion for 38,337 of these users.

5https://stream.twitter.com/1.1/

statuses/sample.json

189For this set of users, we constructed the Au-
thor Text embedding (PCA representation of a
TF-IDF-weighted bag of words from the user) as
well as the Social Network embedding (PCA rep-
resentation of the friend adjacency matrix). For
users with missing social network information, we
replaced their network embedding with the mean
embedding over all other users. This preprocess-
ing was applied before learning Multiview (CCA)
embeddings for all users.

General User Tweets
Is it necessary for our pre-
training set to be topically-related to the stance
task we are trying to improve, or can we consider
a generic set of users? To answer this question
we created a pre-training set of randomly sampled
users, not speciﬁcally related to any of our stance
classiﬁcation topics.
If these embeddings prove
useful, it provides an attractive method whereby
stance classiﬁers are pre-trained to predict gen-
eral user embeddings not speciﬁcally related to the
stance classiﬁcation topic.

We considered the collection of Twitter users
that were described in Benton et al. (2016) to learn
general user embeddings. These users were sam-
pled uniformly at random from the Twitter 1%
stream in April 2015. We collected their past
tweets from January 2015 to March 2015 and col-
lected their friend network exactly as was done in
the SemEval 2016-related user data.

We construct the Author Text and Social Net-
work embeddings, as well as the Multiview
(mean CCA) embeddings. Note that unlike Ben-
ton et al. (2016), we did not consider a generalized
CCA model of all subsets of views so as to narrow
the model search space. Author Text embeddings
were constructed from tweets made in January and
February 2015.

To utilize user embeddings for model pre-
training, we randomly selected three tweets from
each user that occurred in March 2015, so as to
be disjoint from the tweets used to build the Au-
thor Text embeddings. We pre-trained the model
by providing these tweets as input and trained the
model to predict the accompanying embedding. In
total, we constructed a set of 152,751 input tweets
posted by 61,959 unique users.

Guns User Tweets We also kept 49,023 unla-
beled guns tweets for pre-training on the guns
stance task, using the distribution over general
keyphrases that an author posted across the pre-

training set as the user embedding. We pre-trained
on the (Author Text) embedding of these tweets,
along with a friend network embedding (network
data collected identically to above pre-training
datasets).

5 Model Training

We preprocessed all tweets by lowercasing and to-
kenizing with a Twitter-speciﬁc tokenizer (Gim-
pel et al., 2011)6. We replaced usernames with
<user> and URLs with <url>.

For training on the SemEval dataset, we se-
lected models based on four-fold cross valida-
tion macro-averaged F1-score for FAVOR and
AGAINST classes (the ofﬁcial evaluation metric
for this task). For the guns dataset we select mod-
els based on average development set F1-score.
For SemEval, each classiﬁer is trained indepen-
dently for each target. Reported test F1-score is
averaged across each model ﬁt on CV folds.

All neural networks were trained by minibatch
gradient descent with ADAM (Kingma and Ba,
2015) with base step size 0.005, β1 = 0.99, and
β2 = 0.999, with minibatch size of 16 examples,
and the weight updates were clipped to have an (cid:96)2-
norm of 1.0. Models were trained for a minimum
of 5 epochs with early stopping after 3 epochs if
held-out loss did not improve. The per-example
loss was weighted by the inverse class frequency
of the example label7.

The neural model architecture was selected by
performing a grid search over hidden layer width
({25, 50, 100, 250, 500, 1000}), dropout rate ({0,
0.1, 0.25, 0.5}), word embedding width ({25, 50,
100, 200}), number of layers ({1, 2, 3}), and
RNN directionality (forward or bi-directional).
Architecture was selected to maximize cross-fold
macro-averaged F1 on the “Feminist Movement”
topic with the GRU classiﬁer without pre-training.
We performed a separate grid search of architec-
tures for the with-pre-training models.

6 Results and Discussion

6.1 SemEval 2016 Task 6A
Table 2 contains the test performance for each tar-
get in the SemEval 2016 stance classiﬁcation task.

6https://github.com/myleott/

ark-twokenize-py

7This improved performance for tasks with imbalanced

class labels.

190Model

SVM
RNN

RNN-MSG-HASHTAG

RNN-HSET

RNN-TEXT-HSET
RNN-NET-HSET
RNN-MV-HSET
RNN-GENSET

RNN-TEXT-GENSET
RNN-NET-GENSET
RNN-MV-GENSET

Ath
61.2
(cid:79)
54.0
53.4
58.2
58.2
42.7
60.1
56.7
56.7
54.6
57.3

Cli
41.4
39.6
41.0
44.5
44.5
38.8
40.5
41.9
38.2
41.4
41.9

Target
Fem
57.7
(cid:79)
48.5
(cid:79)
48.4
51.2
51.2
48.2
49.9
54.4♦♠
54.4♦♠
47.8
52.1

Hil Abo
59.1
52.0
53.5
58.6
48.0
55.8
60.2
50.9
60.2
50.9
42.0
45.0
56.5
52.5
56.5
51.7
56.5
51.7
50.5
50.6
54.4
50.4

Avg
54.3
50.8
49.3
53.0
53.0
43.3
51.9
52.2
51.5
49.0
51.2

Table 2: Positive/negative class macro-averaged F1 model test performance at SemEval 2016 Task 6A. The ﬁnal
column is macro-averaged F1 across all domains. ♦ means model performance is signiﬁcantly better than a non-
pre-trained RNN, (cid:79) is worse than SVM, and ♠ is better than tweet-level hashtag prediction pre-training (RNN-
MSG-HASHTAG).

Statistically signiﬁcant difference between mod-
els was determined by a bootstrap test of 1,000
samples with 250 examples each (p = 0.05). *-
GENSET corresponds to networks pretrained on
general set user embeddings, and *-HSET corre-
sponds to networks pretrained on user embeddings
from the hashtag-ﬁltered set. The type of pre-
training user embedding is noted by *-TEXT-*
(user text), *-NET-* (friend network), or *-MV-
* (multiview CCA). The RNN-HSET and RNN-
GENSET rows correspond to selecting the best-
performing user embedding based on CV F1 inde-
pendently for each target. RNN denotes the GRU
model without pre-training.

Models with pre-training outperform the non-
pre-trained RNN in four out of ﬁve targets. Pre-
trained models always beat the baseline of tweet-
level hashtag distribution pre-training (RNN-
MSG-HASHTAG) for all targets. While topic spe-
ciﬁc user embeddings (HSET) improve over no-
pre-training in four out of ﬁve cases, the generic
user embeddings (GENSET) improve in three out
of ﬁve cases. Even embeddings for users who
don’t necessarily discuss the topic of interest can
have value in regularizing model weights.

In terms of embedding type, embeddings built
on the author text tended to perform best, but re-
sults are not clear due to small test set size.

The linear SVM baseline with word and char-
acter n-gram features outperforms neural models
in two out of ﬁve tasks, and performs the best on

average. This agrees with the submissions to the
SemEval 2016 6A stance classiﬁcation task, where
the baseline SVM model outperformed all submis-
sions on average – several of which were neural
models.

6.2 Guns

Model

SVM
RNN

RNN-KEY-GUNSET
RNN-TEXT-GUNSET
RNN-TEXT-GENSET
RNN-NET-GENSET
RNN-MV-GENSET

100
79.2
72.2(cid:79)
73.1(cid:79)
72.2(cid:79)
71.7(cid:79)
73.1(cid:79)
75.0

# Train Examples

200
81.1
79.0
76.7
79.0
76.6
77.2
79.1

1000
85.9
84.0
83.6
84.0
83.6
83.3
83.9

2000
87.4
85.3
85.6
85.3
85.3
85.4
85.4

Table 3: Model test accuracy at predicting gun stance.
RNNs were pre-trained on either the guns-related pre-
training set (GUNSET) or the general user pre-training
set (GENSET). The best-performing neural model is
bolded. (cid:79) indicates that the model performs signiﬁ-
cantly worse than the SVM baseline.

We sought to understand how the amount of
training data inﬂuenced the efﬁcacy of model pre-
training in the guns dataset. Table 3 shows the ac-
curacy of different models with varying amounts
of training data. As the amount of training data
increases, so does model accuracy. Additionally,
we tend to see larger increases from pre-training
with less training data overall. It is unclear which
user embedding or pre-training set is most effec-
tive. Although the multiview embedding is most

191Model

TWEET
TEXT
KEY

TWEET+TEXT
TWEET+KEY

100
79.2
72.1(cid:79)
52.2(cid:79)
79.2♣
79.2♣

# Train Examples

200
81.1
74.1(cid:79)
50.8(cid:79)
81.1♣
81.1♣

1000
85.9
76.5(cid:79)
51.0(cid:79)
86.0♣
85.9♣

2000
87.4
76.6(cid:79)
51.8(cid:79)
87.6♣
87.4♣

Table 4: Test accuracy of an SVM at predicting gun
control stance based on guns-related keyphrase distri-
bution (KEY), user’s Author Text embedding (TEXT),
and word and character n-gram features (TWEET). (cid:79)
means a model is signiﬁcantly worse than TWEET and
♣ means the feature set is signiﬁcantly better than
TEXT.

effective at improving the neural classiﬁer, the dif-
ference is not statistically signiﬁcant.

As with SemEval, the SVM always outperforms
neural models, though the improvement is only
statistically signiﬁcant in the smallest data set-
ting. Although we are unable to beat an SVM, the
improvements we observe in RNN performance
after user embedding pre-training are promising.
Neural model architectures offer more ﬂexibil-
ity than SVMs, particularly linear-kernel, and we
only consider a single model class (recurrent net-
works with GRU hidden unit). Further architec-
ture exploration is necessary, and user embedding
pre-training will hopefully play a role in training
state-of-the-art stance classiﬁcation models.

We sought to understand how much stance-
relevant information was contained in the user em-
beddings. The guns data allowes us to investi-
gate this, since the users who had stance anno-
tations and those who had embeddings overlap.
We trained an SVM to predict gun stance but in-
stead of providing the tweet, we either provided
the tweet, one of the embeddings, or both together.
Higher prediction accuracy indicates that the input
is more helpful in predicting stance.

Table 4 shows test accuracy for this task across
different amounts of training data. Unsurprisingly,
the tweet content is more informative at predicting
stance than the user embedding. However, the em-
beddings did quite well, with the “Author Text”
embedding – coming close to the tweet in some
cases. Providing both features had no effect or
only a marginal improvement over the text alone.

7 Conclusion

We have presented a method for incorporating user
information into a stance classiﬁcation model for

improving accuracy on test data, even when no
user embeddings are available during prediction
time. We rely on a pre-training method that can
ﬂexibly utilize embeddings directly correspond-
ing to the annotated stance classiﬁcation dataset,
are distantly related, or have no relation to the
topic. We observe improvements on most of the
SemEval 2016 domains, with mixed results on a
new guns stance dataset – we only see beneﬁt with
fewer than 1,000 training examples.

Future work will explore more effective ways in
which we can represent users, and utilize the in-
formation within the classiﬁcation model. We are
interested in neural models that are more robust to
variation in the input examples such as convolu-
tional neural networks.

Despite having data for six stance classiﬁcation
targets, the datasets are still small and limited. We
plan to evaluating our pre-training technique on
the stance classiﬁcation tasks presented in Hasan
and Ng (2013) and related message-level classiﬁ-
cation tasks such as rumor identiﬁcation (Wang,
2017).

Augenstein et al. (2016) present a stance clas-
siﬁcation model that can be applied to unseen tar-
gets, conditioning stance prediction on an encod-
ing of the target description. Although the exper-
iments we run here only consider models trained
independently for each target, user embedding
pre-training is not restricted to this scenario. We
will also investigate whether user embedding pre-
training beneﬁts models that are trained on many
targets jointly and those designed for unseen tar-
gets.

References
Silvio Amir, Byron C Wallace, Hao Lyu, Paula Car-
valho, and M´ario J Silvia. 2016. Modelling context
with user embeddings for sarcasm detection in social
media. CONLL, page 167.

Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E Fox Tree, Robeson Bowmani, and Michael Mi-
nor. 2011. Cats rule and dogs drool!: Classifying
stance in online debate. In Proceedings of the 2nd
workshop on computational approaches to subjec-
tivity and sentiment analysis, pages 1–9. Association
for Computational Linguistics.

Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the ﬂu: detecting inﬂuenza
epidemics using twitter. In Proceedings of the con-
ference on empirical methods in natural language
processing, pages 1568–1576. Association for Com-
putational Linguistics.

192Isabelle Augenstein, Tim Rockt¨aschel, Andreas Vla-
chos, and Kalina Bontcheva. 2016. Stance detection
with bidirectional conditional encoding. In EMNLP,
pages 876–885.

Eytan Bakshy, Solomon Messing, and Lada A Adamic.
2015. Exposure to ideologically diverse news and
Science, 348(6239):1130–
opinion on facebook.
1132.

Adrian Benton, Raman Arora, and Mark Dredze. 2016.
Learning multiview embeddings of twitter users. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), volume 2, pages 14–19.

Adam Bermingham and Alan Smeaton. 2011. On us-
ing twitter to monitor political sentiment and predict
In Proceedings of the Workshop
election results.
on Sentiment Analysis where AI meets Psychology
(SAAIP 2011), pages 2–10.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classiﬁ-
cation. In Proceedings of the 45th annual meeting of
the association of computational linguistics, pages
440–447.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Nigel Collier and Son Doan. 2011. Syndromic classi-
ﬁcation of twitter messages. In International Con-
ference on Electronic Healthcare, pages 186–195.
Springer.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Jacob Eisenstein,
Dipanjan Das, Daniel Mills,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging for
twitter: Annotation, features, and experiments.
In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
42–47. Association for Computational Linguistics.

Kazi Saidul Hasan and Vincent Ng. 2013. Stance clas-
siﬁcation of ideological debates: Data, models, fea-
In IJCNLP, pages 1348–
tures, and constraints.
1356.

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classiﬁcation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 151–160. Association for Computational
Linguistics.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference for Learning Representations (ICLR).

Irene Kwok and Yuzhou Wang. 2013. Locate the hate:

Detecting tweets against blacks. In AAAI.

Nut Limsopatham and Nigel Henry Collier. 2016.
Bidirectional lstm for named entity recognition in
twitter messages.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Saif M. Mohammad, Svetlana Kiritchenko, Parinaz
Sobhani, Xiaodan Zhu, and Colin Cherry. 2016.
Semeval-2016 task 6: Detecting stance in tweets. In
Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’16, San Diego, Cali-
fornia.

Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: classifying positions in online debates
from reply activities and opinion expressions.
In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869–875.
Association for Computational Linguistics.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classiﬁcation using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.

Bo Pang, Lillian Lee, et al. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R(cid:13) in In-
formation Retrieval, 2(1–2):1–135.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. Semeval-2017 task 4: Sentiment analysis in
In Proceedings of the 11th International
twitter.
Workshop on Semantic Evaluation (SemEval-2017),
pages 502–518.

Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
In Proceed-
ognizing stances in online debates.
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the

193AFNLP: Volume 1-Volume 1, pages 226–234. Asso-
ciation for Computational Linguistics.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Docu-
ment modeling with gated recurrent neural network
In Proceedings of the
for sentiment classiﬁcation.
2015 conference on empirical methods in natural
language processing, pages 1422–1432.

Andranik Tumasjan, Timm Oliver Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with twitter: What 140 characters reveal
about political sentiment. Icwsm, 10(1):178–185.

Soroush Vosoughi, Prashanth Vijayaraghavan, and Deb
Roy. 2016. Tweet2vec: Learning tweet embeddings
using character-level cnn-lstm encoder-decoder. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval, pages 1041–1044. ACM.

William Yang Wang. 2017. ”liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
ACL, volume 2, pages 422–426.

Min Yang, Wenting Tu, Jingxuan Wang, Fei Xu, and
Xiaojun Chen. 2017. Attention based lstm for target
dependent sentiment classiﬁcation. In AAAI, pages
5013–5014.

Guido Zarrella and Amy Marsh. 2016. Mitre at
semeval-2016 task 6: Transfer learning for stance
detection. arXiv preprint arXiv:1606.03784.

194
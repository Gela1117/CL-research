Grammar Induction with Neural Language Models:

An Unusual Replication

Phu Mon Htut1

Kyunghyun Cho1,2

Samuel R. Bowman1,2,3

pmh330@nyu.edu

kyunghyun.cho@nyu.edu

bowman@nyu.edu

1Center for Data Science

New York University

60 Fifth Avenue

New York, NY 10011

2Dept. of Computer Science

New York University

60 Fifth Avenue

New York, NY 10011

3Dept. of Linguistics
New York University
10 Washington Place
New York, NY 10003

Introduction

1
Grammar induction is the task of learning syntac-
tic structure without the expert-labeled treebanks
(Charniak and Carroll, 1992; Klein and Manning,
2002). Recent work on latent tree learning of-
fers a new family of approaches to this problem by
inducing syntactic structure using the supervision
from a downstream NLP task (Yogatama et al.,
2017; Maillard et al., 2017; Choi et al., 2018). In a
recent paper published at ICLR, Shen et al. (2018)
introduce such a model and report near state-of-
the-art results on the target task of language mod-
eling, and the ﬁrst strong latent tree learning re-
sult on constituency parsing. During the analy-
sis of this model, we discover issues that make
the original results hard to trust, including tuning
and even training on what is effectively the test
set. Here, we analyze the model under different
conﬁgurations to understand what it learns and to
identify the conditions under which it succeeds.
We ﬁnd that this model represents the ﬁrst empiri-
cal success for neural network latent tree learning,
and that neural language modeling warrants fur-
ther study as a setting for grammar induction.

analyze

2 Background and Experiments
the Parsing-Reading-Predict-
We
Network (PRPN; Shen et al., 2018), which uses
convolutional networks with a form of structured
attention (Kim et al., 2017) rather than recursive
neural networks (Goller and Kuchler, 1996;
Socher et al., 2011) to learn trees while perform-
ing straightforward backpropagation training on a
language modeling objective. The structure of the
model seems rather suboptimal: Since the parser
is trained as part of a language model, it makes
parsing greedily, with no access to any words to
the right of the point where each parsing decision
must be made.

The experiments on language modeling and

A crusade of NO to the consumption of drugs is imperative .

Parses from PRPN-LM trained on

Figure 1:
AllNLI.
parsing are carried out using different conﬁgura-
tions of the model—PRPN-LM tuned for language
modeling, and PRPN-UP for (unsupervised) pars-
ing. PRPN-LM is much larger than PRPN-UP,
with embedding layer that is 4 times larger and
the number of units per layer that is 3 times larger.
In the PRPN-UP experiments, we observe that the
WSJ data is not split, such that the test data is used
without parse information for training. This im-
plies that the parsing results of PRPN-UP may not
be generalizable in the way usually expected of
machine learning evaluation results.

We train PRPN on sentences from two datasets:
The full WSJ and AllNLI,
the concatenation
of SNLI (Bowman et al., 2015) and MultiNLI
(Williams et al., 2018b). We then evaluate the con-
stituency trees produced by these models on the
full WSJ, WSJ101, and the MultiNLI development
set.

3 Results
Table 1 shows results with all the models un-
der study, plus several baselines, on WSJ and
WSJ10. Unexpectedly,
the PRPN-LM models
achieve higher parsing performance than PRPN-
UP. This shows that any tuning done to sepa-
rate PRPN-UP from PRPN-LM was not necessary,
and that the results described in the paper can be
largely reproduced by a uniﬁed model in a fair
setting. Moreover, the PRPN models trained on
the larger, out-of-domain AllNLI perform better
than those trained on WSJ. Surprisingly, PRPN-
LM tained on out-of-domain AllNLI achieves the
best F1 score on full WSJ among all the models

1A processed subset of WSJ in which the sentences con-

tain no punctuation and no more than 10 words.

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages371–373Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics371Model

Training
Data

Stopping Vocab
Criterion Size

Parsing F1

WSJ10

WSJ

AllNLI Train UP
PRPN-UP
AllNLI Train LM
PRPN-UP
AllNLI Train LM
PRPN-LM
UP
WSJ Full
PRPN-UP
LM
WSJ Full
PRPN-UP
UP
WSJ Train
PRPN-UP
LM
WSJ Train
PRPN-UP
WSJ Train
LM
PRPN-LM
PRPN-LM
UP
WSJ Train
300D ST-Gumbel AllNLI Train NLI
w/o Leaf GRU AllNLI Train NLI
300D RL-SPINN AllNLI Train NLI
w/o Leaf GRU AllNLI Train NLI

CCM
DMV+CCM
UML-DOP
Random Trees
Balanced Trees

WSJ10 Train –
WSJ10 Train –
WSJ10 Train –
–
–
–
–

µ (σ) max

µ (σ) max
67.5 (0.6) 68.6 38.1 (0.7) 39.1
76k
66.3 (0.8) 68.5 39.8 (0.6) 40.7
76k
52.4 (4.9) 58.1 42.5 (0.7) 43.6
76k
15.8k 64.7 (3.2) 70.9 26.6 (1.9) 31.6
15.8k 64.3 (3.3) 70.8 26.5 (1.9) 31.4
15.8k 63.5 (3.5) 70.7 26.6 (2.5) 34.2
15.8k 62.2 (3.9) 70.3 26.4 (2.5) 34.0
10k
70.5 (0.4) 71.3 38.3 (0.3) 38.9
66.1 (0.5) 67.2 34.0 (0.9) 36.3
10k
19.0 (1.0) 20.1
–
22.8 (1.6) 25.0
–
13.2 (0.0) 13.2
–
13.1 (0.1) 13.2
–
–
–
–
–
–

71.9
77.6
82.9
34.7 21.3 (0.0) 21.4
21.3 (0.0) 21.3

–
–
–
–
–
–
–
–
–

–
–
–
–

–
–
–

–
–
–

–

Depth Accuracy on WSJ by Tag
WSJ ADJP NP PP INTJ

5.9
5.9
6.2
5.9
5.9
5.9
5.9
5.9
5.9
–
–
–
–
–
–
–
5.3
4.6

27.8
26.5
34.2
19.3
18.8
21.3
22.3
26.0
32.0
15.6
18.9
1.7
1.6
–
–
–

17.4
22.1

63.0 31.4
53.0 32.9
60.1 60.0
48.7 19.2
48.1 19.1
57.2 19.4
56.2 19.1
64.4 25.5
58.3 19.6
18.8 9.9
24.1 14.2
10.8 4.6
10.9 4.6
–
–
–

–
–
–

22.3 16.0
20.2 9.3

52.9
52.9
64.7
44.1
44.1
47.1
44.1
50.0
44.1
59.4
51.8
50.6
50.0

–
–
–

40.4
55.9

Table 1: Unlabeled parsing F1 test results broken down by training data and by early stopping criterion.
The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond
to constituents in the model parses. Italics mark results that are worse than the random baseline. Results
with RL-SPINN and ST-Gumbel are from Williams et al. (2018a). WSJ10 baselines are from Klein and
Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP).

Model
300D SPINN

w/o Leaf GRU
300D SPINN-NC
w/o Leaf GRU
300D ST-Gumbel
w/o Leaf GRU
300D RL-SPINN
w/o Leaf GRU

PRPN-LM
PRPN-UP
PRPN-UP
Random Trees
Balanced Trees

Stopping
Criterion LB RB SP Depth

F1 wrt.

19.3 36.9 70.2
NLI
21.2 39.0 63.5
NLI
19.2 36.2 70.5
NLI
20.6 38.9 64.1
NLI
32.6 37.5 23.7
NLI
30.8 35.6 27.5
NLI
95.0 13.5 18.8
NLI
NLI
99.1 10.7 18.1
LM 25.6 26.9 45.7
UP
19.4 41.0 46.3
LM 19.9 37.4 48.6
27.9 28.0 27.0
–
–
21.7 36.8 21.3

6.2
6.4
6.1
6.3
4.1
4.6
8.6
8.6
4.9
4.9
4.9
4.4
3.9

Table 2: Unlabeled parsing F1 on the MultiNLI
development set for models trained on AllNLI. F1
wrt. shows F1 with respect to strictly right- and
left-branching (LB/RB) trees and with respect to
the Stanford Parser (SP) trees supplied with the
corpus; The evaluations of SPINN, RL-SPINN,
and ST-Gumbel are from Williams et al. (2018a).
SPINN is a supervised parsing model, and the oth-
ers are latent tree models.

we experimented, even though its performance on
WSJ10 is the lowest of all. Under all the conﬁgu-
rations we tested, PRPN yields much better perfor-
mance than that seen with the baselines from Yo-
gatama et al. (2017, called RL-SPINN) and Choi

et al. (2018, called ST-Gumbel), despite the fact
that the model was tuned exclusively for WSJ10
parsing (Table 1 and 2). This suggests that PRPN
is strikingly effective at latent tree learning.

Additionally, Table 2 shows that both PRPN-UP
models achieve F1 scores of 46.3 and 48.6 respec-
tively on the MultiNLI dev set, setting the state
of the art in parsing on this dataset among latent
tree models. We conclude that PRPN does acquire
some substantial knowledge of syntax, and that
this knowledge agrees with Penn Treebank (PTB)
grammar signiﬁcantly better than chance.

Moreover, we replicate the language model-
ing perplexity of 61.6 reported in the paper us-
ing PRPN-LM trained on WSJ, which indicates
that PRPN-LM is effective at both parsing and lan-
guage modeling.

4 Conclusion
In our analysis of the PRPN model, we ﬁnd sev-
eral experimental problems that make the results
difﬁcult to interpret. However, in the analyses go-
ing well beyond the scope of the original paper,
we ﬁnd that PRPN is nonetheless robust. It repre-
sents a viable method for grammar induction and
the ﬁrst success for latent tree learning. We expect
that it heralds further work on language modeling
as a tool for grammar induction research.

372identify meaningful structure in sentences? Trans-
actions of the Association for Computational Lin-
guistics (TACL).

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018b. A broad-coverage challenge corpus for
sentence understanding through inference. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL).

Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2017. Learning to
Compose Words into Setences with Reinforcement
Learning. Proceedings of the International Confer-
ence on Learning Representations, pages 1–17.

References
Rens Bod. 2006. An All-Subtrees Approach to Un-
supervised Parsing. Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 865–872.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
In Proceedings of the Conference on Em-
ence.
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Eugene Charniak and Glen Carroll. 1992. Two exper-
iments on learning probabilistic dependency gram-
In Proceedings of the AAAI
mars from corpora.
Workshop on Statistically-Based NLP Techniques,
page 113.

Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.
Learning to compose task-speciﬁc tree structures.
In Proceedings of the Thirty-Second Association for
the Advancement of Artiﬁcial Intelligence Confer-
ence on Artiﬁcial Intelligence (AAAI-18), volume 2.

Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of International Conference on Neural Networks
(ICNN’96).

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-

der M. Rush. 2017. Structured attention networks.

Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics - ACL ’02, page 128.

Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern Recognition,
38(9):1407–1419.

Jean Maillard, Stephen Clark, and Dani Yogatama.
Jointly learning sentence embeddings and
arXiv

2017.
syntax with unsupervised Tree-LSTMs.
preprint 1705.09189.

Yikang Shen, Zhouhan Lin, Chin wei Huang, and
Aaron Courville. 2018. Neural language modeling
by jointly learning syntax and lexicon. In Interna-
tional Conference on Learning Representations.

Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and
Chris Manning. 2011. Parsing Natural Scenes and
Natural Language with Recursive Neural Networks.
In Proceedings of the 28th International Conference
on Machine Learning, pages 129–136.

Adina Williams, Andrew Drozdov, and Samuel R.
Bowman. 2018a. Do latent tree learning models

373
GLUE: A Multi-Task Benchmark and Analysis Platform

for Natural Language Understanding

Alex Wang1, Amanpreet Singh1, Julian Michael2, Felix Hill3,

Omer Levy2, and Samuel R. Bowman1

2Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA

1New York University, New York, NY

3DeepMind, London, UK

{alexwang,amanpreet,bowman}@nyu.edu
{julianjm,omerlevy}@cs.washington.edu

felixhill@google.com

Human ability to understand language is gen-
eral, ﬂexible, and robust. In contrast, most NLU
models above the word level are designed for a
speciﬁc task and struggle with out-of-domain data.
If we aspire to develop models with understand-
ing beyond the detection of superﬁcial correspon-
dences between inputs and outputs, then it is crit-
ical to develop a uniﬁed model that can execute a
range of linguistic tasks across different domains.
To facilitate research in this direction, we
present
the General Language Understanding
Evaluation (GLUE, gluebenchmark.com): a
benchmark of nine diverse NLU tasks, an auxil-
iary dataset for probing models for understand-
ing of speciﬁc linguistic phenomena, and an on-
line platform for evaluating and comparing mod-
els. For some benchmark tasks, training data is
plentiful, but for others it is limited or does not
match the genre of the test set. GLUE thus favors
models that can represent linguistic knowledge in
a way that facilitates sample-efﬁcient learning and
effective knowledge-transfer across tasks. While
none of the datasets in GLUE were created from
scratch for the benchmark, four of them feature
privately-held test data, which is used to ensure
that the benchmark is used fairly.

We evaluate baselines that use ELMo (Peters
et al., 2018), a powerful transfer learning tech-
nique, as well as state-of-the-art sentence repre-
sentation models. The best models still achieve
fairly low absolute scores. Analysis with our diag-
nostic dataset yields similarly weak performance
over all phenomena tested, with some exceptions.

The GLUE benchmark GLUE consists of nine
English sentence understanding tasks covering a
broad range of domains, data quantities, and difﬁ-
culties. As the goal of GLUE is to spur develop-
ment of generalizable NLU systems, we design the
benchmark such that good performance should re-

Corpus

|Train| Task

Domain

Single-Sentence Tasks
8.5k
67k

acceptability misc.
sentiment

movie reviews

Similarity and Paraphrase Tasks
news
misc.
online QA

3.7k
7k
364k

paraphrase
textual sim.
paraphrase
Inference Tasks

393k NLI
108k QA/NLI
2.5k NLI
634

coref./NLI

misc.
Wikipedia
misc.
ﬁction books

CoLA
SST-2

MRPC
STS-B
QQP

MNLI
QNLI
RTE
WNLI

Table 1: Task descriptions and statistics. Bold de-
notes tasks for which there is privately-held test
data. All tasks are binary classiﬁcation, except
STS-B (regression) and MNLI (three classes).

quire models to share substantial knowledge (e.g.,
trained parameters) across tasks, while maintain-
ing some task-speciﬁc components. Though it is
possible to train a model per task and evaluate the
resulting set of models on this benchmark, we ex-
pect that inclusion of several data-scarce tasks will
ultimately render this approach uncompetitive.

The nine tasks include two tasks with single-
sentence inputs: Corpus of Linguistic Acceptabil-
ity (CoLA; Warstadt et al. 2018) and Stanford
Sentiment Treebank (SST-2; Socher et al. 2013)
Three tasks involve detecting semantic similarity:
Microsoft Research Paraphrase Corpus (MRPC,
(Dolan and Brockett, 2005)), Quora Question
Pairs1 (QQP), and Semantic Textual Similarity
Benchmark (STS-B; Cer et al. 2017). The remain-
ing four tasks are formatted as natural language in-
ference (NLI) tasks, such as the Multi-Genre NLI
corpus (MNLI; Williams et al. 2018) and Recog-

1

data.quora.com/First-Quora-Dataset-

Release-Question-Pairs

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages353–355Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics353Single Sentence

Similarity and Paraphrase

Natural Language Inference

Model
Single-task
Multi-task
CBoW
Skip-Thought
InferSent
DisSent
GenSen

Avg CoLA SST-2 MRPC
68.8/80.2
64.8
77.3/83.5
69.0
73.4/81.5
58.9
71.7/80.8
61.5
64.7
74.1/81.2
74.1/81.7
62.1
66.6
76.6/83.0

35.0
18.9
0.0
0.0
4.5
4.9
7.7

90.2
91.6
80.0
81.8
85.1
83.7
83.1

QQP

86.5/66.1
85.3/63.3
79.1/51.4
82.2/56.4
81.7/59.1
82.6/59.5
82.9/59.8

STS-B
55.5/52.5
72.8/71.1
61.2/58.7
71.8/69.7
75.9/75.3
66.1/64.8
79.3/79.2

MNLI
76.9/76.7
75.6/75.9
56.0/56.4
62.9/62.8
66.1/65.7
58.7/59.1
71.4/71.3

QNLI RTE WNLI
65.1
65.1
62.3
65.1
65.1
65.1
65.1

61.1
81.7
75.1
74.7
79.8
75.2
82.3

50.4
61.2
54.1
53.1
58.0
56.4
59.2

Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and
mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and
Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks
we report accuracy. All values are scaled by 100. A similar table is presented on the online platform.

nizing Textual Entailment (RTE; aggregated from
Dagan et al. 2006, Bar Haim et al. 2006, Giampic-
colo et al. 2007, Bentivogli et al. 2009), as well
as versions of SQuAD (Rajpurkar et al., 2016)
and Winograd Schema Challenge (Levesque et al.,
2011) recast as NLI (resp. QNLI, WNLI). Table 1
summarizes the tasks. Performance on the bench-
mark is measured per task as well as in aggregate,
averaging performance across tasks.
Diagnostic Dataset To understand the types of
knowledge learned by models, GLUE also in-
cludes a dataset of hand-crafted examples for
probing trained models. This dataset is designed
to highlight common phenomena, such as the use
of world knowledge, logical operators, and lexi-
cal entailments, that models must grasp if they are
to robustly solve the tasks. Each of the 550 ex-
amples is an NLI sentence pair tagged with the
phenomena demonstrated. We ensure that the data
is reasonably diverse by producing examples for
a wide variety of linguistic phenomena, and bas-
ing our examples on naturally-occurring sentences
from several domains. We validate our data by us-
ing the hypothesis-only baseline from Gururangan
et al. (2018) and having six NLP researchers man-
ually validate a random sample of the data.
Baselines To demonstrate the benchmark in use,
we apply multi-task learning on the training data
of the GLUE tasks, via a model that shares a BiL-
STM between task-speciﬁc classiﬁers. We also
train models that use the same architecture but are
trained on a single benchmark task. Finally, we
evaluate the following pretrained models: average
bag-of-words using GloVe embeddings (CBoW),
Skip-Thought (Kiros et al., 2015), InferSent (Con-
neau et al., 2017), DisSent (Nie et al., 2017), and
GenSen (Subramanian et al., 2018).

Tags

Quantiﬁers

Double Negation

Active/Passive

Named Entities
World Knowledge

Sentence Pair
I have never seen a hummingbird
not ﬂying.
I have never seen a hummingbird.
Cape sparrows eat seeds, along
with soft plant parts and insects.
Cape sparrows are eaten.
Musk decided to offer up his per-
sonal Tesla roadster.
Musk decided to offer up his per-
sonal car.

Table 3: Diagnostic set examples. Systems must
predict the relationship between the sentences, ei-
ther entailment, neutral, or contradiction when
one sentence is the premise and the other is the
hypothesis, and vice versa. Examples are tagged
with the phenomena demonstrated. We group each
phenomena into one of four broad categories.

We ﬁnd that our models trained directly on the
GLUE tasks generally outperform those that do
not, though all models obtain fairy low absolute
scores. Probing the baselines with the diagnos-
tic data, we ﬁnd that performance on the bench-
mark correlates with performance on the diag-
nostic data, and that the best baselines similarly
achieve low absolute performance on the linguis-
tic phenomena included in the diagnostic data.
Conclusion We present the GLUE benchmark,
consisting of: (i) a suite of nine NLU tasks, built
on established annotated datasets and covering a
diverse range of text genres, dataset sizes, and
difﬁculties; (ii) an online evaluation platform and
leaderboard, based primarily on private test data;
(iii) an expert-constructed analysis dataset. Exper-
iments indicate that solving GLUE is beyond the
capability of current transfer learning methods.

354References
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second PASCAL recognising
textual entailment challenge.

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
ﬁfth PASCAL recognizing textual entailment chal-
lenge.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity-multilingual and
In 11th Interna-
cross-lingual focused evaluation.
tional Workshop on Semantic Evaluations.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017, pages 681–
691.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. evalu-
ating predictive uncertainty, visual object classiﬁca-
tion, and recognising tectual entailment, pages 177–
190. Springer.

lysozyme. Biochimica et Biophysica Acta (BBA)-
Protein Structure, 405(2):442–451.

Allen Nie, Erin D Bennett, and Noah D Goodman.
2017. Dissent: Sentence representation learning
arXiv preprint
from explicit discourse relations.
1710.04334.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL 2018.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392. Asso-
ciation for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
In Proceedings of the 2013 conference on
bank.
empirical methods in natural language processing,
pages 1631–1642.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J. Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
In Proceedings of
large scale multi-task learning.
ICLR.

William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of IWP.

Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018. Neural network acceptability judg-
ments. arXiv preprint 1805.12471.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018. A broad-coverage challenge corpus for
sentence understanding through inference. In Pro-
ceedings of NAACL 2018.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proceedings
of the ACL-PASCAL workshop on textual entailment
and paraphrasing, pages 1–9. Association for Com-
putational Linguistics.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in nat-
In Proceedings of
ural language inference data.
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-Thought vectors.
In
Advances in neural information processing systems,
pages 3294–3302.

Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The Winograd schema challenge.
In
Aaai spring symposium: Logical formalizations of
commonsense reasoning, volume 46, page 47.

Brian W Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of t4 phage

355
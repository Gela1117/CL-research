EmotionX-AR: CNN-DCNN autoencoder based Emotion Classiﬁer

Sopan Khosla

Big Data Experience Lab

Adobe Research, Bangalore, India

skhosla@adobe.com

Abstract

joint

autoencoder

reconstruction loss

In this paper, we model emotions in Emo-
tionLines dataset using a convolutional-
deconvolutional
(CNN-
DCNN) framework. We show that adding
a
improves
performance. Quantitative evaluation with
jointly trained network, augmented with
linguistic features, reports best accuracies
for emotion prediction;
namely joy,
sadness, anger, and neutral emotion in
text.

Introduction

1
Emotion recognition in content is an extensively
studied area.
It deals with associating words,
phrases or documents with various categories of
emotions. The importance of emotion analysis in
human communication and interactions has been
discussed by Picard (1997). Historically studied
using multi-modal data, the study of human emo-
tion from text and other published content has be-
come an important topic in language understand-
ing. Word correlation with social and psycholog-
ical processes is discussed by Pennebaker (2011).
Preotiuc-Pietro et al. (2017) studied personal-
ity and psycho-demographic preferences through
Facebook and Twitter content. The analysis of
emotion in interpersonal communication such as
emails, chats and longer written articles is neces-
sary for various applications including the study of
consumer behavior and psychology, understand-
ing audiences, and opinions in computational so-
cial science, and more recently for dialogue sys-
tems and conversational agents. This is an active
research space today.

In contrast

to sentiment analysis, emotion
analysis
such as
tweets (Dodds et al., 2011), blogs (Aman and Sz-

in user generated content

pakowicz, 2007) and chats remains a space less
trodden. The WASSA-2017 task on emotion in-
tensity (Mohammad and Bravo-Marquez, 2017)
aims at detecting the intensity of emotion felt by
the author of a tweet. Whereas (Alm et al., 2005;
Aman and Szpakowicz, 2007; Brooks et al., 2013;
Neviarouskaya et al., 2009; Bollen et al., 2011)
provide discrete binary labels to text instances for
emotion classiﬁcation. Typical discrete categories
are a subset of those proposed by Ekman (Ekman,
1992) namely anger, joy, surprise, disgust, sad-
ness, and fear.
Paper Structure: The remainder of the paper is
organized as follows. We summarize the Emo-
tionLines dataset in Section 2. Section 3 describes
different parts of our system. We present our ex-
periments in Section 4. Section 5 discusses the
results of our ﬁnal system submitted to the Emo-
tionX challenge. Finally, we present conclusion
and future directions in section 6.

2 Data

EmotionLines dataset contains dialogues from the
Friends TV series and EmotionPush chat logs.
Both Friends TV scripts and EmotionPush chat
logs contain 1,000 dialogues split
into train-
ing(720), development(80), and testing(200) set
separately. In order to preserve completeness of
any dialogue, the corpus was divided by the di-
alogues, not the utterances. Refer to Chen et
al. (2018) for details on the dataset collection and
construction.

The EmotionX task on EmotionLines dialogue
dataset tries to capture the ﬂow of emotion in a
conversation. Given a dialogue, the task requires
participants to determine the emotion of each ut-
terance (in that dialogue) among four label candi-
dates: joy, sadness, anger, and neutral.

ProceedingsoftheSixthInternationalWorkshoponNaturalLanguageProcessingforSocialMedia,pages37–44,Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics373 System Description

In this section, we provide the technical details of
our model.

3.1 Architecture Overview
We propose a joint learning framework for emo-
tion detection built on a convolutional encoder
(CNN). We introduce a joint learning objective
where the network needs to learn the (1) utter-
ance text (the data itself) and the (2) emotion in-
formation from the labeled data (EmotionLines)
together. The CNN along with a deconvolutional
decoder (DCNN) provides the mechanism for text
reconstruction, i.e. to learn the text sequences. On
the other hand, the learned encoding, augmented
with linguistic features, acts as the input feature
space for emotion detection.

CNN encoder to create a ﬁxed-length vector hL
for the entire input text d. This latent representa-
tion, appended with linguistic features is then sent
to a fully connected layer with a softmax classiﬁer
on top. Along with this, hL is also fed to a decon-
volutional decoder which attempts to reconstruct
d from the latent vector. Therefore, the ﬁnal loss
function: αaeLae + (1− αae)Lc for the model is a
combination of the classiﬁcation error Lc and the
reconstruction error Lae explained in the follow-
ing subsections.

3.2 CNN-DCNN Autoencoder
Zhang et al. (2017) introduce a sequence-to-
sequence convolutional encoder followed by a de-
convolutional decoder (CNN-DCNN) framework
for learning latent representations from text data.
Their proposed framework outperforms RNN-
based networks for text reconstruction and semi-
supervised classiﬁcation tasks. We leverage their
network in our work.
Convolutional Encoder. CNN with L layers, in-
spired from Radford et al. (2015) is used to encode
the document into a latent representation vector,
hL. Former L − 1 convolutional layers create a
feature map which is fed into a fully-connected
layer implemented as a convolutional layer. This
ﬁnal layer produces the latent representation hL
which acts as a ﬁxed-dimensional summarization
of the document.
Deconvolutional Decoder. We leverage the de-
convolutional decoder introduced by Zhang el
al. (2017) as is for our model. The reconstruction
loss is deﬁned as,

Lae =

log p( ˆwt

d = wt

d),

(1)

(cid:88)

(cid:88)

d∈D

t

Figure 1: Architecture Overview

The architecture diagram is shown in ﬁgure 1.
The network aims at emotion classiﬁcation and in
turn also learns the reconstruction objective. Key
components of this approach are: (1) Convolu-
tional Autoencoder (CNN-DCNN), (2) Linguis-
tic features, and (3) Joint-learning objective.

Consider a text input d to the model. Each word
d in d is embedded into a k-dimensional repre-
wt
sentation et = E[wt
d] where E is a learned ma-
trix. The embedding layer is passed through a

where D is the set of observed sentences. wt
d and
d correspond to the words in the input and output
ˆwt
sequences respectively.

3.3 Linguistic Features
Here, we explain the various linguistic features
used in our network.
Inspired from Chhaya et
al, (2018), we use 68 linguistic features further
divided into 4 sub-groups: Lexical, Syntactic,
Derived and Affect-based.
The lexical and
syntactic features include features such as ‘avera-
geNumberofWords per sentence’ and ‘number of
capitalizations’. Features that can help quantify
readability of text are the part of derived features.

38Thus,
this set contains features like Hedges,
Contractions, and Readability scores. The fourth
group of features are the Affect–related features.
These features are lexica–based and quantify the
amount of affective content present in the text.
All features used by Pavlick et al. (2016) for for-
mality detection and by Danescu et al. (2013) for
politeness detection are included in our analysis.
We use Stanford CoreNLP1 and TextBlob2 feature
extraction and pre-processing.

Lexical and Syntactic Features: The lexi-
cal features capture various counts associated
with the content like ’#Question Marks’, ’Average
Word Length’ etc.
Syntactic features include
NER–based features, Number of blank lines, and
text density which is deﬁned as follows:

ρ =

#(sentences)
1 + #(lines)

where ρ is the text density, #(sentences) denotes
number of sentences in the text content and
#(lines) number of lines including blank lines
in the text message. Prior art in NLP extensively
relies on these features for their analysis.

Derived: Readability Features: The derived
features capture information such as readability
of text, existence of hedges, subjectivity, contrac-
tions and sign–offs. Subjectivity, contractions and
hedges are based on the TextBlob implementation.
Readability is measured based on Flesh–Kincaid
readability score. This score is a measure of ease
of reading of given piece of text. We use the
textstat package3 in Python for implementation.

Psycholinguistic Features:
tures used in our analysis include:

The affect

fea-

1. Valence-Arousal-Dominance (VAD) Model
(Mehrabian, 1980): We use the Warriner’s
lexicon (Warriner et al., 2013) for these
features. This lexicon contains real-valued
scores for Valence, Arousal, and Dominance
(VAD) on a scale of 1 9 each for 13915 En-
glish words. 1, 5, 9 correspond to the low,
moderate (i.e. neutral), and high values for
each dimension respectively.

2. Ekman’s Emotions (Ekman, 1992): Ekman
introdcued six fundamental emotions namely
anger,
joy, surprise, disgust, sadness, and
fear. In this work, we use the NRC lexicon
(EMOLEX) (Mohammad et al., 2013) which
provides a measure for the existence of the
emotion as well as the intensity of the de-
tected emotion on word level.

3. PERMA Model (Seligman, 2011): The
PERMA model is a scale to measure posi-
tivity and well–being in humans (Seligman,
2011). This model deﬁnes the 5 dimen-
sions: Positive Emotions, Engagement, Re-
lationships, Meaning, and Accomplishments
as quantiﬁers and indicators of positivity and
well–being. Schwartz et al. (Schwartz et al.,
2013) published a PERMA lexicon. We use
this lexicon in our work.

Formality Lists:We use the formality list, pro-
vided by Brooke et al. (2010), for our experiments.
It contains a set of words usually used to express
formality or informality in text.

3.4 Supervised Classiﬁcation
Traditional affective language studies focus on
analyzing features including lexical (Pennebaker
et al., 2001), syntactic, and psycholinguistic fea-
tures to detect emotions. We augment the latent
vector produced by CNN encoder with the set of
linguistic features (Section 3.3) to capture emo-
tions.

(cid:48)(cid:48)

Let h

denote the representation vector for lin-
(cid:48)
guistic features extracted from the input data d. h
is normalized and concatenated with hL to derive
, producing a probability pn
h
for each neuron in the softmax layer, where yn de-
notes the ground-truth for corresponding class n.
We use cross-entropy based classwise loss as

= hL (cid:95) h

. h

(cid:48)(cid:48)

(cid:48)

(cid:48)

given below:

lossn = −(cid:104)

yn log(pn) + (1 − yn) log(1 − pn)

Since, EmotionLines suffers from class imbal-
ance, we give higher weight (wn) to the losses in-
curred on data samples of minority classes.

an(cid:80)N

i=1 ai

1
wn

=

(cid:105)

1https://stanfordnlp.github.io/CoreNLP/
2https://textblob.readthedocs.io/en/dev/
3https://pypi.python.org/pypi/textstat/0.1.6

where an denote the number of samples of class
n in the training set. Finally, we use a weighted

39Features
Lexical

Feature list
Average Word Length, Average Words per Sentence, # of Upper Case Words, # Ellipses, # Exclamation
marks,
# Question Mark, # Multiple Question Marks, # Words, # Lower Case words, First word upper case,
# NonAlphaChars, # Punctuation Chars

Syntactic

# BlankLines, NER-Person, NER-Location, NER-PersonLength, NER-Organization, TextDensity

Derived

# Contractions, ReadabilityScore- FKgrade, FirstPerson, Hedge, Subjectivity,
Sentiment, ThirdPerson, SignOff

Psycholingistic
Features

ANEW-arousal, ANEW-dominance, ANEW-valence,

EmolexIntensity-anger, EmolexIntensity-fear, EmolexIntensity-joy, EmolexIntensity-sadness, Emolex-
anger, Emolex-anticipation,
Emolex-disgust, Emolex-fear, Emolex-joy, Emolex-negative, Emolex-positive, Emolex-sadness,
Emolex-surprise, Emolex-trust,
Perma-NEG-A, Perma-NEG-E, Perma-NEG-M, Perma-NEG-P, Perma-NEG-R, Perma-POS-A,
Perma-POS-E, Perma-POS-M, Perma-POS-P, Perma-POS-R

Formal Words

formal-words, informal-words (Brooke et al., 2010)

Table 1: Summary of feature groups used in our model.

Friends TV Series Script WA
67.67
CNN + MLP (S)
S + Joint Learning (J)
67.40
65.30
S + Linguistic Features (L)
S + J + L
60.97
EmotionPush Chat Logs WA
CNN + MLP (S)
68.89
70.44
S + Joint Learning (J)
67.54
S + Linguistic Features (L)
S + J + L
65.69

UWA
57.61
58.47
59.48
59.39
UWA
59.22
59.58
64.03
65.08

Joy
66.67
63.41
66.67
59.35
Joy
69.37
68.75
70.00
71.88

Sad
38.70
45.16
43.55
58.06
Sad
76.31
76.31
63.16
68.42

Ang
38.82
38.82
47.06
44.71
Ang
22.22
22.22
55.56
55.56

Neu
76.58
76.17
70.88
64.56
Neu
68.97
71.03
67.39
64.48

Table 2: Weighted (WA) and Unweighted (UWA) accuracies(%) on Friends and EmotionPush validation
sets provided by the challenge authors. S: Supervised learning using CNN encoder trained on labeled

data only, J: Joint learning with reconstruction task using DCNN decoder, L: Linguistic features.

N(cid:88)

n=1

cross entropy loss deﬁned by

Lc = − 1
N

wn ∗ lossn

(2)

Table 1 provides a summary of the features con-
sidered. Ngrams and other semantic features are
ignored as they introduce domain-speciﬁc biases.
Word-embeddings are treated separately and con-
sidered as raw features to train a supervised model.

mechanism to learn shared representations during
the network training. We implement joint learn-
ing using simultaneous optimization for both se-
quence reconstruction (CNN-DCNN) and emotion
detection (linguistic features). The combined loss
function is given by,

L = αaeLae + (1 − αae)Lc.

(3)

Joint learning

3.5
The CNN-DCNN network learns the text informa-
tion i.e. sequences, the linguistic features learn
the emotional aspect. Joint learning introduces the

where αae is a balancing hyperparameter with 0 ≤
αae ≤ 1. Higher the value of αae, higher is the im-
portance given to the reconstruction loss Lae while
training and vice versa.

40Friends WA
S
64.90
69.54
S + J
S + L
62.78
S + J + L 65.83
EmPush WA
68.89
S
70.44
S + J
S + L
67.54
S + J + L 65.69

UWA
59.09
60.54
59.16
60.48
UWA
64.62
60.53
62.95
64.89

Joy
69.10
71.54
62.60
68.29
Joy
80.62
85.00
76.87
75.62

Sad
53.22
51.67
54.10
48.33
Sad
52.63
58.97
56.41
63.16

Ang
45.88
45.88
56.47
58.82
Ang
66.67
33.33
44.44
55.56

Neu
68.24
75.20
64.75
68.44
Neu
58.54
63.27
72.60
65.21

Table 3: Accuracy(%) for models trained on Friends + EmotionPush data, tested on individual

validation sets.

4 Experiments

In this section, we show the experimental evalua-
tion of our system on the EmotionLines dataset.

4.1 Experimental Setup
CNN encoder with MLP Classiﬁer: We use 300-
dimensional pre-trained glove word-embeddings
(Pennington et al., 2014) as input to the model.
The encoder contains two convolutional layers.
Size of the latent representation is set to 600. The
MLP classiﬁer contains one fully-connected layer
followed by a softmax layer.
Joint Training: We set αae = 0.5 as this gives
equal importance to both objectives and reports
best results.
Linguistic Features: We concatenate a full set of
68 linguistic features with the latent representation
for emotion detection.

Friends
Our Model
Highest
EmPush
Our Model
Highest

UWA Joy
71.1
62.5
62.5
71.1
UWA Joy
62.5
76.0
76.0
62.5

55.3
55.3

Sad Ang Neu
55.3
68.3
99.5
55.3
Sad Ang Neu
51.7
76.3
99.0
54.0

45.9
45.9

Table 4: Results on the EmotionX challenge test

sets for Friends and EmotionPush datasets.

Accuracy(%) rounded off to one decimal point.

4.2 Results
Table 2 shows the results for models trained on
individual training sets using our weighted loss
function. The performance is evaluated using
both, the weighted accuracy (WA) and the un-
weighted accuracy (UWA), as deﬁned by the chal-

lenge authors (Chen et al., 2018).

(cid:88)

c∈C

1
|C|

W A =

U W A =

pcac

(cid:88)

c∈C

ac

(4)

(5)

where ac denotes the accuracy of emotion class
c and pc denotes the percentage of utterances in
emotion class c.
Adding a reconstruction loss with classiﬁcation
loss improves performance. We attribute this
to improved generalizability provided by a semi-
supervised loss. Concatenating linguistic fea-
tures improves minority class accuracies for both
Friends TV dialogues and EmotionPush chats.
The improvements due to joint loss and linguistic
features are more signiﬁcant for EmotionPush chat
log dataset. Accuracies of majority class (Neutral)
take a considerable hit with the addition of J and L
for both datasets, whereas minority emotions like
Sadness and Anger consistently beneﬁt from addi-
tion of linguistic features.

Table 3 contains results for models trained on
both Friends and EmotionPush training data. In-
crease in training data, even though from a dif-
ferent domain, improves performance for Joy and
Anger emotions. Accuracy on sadness dips signif-
icantly for EmotionPush. Overall WA and UWA
also increase slightly for Friends dataset.

5 EmotionX Submission and Analysis
We implement an ensemble of the four model vari-
ants trained on the Friends + EmotionPush data as
our ﬁnal submission for the EmotionX challenge.
We arrive at the ﬁnal class predictions using the
algorithm explained in Algorithm 1. For each test

41end if

(cid:46) For each base model

candidates.add(arg max(p)))

if max(p) > threshold then

end for
return candidates

candidates ← []
for p ∈ p x do

ensemble pred ← []
for x ∈ data do

Algorithm 1 Ensemble Algorithm
1: procedure FILTER(p x, threshold)
2:
3:
4:
5:
6:
7:
8:
9: end procedure
10:
11: procedure ENSEMBLE(data, p sof tmax) (cid:46) p softmax.shape = (#test samples, #models, #classes)
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end procedure

ensemble pred.add(most common(candidate classes)))
candidate classes ← F ILT ER(p sof tmax[x], 0.00)
ensemble pred.add(most common(candidate classes))

ensemble pred.add(most common(candidates))
candidate classes ← F ILT ER(p sof tmax[x], 0.50)
if len(candidate classes) > 0 then

candidate classes ← F ILT ER(p sof tmax[x], 0.75)
if len(candidate classes) > 0 then

end for
return ensemble predictions

(cid:46) Moderate Conﬁdence

(cid:46) High Conﬁdence

(cid:46) Low Conﬁdence

end if

end if

else

else

sample, we ﬁnd models for which the maximum
output probability associated with a class is greater
than a threshold of 0.75 (High Conﬁdence). Pre-
dictions from this subset are considered as the can-
didate high conﬁdence classes. The most common
class in this subset is taken as the ﬁnal prediction
for EmotionX submission. If the subset is empty,
a similar approach is followed but with a reduced
threshold of 0.50 (Moderate Conﬁdence). Predic-
tions for samples which do not satisfy any of the
above thresholds are termed as Low Conﬁdence
Predictions.

The results on the test-set for both datasets are
shown in Table 4. Comparison with the best re-
sults in each class shows that for Friends dataset,
our model tops for all emotions except Neutral.
Whereas, for the EmotionPush dataset, we per-
form well on Joy and Anger. Our model had the
best unweighted accuracy (UWA) for both datasets
in the EmotionX challenge.

Text

Prediction

Label

Come on, Lydia, you can
do it.
Push!
Push ’em out, push ’em
out, harder, harder.
Push ’em out, push ’em
out, way out!
Let’s get that ball and re-
ally move, hey, hey, ho,
ho.
Let’s– I was just–yeah,
right.
Push!
Push!

Neutral

Neutral

Anger

Anger

Anger

Joy

Joy

Anger
Anger

Joy
Joy

Joy

Joy

Joy
Joy
Joy

Table 5: An example dialogue from Friends

dataset with corresponding predictions and labels.
5.1 Error Analysis
Our model does not explicitly import contextual
information from other utterances in the conversa-
tion. Therefore, quite expectedly, we found that

42most of the utterances misclassiﬁed by our model
occur in dialogues where the current utterance
does not exhibit the emotion it is tagged with.

Another set of errors occur where the whole
conversation is not able to explain the respective
emotions of each utterance. Table 5 shows an ex-
ample conversation where it might be difﬁcult for
even a human to classify the utterances without the
associated multi-modal cues.

6 Conclusion and Future Work

We propose a CNN-DCNN autoencoder based ap-
proach for emotion detection on EmotionLines
dataset. We show that addition of a semi-
supervised loss improves performance. We pro-
pose multiple linguistic features which are con-
catenated to the latent encoded representation for
classiﬁcation. The results show that our model de-
tects emotions successfully. The network, using a
weighted classiﬁcation loss function, tries to han-
dle the class imbalance in the dataset.

In future, we plan to include results of model-
ing emotion on the whole dialog using an LSTM
layer over our network. We would experiment
with concatenating subsets of linguistic features
to better estimate the contribution of each feature
group. We also plan to use data-augmentation
techniques such as backtranslation and word sub-
stitution using Wordnet and word-embeddings in
order to handle class-imbalance in the dataset.

References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In Proceed-
ings of the conference on human language technol-
ogy and empirical methods in natural language pro-
cessing. Association for Computational Linguistics,
pages 579–586.

Julian Brooke, Tong Wang, and Graeme Hirst. 2010.
Automatic acquisition of lexical formality. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters. Association for
Computational Linguistics, pages 90–98.

Michael Brooks, Katie Kuksenok, Megan K Torkild-
son, Daniel Perry, John J Robinson, Taylor J Scott,
Ona Anicello, Ariana Zukowski, Paul Harris, and
Cecilia R Aragon. 2013. Statistical affect detec-
In Proceedings of the
tion in collaborative chat.
2013 conference on Computer supported coopera-
tive work. ACM, pages 317–328.

Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Lun-Wei Ku, et al. 2018. Emotionlines: An emotion
corpus of multi-party conversations. arXiv preprint
arXiv:1802.08379 .

Niyati Chhaya, Kushal Chawla, Tanya Goyal, Projjal
Chanda, and Jaya Singh. 2018. Frustrated, polite
or formal: Quantifying feelings and tone in emails.
In Second Workshop on Computational Modeling of
Peoples Opinions, Personality, and Emotions in So-
cial Media, NAACL HLT.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness
with application to social factors. arXiv preprint
arXiv:1306.6078 .

Peter Sheridan Dodds, Kameron Decker Harris, Is-
abel M Kloumann, Catherine A Bliss, and Christo-
pher M Danforth. 2011. Temporal patterns of hap-
piness and information in a global social network:
Hedonometrics and twitter. PloS one 6(12):e26752.

Paul Ekman. 1992. An argument for basic emotions.

Cognition & Emotion 6(3-4):169–200.

Albert Mehrabian. 1980. Basic dimensions for a gen-
eral psychological theory implications for personal-
ity, social, environmental, and developmental stud-
ies .

Saif M Mohammad and Felipe Bravo-Marquez. 2017.
Wassa-2017 shared task on emotion intensity. arXiv
preprint arXiv:1708.03700 .

Saima Aman and Stan Szpakowicz. 2007.

Identify-
ing expressions of emotion in text. In International
Conference on Text, Speech and Dialogue. Springer,
pages 196–205.

Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
arXiv
of-the-art in sentiment analysis of tweets.
preprint arXiv:1308.6242 .

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016.
Enriching word vectors
with subword information. CoRR abs/1607.04606.
http://arxiv.org/abs/1607.04606.

Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of ﬁne-grained emotions from text.
In
ICWSM.

Johan Bollen, Huina Mao, and Alberto Pepe. 2011.
Modeling public mood and emotion: Twitter sen-
Icwsm
timent and socio-economic phenomena.
11:450–453.

Ellie Pavlick and Joel Tetreault. 2016. An empiri-
cal analysis of formality in online communication.
Transactions of the Association for Computational
Linguistics 4:61–74.

43James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates 71:2001.

J.W. Pennebaker. 2011. The Secret Life of Pronouns:
What Our Words Say About Us. Bloomsbury USA.
https://books.google.com/books?id=Avz4rthHySEC.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Rosalind W. Picard. 1997. Affective Computing. MIT

Press, Cambridge, MA, USA.

Daniel Preotiuc-Pietro, Ye Liu, Daniel J. Hopkins, and
Lyle Ungar. 2017. Personality Driven Differences in
Paraphrase Preference. In Proceedings of the Work-
shop on Natural Language Processing and Compu-
tational Social Science (NLP+CSS). ACL.

Alec Radford, Luke Metz, and Soumith Chintala.
2015. Unsupervised representation learning with
deep convolutional generative adversarial networks.
arXiv preprint arXiv:1511.06434 .

Sascha Rothe, Sebastian Ebert, and Hinrich Sch¨utze.
2016.
Ultradense word embeddings by or-
thogonal transformation. CoRR abs/1602.07572.
http://arxiv.org/abs/1602.07572.

H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one 8(9):e73791.

Jo˜ao Sedoc, Jean Gallier, Lyle H. Ungar, and Dean P.
Foster. 2016. Semantic word clusters using signed
CoRR abs/1601.05403.
normalized graph cuts.
http://arxiv.org/abs/1601.05403.

Martin EP Seligman. 2011. Flourish: a visionary new
understanding of happiness and well-being. Policy
27(3):60–1.

Ivan Vulic, Nikola Mrksic, Roi Reichart, Diarmuid ´O
S´eaghdha, Steve J. Young, and Anna Korhonen.
2017. Morph-ﬁtting: Fine-tuning word vector
spaces with simple language-speciﬁc rules. CoRR
abs/1706.00377. http://arxiv.org/abs/1706.00377.

Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 english lemmas. Behavior Re-
search Methods 45(4):1191–1207.

Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan,
Ricardo Henao, and Lawrence Carin. 2017. Decon-
volutional paragraph representation learning. CoRR
abs/1708.04729. http://arxiv.org/abs/1708.04729.

44
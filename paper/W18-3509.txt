EmotionX-Area66: Predicting Emotions in Dialogues using Hierarchical

Attention Network with Sequence Labeling

Rohit Saxena
TCS Research

Savita Bhat
TCS Research

Niranjan Pedanekar

TCS Research

rohit.saxena2@tcs.com

savita.bhat@tcs.com

n.pedanekar@tcs.com

Abstract

This paper presents our system submit-
ted to the EmotionX challenge.
It is an
emotion detection task on dialogues in the
EmotionLines dataset. We formulate this
as a hierarchical network where network
learns data representation at both utterance
level and dialogue level. Our model is in-
spired by Hierarchical Attention network
(HAN) and uses pre-trained word embed-
dings as features. We formulate emotion
detection in dialogues as a sequence la-
beling problem to capture the dependen-
cies among labels. We report the perfor-
mance accuracy for four emotions (anger,
joy, neutral and sadness). The model
achieved unweighted accuracy of 55.38%
on Friends test dataset and 56.73% on
EmotionPush test dataset. We report an
improvement of 22.51% in Friends dataset
and 36.04% in EmotionPush dataset over
baseline results.

Introduction

1
Emotion detection and classiﬁcation constitutes a
signiﬁcant part of research in the area of natural
language processing (NLP). The research aims to
detect presence of an emotion in a text snippet and
correctly categorize the same. The emotions are
typically classiﬁed using categories proposed by
(Ekman et al., 1987), namely anger, disgust, fear,
joy, sadness, surprise. Signiﬁcant amount of re-
search has been dedicated to emotion classiﬁca-
tion in variety of texts like news and news head-
lines (Strapparava and Mihalcea, 2008; Staiano
and Guerini, 2014), blogposts (Mishne, 2005), ﬁc-
tion (Mohammad, 2012b).

With the advent of social media and dialogue
systems like personal assistants and chatbots,

Speaker Utterance
Joey

Rachel

Joey

Monica

Rachel

Joey

Treeger

Whoa-whoa,
made you cry?
Yes! And he said really
mean things that were
only partly true.
I’m gonna go down there
and teach that guy a les-
son.
Joey, please don’t do
that. I think it’s best that
we just forget about it.
That’s easy for you to
say, you weren’t almost
just killed.
All right that’s it, school
is in session!

Emotion
surprise

sadness

anger

fear

anger

neutral

Table 1: Example of a dialogue from Friends
dataset

emotion analysis of short texts has garnered a lot
of attention. Short texts are deﬁned as small text
chunks in the form of tweets, messenger conver-
sations, social network posts, conversational dia-
logues etc. Unlike large documents, these texts
have unique set of characteristics such as infor-
mal language, incomplete sentences, use of emoti-
cons. Different approaches for emotion detection
in short texts are proposed in (Krcadinac et al.,
2013) for instant messages, (Mohammad, 2012a)
and (Wang et al., 2012) for Twitter and (Preotiuc-
Pietro et al., 2016) for status updates in Facebook.
Conversational short texts consist of dialogues
between two or more entities. A dialogue naturally
has a hierarchical structure, with words contribut-
ing to an utterance and a set of utterances con-
tributing to a dialogue (Kumar et al., 2017). Ta-
ble 1 shows an example of a dialogue which con-
sists of 6 utterances with corresponding speakers

ProceedingsoftheSixthInternationalWorkshoponNaturalLanguageProcessingforSocialMedia,pages50–55,Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics50Figure 1: An illustration of proposed Hierarchical Attention Network

and emotions. In these dialogues, context builds
as the dialogue progresses. There is a depen-
dency between consecutive utterances and hence
the classiﬁcation of such utterances can be treated
as a sequence labeling problem.
In particular,
(Stolke et al., 2000; Venkataraman et al., 2003)
and (Kim et al., 2010; Chen and Eugenio, 2013;
Kumar et al., 2017) have captured dependencies
in utterances for dialogue act classiﬁcation using
Hidden Markov Model (HMM) and Conditional
Random Field (CRF) respectively. Also, several
ways of incorporating such context information in
artiﬁcial neural networks have been proposed in
(Liu, 2017).

The EmotionX shared task consists of detecting
emotions for each utterance from EmotionLines
dataset. The dataset (Chen et al., 2018) contains
dialogues collected from Friends TV show scripts
and private Facebook messenger chats. Each of
the utterances has been annotated for one of the
eight emotions viz. six basic emotions proposed
by (Ekman et al., 1987) and two other emotions
viz. neutral, non-neutral. The shared task focuses
on detecting only four of these eight emotions,
namely joy, sadness, anger and neutral.
In this
paper, we present our approach to detect emotions
in utterances.
Inspired by (Kumar et al., 2017),
we use Hierarchical Attention Network (HAN) to
build context both at utterance and dialogue level.
We treat emotion detection at utterance level as a

sequence labeling problem and use a linear chain
CRF as a classiﬁer.

2 Proposed Model
The dataset for the task consists of dialogues, each
dialogue (Di) consists of sequence of utterances
denoted as Di = (u1, u2, . . . un), where n is the
number of utterances in a given dialogue. Each ut-
terance uj is associated with a target emotion label
yj ∈ Y. To build context within a dialogue, we
consider a moving context window Nk of length
k and combine all the utterances within the win-
dow with their target labels to create multiple sets
of context utterances. These sets of utterances are
given as input to our model.

The model consists of HAN (Yang et al., 2016),
where the ﬁrst part is a word-level encoder with
the attention layer, encoding each word in an ut-
terance. The second part is an utterance-level en-
coder, encoding each utterance in the dialogue.
The HAN is combined with a linear chain CRF
classiﬁcation layer for detecting emotions. The ut-
terance level emotion detection is treated as a se-
quence labeling problem based on the fact that the
emotion in an utterance depends on emotions of
previous utterances. An illustration of complete
model comprising of embedding layer, word level
encoder, attention layer , utterance level encoder
with ﬁnal layer of CRF classiﬁcation is depicted
in Figure 1.

513 Model Desscription
Embedding Layer: A context window Nk con-
sists of k utterances each having l number of
words. Each word wij in an utterance uj, where
j ∈ [1, k] , is embedded to a low-dimensional vec-
tor space Rd using an embedding layer (fembed)
of size d. It projects the word into representative
word vector xij. We initialize the weights of the
embedding layer with pre-trained GloVe embed-
dings1.

xij = fembed(wij)

Word-level Encoder: We use a bidirectional
Gated Recurrent Unit (GRU) (Cho et al., 2014) as
the word-level encoder in the hierarchical network
to summarize information from both directions for
words. The bidirectional GRU contains the for-
ward GRU which reads the utterance uj from w1j
to wlj and a backward GRU which reads from wlj
to w1j:

−→
h ij =

−−−→
GRU (xij), i ∈ [1, l]

←−
h ij =

←−−−
GRU (xij), i ∈ [l, 1]

−→
h ij and backward
←−
h ij are concatenated to obtain word

The forward hidden state
hidden state
encoded representation hij.

i.e.

Attention Layer: The intuition for using an
attention layer is that a few words in an utterance
are more important in identifying an emotion.
Moreover, the informativeness of words is context
dependent
same set of words contribute
differently in different context. We augment the
Word-level Encoder with a deep self-attention
mechanism (Bahdanau et al., 2014; Baziotis
et al., 2017) to obtain a more accurate estimation
of the importance of each word. The attention
mechanism assigns a weight αij to each word
representation. Formally:

rij = tanh(W hij + b)

(cid:80)l
(cid:88)

αij =

exp(rij)
i=1 exp(rij)

sj =

αijhij

1https://nlp.stanford.edu/projects/glove/

where sj is the utterance representation.

Similar

Utterance-Level Encoder:
to Word
Level Encoder, the set of utterance representations
sj is passed to a bidirectional GRU to obtain the
ﬁnal representation gj at utterance level. These
representations are passed to CRF classiﬁcation
layer.

Linear Chain CRF: Bidirectional
encoder
To
captures dependencies among utterances.
model the dependency among labels,
the ﬁnal
utterance representations are passed to the linear
chain CRF classiﬁer layer. CRFs are undirected
graphical models that predict the optimal label
sequence given an observed sequence. For a given
context window Nk, the probability of predicting
sequence of emotion labels for a set of utterance
representations g and corresponding emotion
label set y is

(cid:88)
(cid:88)

j

j

exp(

wjFj(g, y))

exp(

wjFj(g, y(cid:48)))

(cid:88)

y(cid:48)∈Y

P (y|g; w) =

where wj is the set of parameters corresponding
to CRF layer and Fj(g, y) is the feature function
(Maskey, Spring 2010).

4 Data Preparation
The dataset consists of two sets, viz.
1) di-
alogues collected from Friends TV show script
and 2) Facebook messenger private chats. Both
these datasets have characteristics of short texts.
We describe our preprocessing strategies for these
datasets below.

4.1 Pre-processing
EmotionPush: These are informal chats between
two individuals. This data has typical character-
istics of short texts.
It contains incomplete sen-
tences, informal language, use of emoticons, ex-
cessive use of punctuations like ’?’ and ’!’. As
a part of preprocessing, we convert all the emoti-
cons to appropriate emotion word. We also replace
all occurrences of date and time with named en-
tities ’DATE’ and ’TIME’. We convert all con-
tracted forms like ’can’t’,’haven’t’ to appropri-
ate expanded forms like ’can not’ and ’have
not’. The dataset contains named entities such
as ’PERSON 354’, ’ORGANIZATION 78’ and

52’LOCATION 8’. These entities are important to
build the context but they do not appear in word
embeddings. We convert all these named enti-
ties to pseudo entities which are present in word
embeddings but not present in the EmotionPush
dataset vocabulary.

Accuracy
(%)
Unweighted
neutral
anger
joy
sadness

EmotionPush

Friends

56.73
88.2
21.6
63.1
54

55.38
73.5
39.8
57.6
50.6

Table 2: Final results on Test Sets.

Emotion Precision

anger
joy
neutral
sadness

(%)
31
59
82
30

Recall
(%)
44
64
85
61

F1
(%)
36
61
84
40

Accuracy
(%)
44
64
85
61

Table 3: Experimental results on EmotionPush
Development Set.

Emotion Precision

anger
joy
neutral
sadness

(%)
36
47
67
25

Recall
(%)
34
67
78
47

F1
(%)
35
55
72
33

Accuracy
(%)
34
67
78
47

Table 4: Experimental results on Friends Devel-
opment Set.

Friends - TV Show scripts: This dataset contains
scene snippets having interaction between two or
more speakers. Some of the utterances are incom-
plete and some have excessive use of punctuations.
Unlike EmotionPush dataset, there are no emoti-
cons and tagged named entities in this data. We
convert the contracted forms as mentioned above
and remove extra punctuations.
In this dataset,
speaker and words uttered by the speaker play an
important role in building the context. To incor-
porate this, we concatenate speaker information to
every utterance.

5 Experiments and Results
The EmotionX challenge consists of detecting
emotions for each utterance from EmotionLines
dataset. Each of the utterances has been anno-
tated for one of the eight emotions, anger, sad-
ness, joy, fear, disgust, surprise, neutral and non-
neutral. Even though the shared task consists of
joy, sad-
detection of only four emotions, viz.
ness, anger and neutral, we consider all emotions
in our model. We train the model separately for
each dataset. We use pre-trained 100-dimensional
GloVe-Tweet embedding for both datasets. These
embeddings are used to initialize weights of the
embedding layer.

We also consider word priors as features. Word

prior for a word is computed as

p(wi|cj) =

count(wi, cj)

count(cj)

where count(wi, cj) is frequency of word wi in
class cj and count(cj) is total number of words in
class cj. We determine word priors for every word
for all 8 emotion classes and concatenate these 8
features to embedding feature vectors.

The hyper-parameters such as window length
for context window, learning rate, optimizer, early
stopping and dropout were tuned for performance
during experimentation.

Results on both EmotionPush and Friends test
sets are listed in Table 2. We also report model
performance on both the development datasets in
Table 3 and Table 4. The model achieved improve-
ment of 22.51% in Friends dataset and 36.04% in
EmotionPush dataset over baseline (Chen et al.,
2018) results. We report overall unweighted accu-
racy of 56.73% on EmotionPush test dataset and
accuracy of 55.38% on Friends test dataset.

6 Discussion
To understand how the context builds over the dia-
logues, we performed exploratory analysis on both
the datasets. In Friends dataset, we found some
anomalies which can impact the performance of
our system.
1. A few dialogues consist of utterances from dif-
ferent scenes which breaks the continuity of the
dialogue.
2. Some utterances have scene descriptions as
part of the utterance. For example,
in record
{”speaker”: ”Joey”, ”utterance”: ”and Phoebe
picks up a wooden baseball bat and starts to

53swing as Chandler and Monica enter.)”, ”emo-
tion”: ”non-neutral”}, utterance is a scene descrip-
tion and not spoken by any speaker.
3. We also found few utterances having no words
but only a punctuation (’.’ or ’!’) which is attached
with an emotion. For example,
a) {”speaker”: ”Rachel”, ”utterance”: ”!”,
”emotion”: ”non-neutral”}
b) {”speaker”: ”Phoebe”, ”utterance”: ”.”,
”emotion”: ”non-neutral”}

We did not ﬁnd such anomalies in EmotionPush

dataset.

The word embeddings do not have explicit emo-
tion information for words. To incorporate this,
we added word priors per class to word vectors
and examined their effect on the performance of
our model. Word priors improve the model perfor-
mance by 17% in EmotionPush dataset and 19% in
Friends dataset. For example, utterances like ”Lol
weird” and ”I also have no shoes lol” belonging to
emotion class ’joy’ were misclassiﬁed without us-
ing word priors as features. Similarly, utterances
such as ”Sorry he cannot” and ”Sorry about that
person 107” belonging to emotion class ’sadness’
were also misclassiﬁed.

7 Conclusion
In this paper, we present our submission for Emo-
tionX emotion detection challenge. We use Hier-
archical Attention Network (HAN) model to learn
data representation at both utterance level and di-
alogue level. Additionally, we formalize the prob-
lem as sequence labeling task and use a linear
chain Conditional Random Field (CRF) as a clas-
siﬁcation layer to classify the dialogues in both
Friends and EmotionPush dataset. The model
achieved improvement of 22.51% in Friends
dataset and 36.04% in EmotionPush dataset over
baseline results. In future, we would like to ex-
plore the speaker-listener relation with emotion
and lexical features to improve the performance of
the system.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

In Proceedings of
topic-based sentiment analysis.
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754.

Lin Chen and Barbara Di Eugenio. 2013. Multimodal-
ity and dialogue act classiﬁcation in the robohelper
project. In Proceedings of the SIGDIAL.

Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Ting-Hao (Kenneth) Huang, and Lun-Wei Ku. 2018.
Emotionlines: An emotion corpus of multi-party
conversations. In Proceedings of the 11th edition of
the Language Resources and Evaluation Conference
(LREC 2018).

Kyunghyun Cho, Bart Van Merrinboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Paul Ekman, Wallace V. Friesen,

and Mau-
Universals and
reen O’Sullivan et al. 1987.
cultural differences in the judgments of
facial
expressions of emotion. Journal of Personality and
Social Psychology, 53(4):712–717.

Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing.

Uros Krcadinac, Philippe Pasquier, Jelena Jovanovic,
and Vladan Devedzic. 2013. Synesketch: An open
source library for sentence-based emotion recogni-
IEEE Transactions on Affective Computing,
tion.
4(3):312–325.

Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta,
Sachindra Joshi, and Arun Kumar. 2017. Dialogue
act sequence labeling using hierarchical encoder
arXiv preprint, arXiv:1709.04250:712–
with crf.
717.

Yang Liu. 2017. Using context information for dia-
log act classiﬁcation in dnn framework. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2170–2178.

Sameer Maskey. Spring 2010. Statistical methods for
natural language processing. Course Slides- Week
13 - Language Models, Graphical Models.

Gilad Mishne. 2005. Experiments with mood classiﬁ-
cation in blog posts. In Proceedings of ACM SIGIR
2005 Workshop on Stylistic Analysis of Text for In-
formation Access, pages 321–327.

Saif M Mohammad. 2012a. #emotional tweets. In Pro-
ceedings of the 1st Joint Conference on Lexical and
Computational Semantics (*SEM), pages 246–255.

Christos Baziotis, Nikos Pelekis, and Christos Doulk-
eridis. 2017. Datastories at semeval-2017 task
4: Deep lstm with attention for message-level and

Saif M Mohammad. 2012b. From once upon a time
to happily ever after: Tracking emotions in mail and
books. Decision Support Systems, 53(4):730–741.

54Daniel Preotiuc-Pietro, H Andrew Schwartz, Gregory
Park, Johannes C Eichstaedt, Margaret Kern, Lyle
Ungar, and Elizabeth P Shulman. 2016. Modelling
valence and arousal in facebook posts. In Proceed-
ings of Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis.

Jacopo Staiano and Marco Guerini. 2014.

De-
pechemood: A lexicon for emotion analysis from
crowdannotated news. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 427–433.

Andreas Stolke, Klaus Ries, and Noah Coccaro. 2000.
Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computation
Linguistics, pages 339–373.

Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM symposium on Applied Computing, pages
1556–1560.

Anand Venkataraman, Lucianna Ferrer Andreas Stol-
Training a
cke, and Elizabeth Shriberg. 2003.
prosody based dialog act tagger from unlabeled data.
In Proceedings of the 2003 IEEE International Con-
ference on Acoustics, Speech, and Signal Process-
ing.

Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P Sheth. 2012. Harnessing twitter big data
In Proceed-
for automatic emotion identiﬁcation.
ings of the 2012 International Conference on Pri-
vacy, Security, Risk and Trust (PASSAT) and 2012
International Confernece on Social Computing (So-
cialCom), pages 587–592.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

55
UKP-Athene: Multi-Sentence Textual Entailment for Claim Veriﬁcation

Andreas Hanselowski†∗, Hao Zhang∗, Zile Li∗, Daniil Sorokin†∗,

Benjamin Schiller∗, Claudia Schulz†∗, Iryna Gurevych†∗

†Research Training Group AIPHES

Computer Science Department, Technische Universit¨at Darmstadt

https://www.aiphes.tu-darmstadt.de

∗Ubiquitous Knowledge Processing Lab (UKP-TUDA)

Computer Science Department, Technische Universit¨at Darmstadt

https://www.ukp.tu-darmstadt.de/

Abstract

The Fact Extraction
and VERiﬁcation
(FEVER) shared task was launched to sup-
port
the development of systems able to
verify claims by extracting supporting or
refuting facts from raw text.
The shared
task organizers provide a large-scale dataset
for the consecutive steps involved in claim
veriﬁcation, in particular, document retrieval,
fact extraction, and claim classiﬁcation.
In
this paper, we present our claim veriﬁcation
pipeline approach, which, according to the
preliminary results, scored third in the shared
task, out of 23 competing systems. For the
document retrieval, we implemented a new
entity linking approach.
In order to be able
to rank candidate facts and classify a claim
on the basis of several selected facts, we
introduce two extensions to the Enhanced
LSTM (ESIM).

Introduction

1
In the past years, the amount of false or misleading
content on the Internet has signiﬁcantly increased.
As a result, information evaluation in terms of
fact-checking has become increasingly important
as it allows to verify controversial claims stated
on the web. However, due to the large number of
fake news and hyperpartisan articles published on-
line every day, manual fact-checking is no longer
feasible. Thus, researchers as well as corporations
are exploring different techniques to automate the
fact-checking process1.

In order to advance research in this direction,
the Fact Extraction and VERiﬁcation (FEVER)
shared task2 was launched. The organizers of

1https://fullfact.org/media/uploads/

full_fact-the_state_of_automated_
factchecking_aug_2016.pdf

2http://fever.ai/task.html

the FEVER shared task constructed a large-scale
dataset (Thorne et al., 2018) based on Wikipedia.
This dataset contains 185,445 claims, each of
which comes with several evidence sets. An ev-
idence set consists of facts, i.e. sentences from
Wikipedia articles that jointly support or contra-
dict the claim. On the basis of (any one of) its
evidence sets, each claim is labeled as Supported,
Refuted, or NotEnoughInfo if no decision about
the veracity of the claim can be made. Supported
by the structure of the dataset, the FEVER shared
task encompasses three sub-tasks that need to be
solved.
Document retrieval: Given a claim, ﬁnd (En-
glish) Wikipedia articles containing information
about this claim.
Sentence selection: From the retrieved articles,
extract facts in the form of sentences that are rele-
vant for the veriﬁcation of the claim.
Recognizing textual entailment: On the basis of
the collected sentences (facts), predict one of three
labels for the claim: Supported, Refuted, or NotE-
noughInfo. To evaluate the performance of the
competing systems, an evaluation metric was de-
vised by the FEVER organizers: a claim is con-
sidered as correctly veriﬁed if, in addition to pre-
dicting the correct label, a correct evidence set was
retrieved.
In this paper, we describe the pipeline system
that we developed to address the FEVER task.
For document retrieval, we implemented an en-
tity linking approach based on constituency pars-
ing and handcrafted rules. For sentence selection,
we developed a sentence ranking model based on
the Enhanced Sequential Inference Model (ESIM)
(Chen et al., 2016). We furthermore extended the
ESIM for recognizing textual entailment between
multiple input sentences and the claim using an at-
tention mechanism.
According to the preliminary results of

the

ProceedingsoftheFirstWorkshoponFactExtractionandVERiﬁcation(FEVER),pages103–108Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics103FEVER shared task, our systems came third out
of 23 competing teams.

2 Background

In this section, we present underlying methods that
we adopted for the development of our system.

2.1 Entity linking
The document retrieval step requires matching a
given claim with the content of a Wikipedia arti-
cle. A claim frequently features one or multiple
entities that form the main content of the claim.

Furthermore, Wikipedia can be viewed as a
knowledge base, where each article describes a
particular entity, denoted by the article title. Thus,
the document retrieval step can be framed as an
entity linking problem (Cucerzan, 2007). That is,
identifying entity mentions in the claim and link-
ing them to the Wikipedia articles of this entity.
The linked Wikipedia articles can then be used as
the set of the retrieved documents for the subse-
quent steps.

2.2 Enhanced Sequential Inference Model
Originally developed for the SNLI task (Bow-
man et al., 2015) of determining entailment be-
tween two statements, the ESIM (Enhanced Se-
quential Inference Model) (Chen et al., 2016) cre-
ates a rich representation of statement-pairs. Since
the FEVER task requires the handling of claim-
sentence pairs, we use the ESIM as the basis
for both sentence selection and textual entailment.
The ESIM solves the entailment problem in three
consecutive steps, taking two statements as input.
Input encoding: Using a bidirectional LSTM
(BiLSTM) (Graves and Schmidhuber, 2005), rep-
resentations of the individual tokens of the two in-
put statements are computed.
Local inference modeling: Each token of one
statement is used to compute attention weights
with respect to each token in the other statement,
giving rise to an attention weight matrix. Then,
each token representation is multiplied by all of
its attention weights and weighted pooling is ap-
plied to compute a single representation for each
token. This operation gives rise to two new repre-
sentations of the two statements.
Inference composition: These two statement
representations are then fed into two BiLSTMs,
which again compute sequences of representations
for each statement. Maximum and average pool-

ing is applied to the two sequences to derive two
representations, which are then concatenated (last
hidden state of the ESIM) and fed into a multilayer
perceptron for the classiﬁcation of the entailment
relation.

3 Our system for fact extraction and

claim veriﬁcation

In this section, we describe the models that we de-
veloped for the three FEVER sub-tasks.

3.1 Document retrieval
As explained in Section 2.1, we propose an entity
linking approach to the document retrieval sub-
task. That is, we ﬁnd entities in the claims that
match the titles of Wikipedia articles (documents).
Following the typical entity linking pipeline, we
develop a document retrieval component that has
three main steps.
Mention extraction: Named entity recognition
tools focus only on the main types of entities (Lo-
cation, Organization, Person). In order to ﬁnd en-
tities of different categories, such as movie titles,
that are numerous in the shared task data set, we
employ the constituency parser from AllenNLP
(Gardner et al., 2017). After parsing the claim, we
consider every noun phrase as a potential entity
mention. However, a movie or a song title may be
an adjective or any other type of syntactic phrase.
To account for such cases, we use a heuristic that
adds all words in the claim before the main verb
as well as the whole claim itself as potential en-
tity mentions. For example, a claim “Down With
Love is a 2003 comedy ﬁlm.” contains the noun
phrases ‘a 2003 comedy ﬁlm’ and ‘Love’. Neither
of the noun phrases constitutes an entity mention,
but the tokens before the main verb, ‘Down With
Love’, form an entity.
Candidate article search: We use the MediaWiki
API3 to search through the titles of all Wikipedia
articles for matches with the potential entity men-
tions found in the claim. The MediaWiki API uses
the Wikipedia search engine to ﬁnd matching ar-
ticles. The top match is the article whose title
has the largest overlap with the query. For each
entity mention, we store the seven highest-ranked
Wikipedia article matches.

The MediaWiki API uses the online version of
Wikipedia and since there are some discrepancies

3https://www.mediawiki.org/wiki/API:

Main_page

104nected to a single neuron for the prediction of the
ranking score. As a result, we are able to rank all
sentences of the retrieved documents according to
the computed ranking scores.
In order to ﬁnd a
potential evidence set, we select the ﬁve highest-
ranked sentences.
Training: Our adaptation of the ESIM is illus-
trated in Fig. 1. In the training mode, the ESIM
takes as input a claim and the concatenated sen-
tences of an evidence set. As a loss function, we
use a modiﬁed hinge loss with negative sampling:

(cid:80) max(0, 1 + sn − sp), where sp indicates the

positive ranking score and sn the negative rank-
ing score for a given claim-sentence pair. To get
sp, we feed the network a claim and the concate-
nated sentences of one of its ground truth evidence
sets. To get sn, we take all Wikipedia articles
from which the ground truth evidence sets of the
claim originate, randomly sample ﬁve sentences
(not including the sentences of the ground truth ev-
idence sets for the claim), and feed the concatena-
tion of these sentences into the same ESIM. With
our modiﬁed hinge loss function, we then try to
maximize the margin between positive and nega-
tive samples.
Testing: At testing time, we calculate the score
between a claim and each sentence in the retrieved
documents. For this purpose, we deploy an en-
semble of ten models with different random seeds.
Then, the mean score of a claim-sentence pair over
all ten models of the ensemble is calculated and
the scores for all pairs are ranked. Finally, the ﬁve
sentences of the ﬁve highest-ranked pairs are taken
as an output of the model.

3.3 Recognizing textual entailment

In order to classify the claim as Supported,
Ref uted or N otEnoughInf o, we use the ﬁve
sentences retrieved by our sentence selection
model described in the previous section. For the
classiﬁcation, we propose another extension to the
ESIM, which can predict the entailment relation
between multiple input sentences and the claim.
Fig. 2 gives an overview of our extended ESIM
for the FEVER textual entailment task.

As word representation for both claim and sen-
tences, we concatenate the Glove (Pennington
et al., 2014) and FastText (Bojanowski et al., 2016)
embeddings for each word. Since both types of
embeddings are pretrained on Wikipedia, they are
particularly suitable for our problem setting.

Figure 1: Sentence selection model

between the 2017 dump used in the shared task
and the latest version, we also perform an exact
search over all Wikipedia article titles in the dump.
We add these results to the set of the retrieved ar-
ticles.
Candidate ﬁltering: The MediaWiki API re-
trieves articles whose title overlaps with the query.
Thus, the results may contain articles with a ti-
tle longer or shorter than the entity mention used
in the query. Similarly to previous work on en-
tity linking (Sorokin and Gurevych, 2018), we re-
move results that are longer than the entity men-
tion and do not overlap with the rest of the claim.
To check this overlap, we ﬁrst remove the con-
tent in parentheses from the Wikipedia article ti-
tles and stem the remaining words in the titles and
the claim. Then, we discard a Wikipedia article if
its stemmed article title is not completely included
in the stemmed claim.

We collect all retrieved Wikipedia articles for
all identiﬁed entity mentions in the claim after
ﬁltering and supply them to the next step in the
pipeline. The evaluation of the document retrieval
system on the development data shows the effec-
tiveness of our ad-hoc entity linking approach (see
Section 4).

3.2 Sentence selection
In this step, we select candidate sentences as a po-
tential evidence set for a claim from the Wikipedia
articles retrieved in the previous step. This is
achieved by extending the ESIM to generate a
ranking score on the basis of two input statements,
instead of predicting the entailment relation be-
tween these two statements.
Architecture: The modiﬁed ESIM takes as input
a claim and a sentence. To generate the ranking
score, the last hidden state of the ESIM (see Sec-
tion 2.2) is fed into a hidden layer which is con-

105claimevidencesentencesclaimsampledsentencesInputEncodingLocalInferenceCompositionInputEncodingLocalInferenceCompositionspsnPmax(0,1+sn−sp)InputESIMtanh+DropoutRankingscoreHingeLoss4 Results

Table 1 shows the performance of our document
retrieval and sentence selection system when re-
trieving different numbers of the highest-ranked
Wikipedia articles.
In contrast to the results re-
ported in Table 3, here we consider a single model
instead of an ensemble. The results show that both
systems beneﬁt from a larger number of retrieved
articles.

#search results
3
5
7

doc. accuracy
92.60
93.30
93.55

sent. recall

85.37
86.02
86.24

Table 1: Performance of the retrieval systems using dif-
ferent numbers of MediaWiki search results

For the subtask of recognizing textual entail-
ment, we also experiment with different numbers
of selected sentences. The results in Table 2
demonstrate that our model performs best with all
ﬁve selected sentences.

#sentence(s)
1
2
3
4
5

label accuracy FEVER score
67.94
68.33
67.82
67.61
68.49

63.64
64.30
63.72
63.59
64.74

Table 2: Performance of the textual entailment model
using different numbers of sentences

In Table 3, we compare the performance of our
three systems as well as the full pipeline to the
baseline systems and pipeline implemented by the
shared task organizers (Thorne et al., 2018) on the
development set. As the results demonstrate, we
were able to signiﬁcantly improve upon the base-
line on each sub-task. The performance gains over
the whole pipeline add up to an improvement of
about 100% with respect to the baseline pipeline.

5 Error analysis

In this section, we present the error analysis for
each of the three sub-tasks, which can serve as a
basis for further improvements of the system.

5.1 Document retrieval
The typical errors encountered for the document
retrieval system can be divided into three classes.

Figure 2: Extended ESIM for recognizing textual en-
tailment

To process the ﬁve input sentences using the
ESIM, we combine the claim with each sentence
and feed the resulting pairs into the ESIM. The last
hidden states of the ﬁve individual claim-sentence
runs of the ESIM are compressed into one vector
using attention and pooling operations.
The attention is based on a representation of the
claim that is independent of the ﬁve sentences.
This representation is obtained by summing up the
input encodings of the claim in the ﬁve ESIM runs.
In the same way, we derive ﬁve sentence repre-
sentations, one from each of the ﬁve runs of the
ESIM, which are independent of the claim. For
each claim-sentence pair, the single sentence rep-
resentation and the claim representation are then
individually fed through a single layer perceptron.
The cosine similarity of these two vectors is then
used as an attention weight. The ﬁve output vec-
tors of all ESIMs are multiplied with their respec-
tive attention weights and we apply average and
max pooling on these vectors in order to reduce
them to two representations. Finally, the two rep-
resentations are concatenated and fed through a 3-
layer perceptron to predict one of the three classes
Supported, Refuted or NotEnoughInfo. The idea
behind the described attention mechanism is to al-
low the model to extract information from the ﬁve
sentences that is most relevant for the classiﬁca-
tion of the claim.

106S1InputEncodingLocalInferenceComposition·S5InputEncodingLocalInferenceComposition·S2S3S4.........claimC⊕maxavgInputESIMOutputofESIMAttendedVectorsPoolingAttentionClassiﬁerscore (%)

claim, thus, it is ranked very low by our model.

Task (metric)
Document retrieval
(accuracy)
Sentence selection
(recall)
Textual entailment
(label accuracy)
Full pipeline
(FEVER score)

system
baseline
our system
baseline
our system
baseline
our system
baseline
our system

70.20
93.55
44.22
87.10
52.09
68.49
32.27
64.74

Table 3: Performance comparison of our system and
the baseline system on the development set

Spelling errors: A word in the claim or in the
article title is misspelled. E.g. “Homer Hickman
wrote some historical ﬁction novels.” vs. “Homer
Hickam”. In this case, our document retrieval sys-
tem discards the article during the ﬁltering phase.
Missing entity mentions: The entity mention rep-
resented by the title of the article, which needs to
be retrieved, is not related to any entity mention
in the claim. E.g. Article title: “Blue Jasmine”
Claim: “Cate Blanchett ignored the offer to act in
Cate Blanchett.”.
Search failures: Some article titles contain a cat-
egory name in parentheses for the disambiguation
of the entity. This makes it difﬁcult to retrieve the
exact article title using the MediaWiki API. E.g.
the claim “Alex Jones is apolitical.” requires the
article “Alex Jones (radio host)”, but it is not con-
tained in the MediaWiki search results.

5.2 Sentence selection
The most frequent case, in which the sentence se-
lection model fails to retrieve a correct evidence
set, is that the entity mention in the claim does not
occur in the annotated evidence set. E.g. the only
evidence set for the claim “Daggering is nontradi-
tional.” consists of the single sentence “This dance
is not a traditional dance.”. Here, “this dance”
refers to “daggering” and cannot be resolved by
our model, since the information that “daggering”
is a dance is not mentioned in the evidence sen-
tence or in the claim. Some evidence sets contain
two sentences one of which is less related to the
the claim “Herry II of France has
claim. E.g.
three cars.” has an evidence set that contains the
two sentences “Henry II died in 1559.” and “1886
is regarded as the birth year of the modern car.”.
The second sentence is not directly related to the

5.3 Recognizing textual entailment
A large number of claims are misclassiﬁed due to
the model’s disability to interpret numerical val-
ues. For instance, the claim “The heart beats at a
resting rate close to 22 beats per minute.” is not
classiﬁed as refuted on the basis of the evidence
sentence “The heart beats at a resting rate close
to 72 beats per minute.”. The only information re-
futing the claim is the number, but neither GloVe
nor FastText embeddings can embed numbers dis-
tinctly enough. Another problem are challeng-
ing NotEnoughInfo cases. For instance, the claim
“Terry Crews played on the Los Angeles Charg-
ers.” (annotated as NotEnoughInfo) is classiﬁed
as refuted, given the sentence “In football, Crews
played ...
for the Los Angeles Rams, San Diego
Chargers and Washington Redskins, ...”. The sen-
tence is related to the claim but does not exclude
it, which makes this case difﬁcult.

6 Conclusion

In this paper, we presented the system for fact ex-
traction and veriﬁcation, which we developed in
the course of the FEVER shared task. According
to the preliminary results, our system scored third
out of 23 competing teams. The shared task was
divided into three parts: (i) Given a claim, retrieve
Wikipedia documents that contain facts about the
claim. (ii) Extract these facts from the document.
(iii) Verify the claim on the basis of the extracted
facts. To address the problem, we developed mod-
els for the three sub-tasks. We framed document
retrieval as entity linking by identifying entities in
the claim and linking them to Wikipedia articles.
To extract facts in the articles, we developed a sen-
tence ranking model by extending the ESIM. For
claim veriﬁcation we proposed another extension
to the ESIM, whereby we were able to classify the
claim on the basis of multiple facts using attention.
Each of our three models, as well as the combined
pipeline, signiﬁcantly outperforms the baseline on
the development set.

7 Acknowledgements

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group Adaptive Preparation of Information from
Heterogeneous Sources (AIPHES) at the Technis-
che Universit¨at Darmstadt grant No. GRK 1994/1.

107References
Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
arXiv preprint
tors with subword information.
arXiv:1607.04606.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2016. Enhanced LSTM
arXiv preprint
for natural
arXiv:1609.06038.

language inference.

Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
In Pro-
ambiguation Based on Wikipedia Data.
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708–716, Prague, Czech Republic.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A deep semantic natural language
processing platform.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Daniil Sorokin and Iryna Gurevych. 2018. Mixing
Context Granularities for Improved Entity Linking
on Question Answering Data across Entity Cate-
gories. In Proceedings of the 7th Joint Conference
on Lexical and Computational Semantics (*SEM
2018), pages 65–75. Association for Computational
Linguistics.

James

Thorne,

Andreas Vlachos,

Christos
Christodoulopoulos,
and Arpit Mittal. 2018.
FEVER: A large-scale dataset for fact extraction
and veriﬁcation. In NAACL-HLT.

108
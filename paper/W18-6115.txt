Detecting Code-Switching between Turkish-English Language Pair

Zeynep Yirmibes¸o˘glu

Dep. of Computer Engineering
Istanbul Technical University

Maslak, Istanbul 34369

G¨uls¸en Eryi˘git

Dep. of Computer Engineering
Istanbul Technical University

Maslak, Istanbul 34369

yirmibesogluz@itu.edu.tr

gulsen.cebiroglu@itu.edu.tr

Abstract

Code-switching (usage of different languages
within a single conversation context in an al-
ternative manner) is a highly increasing phe-
nomenon in social media and colloquial us-
age which poses different challenges for natu-
ral language processing. This paper introduces
the ﬁrst study for the detection of Turkish-
English code-switching and also a small test
data collected from social media in order to
smooth the way for further studies. The pro-
posed system using character level n-grams
and conditional random ﬁelds (CRFs) obtains
95.6% micro-averaged F1-score on the intro-
duced test data set.
Introduction

1
Code-switching is a common linguistic phe-
nomenon generally attributed to bilingual commu-
nities but also highly observed among white collar
employees. It is also treated as related to higher
education in some regions of the world (e.g. due to
foreign language usage at higher education). Al-
though the social motivation of code-switching us-
age has been still under investigation and there ex-
ist different reactions to it (Hughes et al., 2006;
Myers-Scotton, 1995), the challenges caused by
its increasing usage in social media are not neg-
ligible for natural language processing studies fo-
cusing on this domain.

Social media usage has increased tremendously,
bringing with it several problems. Analysis and in-
formation retrieval from social media sources are
difﬁcult, due to usage of a noncanonical language
(Han and Baldwin, 2011; Melero et al., 2015). The
noisy character of social media texts often require
text normalization, in order to prepare social me-
dia texts for data analysis. Eryi˘git and Toruno˘glu-
Selamet (2017) is the ﬁrst study which introduces
a social media text normalization approach for
Turkish. In this study, similar to Han and Baldwin

(2011) their candidate word (solution) generation
stage comes after an initial ill-formed word detec-
tion stage where they use a Turkish morphologi-
cal analyzer as the language validator. Although
this approach works quite well for Turkish posts,
it is obvious that it would encounter difﬁculties in
case of code-switching where the language valida-
tor would detect every foreign word as ill-formed
and the normalizer would try to propose a candi-
date correction for each of these. A similar situ-
ation may be observed at the behavior of spelling
checkers within text editors. These also detect the
foreign words (purposely written) as out of vocab-
ulary and insist on proposing a candidate correc-
tion which makes the detection of actual spelling
errors difﬁcult for the users.

In recent years, the use of code-switching be-
tween Turkish and English has also become very
frequent speciﬁcally in daily life conversations
and social media posts of white collars and youth
population. Ex. (1) introduces such an example
which is not unusual to see.
(1) (cid:28)Original code-switched version(cid:29)

Serverlarımızın update is¸lemleri ic¸in bu do-
maindeki expert arayıs¸ımız devam etmektedir.
(cid:28)Turkish version and literal translation(cid:29)
Sunucularımızın (of our servers) g¨uncelleme
(update) is¸lemleri (process) ic¸in (for), bu (this)
alandaki (on domain) uzman (expert) arayıs¸ımız
(search) devam etmektedir (continues).

(cid:28)English version(cid:29)
For the update processes of our servers, we keep

on searching an expert on this domain.

To the best of our knowledge, this is the ﬁrst
study working on automatic detection of code-
switching between Turkish and English. We in-
troduce a small test data set composed of 391 so-
cial media posts each consisting of code-switched

Proceedingsofthe2018EMNLPWorkshopW-NUT:The4thWorkshoponNoisyUser-generatedText,pages110–115Brussels,Belgium,Nov1,2018.c(cid:13)2018AssociationforComputationalLinguistics110sentences and their word-by-word manual annota-
tion stating either the word is Turkish or English.
The paper presents our ﬁrst results on this data set
which is quite promising with a 95.6% micro av-
erage F1-score. Our proposed system uses condi-
tional random ﬁelds using character n-grams and
word look-up features from monolingual corpora.

2 Related Work

Code-switching is a spoken and written phe-
nomenon. Hence, its investigation by linguists had
started long before the Internet era, dating to 1950s
(Solorio et al., 2014). However, code-switching
researches concerning Natural Language Process-
ing has started more recently, with the work of
Joshi (1982), where a “formal model for intra-
sentential code-switching” is introduced.

Analysis of code-switched data requires an an-
notated, multilingual corpus. Although collec-
tion of code-switched social media data is not an
easy task, there has been worthy contributions.
A Turkish-Dutch corpus (Nguyen and Do˘gru¨oz,
2013), a Bengali-English-Hindi corpus (Barman
et al., 2014), Modern Standard Arabic - Dialec-
tal Arabic, Mandarin - English, Nepali-English,
and Spanish-English corpora for the First and
Second Workshops on Computational Approaches
to Code-switching (Solorio et al., 2014; Molina
et al., 2016), a Turkish-German corpus ( ¨Ozlem
C¸ etino˘glu, 2016), a Swahili-English corpus (Pier-
gallini et al., 2016) and an Arabic-Moroccan Dar-
ija corpus (Samih and Maier, 2016) were intro-
duced. Social media sources are preferred, due
to the fact that social media users are not aware
that their data are being analyzed and thus gener-
ate text in a more natural manner (C¸ etino˘glu et al.,
2016). To our knowledge, a Turkish-English code-
switching social media corpus has not yet been in-
troduced.

Word-level

language identiﬁcation of code-
switched data has proved to be a popular re-
search area with the ascent of social media. Das
and Gamb¨ack (2013) applied language detection
to Facebook messages in mixed English-Bengali
and English-Hindi. Chittaranjan et al. (2014) car-
ried out the task of language detection for code-
switching feeding character n-grams to CRF, with
addition to lexical, contextual and other special
character features, and reached 95% labeling ac-
curacy. Nguyen and Do˘gru¨oz (2013) identiﬁed
Dutch-Turkish words using character n-grams and

dictionary lookup as CRF features along with con-
textual features, reaching 98% accuracy, and intro-
ducing new methods to measure corpus complex-
ity. These researches mostly depend on monolin-
gual training data (Solorio et al., 2014). As op-
posed to monolingual training data, Lignos and
Marcus (2013) used code-switched data for lan-
guage modeling where they use a Spanish data
set containing 11% of English code-switched data.
However,
the usage of code-switched data for
training is problematic, since its size is generally
low, and may be insufﬁcient for training (Mahar-
jan et al., 2015).

Shared tasks on code-switching (Solorio et al.,
2014; Molina et al., 2016) contributed greatly to
the research area. First Workshop on Computa-
tional Approaches to Code-switching (FWCAC)
showed that, when typological similarities are
high between the two languages (Modern Stan-
dard Arabic-Dialectal Arabic (MSA-DA) for in-
stance), and they share a big amount of lexical
items, language identiﬁcation task becomes con-
siderably difﬁcult (Solorio et al., 2014). It is easier
to deﬁne languages when the two are not closely
related (Nepali-English for instance).

3 Language Identiﬁcation Models

This section presents our word-level identiﬁcation
models tested on Turkish-English language pair.

3.1 Character N-gram Language Modeling
Our ﬁrst model “Ch.n-gram” uses SRI Language
Modeling Toolkit (SRILM) for character n-gram
modeling, with Witten-Bell smoothing (Stolcke,
2002). Unigrams, bigrams and trigrams (n=1,2,3)
are extracted from Turkish and English training
corpora (ETD, TNC and TTC to be introduced in
§4).

In order to observe how corpora with different
sources (formal or social media) affect language
modeling and word-level language identiﬁcation
on social media texts, TNC and TTC are paired
with the ETD, and the model perplexities are cal-
culated against the code-switched corpus (CSC).
Language labels are decided upon the comparison
of English and Turkish model perplexities for each
token in the test set.

3.2 Conditional Random Fields (CRF)
Conditional Random Fields (CRF) perform effec-
tively in the sequence labeling problem for many

111NLP tasks, such as Part-of-Speech (POS) tagging,
information extraction and named entity recogni-
tion (Lafferty et al., 2001). CRF method was em-
ployed by Chittaranjan et al. (2014) for word-level
language detection, using character n-gram prob-
abilities among others as a CRF feature, reaching
80% - 95% accuracy in different language pairs.
In this research we also experiment with CRF
for word-level language identiﬁcation, where lan-
guage tagging is considered as a sequence labeling
problem of labeling a word either with English or
Turkish language tags.

Our ﬁrst CRF model “CRF†” uses lexicon
lookup (LEX), character n-gram language model
(LM) features and the combination of these for the
current and neighboring tokens (provided as fea-
ture templates to the used CRF tool (Kudo, 2005)).
LEX features are two boolean features stating the
presence of the current token in the English (ETD)
or Turkish dictionary (TNC or TTC). LM feature
is a single two-valued (T or E) feature stating the
label assigned by our previous (Ch.n-gram) model
introduced in §3.1.

Turkish is an agglutinative language. Turkish
proper nouns are capitalized and an apostrophe
is inserted between the noun and any following
inﬂectional sufﬁx.
It is frequently observed that
code-switching people apply the same approach
while using foreign words in their writings. Ex.
(2) provides such an example usage:
(2) action’lar (cid:28)code-switched version(cid:29)

eylemler (cid:28)Turkish version(cid:29)
actions (cid:28)English version(cid:29)
it
In such circumstances,

is hard for our
character-level and lexicon look-up models to as-
sign a correct
tag where an intra-word code-
switching occurs and the apostrophe sign may be
a good clue for detecting these kinds of usages.
In order to reﬂect this know-how to our machine
learning model, we added new features (APOS) to
our last model “CRFφ” (in addition to previous
ones). APOS features are as follows: a boolean
feature stating whether the token contains an apos-
trophe (’) sign or not, a feature stating the lan-
guage tag (E or T) assigned by ch.n-gram model to
the word sub-part appearing before the apostrophe
sign (this feature is assigned an ‘O’ (other) tag if
the previous boolean feature is 0) and a ﬁnal fea-
ture which is similar to the previous one but this
time stating whether this sub-part appears in one

of the language dictionaries (E/T/O).

4 Data

Our character-level n-gram models were trained
on monolingual Turkish and English corpora re-
trieved from different sources. We also collected
and annotated a Turkish-English code-switched
test data-set and used it both for testing of our n-
gram models and training (via cross-validation) of
our sequence labeling model.

The monolingual English training data (ETD)
was acquired from the Leipzig Corpora Collec-
tion (Goldhahn et al., 2012), containing English
text from news resources, incorporating a formal
language, with 10M English tokens. For the Turk-
ish training data, two different corpora were used.
The ﬁrst corpus was artiﬁcially created using the
word frequency list of the Turkish National Cor-
pus (TNC) Demo Version (Aksan et al., 2012).
TNC mostly consists of formally written Turkish
words. Second Turkish corpus (TTC) (6M tokens)
was extracted using the Twitter API aiming to ob-
tain a representation of the non-canonical user-
generated context.

For the code-switched test corpus (CSC), 391
posts all of which containing Turkish-English
code-switching were collected from Twitter posts
and Eks¸i S¨ozl¨uk website1. The data was cross-
annotated by two human annotators. A baseline
assigning the label “Turkish” to all tokens in this
dataset would obtain a 72.6% accuracy score and a
42.1% macro-average F1-measure. Corpus statis-
tics and characteristics are provided in Table 1.2

5 Experiments and Discussions

We evaluate our token-level language identiﬁca-
tion models using precision, recall and F1 mea-
sures calculated separately for both language
classes (Turkish and English). We also provide
micro3 and macro averaged F1 measures for each
model.
the ﬁrst row
“base T” (introduced in §4) provides the scores of
1an online Turkish forum, containing informal, user-

Table 2 provides two baselines:

generated information

2All punctuations (except for “ ‘ ” and “-”), smileys and
numbers were removed from the corpora using regular ex-
pressions (regex) and all characters were lowercased.

3 Since the accuracy scores in this classiﬁcation scenario
are most of the time the same (except for baseline model
scores provided in §4) with micro-averaged F1 measures, we
do not provide them separately in Table 2.

112English
tokens

TNC -
TTC -
ETD 10,799,547 -
CSC 1488

Total
tokens

Turkish
tokens
10,943,259 10,943,259 Formal
5,940,290

Language
Type

Informal

5,940,290
10,799,547 Formal
5430

Informal

3942

Use

Training of Turkish language model
Training of Turkish language model
Training of English language model

Testing & Training of sequence models

Table 1: Corpus Characteristics

System

LM/
Dict.

Turkish

English

Avg. F1

-

P

P

R

R

F1

72.6% 100.0% 84.1% 0.0% 0.0%

F1
Micro Macro
base T
0.0% 61.1% 42.1%
base LL
ETD-TNC 91.4% 98.7% 94.9% 95.5% 75.5% 84.4% 92.0% 89.6%
Ch.n-gram ETD-TNC 98.1% 88.4% 93.0% 75.6% 95.5% 84.4% 90.6% 88.7%
Ch.n-gram ETD-TTC 95.9% 94.1% 95.0% 85.1% 89.4% 87.2% 92.9% 91.1%
ETD-TNC 96.3% 97.2% 96.7% 91.9% 89.6% 90.6% 95.0% 93.7%
ETD-TTC 96.3% 96.9% 96.6% 91.2% 90.3% 90.7% 95.0% 93.6%
ETD-TNC 97.2% 96.8% 97.0% 91.7% 92.2% 91.9% 95.6% 94.5%
ETD-TTC 96.8% 96.6% 96.7% 91.5% 90.9% 91.1% 95.1% 93.9%

CRF†
CRF†
CRFφ
CRFφ

Table 2: Token-level language identiﬁcation results.
LM/Dict. refers to the data used as dictionaries and training data for n-gram language models.

the baseline model which assigns the label “Turk-
ish” to all tokens and the second row provides the
results of a rule-based lexicon lookup (base LL)
which assigns the language label for each word by
searching it in TNC and ETD used as Turkish and
English dictionaries. If a word occurs in both or
none of these dictionaries, it is tagged as Turkish
by default.

We observe from the results that the character-
level n-gram models trained on a formal data
set (TNC) fall behind our second baseline (with
88.7% macro avg. F1) whereas the one trained on
social media data (TTC) performs better (91.1%).
It can also be observed that the performances of
character n-gram language models turned out to be
considerably high, aided by the fact that Turkish
and English are morphologically distant languages
and contain differing alphabetical characters such
as “s¸,˘g,¨u,¨o,c¸,ı”in Turkish and “q,w,x” in English.
CRF models’ performances are calculated via
10 fold cross-validation over code-switched cor-
pus (CSC). One may see from the table that all
of our CRF models perform higher than our base-
lines and character n-gram models. The best per-
formances (95.6% micro and 94.5% macro avg.
F1) are obtained with CRFφ trained with LEX +
LM + APOS features. Contrary to the above ﬁnd-
ings with character level n-gram models, we see

that CRFφ performs better when TNC is used for
character-level n-gram training and look-up. The
use of TTC (monolingual Turkish data collected
from social media) was revealing better results in
Ch.n-gram models and similar results in CRF†.
This may be attributed to the fact that our hy-
pothesis regarding the use of apostrophes in code-
switching of Turkish reveals a good point and the
validation of the word sub-part before the apostro-
phe sign (from a formally written corpus - TNC)
brings out a better modeling.

6 Conclusion
In this paper, we presented the ﬁrst
results
on code-switching detection between Turkish-
English languages. With the motivation of social
media analysis, we introduced the ﬁrst data set
which consists 391 posts with 5430 tokens (hav-
ing ∼30% English words) collected from social
media posts4. Our ﬁrst trials with conditional ran-
dom ﬁelds revealed promising results with 95.6%
micro-average and 94.5 macro-average F1 scores.
We see that there is still room for improvement
for future studies in order to increase the rela-
tively low F1 (91.9%) scores on English. As fu-
ture works, we aim to increase the corpus size

4The

code-switched

corpus

is

available

via

http://tools.nlp.itu.edu.tr/Datasets (Eryi˘git, 2014)

113and to test with different sequence models such as
LSTMs.

References
Yes¸im Aksan, Mustafa Aksan, Ahmet Koltuksuz, Taner
Sezer, ¨Umit Mersinli, Umut Ufuk Demirhan, Hakan
Yılmazer, G¨uls¨um Atasoy, Seda ¨Oz, ˙Ipek Yıldız,
and ¨Ozlem Kurto˘glu. 2012. Construction of the
In Proceedings of
Turkish national corpus (tnc).
the Eight International Conference on Language
Resources and Evaluation (LREC 2012), Istanbul,
Turkey. European Language Resources Association
(ELRA).

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identiﬁcation in the language of so-
In Proceedings of the First Workshop
cial media.
on Computational Approaches to Code-Switching
(EMNLP 2014), pages 13–23. Association for Com-
putational Linguistics.

¨Ozlem C¸ etino˘glu, Sarah Schulz, and Ngoc Thang Vu.
2016. Challenges of computational processing of
code-switching. In Proceedings of the Second Work-
shop on Computational Approaches to Code Switch-
ing (EMNLP 2016), pages 1–11, Austin, Texas. As-
sociation for Computational Linguistics.

¨Ozlem C¸ etino˘glu. 2016. A Turkish-German code-
In Proceedings of the Tenth In-
switching corpus.
ternational Conference on Language Resources and
Evaluation (LREC 2016), Paris, France. European
Language Resources Association (ELRA).

Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. Word-level language
identiﬁcation using crf: Code-switching shared task
In Proceedings of the
report of msr India system.
First Workshop on Computational Approaches to
Code Switching (EMNLP 2014), pages 73–79. As-
sociation for Computational Linguistics.

Amitava Das and Bj¨orn Gamb¨ack. 2013. Code-mixing
in social media text: The last language identiﬁca-
tion frontier? Traitement Automatique des Langues
(TAL): Special Issue on Social Networks and NLP,
54(3).

G¨uls¸en Eryi˘git and Dilara Toruno˘glu-Selamet. 2017.
Social media text normalization for Turkish. Nat-
ural Language Engineering, 23(6):835–875.

G¨uls¸en Eryi˘git. 2014. ITU Turkish NLP web service.
In Proceedings of the Demonstrations at the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL), Gothen-
burg, Sweden. Association for Computational Lin-
guistics.

Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.
2012. Building large monolingual dictionaries at the

Leipzig corpora collection: From 100 to 200 lan-
guages. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC 2012).

Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368–378.
Association for Computational Linguistics.

Claire E Hughes, Elizabeth S Shaunessy, Alejandro R
Brice, Mary Anne Ratliff, and Patricia Alvarez
McHatton. 2006. Code switching among bilingual
and limited English proﬁcient students: Possible in-
dicators of giftedness. Journal for the Education of
the Gifted, 30(1):7–28.

Aravind K. Joshi. 1982. Processing of sentences with
In Proceedings of
intra-sentential code-switching.
the 9th Conference on Computational Linguistics -
Volume 1, COLING ’82, pages 145–150, Czechoslo-
vakia. Academia Praha.

Taku Kudo. 2005. Crf++: Yet another crf toolkit.

http://crfpp. sourceforge. net/.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
In Proceedings of the 18th Interna-
quence data.
tional Conference on Machine Learning, pages 282–
289. Morgan Kaufmann, San Francisco, CA.

Constantine Lignos and Mitch Marcus. 2013. Toward
In 87th An-

web-scale analysis of codeswitching.
nual Meeting of the Linguistic Society of America.

Suraj Maharjan, Elizabeth Blair, Steven Bethard, and
Thamar Solorio. 2015. Developing language-tagged
corpora for code-switching tweets. In Proceedings
of LAW IX - The 9th Linguistic Annotation Work-
shop, pages 72–84, Denver, Colorado.

Maite Melero, Marta.R. Costa-Juss`a, P. Lambert, and
M. Quixal. 2015. Selection of correction candidates
for the normalization of Spanish user-generated con-
tent. Natural Language Engineering, FirstView:1–
27.

Giovanni Molina, Fahad AlGhamdi, Mahmoud
Ghoneim, Abdelati Hawwari, Nicolas Rey-
Villamizar, Mona Diab, and Thamar Solorio. 2016.
Overview for the second shared task on language
identiﬁcation in code-switched data. In Proceedings
of
the Second Workshop on Computational Ap-
proaches to Code Switching (EMNLP 2016), pages
40–49. Association for Computational Linguistics.

Carol Myers-Scotton. 1995.

Social motivations for
codeswitching: Evidence from Africa. Oxford Uni-
versity Press.

114Dong Nguyen and A. Seza Do˘gru¨oz. 2013. Word level
language identiﬁcation in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 857–862, Seattle, Washington, USA.
Association for Computational Linguistics.

Mario Piergallini, Rouzbeh Shirvani, Gauri S. Gau-
tam, and Mohamed Chouikha. 2016. Word-level
language identiﬁcation and predicting codeswitch-
ing points in Swahili-English language data.
In
Proceedings of the Second Workshop on Compu-
tational Approaches to Code Switching (EMNLP
2016), pages 21–29. Association for Computational
Linguistics.

Younes Samih and Wolfgang Maier. 2016. An Arabic-
In Pro-
Moroccan Darija code-switched corpus.
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
Paris, France. European Language Resources Asso-
ciation (ELRA).

Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona T. Diab, Mahmoud
Ghoneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, and Pascale Fung.
2014. Overview for the ﬁrst shared task on language
In Proceed-
identiﬁcation in code-switched data.
ings of The First Workshop on Computational Ap-
proaches to Code Switching (EMNLP 2014), pages
62–72, Doha, Qatar.

Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proceedings of the 7th Interna-
tional Conference on Spoken Language Processing
(ICSLP 2002), pages 901–904.

115
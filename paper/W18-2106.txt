Marcello Federico

MMT Srl / FBK  Trento, Italy

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 207Symbiotic Human and Machine Translation

MT seamlessly 
• adapts to user data
• learns from post-editing

user enjoys 
• enhanced productivity
• better user experience

3

Usable technology for the translation industry

• easy to install and deploy
• fast to set-up for a new project
• effective, also on small projects
• scalable with data and users
• works with commodity hardware

4

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 208The Modern MT way

(1)
(2)
(3)

 connect your CAT with a plug-in
 drag & drop your private TMs
 start translating!

Modern MT in a nutshell

zero training time 
adapts to context
learns from user corrections
scales with data and users

5

6

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 209Training data is a dynamic collection of Translation Memories

At any time: 

● new TMs are added 
● existing TMs are extended

Training time comparable to uploading time!

Context aware translation

SENTENCE

party

CONTEXT

CONTEXT

We are going out.

We approved the law 

TRANSLATION

fête

TRANSLATION

parti

7

8

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 210      requests

  suggestions

Machine 

Translation 

 
 
 
 

    post-edits 

Incremental Learning

9

Core technology [original plan]

context analyser
phrase-based decoder
adaptive models
incremental structures
parallel processing 

Simple. Adaptive. Neural. 

10

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 211Language support

● 45 languages
● fast pre-/post-processing
● simple interfaces
● tags and XML management
● localization of expressions
● TM cleaning

Simple. Adaptive. Neural. 

Context Analyzer

50%

A
B
C

45%

5%

● analyze input text

● retrieve best matching TMs

● compute matching scores

● dynamic structure

Simple. Adaptive. Neural. 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 212Adaptive Phrase Table

● suffix array indexed with TMs 
● phrases sampled on demand
● priority sampling over TMs 
● dynamic structure 

Simple. Adaptive. Neural. 

1000

Suffix Array with 
Ranked Sampling

Adaptive Language Model

A

B

C

∑ w • p

● large static background model
● n-grams stats indexed with TMs  
● combination of active TM LMs
● TM LMs computed on the fly
● dynamic structure 

Simple. Adaptive. Neural. 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 213TED Talks  
English-French

M. Cettolo, et al. (2016), The IWSLT 2016 Evaluation Campaign, IWSLT. 

15

Second Prototype  (0.14 January 2017) 

Open benchmark:

- Training speed:

12x Moses - 100x NMT

- MT quality (BLEU):

+1 vs Moses   
-0.5 vs NMT Ada

Domains: ECB, Gnome, JRC, KDE, 
OpenOffice, PHP, Ubuntu, UN-TM

Simple. Adaptive. Neural. 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 214What happened

Research on adaptive neural MT 
Believed PBMT was competitive on technical translation
 Finally realised superiority of NMT quality
Completed PBMT release and switched to NMT
 Data collection for 14 translation directions 

Simple. Adaptive. Neural. 

Roadmap from last review meeting 

2015 Q2

2016 Q2

2016 Q4

technology 

switch

2017 Q4

minimum 
viable product

first alpha 
release

first beta 
release

context aware 
1 lang pair

fast training,
context aware,
distributed,
1 lang pair

online learning
plug-in,
3 lang pairs

final release 

neural MT,
enterprise 
ready,
14 lang pairs

Simple. Adaptive. Neural. 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 215Multi-user scenario

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 216Multi-user scenario

Multi-user scenario

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 217All we need is a memory

24

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 218All we need is a memory

All we need is a memory

25

26

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 219All we need is a memory

All we need is a memory

27

28

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 220Multi-user adaptive NMT

Multi-user adaptive NMT

29

Instances are 
selected by 
combining 
context scores and 
similarity scores 

30

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 221Adaptation, too!

Source: Farajian et al, “Multi-Domain Neural MT 

through Unsupervised Adaptation”, Proc. WMT 2017.

Farajian et al. (2017) “Multi-domain NMT through unsupervised adaptation”, WMT.

31

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 222Sep: integration of MateCat
Oct: NMT code released 
Nov: co-development 

release of 14 engines

Dec: performance boost

33

Relative BLEU 
scores wrt
Google Translate 

34

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 223Progression in one month on English-Italian

Performance of 
generic MMT 
1-6 scale 
(w/o adaptation)

35

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 22437

38

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 225Noisy training data

EN: What history teaches us
IT: === Storia ==================================

Simple. Adaptive. Neural. 

Data Cleaning

We added a simple QE module to filter out bad examples:

● Apply Fast-Align in two directions
● Compute Model 1 scores in two directions
● Combine and normalize scores
● Filter out on the distribution of scores

Simple. Adaptive. Neural. 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 226Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 22743

44

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 228We compare:

● Generic MT:  production engine [En-It]
● Custom MT: Generic MT tuned on TM [takes hours]
● +Adaptive MT: Generic MT adapted on TM [real-time]
● +Incremental MT: TM updated with simulated PE

45

46

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 22947

48

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23049

50

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 231Use post-editing as new training instances 

Perform one/more iterations 

Can be combined with a priori adaptation

Updates generic or adapted model

 

Turchi et al. (2017), Continuous learning from human post-edits for NMT, EAMT.

52

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23253

54

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23355

56

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23457

58

Online-learning contribution is consistent

Does it scale with number of domains?

Incremental learning contributes marginally

Probably depends on test set size 

We are not always able to beat specialized models 

How to improve further adaptation ? 

 

Source: Turchi et al. (2017) “Continuous learning from human post-edits for NMT”, EAMT.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 235Can improve MT without touching it inside 

We can adapt an “external” MT  service!

Similar to NMT: two inputs (src,mt), one output (ape)

Can be trained with less data than NMT

We can deploy instance based adaptation 

 
Chatterjee  et al. (2017), Multi-source Neural APE: FBK’s participation …. , WMT.

60

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23661

Neural APE uses two 
encoders and two 
attention models, 
which are merged and 
used by one decoder. 

62

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23763

64

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23865

66

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 23967

68

Can improve on top of static and adaptive engine! 

Uses incremental learning, adaptation and online learning

Portable (in principle) on the multi-domain setting  

Limited gain on top of full-fledged adaptive NMT 

Can be an extra component to manage

 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 240Multi-user scenario goes beyond simple domain adaptation  

We need to handle multiple evolving domains

Domain customization is not an option

 Real-time adaptation/learning works! 

But, there is still room for improvement! 

 

70

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 241Thank You

Website
www.ModernMT.eu

Github
github.com/ModernMT/MMT

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-EditingBoston, March 21, 2018   |   Page 242
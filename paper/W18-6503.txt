Syntactic Manipulation for Generating More Diverse and Interesting

Texts

Jan Deriu

Zurich University of Applied Sciences

Mark Cieliebak

Zurich University of Applied Sciences

jan.deriu@zhaw.ch

mark.cieliebak@zhaw.ch

Abstract

Natural Language Generation plays an im-
portant role in the domain of dialogue sys-
tems as it determines how users perceive
the system. Recently, deep-learning based
systems have been proposed to tackle this
task, as they generalize better and re-
quire less amounts of manual effort to im-
plement them for new domains. How-
ever, deep learning systems usually adapt a
very homogeneous sounding writing style
which expresses little variation.
In this work, we present our system for
Natural Language Generation where we
control various aspects of the surface re-
alization in order to increase the lexical
variability of the utterances, such that they
sound more diverse and interesting. For
this, we use a Semantically Controlled
Long Short-term Memory Network (SC-
LSTM), and apply its specialized cell to
control various syntactic features of the
generated texts. We present an in-depth
human evaluation where we show the ef-
fects of these surface manipulation on the
perception of potential users.

Introduction

1
In this paper, we describe our end-to-end train-
able neural network for producing natural lan-
guage descriptions of restaurants from meaning
representations (MR). Recently, data-driven nat-
ural
language generation (NLG) systems have
shown great promise, especially as they can be
easily adapted to new data or domains. End-to-
end systems based on deep learning can jointly
learn sentence planning and sentence realization
from unaligned data. However, a recurrent prob-
lem, which we found with the existing solutions

for NLG, is that the generated utterances express
a very homogeneous writing style. More pre-
cisely, most utterances start by using the restaurant
name, the follow-up sentences usually begin with
the pronoun “It”, and each attribute-value pair is
expressed using the same formulation across dif-
ferent utterances (see Table 1).

Green Man is a family friendly japanese restaurant in riverside
near Express by Holiday Inn.
Clowns is a pub near Crowne Plaza Hotel with a customer rating of 5 out of 5.
Wildwood is an italian pub located near Raja Indian Cuisine in the city centre.
It is not family-friendly.
The Cricketers provides chinese food in the 20-25 price range.
It is located in the riverside. It is near All Bar One. Its customer rating is high.

Table 1: Examples to highlight the homogeneity of the utter-
ances generated by state-of-the-art systems.

The

publicly

available E2E dataset

by
(Novikova et al., 2017) provides pairs of
Meaning Representations (MR’s) and several
human generated reference utterances for the
restaurant-domain. It is the ﬁrst dataset to provide
large amounts of training data with an open
vocabulary, complex syntactic structures, and
more variabilty in expressing the attributes.
In
this work, we exploit
these characteristics of
the dataset to generate utterances which express
a higher diversity in their writing style.
For
this, we extend the Semantically Conditioned
Long Short-term Memory Network (SC-LSTM)
proposed by (Wen et al., 2015b) with surface
features to control the manipulation of the surface
realization.
Since the data contains a large variety of for-
mulations for an attribute-value pair, a simple
delexicalization of the utterance is not possible.
This fact also increases the difﬁculty of evaluating
the utterances for their correctness. Thus, we in-
troduce a semantic reranking procedure based on
classiﬁcation algorithms trained to rate whether

ProceedingsofThe11thInternationalNaturalLanguageGenerationConference,pages22–34,Tilburg,TheNetherlands,November5-8,2018.c(cid:13)2018AssociationforComputationalLinguistics22the attributes are rendered correctly.
We evaluate our model on the E2E dataset and
report the BLEU, NIST, METEOR, ROUGE-L
and CIDEr scores. We measure the diversity of
the generated utterances by counting the number
of different uni- and bi-grams. Further, to evaluate
the correctness of the generated utterances, we
employ a soft metric based on the aforementioned
classiﬁers. Finally, we present an in-depth human
evaluation where we measured the effects of these
more diverse utterances on the perceptions of
potential users. More precisely, humans evaluated
the quality and naturalness of an utterance, which
of the attributes comprehensible, concise, elegant,
and professional ﬁts to the text, and which of the
different systems generated the most preferred
outputs. We release the code and all the scripts.1

2 Related Work

The task of NLG is usually divided into separate
subtasks such as content selection, sentence plan-
ning, and surface realization (Stent et al., 2004).
Traditionally, the task has been solved by relying
on rule-based methods, but these methods do not
scale and are hardly adaptable to new domains.
Recently, deep learning techniques have become
more prominent for NLG. With these techniques,
there now exists a large variety of different net-
work architectures, each tackling a different aspect
of NLG: (Wen et al., 2015b) propose an extension
to the vanilla LSTM (Hochreiter and Schmidhu-
ber, 1997) to control the semantic properties of
an utterance, whereas (Hu et al., 2017) use varia-
tional autoencoder (VAE) and generative adversar-
ial networks to control the generation of texts by
manipulating the latent space; (Mei et al., 2016)
employ an encoder-decoder architecture extended
by a coarse-to-ﬁne aligner to solve the problem of
content selection; (Wen et al., 2016) apply data
counter-ﬁtting to generate out-of-domain training
data for pretraining a model where there is little
in-domain data available; (Semeniuta et al., 2017;
Bowman et al., 2015) use a VAE trained in an
unsupervised fashion on large amounts of data to
sample texts from the latent space; and (Duˇsek and
Jurcicek, 2016) use a sequence-to-sequence model
with attention to generate natural language strings
as well as deep syntax dependency trees from di-
alogue acts. All these approaches solve different
aspects of the NLG task.

1https://github.com/jderiu/e2e nlg

In our work, we tackle the aspect of generating
texts that display more complex and diverse syn-
tactic structures. The dialogue system commu-
nity has proposed most work on this topic, as
the end-to-end trainable algorithms tend to pro-
duce the same universal answer to each input. In
(Li et al., 2016a) the authors develop a new loss
function based on mutual information, (Li et al.,
2016b) propose a new decoding algorithm based
on a modiﬁed beam search, which favors hypothe-
ses from different parent nodes. In (Li et al., 2017)
the authors aim to increase the diversity by remov-
ing training examples, which are similar to the
most commonly used utterances. In (Shao et al.,
2017) the authors propose a sequence-to-sequence
model with an augmented attention mechanism,
which takes into account parts of the target sen-
tence. Finally, the authors adapt the beam-search
ranking to work at a segment level and, thus, in-
jecting diversity earlier during the decoding.

3 Task Deﬁnition

Natural language generation for dialogue systems
describes the task of converting a meaning repre-
sentation (MR) into an utterance in a natural lan-
guage. The E2E training data consist of 50k in-
stances in the restaurant domain, where one in-
stance is a pair of a MR and an example utter-
ance or reference. The data is split into training,
development and test in a 76.5%-8.5%-15%-ratio.
Each MR consists of 3-8 attributes and their val-
ues, see Table 2 for the domain ontology. The split
ensures that the MRs in the different dataset-splits
are distinct. The dataset contains an open vocab-
ulary and more complex syntactic structures than
other similar datasets, as shown in the dataset def-
inition (Novikova et al., 2017). Especially, it con-
tains various ways of expressing a single value of
an attribute: for instance, the value 1 of 5 is ex-
pressed in the data as “one star rated”, “rated with
1 of 5 stars”, or “rated one out of ﬁve”.
In this
work, we exploit this variety of formulation to pro-
duce utterances that express a more varied writing
style.

4 Model

The goal of our model is to generate a text while
providing the ability of controlling various seman-
tic and syntactic properties of this text. Our model
has two components: i) the generator and ii) se-
mantic classiﬁers that rate the correctness of an ut-

23Attribute

Type

Example Values

name
eatType
familyFriendly
food
near
area

verbatim string Alimentum, ..
dictionary
boolean
dictionary
verbatim string Burger King
dictionary

restaurant, pub, coffee shop
yes, no
Italian, French, English, ...

customerRating

dictionary

priceRange

dictionary

riverside, city center
1 of 5, 3 of 5, 5 of 5,
low, average, high
<£20, £20-25, >£30
cheap, moderate, high

Table 2: Domain ontology of the E2E dataset.

terance.
We use the Semantically Conditioned Long Short-
term Memory Network (SC-LSTM) proposed by
(Wen et al., 2015b) as our generator, which has
a specialized cell to process the one-hot encoded
MR-vector. The semantic classiﬁers (SC) are
trained for each attribute separately: they classify
which value the generator rendered. With this,
the correctness of an utterance can be determined,
which is relevant when dealing with contradictory
constraints during the generation of more diverse
texts.

4.1 Semantically Conditioned LSTM

The SC-LSTM (Wen et al., 2015b) extends the
original LSTM (Hochreiter and Schmidhuber,
1997) cell with a specialized cell, which processes
the MR. The MR is represented as a one-hot en-
coded MR-vector d0, which represents the value
for each attribute. This cell assumes the task of
the sentence planner, as it treats the MR-vector as
a checklist to ensure that the information is fully
represented in the utterance. The cell acts as a for-
get gate, keeping track of which information has
already been consumed.
We brieﬂy introduce the SC-LSTM as deﬁned in
(Wen et al., 2015b), which we will later on mod-
ify to meet our needs. Let wt ∈ RM be the input
vector at time t, dt ∈ RD the MR-vector at time
t, and N be the number of units of an SC-LSTM
cell, then the formulation of the forward pass is

deﬁned as:

it
ft
ot
rt
gt

 =



σ
σ
σ
σ

 W5n,2n

tanh

(cid:19)

(cid:18) wt

ht−1

dt = rt ∗ dt−1
ct = it ∗ gt + ft ∗ ct−1 + tanh(Wddt)
ht = ot ∗ tanh(ct)

where σ is the sigmoid function, and it, ft, ot, rt ∈
[0, 1]N are the input, forget, output, and MR-
reading gates, and ht, ct ∈ [0, 1]N are the hid-
den state and the cell state. The weights W5n,2n
and Wd ∈ RD×M are the model parameters to be
learned.
The prediction of the next token is performed by
sampling from the probability distribution:

wt ∼ P (wt|w0:t−1, dt) = softmax(Wsht)

where Ws ∈ RN×M is a weight matrix to be
learned during training. During the training pro-
cedure the inputs to the SC-LSTM are the original
tokens wt from the training set. On the other hand,
when generating new utterances we use the previ-
ously generated token as input to generate the next
token.
Loss To ensure that the SC-LSTM consumes the
MR correctly, two conditions are deﬁned: i) the
MR-vector at the last time step dT has to be zero,
which ensures that all the required information has
been rendered, and ii) the gate should not con-
sume too much of the dialogue act in one time
the difference (cid:107)dt − dt−1(cid:107) should be
step, i.e.
minimised. From these criteria, the reconstruction
loss is adapted to:

F (θ) =

t log(yt) + (cid:107)dT(cid:107) +
pT

ηξ(cid:107)dt−dt−1(cid:107)

t

t=0

where the ﬁrst term is the reconstruction error,
which sums the cross-entropy loss for each time
step and the following two terms ensure the two
criteria deﬁned above.
Semantic Classiﬁers For each attribute a we
train a CNN-based classiﬁer Da. Each classiﬁer
is trained to detect which of the possible values
for the attribute a is rendered in the utterance or if
the attribute is present in the utterance at all. We
train the classiﬁers on the training set, where the
input is the utterance and the output is the value for
the attribute a, which is deﬁned in the MR. These
classiﬁers measure the semantic correctness of the
produced utterances by comparing the output of
the classiﬁer to the MR. If the classiﬁer output cor-
responds to the value deﬁned in the MR then we
regard the attribute as being rendered correctly.

T−1(cid:88)

(cid:88)

245 Syntactic Control

The utterances produced by the basic model de-
scribed in Section 4 lack syntactic variety, they all
follow the trivial structure. To control the syntac-
tic expressions of an utterance we expand the MR-
vector with syntax speciﬁc features. More specif-
ically, in this work we control three different sur-
face features: i) the ﬁrst word of the utterance, ii)
the ﬁrst word of each follow-up sentence in the ut-
terance, and iii) for each attribute-value pair the
formulation used to express it. For each of these
control mechanisms, we produce one-hot encoded
vectors and append these vectors to the MR-vector
d0. Through this mechanism, we provide the SC-
LSTM with more prior information on the struc-
ture of the utterance. Thus, it learns to correlate
how to render the surface based on the surface in-
formation provided. In the following, we describe
the three control mechanisms in detail.

First Word Control Most utterances generated
by the vanilla SC-LSTM begin by using the restau-
rant name. The main reason for this behaviour
is that 59% of all utterances in the dataset have
this characteristic. All the other starting words are
used much less frequently: e.g. only 7% of all
utterances start with the word “There”, which is
the second most used word. The model optimizes
to generate the utterance, which yields the low-
est average loss. Without additional information,
this equates to the most common structure of utter-
ances found in the training set. The ﬁrst word used
in an utterance greatly impacts how the rest of the
utterance is rendered. Thus, using different ﬁrst
words increases the diversity of the rendered ut-
terances. To generate more uncommon utterances,
we provide the model with the information about
the ﬁrst word in the utterance during training. For
this, we select all the words that appear more than
t = 60 times as ﬁrst word in the training data,
which results in a set of n = 20 different words2.
We then extend the MR-vector by adding a one-
hot encoded vector u0 ∈ Rn+1, where the vector
is set to ’1’ at the index of the ﬁrst word in the
utterance of the training sample. During the train-
ing, we use a dummy-index at n + 1 in case the
ﬁrst word of the utterance is not present in the list
of ﬁrst words. During test-time the ﬁrst word is
sampled from the set of n ﬁrst words. To improve

2$Name, Located, For, In, A, $Near, An, Near, There, On,
$Food,The ,With ,Serving , If, At, Riverside, By, You, Family

the semantic correctness we use the sampling pro-
cedure to over-generate, i.e. m different words are
sampled to generate m different utterances. Using
the semantic classiﬁers, the produced utterances
are ranked by their correctness score.

Follow-up First Word Control We observe that
the follow-up sentences in an utterance, which
are produced by the vanilla SC-LSTM also follow
the same pattern. More precisely, in cases where
the utterance uses multiple sentences, the follow-
up sentences usually begin with the pronoun ’It’
which refers to the restaurant name mentioned in
the ﬁrst sentence. Similarly, to the First-Word-
Control, we control the ﬁrst word of follow-up
sentences by using one-hot encoded vectors. The
encoding states which word is used as ﬁrst word
of each follow-up sentence. As most utterances
are composed between one and four sentences, we
use three vectors to encode the ﬁrst word of the
ﬁrst three follow-up sentences.
There are n = 22 different ﬁrst words used in
follow-up sentences, thus, each vector fi is of
length n + 1, where i ∈ {2, 3, 4} denotes the sen-
tence enumeration. We add an extra dimension to
denote the case where the number of sentences is
less than i. This representation provides the abil-
ity to control the ﬁrst word used in each follow-up
sentence as well as the number of sentences ren-
dered.

Attribute-Value Formulation Control We ob-
serve that the vanilla SC-LSTM learns to use the
most common formulation for an attribute-value
pair. On average over all the attribute-value pairs,
the most common formulation is used in 76% of
the cases in the training set.
It turns out that
the most used formulation for most attribute-value
pairs is equivalent to the surface form of the value
itself. For example, the value “5 out of 5” is
mostly expressed using the formulation: “... with a
customer rating of 5 out of 5”, instead of “It has an
excellent customer rating” or other formulations.
To extract
formulations of an
attribute-value pair, we use a simple TF-IDF ap-
proach based on unigrams. For the complete list
of formulations refer to Table 11 in Appendix A.
For each attribute, we treat the utterances for each
value as one document, thus, the corpus is made
of as many documents as there are values for this
iv) ∗
attribute. The score is computed as 1 + log(tfa
iv is the term frequency of
log(1 + N
dfa
i

the different

) where tfa

25term i for value v and dfa
is the document fre-
i
quency of term i in the documents of attribute a.
We keep only those terms whose score is higher
than 3. We apply manual ﬁltering to clean the list
from terms, which do not describe the attribute-
value pair. With this method, we get on average
4.2 terms per attribute-value pair. We extend the
MR-vector with one one-hot encoded vector for
each attribute-value pair.

6 Experimental Setting

The goal for our application is to generate descrip-
tions for restaurants. The dataset from (Novikova
et al., 2017) contains 50k utterances for 5,751 dif-
ferent MRs. On average, each MR is composed
of 5.43 attributes and there are 8.1 different ref-
erences for each MR on average. For the eval-
uation, we report various corpus-based metrics:
BLEU-4 (Papineni et al., 2002), NIST (Dodding-
ton, 2002) METEOR (Lavie and Agarwal, 2007),
ROUGE-L (Lin, 2004), and CIDEr (Vedantam
et al., 2015). Furthermore, we report various mea-
sures for lexical diversity: number of different to-
kens (#tokens), the type-token ratio (TTR) (Chot-
los, 1944), the moving average type-token ratio
(MSTTR) (Covington and McFall, 2010), and the
measure of lexical diversity(MLTD) (McCarthy,
2005). Finally, we perform a human evaluation to
measure the effect of the proposed manipulations
on the user’s perception.

Preprocessing Each utterance is treated as a
string of characters, where each character is repre-
sented as a one-hot encoded vector. We replace the
name and near values with the tokens ‘X-name”
and “X-near” respectively. The high diversity of
the various formulations found for the attribute-
value pairs, impedes us from replacing other at-
tributes with placeholders. To generate the lexical
features, we apply the Spacy-API3 for word and
sentence tokenization.

System Setup We train the SC-LSTM and the
classiﬁers using AdaDelta (Zeiler, 2012) to opti-
mize the loss function. We apply a softmax with
decreasing temperature as proposed in (Hu et al.,
2017) to approximate the discrete representation,
which is used as input to the LSTM during the de-
coding stage. For the LSTM cell we use a hidden
state of size 1024 and apply dropout as suggested

3https://spacy.io/

BLEU NIST METEOR ROUGE L CIDEr
System
0.634
1.9281
vanilla
2.201
0.661
tgen
1.810
utt-fw
0.581
1.819
follow-fw 0.572
form
1.992
0.623
full
0.505
1.616

8.270
8.550
7.983
7.665
8.161
7.455

0.428
0.446
0.427
0.436
0.432
0.422

0.653
0.687
0.591
0.643
0.657
0.558

Table 3: Scores achieved for the corpus-based metrics by the
different systems. The value of the best system for each score
is highlighted in bold.

in (Yarin and Ghahramani, 2016). For the classi-
ﬁers we use a 2-layer CNN with 256 kernels of
length 3.
We use our character-based version of the SC-
LSTM (vanilla) as well as the sequence-to-
sequence model by (Duˇsek and Jurcicek, 2016)
(tgen) as baseline. We evaluate different ver-
sions of our model: the model where we control
only the ﬁrst word of the utterance (utt-fw), the
model where we only control the ﬁrst words of the
follow-up sentences (follow-fw), the model where
we only control the formulations of the attribute-
value pairs (form), and the model where we con-
trol all three factors (full).

Output Generation The input to the system is a
meaning representation (MR) which is converted
into the MR-vector d0. For each MR, the system
samples the syntactic control values at random, i.e.
it samples the ﬁrst word of the utterance, the ﬁrst
words of each of the follow-up sentences and the
formulation for each attribute-value pair randomly
from the list of their respective possibilities. Then,
these syntactic features are encoded into the one-
hot format as described above. The input to the
SC-LSTM is composed of both the MR-vector and
the syntactic control vector. To ensure that the
sampling of the syntactic features did not intro-
duce semantic error, the system samples 10 differ-
ent values for each of the three control types and
produces one utterance for each combination, e.g.
the full system produces 1000 sentences for each
MR. We then use the classiﬁers (previously trained
to evaluate if the utterance rendered the MR cor-
rectly) to rank the 1000 utterances w.r.t. their cor-
rectness. Finally, the system samples the ﬁnal ut-
terance from the set of utterances with the highest
score (as there can be multiple utterances with the
same score).

26name
1.0

eatType
0.97

price
0.90

rating
0.84

near
0.99

food
0.95

area
0.94

fam.
0.91

Table 4: Validation Accuracy scores for each classiﬁer.

System vanilla
ERRsc
0.158
ERRrule
0.086

tgen
0.192
0.059

utt-fw follow-fw form full
0.093
0.028

0.100
0.040

0.100
0.054

0.056
0.015

Table 5: Error Rate for each system, best system is high-
lighted in bold. The sc subscript denotes the scores computed
by the classiﬁers.

7 Results
7.1 Evaluation Metrics
We report the scores for the automatic evaluation.
This includes the metrics BLEU, ROUGE-L, ME-
TEOR, NIST, and CIDEr score, which rely on the
comparison between the predicted utterance and
multiple reference utterances. Table 3 shows that
the surface manipulation leads to a decrease in all
of these scores. The best scores for each metric
is achieved by the tgen system. Its BLEU score
is 3 points above the score achieved by vanilla.
The full system achieved the lowest scores in each
metric. Generally speaking, the deeper the impact
of the syntactic manipulation the lower the word-
overlap based score. This behaviour is explained
by the fact that the baseline systems generate utter-
ances which are syntactically similar to the most
used structure in the gold-standard. The other sys-
tems generate sentences whose style and structure
is much rarer in the gold-standard. For example,
59% of the reference utterances start with the stan-
dard pattern, whereas only 3% of the sentences
generated by the full system follow this pattern.
Although there are multiple reference utterances,
it is not likely that one of these follows the syntac-
tic choices of the syntactically controlled systems.
Table 6 displays the various lexical diversity
scores for each system as well as for the human-
written text for reference. As expected,
the

#tokens TTR
System
106
vanilla
120
tgen
131
utt-fw
follow-fw 141
155
form
224
full
human
425

0.0070
0.0081
0.0082
0.0084
0.0098
0.0134
0.0280

MATTR MTLD
31.4811
0.5410
30.5444
0.5175
0.5980
34.2865
33.5055
0.5745
33.4892
0.5748
35.7831
0.6310
0.6373
36.4466

Table 6: Diversity scores for each system and the human
texts. The highest score of a system is marked in bold.

human-written texts display the highest diversity
across all scores. The full system achieves the
highest scores out of all systems. Furthermore,
both the vanilla and the tgen system obtain the
lowest scores,
thus, showing that the syntactic
control mechanisms generate more diverse texts.

7.2 Classiﬁer Performance
Since we use semantic classiﬁers to evaluate the
correctness of the generated sentences, it is impor-
tant to assess the quality of these classiﬁers. Table
4 shows the accuracy score for each of the clas-
siﬁers on the testset. We note that all classiﬁers
have a score greater than 0.9 except for the cus-
tomer rating. The errors of the customer rating
and the price classiﬁers stem from the semantic
equivalence between the numerical and the verbal
values which were used interchangeably in the ref-
erences, e.g. when “price range is over £30” is
expressed as “high-priced”.

7.3 Correctness
We evaluate the correctness using a rule based sys-
tem. We report the average error rate achieved
by a system, as proposed by (Wen et al., 2015a),
in Table 5, line ERRrule . The best error-rate is
achieved by the full system, followed by utt-fw and
form. This shows that our approach to rerank the
utterances with the semantic classiﬁers works very
well. For comparison, we also report the error-
rates when using the semantic classiﬁers them-
selves to determine the correctness of an utterance
ERRsc . It turns out that there is a mismatch be-
tween the scores achieved by the two metrics, es-
pecially for the tgen and vanilla system. This is
due to the fact that the classiﬁers are used to ﬁl-
ter the incorrect utterances, which leads the scores
to be biased. Thus, it shows that the classiﬁers
themselves are not suitable to compute a correct-
ness score.

7.4 Qualitative Evaluation
In Table 8 two representative (cherry picked) ex-
amples are shown. For one MR we compare the
outputs of all systems. In both examples the tgen
and vanilla system produce utterances which fol-
low the trivial pattern. The uff-fw and full systems
produce a different style of utterance by starting
the sentence with a preposition. The follow-fw
system adds more variability to the utterance by
starting the follow-up sentences with verbs (e.g.

27“Located”) or nouns (“Children”) instead of pro-
nouns referring to the restaurant name. The form
system adds more variability by using different
ways of phrasing an attribute-value pair (e.g. re-
placing “high price range” with “expensive”). We
added a list of randomly sampled (non-cherry-
picked) examples in Appendix B.

signiﬁcant difference between tgen and vanilla.
In fact, the vanilla system is rated signiﬁcantly
higher in terms of naturalness than any other sys-
tem. For both metrics, the scores of all systems
are very high, thus, we conclude that the syntacti-
cal control mechanisms do not deteriorate the ut-
terances.

System
vanilla
tgen
utt-fw
follow-fw
form
full

Quality Naturalness
3.979
4.013
4.007
3.992
4.035
4.033

2.732∗
2.591
2.605
2.576
2.577
2.540

Table 7: Quality and naturalness results from the user study.
Here, * implies a statistical signiﬁcant difference between a
system and the tgen system, measured with two-tailed Stu-
dent’s t-test with p < 0.05

7.5 Human Evaluation
To measure the effectiveness of our approach, we
performed an extensive human evaluation. For
this, we recruited judges from the Figure-Eight4
platform. For each experiment the sentence is
rated by three different judges.

Quality and Naturalness To show that the syn-
tactic manipulations do not deteriorate the utter-
ances, we evaluated the quality and naturalness of
the utterances produced by the different systems.
Here, quality is deﬁned to measure the grammat-
ical correctness, the ﬂuency and the correctness
of the content, whereas naturalness measures the
likelihood that the utterance was written by a hu-
man. For this, we sampled 250 MR’s and gener-
ated the respective utterances for each system. The
judges rated all utterances on a Likert scale from
1 to 5 for quality and on a scale from 1 to 3 for
naturalness5. Table 7 shows the results for both
the quality and naturalness evaluation. Statistical
signiﬁcance is measured by means of a two-tailed
Student’s t-test between the tgen system and the
other systems. For quality there is no statistically
signiﬁcant difference between the tgen system and
any other system. For naturalness there is no sta-
tistically signiﬁcant between tgen and the syntac-
tically controlled systems. However, there is a

4www.ﬁgure-eight.com
5For naturalness we asked if the utterance is likely to be

written by a human, by a machine or if it is not clear

Subjective Analysis The main goal of the hu-
man evaluation is to understand how humans per-
ceive the new utterances. For this, we compare
the utterances of tgen and the full system by ﬁrst
sampling a MR, generate the utterance for each
system, and let the human judges decide which
of the two utterances they prefer. Since prefer-
ence is a very subjective measure that might not
give complete insight, we asked the judges to also
state which utterance they ﬁnd more comprehensi-
ble (is the utterance easier to understand), more
concise (does the utterance convey the informa-
tion clearly with as little text as possible), more
elegant (is the utterance more nicely written, more
poetic, display higher variability) and more profes-
sional (could this text be written by an experienced
and well trained writer). Table 9 shows the ratio
at which the system was selected for each of the
ﬁve aforementioned categories alongside the inter-
annotator agreement computed with the Fleiss’ κ.
The results show that none of the two systems is
signiﬁcantly preferred by the judges, nor is any of
the two systems rated as being more comprehen-
sible. However, the judges perceive the full sys-
tem to produce signiﬁcantly more elegant and pro-
fessional utterances, i.e. in 71.6% of the compar-
isons the utterances by the full system were rated
as more elegant and in 66.6% as being more pro-
fessional. On the other hand, the judges rate the
utterances of the tgen system as being signiﬁcantly
more concise (75%).
There is a moderate correlation between the pref-
erence of an utterance and the elegance (Spear-
man’s Rho ρ = 0.557 ) or professionalism (
ρ = 0.569 ). Furthermore, there is a weak corre-
lation between comprehensibility of a system and
the professionalism ( ρ = 0.468). However, we
found that there is a strong correlation between the
preference and the comprehensibility of an utter-
ance (ρ = 0.719). The evaluation shows that al-
though the utterances by the full system are rated
as being more elegant and professional, they are
not necessarily preferred. The strongest indicator
for preference is, thus, the comprehensibility.

28System Utterance

#
1 MR

vanilla

tgen
utt-
fw
follow-
fw
form

full
2 MR

vanilla

tgen
utt-
fw
follow-
fw
form

full

fami-

eatType=pub,

food=English,

area=city-centre,

priceRange=high,

name=The Punter,
lyFriendly=no, near=Raja Indian Cuisine
The Punter is a pub that serves English food in the high price range and is located in the city centre
near Raja Indian Cuisine.
The Punter is an english pub in the city centre near Raja Indian Cuisine. It has a high price range and
is not child friendly.
In the city centre near Raja Indian Cuisine is a pub called The Punter. It serves English food and is
not children friendly. The price range is high.
The Punter is a pub that serves English food. Located in the city centre near Raja Indian Cuisine, it is
not children friendly and has a high price range.
The Punter is an expensive English Pub near Raja Indian Cuisine in the city centre. It is not child
friendly.
If you are looking for a pub serving English food, try The Punter. It is located in the city centre near
Raja Indian Cuisine. Prices are on the higher end and it is not child friendly.
name=Giraffe, eatType=restaurant, food=French, area=riverside, familyFriendly=yes, near=Raja In-
dian Cuisine
Giraffe is a family friendly restaurant that serves French food. It is located near Raja Indian Cuisine.

Giraffe is a family friendly french restaurant near Raja Indian Cuisine in riverside.
A French restaurant called Giraffe is located in the riverside area near Raja Indian Cuisine. It is child
friendly.
Giraffe is a restaurant that serves French food. The restaurant is located near Raja Indian Cuisine in
the riverside area. Children are welcome.
Giraffe is a French restaurant in the riverside area near Raja Indian Cuisine. It is family friendly.
In the riverside area there is a French restaurant called Giraffe. You will ﬁnd it near Raja Indian
Cuisine. Yes, it is family friendly.

Table 8: Sample output of the vanilla SC-LSTM (V) and the First Word Control (F) for four different MRs where one attribute-
value is changed.

Question
Preference
Comprehensibility
Conciseness
Elegance
Professional

tgen
0.476
0.476
0.750∗
0.283
0.333

full
0.523
0.523
0.250
0.716∗
0.666∗

κ
0.587
0.555
0.545
0.545
0.529

Table 9: Results of the native speaking preference test.
Signiﬁcance is computed using a two-tailed binomial test.
Where * denotes p < 0.005 and N = 200

Question
Preference
Comprehensibility
Conciseness
Elegance
Professional

tgen
0.593
0.682∗
0.949∗∗
0.424
0.740∗∗

full
0.406
0.317
0.050
0.575
0.259

κ
0.456
0.453
0.312
0.497
0.342

Table 10: Results of the non-native speaking preference test.
Signiﬁcance is computed using a two-tailed binomial test,
here * denotes p < 0.05 and ** denotes p < 0.005 and
N = 200

Native vs. non-native speakers We observed
that depending on whether the judges were native
speaker or not the results were different. Thus, we
repeated the same experiment by recruiting judges

from non-native speaking countries6. Table 10
shows the results of the evaluation performed by
the non-native speaking group. The differences of
the ratings are signiﬁcant. The non-native speak-
ers rate the tgen system as signiﬁcantly more com-
prehensible, more concise as well as more profes-
sional. There is still a high correlation between
the preference and the comprehensibility of an ut-
terance (Spearman’s Rho ρ = 0.709). However,
for the non-native group there is a signiﬁcantly
higher correlation between the comprehensibility
and the professionalism of an utterance (Spear-
man’s Rho ρ = 0.628) and a very high correla-
tion between the preference and the professional-
ism (Spearman’s Rho ρ = 0.714). This shows that
the non-native speaking group ﬁnds it easier to un-
derstand the utterances produced by tgen and rates
them as more preferable and more professional.
The evaluation shows that the two groups have
different preferences and perceptions of the utter-
ances. An in-depth analysis on the reasons behind
these differences is left to future work. Our ex-
periments indicate that the differences are due to
the differences in language proﬁciency, as there is

6Judges were mostly recruited from eastern European

countries and Asia.

29a high correlation between the preference and the
comprehensibility. However, to test this assump-
tion, more characteristics about the judges need to
be known.
8 Conclusion
In this work, we presented an end-to-end train-
able deep-learning based system for the natural
language generation task. With a simple control
mechanism the utterances can be rendered more
diverse and interesting. The human evaluation re-
vealed that this control mechanism does not de-
teriorate the quality of the utterances in terms of
semantic or grammatical errors. It further revealed
that more diverse utterances are perceived as being
more elegant and professional sounding to native
speakers. Not surprisingly, the corpus-based met-
rics deteriorate when a more diverse vocabulary is
used. One major challenge of this approach is the
fact that during the generation the syntactic control
features have to be sampled randomly to generate
many utterances which have to be ranked and ﬁl-
tered. The solution to this inefﬁciency is part of
future work.

References
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349.

John W Chotlos. 1944. Iv. a statistical and compara-
tive analysis of individual written language samples.
Psychological Monographs, 56(2):75.

Michael A. Covington and Joe D. McFall. 2010. Cut-
ting the gordian knot: The moving-average typeto-
ken ratio (mattr). Journal of Quantitative Linguis-
tics, 17(2):94–100.

George Doddington. 2002.

Automatic evaluation
of machine translation quality using n-gram co-
In Proceedings of the Sec-
occurrence statistics.
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

Ondˇrej Duˇsek and Filip Jurcicek. 2016. Sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
45–51. Association for Computational Linguistics.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation, pages
1735–1780.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of tex. International Conference
on Machine Learning, pages 1587–1596.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110–119. Association for Computational Lin-
guistics.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. A
simple, fast diverse decoding algorithm for neural
generation. CoRR, abs/1611.08562.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2017. Data
distillation for controlling speciﬁcity in dialogue
generation. CoRR, abs/1702.06703.

Chin-Yew Lin. 2004. Rouge: A package for automatic
In Text Summarization
evaluation of summaries.
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain. Association
for Computational Linguistics.

Philip M McCarthy. 2005. An assessment of the range
and usefulness of lexical diversity measures and the
potential of the measure of textual, lexical diver-
sity (mtld). Dissertation Abstracts International,
66(12).

Hongyuan Mei, TTI UChicago, Mohit Bansal, and
Matthew R Walter. 2016. What to talk about and
how? selective generation using lstms with coarse-
to-ﬁne alignment. In Proceedings of NAACL-HLT,
pages 720–730.

Jekaterina Novikova, Ondrej Duˇsek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-
the 18th
to-end generation.
Annual Meeting of the Special Interest Group on
Discourse and Dialogue, Saarbr¨ucken, Germany.
ArXiv:1706.09254.

In Proceedings of

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational
arXiv preprint
autoencoder for text generation.
arXiv:1702.02390.

30Yuanlong Shao, Stephan Gouws, Denny Britz, Anna
Goldie, Brian Strope, and Ray Kurzweil. 2017.
Generating high-quality and informative conversa-
tion responses with sequence-to-sequence models.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2210–2219. Association for Computational Linguis-
tics.

Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Proceedings of the 42Nd Annual Meeting on As-
sociation for Computational Linguistics, ACL ’04,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. Cider: Consensus-based image
description evaluation. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola
Mrksic, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a. Stochastic language generation in di-
alogue using recurrent neural networks with convo-
lutional sentence reranking. pages 275–284. Asso-
ciation for Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c,
Lina M. Rojas-Barahona, Pei-Hao Su, David
Vandyke, and Steve Young. 2016. Multi-domain
neural network language generation for spoken di-
In Proceedings of the 2016 Con-
alogue systems.
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 120–129. Association for
Computational Linguistics.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Gal Yarin and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. Advances in neural information
processing systems, pages 1019–1027.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-

ing rate method. arXiv preprint arXiv:1212.5701.

31A Formulations of Attribute-Values

Attribute

customer rating

familyFriendly

food

priceRange

Value
1 out of 5
3 out of 5
low
5 out of 5
average
high
no
English
Fast food
French
Italian
Japanese
cheap
high
moderate

less than£20

£20-25

more than £30

Formulations
1, one, poor
3, three
low, one, poor, poorly
5, ﬁve, excellent
average, an, three, averagely
high, highly, between, ranging
not, non, adult, adults, no, allowed, allow
English, British, breakfast, traditional
fast, fries, joint, American, burger
French, wine, cheese, ﬁne, drinks
Italian, pasta
Japanese, sushi, bar
cheap, inexpensive
high, expensive, higher, end
moderate, moderately, mid, medium, pricing
20, less, than, under, pounds,
inexpensive, below, lower
20, from, between, mid, 20-25, ranging, around
30, more, than, expensive, over,
higher, above, costs, euros, costing

Table 11: The most important formulations that appear in the training set for each attribute-value pair. Pairs with just a single
formulation were omitted.

32B Sampled Utterances

System Utterance

#
1 MR

name=The Wrestlers, eatType=restaurant, food=Italian, priceRange=moderate, area=riverside, fami-
lyFriendly=no, near=Raja Indian Cuisine,
The Wrestlers is a moderately priced Italian restaurant near Raja Indian Cuisine that is not kid friendly.
The Wrestlers is a moderately priced italian restaurant in riverside near Raja Indian Cuisine. It is not
kid friendly.
There is a moderately priced Italian restaurant called The Wrestlers in the riverside area near Raja
Indian Cuisine. It is not kid friendly.
The Wrestlers is a restaurant providing Italian food in the moderate price range. The are located in
the riverside area near Raja Indian Cuisine. The restaurant is not kid friendly.
The Wrestlers is a restaurant that serves Italian food at a moderate price range. It is located in riverside
near Raja Indian Cuisine and is not kid friendly.
With a moderate price range, The Wrestlers is a restaurant that serves Italian food. It is located near
Raja Indian Cuisine in the riverside area. No children allowed.
The Wrestlers is a restaurant that serves Italian food. Has a moderate price range, and is kids friendly.
Located in the riverside area near Raja Indian Cuisine.

vanilla

tgen

utt

follow

form

full

human

2 MR

name=Zizzi, eatType=coffee shop, customer rating=average, near=Burger King,

vanilla

Zizzi is a coffee shop near Burger King with an average customer rating.

tgen

utt

follow

form

full

human

3 MR

vanilla

tgen

utt

follow

form

full

human

Zizzi is a coffee shop near Burger King. It has an average customer rating.

If you are looking for a coffee shop near Burger King, try Zizzi. It has an average customer rating.

Zizzi is a coffee shop near Burger King. Zizzi has an average customer rating.

Zizzi is a coffee shop near Burger King. It has an average customer rating.
If you are looking for a coffee shop near Burger King, then you could try Zizzi. It has an average
customer rating.
Customers rate Zizzi coffee shop, near Burger King, average.

name=The Punter, eatType=restaurant, food=Italian, priceRange=cheap, customer rating=average,
area=city centre, familyFriendly=yes, near=Rainbow Vegetarian Caf´e,
The Punter is a family-friendly restaurant located in the city centre near Rainbow Vegetarian Caf´e. It
is cheap and has an average customer rating.
The Punter is an italian restaurant near Rainbow Vegetarian Caf´e in the city centre. It is family-friendly
and has a cheap price range and an average customer rating.
Rainbow Vegetarian Caf´e is a family-friendly restaurant called The Punter that serves Italian food and
has an average customer rating. It is located in the city centre.
The Punter is a cheap Italian restaurant in the city centre near Rainbow Vegetarian Caf´e. The Punter
is family friendly and has an average customer rating.
The Punter is an inexpensive Italian restaurant in the city centre near Rainbow Vegetarian Caf´e. It is
family friendly and has an average customer rating.
In the city centre is a family-friendly restaurant called The Punter. This is a cheap Italian restaurant
near Rainbow Vegetarian Caf´e. It has an average customer rating.
There is a cheap, restaurant that serves Italian, named The Punter, in the city centre near Rainbow
Vegetarian Caf´e. It has an average customer rating and is family friendly

Table 12: Randomly sampled output. A meaning representation is sampled at random, the respective utterance from each
system is displayed.

33System Utterance

#
1 MR

vanilla

tgen

utt

follow

form

full

human

2 MR

vanilla

tgen

utt

follow

form

full

human

3 MR

vanilla

tgen

utt

follow

form

full

human

customer

rating=average,

familyFriendly=yes,

eatType=coffee shop,

name=The Cricketers,
near=Caf´e Sicilia,
The Cricketers is a coffee shop near Caf´e Sicilia. It is family friendly and has an average customer
rating.
The Cricketers is a family friendly coffee shop near Caf´e Sicilia with an average customer rating.
If you are looking for a child friendly coffee shop near Caf´e Sicilia, try The Cricketers. It has an
average customer rating.
The Cricketers is a coffee shop near Caf´e Sicilia that is family friendly and has an average customer
rating.
The Cricketers is a coffee shop near Caf´e Sicilia. It is family friendly and has an average customer
rating.
For a family friendly coffee shop near Caf´e Sicilia, try The Cricketers. Customers have rated it as
average.
Near Caf´e Sicilia there is a coffee shop named The Cricketers. It’s family friendly with an average
rating.
name=The Waterman, eatType=restaurant, food=Italian, priceRange=less than 20, area=riverside,
familyFriendly=no, near=Raja Indian Cuisine,
The Waterman is a restaurant providing Italian food in the less than 20 price range. It is located in the
riverside. It is near Raja Indian Cuisine.
The Waterman is an italian restaurant in the riverside area near Raja Indian Cuisine. It is not family-
friendly and has a price range of less than 20.
Italian restaurant The Waterman is located in the riverside area near Raja Indian Cuisine. It is not
family-friendly and has a price range of less than 20.
The Waterman is a restaurant located near Raja Indian Cuisine in the riverside area. The price range
is less than 20. They serve Italian food and are not family-friendly.
The Waterman is a restaurant providing Italian food in the low price range. It is located in the riverside
area near Raja Indian Cuisine. It is not family friendly.
Italian food is served at The Waterman restaurant in the riverside area near Raja Indian Cuisine. Prices
are under 20. Not family-friendly.
The Waterman is a mature Italian restaurant near the riverside with a competitive menu for under 20,
near the Raja Indian Cuisine.
name=The Wrestlers, eatType=pub, food=Italian, priceRange=less than 20, area=riverside, fami-
lyFriendly=yes, near=Raja Indian Cuisine,
The Wrestlers is a family friendly pub near Raja Indian Cuisine in the riverside area that serves Italian
food for less than 20.
The Wrestlers is a family-friendly pub near Raja Indian Cuisine in the riverside area. It serves italian
food for less than 20.
Italian food is served at The Wrestlers pub located near Raja Indian Cuisine in the riverside area. It is
family friendly and has a price range of less than 20.
The Wrestlers is a pub that serves Italian food. They are located in the riverside area near Raja Indian
Cuisine. They are family friendly and the price range is less than 20.
The Wrestlers is a family friendly pub serving Italian food in the low price range. It is located in the
riverside area near Raja Indian Cuisine.
On the riverside near Raja Indian Cuisine is a family friendly pub called The Wrestlers. The price
range is less than 20 and they serve Italian food.
The Wrestlers is a pub in the low price range that serves pasta. It is located near Raja Indian Cuisine
and has a public restroom.

Table 13: Randomly sampled output. A meaning representation is sampled at random, the respective utterance from each
system is displayed.

34
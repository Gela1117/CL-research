Understanding Convolutional Neural Networks for Text Classiﬁcation

Alon Jacovi1,2

Oren Sar Shalom2,3

Yoav Goldberg1,4

1 Computer Science Department, Bar Ilan Univesity, Israel

2 IBM Research, Haifa, Israel
3 Intuit, Hod HaSharon, Israel

{alonjacovi,oren.sarshalom,yoav.goldberg}@gmail.com

4 Allen Institute for Artiﬁcial Intelligence

Abstract

We present an analysis into the inner workings
of Convolutional Neural Networks (CNNs) for
processing text. CNNs used for computer vi-
sion can be interpreted by projecting ﬁlters
into image space, but for discrete sequence in-
puts CNNs remain a mystery. We aim to un-
derstand the method by which the networks
process and classify text. We examine com-
mon hypotheses to this problem: that ﬁlters,
accompanied by global max-pooling, serve as
ngram detectors. We show that ﬁlters may
capture several different semantic classes of
ngrams by using different activation patterns,
and that global max-pooling induces behav-
ior which separates important ngrams from the
rest. Finally, we show practical use cases de-
rived from our ﬁndings in the form of model
interpretability (explaining a trained model by
deriving a concrete identity for each ﬁlter,
bridging the gap between visualization tools
in vision tasks and NLP) and prediction inter-
pretability (explaining predictions).
Introduction

1
Convolutional Neural Networks (CNNs), origi-
nally invented for computer vision, have been
shown to achieve strong performance on text clas-
siﬁcation tasks (Bai et al., 2018; Kalchbrenner
et al., 2014; Wang et al., 2015; Zhang et al.,
2015; Johnson and Zhang, 2015; Iyyer et al.,
2015) as well as other traditional Natural Lan-
guage Processing (NLP) tasks (Collobert et al.,
2011), even when considering relatively simple
one-layer models (Kim, 2014).

As with other architectures of neural networks,
explaining the learned functionality of CNNs is
still an active research area. The ability to inter-
pret neural models can be used to increase trust in
model predictions, analyze errors or improve the
model (Ribeiro et al., 2016). The problem of inter-
pretability in machine learning can be divided into

two concrete tasks: Given a trained model, model
interpretability aims to supply a structured expla-
nation which captures what the model has learned.
Given a trained model and a single example, pre-
diction interpretability aims to explain how the
model arrived at its prediction. These can be fur-
ther divided into white-box and black-box tech-
niques. While recent works have begun to sup-
ply the means of interpreting predictions (Alvarez-
Melis and Jaakkola, 2017; Lei et al., 2016; Guo
et al., 2018), interpreting neural NLP models re-
mains an under-explored area.

Accompanying their rising popularity, CNNs
have seen multiple advances in interpretability
when used for computer vision tasks (Zeiler and
Fergus, 2014). These techniques unfortunately do
not trivially apply to discrete sequences, as they
assume a continuous input space used to represent
images. Intuitions about how CNNs work on an
abstract level also may not carry over from image
inputs to text—for example, pooling in CNNs has
been used to induce deformation invariance (Le-
Cun et al., 1998, 2015), which is likely different
than the role it has when processing text.

In this work, we examine and attempt to under-
stand how CNNs process text, and then use this in-
formation for the more practical goals of improv-
ing model-level and prediction-level explanations.
We identify and reﬁne current intuitions as to
how CNNs work. Speciﬁcally, current common
wisdom suggests that CNNs classify text by work-
ing through the following steps (Goldberg, 2016):
1) 1-dimensional convolving ﬁlters are used as
ngram detectors, each ﬁlter specializing in a
closely-related family of ngrams.

2) Max-pooling over time extracts the relevant

ngrams for making a decision.

3) The rest of the network classiﬁes the text

based on this information.

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages56–65Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics56We reﬁne items 1 and 2 and show that:
• Max-pooling induces a thresholding behav-
ior, and values below a given threshold are
ignored when (i.e.
irrelevant to) making a
prediction. Speciﬁcally, we show an exper-
iment for which 40% of the pooled ngrams
on average can be dropped with no loss of
performance (Section 4).

• Filters are not homogeneous, i.e. a single ﬁl-
ter can, and often does, detect multiple dis-
tinctly different families of ngrams (Section
5.3).

• Filters also detect negative items in ngrams—
they not only select for a family of ngrams
but often actively suppress a related family
of negated ngrams (Section 5.4).

We also show that the ﬁlters are trained to work
with naturally-occurring ngrams, and can be eas-
ily misled (made to produce values substantially
larger than their expected range) by selected non-
natural ngrams.

These ﬁndings can be used for improving
model-level and prediction-level interpretability
(Section 6). Concretely: 1) We improve model
interpretability by deriving a useful summary for
each ﬁlter, highlighting the kinds of structures it
is sensitive to. 2) We improve prediction inter-
pretability by focusing on informative ngrams and
taking into account also the negative cues.
2 Background: 1D Text Convolutions
We focus on the task of text classiﬁcation. We con-
sider the common architecture in which each word
in a document is represented as an embedding vec-
tor, a single convolutional layer with m ﬁlters is
applied, producing an m-dimensional vector for
each document ngram. The vectors are combined
using max-pooling followed by a ReLU activation.
The result is then passed to a linear layer for the ﬁ-
nal classiﬁcation.

For an n-words input text w1, ..., wn we embed
each symbol as d dimensional vector, resulting in
word vectors w1, ..., wn ∈ Rd. The resulting d×n
matrix is then fed into a convolutional layer where
we pass a sliding window over the text. For each
l-words ngram:
ui = [wi, ..., wi+(cid:96)−1] ∈ Rd×(cid:96) ; 0 ≤ i ≤ n − (cid:96)
And for each ﬁlter fj ∈ Rd×(cid:96) we calcu-
late (cid:104)ui, fj(cid:105). The convolution results in matrix

F ∈ Rn×m. Applying max-pooling across the
ngram dimension results in p ∈ Rm which is fed
into ReLU non-linearity. Finally, a linear fully-
connected layer W ∈ Rc×m produces the distri-
bution over classiﬁcation classes from which the
strongest class is outputted. Formally:

ui = [wi; ...; wi+(cid:96)−1]
Fij = (cid:104)ui, fj(cid:105)
pj = ReLU(max

Fij)

i

o = softmax(Wp)

In practice, we use multiple window sizes (cid:96) ∈ L,
L (cid:40) N by using multiple convolution layers in
parallel and concatenating the resulting p(cid:96) vectors.
We note that the methods in this work are applica-
ble for dilated convolutions as well.

3 Datasets and Hyperparameters

For our empirical experiments and results pre-
sented in this work we use three text classiﬁca-
tion datasets for Sentiment Analysis, which in-
volves classifying the input text (user reviews in
all cases) between positive and negative. The spe-
ciﬁc datasets were chosen for their relative variety
in size and domain as well as for the relative sim-
plicity and interpretability of the binary sentiment
analysis task.

The three datasets are: a) MR: sentence polarity
dataset v1.0 introduced by Pang and Lee (2005),
containing 10k evenly split short (sentences or
snippets) movie reviews. b) Elec: electronic prod-
uct reviews for sentiment classiﬁcation introduced
by Johnson and Zhang (2015), assembled from the
Amazon review dataset (McAuley and Leskovec,
2013; McAuley et al., 2015), containing 200k train
and 25k test evenly split reviews. c) Yelp Review
Polarity: introduced by Zhang et al. (2015) from
the Yelp Dataset Challenge 2015, containing 560k
train and 38k test evenly split business reviews.

For word embeddings, we use the pre-trained
GloVe Wikipedia 2014—Gigaword 5 embeddings
(Pennington et al., 2014), which we ﬁne-tune with
the model.

We use embedding dimension of 50, ﬁlter sizes
of (cid:96) ∈ {2, 3, 4} words, and m ∈ {10, 50} ﬁlters.
Models are implemented in PyTorch and trained
with the Adam optimizer.

574

Identifying Important Features

Current common wisdom posits that ﬁlters serve
as ngram detectors: each ﬁlter searches for a spe-
ciﬁc class of ngrams, which it marks by assigning
them high scores. These highest-scoring detected
ngrams survive the max-pooling operation. The ﬁ-
nal decision is then based on the set of ngrams in
the max-pooled vector (represented by the set of
corresponding ﬁlters). Intuitively, ngrams which
any ﬁlter scores highly (relative to how it scores
other ngrams) are ngrams which are highly rele-
vant for the classiﬁcation of the text.

In this section we reﬁne this view by attempting
to answer the questions: what information about
ngrams is captured in the max-pooled vector, and
how is it used for the ﬁnal classiﬁcation?1

Informative vs. Uninformative Ngrams

4.1
Consider the pooled vector p ∈ Rm on which
the classiﬁcation is based. Each value pj =
ReLU(maxi(cid:104)ui, fj(cid:105)) stems from a ﬁlter-ngram in-
teraction, and can be traced back to the ngram
ui = [wi, ..., wi+(cid:96)−1] that triggered it. Denote the
set of ngrams contributing to p as Sp. Ngrams not
in Sp do not inﬂuence the decision of the classiﬁer.
But what about the ngrams that are in Sp? Previ-
ous attempts in prediction-based interpretation of
CNNs for text highlight the ngrams in Sp and their
scores as means of explaining the prediction. We
take here a more reﬁned view. Note that the ﬁnal
classiﬁcation does not observe the ngram identi-
ties directly, but only through the scores assigned
to them by the ﬁlters. Hence, the information in p
must rely on the assigned scores.

Conceptually, we separate ngrams in Sp into

two classes, deliberate and accidental.
Deliberate ngrams end up in Sp because they
were scored high by their ﬁlter, likely because they
are informative regarding the ﬁnal decision.
In
contrast, accidental ngrams end up in Sp despite
having a low score, because no other ngram scored
higher than them. These ngrams are likely not in-
formative for the classiﬁcation decision. Can we
tease apart the deliberate and accidental ngrams?

1Although this work focuses on text classiﬁcation, the
ﬁndings in this section apply to any neural architecture which
utilizes global max pooling, for both discrete and continuous
domains. To our knowledge this is the ﬁrst work that exam-
ines the assumption that max-pooling induces classifying be-
havior. Previously, Ruderman et al. (2018) showed that other
assumptions to the functionality of max-pooling as deforma-
tion stabilizers (relevant only in continuous domains) do not
necessarily hold true.

We assume that there is threshold for each ﬁlter,
where values above the threshold signal informa-
tive information regarding the classiﬁcation, while
values below the threshold are uninformative and
can be ignored for the purpose of classiﬁcation.
We thus search for the threshold that separate the
two classes. However, as we cannot measure di-
rectly which values pj inﬂuence the ﬁnal decision,
we opt instead for measuring correlation between
pj values and the predicted label for the vector p.
The linearity of the decision function Wp al-
lows to measure exactly how much pj is weighted
for the logit of label class k. The class which ﬁlter
2. We refer
fj contributes to is cj = arg maxk Wkj
to class cj as the class identity of ﬁlter fj.

By assigning each ﬁlter a class identity cj and
comparing it to the predicted label we arrive at
a correlation label—whether the ﬁlter’s identity
class matches the ﬁnal decision by the network.
Concretely, we run the classiﬁer over a set of texts,
resulting in pooled vectors pi and network predic-
tions ci. For each ﬁlter j we then consider the val-
j and whether ci = cj. For each ﬁlter, we
ues pi
obtain a dataset (p1
j , cD = cj),
and we look for a threshold tj that separates pi
j for
which ci = cj from those where ci (cid:54)= cj.

j , c1 = cj), ..., (pD

j, ci = cj) | j < m & i < D}

(X, Y )j = {(pi
In an ideal case, the set is linearly separable
and we can easily separate informative from un-
j > tj then the classiﬁer’s
informative values: if pi
prediction agrees with the ﬁlter’s label, and oth-
erwise they disagree.
In practice, the set is not
separable. We instead work with the purity of a
ﬁlter-threshold combination, deﬁned as the per-
centage of informative (correlative) ngrams which
were scored above the threshold3. Formally, given
threshold dataset (X, Y ):

purity(f, t) =

|{(x, y) ∈ (X, Y )f | x ≥ t & y = true}|

|{(x, y) ∈ (X, Y )f | x ≥ t}|

We heuristically set the threshold of a ﬁlter to
the lowest value that achieves a sufﬁciently high

2In the case of non-linear fully-connected layers,

the
question of how each feature contributes to each class is
signiﬁcantly harder to answer. Possible methods include
saliency map methods or gradient-based methods. Re-
cently, Guo et al. (2018) has attributed labels to ﬁlters using
Bayesian inference and other image annotations.

3The purity metric can be considered as the precision met-

ric for this task.

58purity (we experimentally ﬁnd that a purity value
of 0.75 works well).

In Figure 2b,c we show examples for threshold
datasets for a model trained on the MR sentiment
analysis task.

described

is the threshold assumption—that

Threshold Effectiveness We
a
method for obtaining per-ﬁlter threshold values.
But
items
below a given threshold do not participate in the
decision—even correct? To assess the quality of
threshold obtained by our proposal and validate
the thresholding assumption, we discard values
that do not pass the threshold for each ﬁlter and
observe the performance of the model. Practi-
cally, we replace the ReLU non-linearity with a
threshold function:

(cid:40)

if x ≥ t
otherwise

x,
0,

threshold(x, t) =

Figure 1 presents the results on the MR dataset
(we observed similar results on the Elec dataset).
where the threshold is set for each ﬁlter separately,
based on a shared purity value. If the threshold-
ing assumption is correct and our way of deriv-
ing the threshold is effective, we expect to not see
a drop in accuracy.
Indeed, for purity value of
0.75, we observe that the model performance im-
proves slightly when replacing the ReLU with a
per-ﬁlter threshold, indicating that the threshold-
ing model is indeed a good approximation for the
feature behavior. The percentage of informative
(non-accidental) values in p is roughly a linear
function of the purity (Figure 1c). With a purity
value of 0.754, we discard roughly 44% of the val-
ues in p—and hence 44% of the ngrams in Sp.

Not all ﬁlters behave in a similar way, however.
In Figure 2 we show an example for a ﬁlter—#6
in the ﬁgure—which is especially uninformative:
by applying the lowest threshold which satisﬁes a
purity of 0.75, we discard 99.99% of activations.
Therefore in the experiments in Figure 1, this ﬁlter
is effectively unused, yet it does not cause loss in
performance. In essence, the threshold classiﬁer

4We note that empirically and intuitively, the more ﬁlters
we utilize in the network, the less correlation there is between
each ﬁlter’s class and the ﬁnal classiﬁcation, as the decision is
being made by a greater consensus. This means that demand-
ing a higher purity will be accompanied by lower coverage,
relative to other experiments, and more ngrams will be dis-
carded. The “correct” purity level for a ﬁlter then is a func-
tion of the model and dataset used, and should be investigated
using the train or validation datasets.

identiﬁed and effectively discarded a ﬁlter which
is not useful to the model.

To summarize, we validated our assumptions
and shown empirically that global max-pooling in-
deed induces a functionality of separating impor-
tant and not important activation signals using a
latent (presumably soft) threshold. For the rest of
this work we will assume a known threshold value
for every ﬁlter in the model which we can use to
identify important ngrams.

5 What is captured by a ﬁlter?

Previous work looked at the top-k scoring ngrams
for each ﬁlter. However, focusing on the top-k
does not tell a complete story. We insead look at
the set of deliberate ngrams: those that pass the ﬁl-
ter’s threshold value. Common intuition suggests
that each ﬁlter is homogeneous and specializes in
detecting a speciﬁc classes of ngrams. For exam-
ple, a ﬁlter may specializing in detecting ngrams
such as “had no issues”, “had zero issues”, and
“had no problems”. We challenge this view and
show that ﬁlters often specialize in multiple dis-
tinctly different semantic classes by utilizing ac-
tivation patterns which are not necessarily max-
imized. We also show that ﬁlters may not only
identify good ngrams, but may also actively su-
press bad ones.

5.1 Slot Activation Vectors
As discussed in Section 2, for each ngram u =
[w1, ..., w(cid:96)] and for each ﬁlter f we calculate the
score (cid:104)u, f(cid:105). The ngram score can be decomposed
as a sum of individual word scores by considering
the inner products between every word embedding
wi in u and every parallel slice in f:

(cid:104)u, f(cid:105) =

(cid:104)wi, fid:i(d+1)(cid:105)

i=0

We refer to slice fid:i(d+1) as slot i of the ﬁl-
ter weights, denoted as f (i). Instead of taking the
sum of these inner products, we can instead inter-
pret them directly—saying that (cid:104)wi, f (i)(cid:105) captures
how much slot i in f is activated by the ith word
in the ngram5.

We can now move from examining the
activation of
:=
[w1; ...; w(cid:96)], f(cid:105) to examining its slot activation
vector: ((cid:104)w1, f (1)(cid:105), ...,(cid:104)w(cid:96), f ((cid:96))(cid:105)). The slot ac-

an ngram-ﬁlter pair

(cid:104)u

(cid:96)−1(cid:88)

59(a)

(b)

(c)

Figure 1: Evaluation results for identifying important ngrams on the MR model.

(a)

(b)

(c)

Figure 2: Visualization of informative and uninformative ﬁlters for the MR model and a universal purity of 0.75.
In (a) we show the percentage of pooled ngrams which pass the threshold per ﬁlter. The threshold datasets of ﬁlters
#0 and #6 are shown in (b) and (c) respectively.

tivation vector captures how much each word in
the ngram contributes to its activation.

top-scoring natural ngrams almost never fully ac-
tivate all slots in a ﬁlter.

5.2 Naturally occurring vs. possible ngrams
We distinguish naturally occurring or observed
ngrams, which are ngrams that are observed in a
large corpus, from possible ngrams which are any
combination of (cid:96) words from the vocabulary. The
possible ngrams are a superset of the naturally oc-
curring ones. Given a ﬁlter, we can ﬁnd its top-
scoring naturally occurring ngram by searching
over all ngrams in a corpus. We can ﬁnd its top-
scoring possible ngram by maximizing each slot
value individually. We observe there is a big and
consistent gap in scores between the top-scoring
natural ngrams and top-scoring possible ngrams.
In our Elec model, when averaging over all ﬁlters,
the top naturally-occurring ngrams score 30% less
Interestingly, the
than the top possible ngrams.

5 We note that this breakdown does not consider the ﬁl-
ter’s bias, if one is used. This bias is a single number (per
ﬁlter) which is added to the sum of slot activations to arrive
at the ngram activation which is passed to the max-pooling
layer. Bias can be accommodated by appending an additional
“bias word” with an embedding vector of [1, ..., 1] to every
ngram. Regardless, as this bias is identical for all ngrams
for the ﬁlter in question, it has no role in identifying which
ngrams the ﬁlter is most similar to, and we can ignore it in
this context.

Table 1 shows the top-scoring naturally occur-
ring and possible ngrams for nine ﬁlters in the Elec
model. In each of the top scoring natural ngrams,
at least one slot receives a low activation. Table 2
zooms in on one of the ﬁlters and shows its top-
7 naturally occurring ngrams and top-7 most acti-
vated words in each slot. Here, most top-scoring
ngrams maximize slot #3 with words such as in-
valuable and perfect, however some ngrams such
as “works as good” and “still holding strong” max-
imize slots #1 and #2 respectively, instead.

Additionally, most top-scoring words do not ap-
pear to be utilized in high-scoring ngrams at all.
This can be explained with the following:
if a
word such as crt rarely or never appears in slot #1
alongside other high-scoring words in other slots,
then crt can score highly with no consequence.
Since an ngram containing crt at slot #1 will rarely
pass the max-pooling layer, its score at that slot is
essentially random.

On naturally occurring ngrams, the ﬁlters do not
achieve maximum values in all slots but only on
some of them. Why? We consider two hypotheses
to explain this behavior:

600.00.20.40.60.8purity0.004750.005000.005250.005500.00575train loss0.00.20.40.60.8purity0.7450.7500.7550.7600.7650.770test accuracy0.50.60.70.80.9purity406080100average n-gram coverage02468filter0102030405060n-gram coverage02000400060008000training example0123max n-gram activationpositivenegativethreshold02000400060008000training example0.0500.0250.0000.0250.0500.075max n-gram activationpositivenegativethresholdtop ngrams

ﬁlter

0
1
2
3
4
5
6
7
8

ngram
poorly designed junk
simply would not
a minor drawback
still working perfect
absolutely gorgeous .
one little hitch
utterly useless .
deserves four stars
a mediocre product

score
7.31
5.75
6.11
6.42
5.36
5.72
6.33
5.56
6.91

slot scores

slot #1

5.47
2.16
0.88
1.58
1.09
0.98
2.03
0.44
0.35

0.97
1.28
1.85
1.22
3.84
3.43
3.49
1.69
3.11

poorly
chapters

0.87
2.3
3.38 workstation
3.62
0.42
1.31
0.81
3.44
3.45

saves
complain
path
stopped
excelente
began

top words by slot

slot #2

slot #3

displaying
5.47
avoid
2.31
high-quality
2.06
delight
2.52
gorgeous
2.57
delight
2.81
refund
2.77
1.89
crossover
1.86 mediocre

3.06
3.07
3.82
2.29
3.84
4.09
3.81
1.93
3.11

landﬁll
impossible
drawback
invaluable
expect
everyday
disabled
incredible
product

1.75
3.06
3.39
4.19
1.22
2.64
1.38
3.96
3.45

sum
10.28
8.44
9.27
9.0
7.63
9.54
7.96
7.78
8.42

Table 1: Top ngrams and words by ﬁlter from a sample of nine ﬁlters from the Elec model. The average difference
between the top natural ngram activation and the top possible ngram activation for this model is 2.5, or a 30%
average reduction.

ngram
still working perfect
works - perfect
isolation proves invaluable
still near perfect
still working great
works as good
still holding strong

top ngrams
score
6.42
5.78
5.61
5.6
5.45
5.44
5.37

rank

1
2
3
4
5
6
7

slot scores

1.58
1.91
0.39
1.58
1.58
1.91
1.58

1.22
0.25
1.03
0.4
1.22
1.45
1.81

3.62
3.62
4.19
3.62
2.65
2.08
1.98

slot #1

top words by slot

slot #2

saves
crt
beginner
mics
genius
ﬁnal
works

delight
2.52
holding
2.1
2.09 welcome
2.08
2.07
2.01
1.91 well-made

dhcp
completely
cradle

2.29
1.81
1.8
1.72
1.64
1.56
1.51

slot #3
invaluable
perfect
cm
pleasant
simplicity
england
daily

4.19
3.62
3.61
3.38
3.14
3.09
3.04

Table 2: Top-k words by slot scores and top-k ngrams by ﬁlter scores from the Elec model. In bold are words from
the top-k ngrams which appear in the top-k slot words - i.e. words which maximize their slot.

(i) Each ﬁlter captures multiple semantic classes
of ngrams, and each class has some domi-
nating slots and some non-dominating slots
(which we deﬁne as a slot activation pattern).

(ii) A slot may not be maximized because it’s not
used to detect word existence, but rather lack
of existence—ensuring that speciﬁc words do
not occur.

We investigate both hypotheses in Sections 5.3 and
5.4 respectively.
Adversarial potential We note in passing that
this discrepancy in scores between naturally oc-
curring and possible ngrams can be used to derive
adversarial examples that cause a trained model
to misclassify. By inserting a few seemingly ran-
dom ngrams, we can cause ﬁlters to activate be-
yond their expected range, potentially driving the
model to misclassiﬁcation. We reserve this area of
exploration for future work.

5.3 Clustering (Hypothesis (i))
We explore hypothesis (i) by clustering threshold-
passing (naturally occurring) ngrams in each ﬁl-
ter according to their activation vectors. We use
Mean Shift Clustering (Fukunaga and Hostetler,

1975; Cheng, 1995), an algorithm that does not
require specifying an a-priori number of clusters,
and does not make assumptions about their shapes.
Mean Shift considers the feature vectors as sam-
pled from an underlying probability density func-
tion6. Each cluster captures a different slot activa-
tion pattern. We use the cluster’s centroid as the
prototypical slot activation for that cluster.

Table 3 shows a sample clustering output. The
clustering algorithm identiﬁed two clusters: one
primarily containing ngrams of the pattern DET
INTENSITY-ADVERB POSITIVE-WORD, while
the second contains ngrams that begin with
phrases like go wrong.7

The centroids for these clusters capture the acti-
vation patterns well: low-medium-high and high-
high-low for clusters 1 and 2 respectively.

To summarize,
by discarding noisy ngrams
which do not pass the ﬁlter’s threshold and then
clustering those that remain according to their slot
activation patterns, we arrived at a clearer image

6Intuitively, we can think of the sampling noise as the
ngram embeddings, and the probability distribution as de-
ﬁned by a function of the ﬁlter weights.

7In the Yelp dataset, go wrong overwhelmingly occurs in
a negated context such as “can’t go wrong” and “won’t go
wrong”, which explains why it is detected by a positive ﬁlter.

61ngram
centroid
was super intriguing
am so grateful
overall very worth
also well worth
- super compassionate
a well oiled
centroid
go wrong bringing
go wrong pairing
go wrong when

slot #1
0.75
1.01
2.59
3.84
1.83
0.51
0.75
2.87
3.97
3.97
3.97

slot #2
1.97
3.16
3.27
1.86
3.06
3.17
3.06
2.17
4.12
4.12
4.12

slot #3
2.79
5.84
4.07
4.22
4.22
5.01
4.84
0.12
1.81
1.65
-0.4

cluster

1
1
1
1
1
1
1
2
2
2
2

Table 3: Example clustering results on the Yelp dataset.
After applying thresholds, the ngrams for this ﬁlter
were split into two clusters of sizes 83% and 17% re-
spectively. The table shows top-scoring ngrams for this
ﬁlter with their clustering results, sorted by their acti-
vation strength.

of the semantic classes of ngrams that a given ﬁl-
ter specializes in capturing. In particular, we re-
veal that ﬁlters are not necessarily homogeneous:
a single ﬁlter may detect several different seman-
tic patterns, each one of them relying on a different
slot activation pattern.

5.4 Negative Ngrams (Hypothesis (ii))
Our second theory to explain the discrepancy be-
tween the activations of naturally occurring and
possible ngrams is that certain ﬁlter slots are not
used to detect a class of highly activating words,
but rather to rule out a class of highly negative
words. We refer to these as negative ngrams.

For example, Table 3 shows an ngram pattern
for which slot #1 contains determiners and other
“ﬁller” tokens such as hyphens, periods and com-
mas with relatively weak slot activations. Hypoth-
esis (ii) suggests that this slot may receive a strong
negative score for words such as not and n’t, caus-
ing such negated patterns to drop below the thresh-
old. Indeed, ngrams containing not or n’t in slot #1
do not pass the threshold for this ﬁlter.

We are interested in a more systematic method
of identifying these cases.
Identifying negative
slot activations would be very useful for under-
standing the semantics captured by a ﬁlter and the
reasoning behind the dismissal of an ngram, as we
discuss in Sections 6.1 and 6.2 respectively.

We achieve this by searching the below-
threshold ngram space for ngrams which are
“ﬂipped versions” of above-threshold ngrams.
Concretely: Given ngram u which was scored
highly by ﬁlter f, we search for low-scoring

ngrams u(cid:48) such that the hamming distance be-
tween u and u(cid:48) is low. By doing this for the top-
k scoring ngrams per cluster, we arrive at a com-
prehensive set of negative ngrams. In Table 4 we
show a sample output of this algorithm.

Furthermore, we can divide negative ngrams
into two cases: 1) Lowering the ngram score be-
low the threshold by replacing high-scoring words
with low-scoring words. 2) Lowering the ngram
score below the threshold by replacing words with
a low positive score with words with a highly-neg-
ative score. Case 2 is more interesting because
it embodies cases where hypothesis (ii) is rele-
vant. Additionally, it highlights ngrams where a
strongly positive word in one slot was negated
with another strongly negative word in another
slot. Table 4 shows examples in bold.

In order to identify “Case 2” negative ngrams,
we heuristically test whether
the “changed”
words’ scores directly inﬂuence the status of the
activation relative to the threshold: given an al-
ready identiﬁed negative ngram,
if the ngram
score—sans the bottom-k negative slot activations
(considering a hamming distance of k and given
that there are k negative slot activations)—passes
the threshold, yet it does not pass the threshold
by including the negative slot activations, then the
ngram is considered a “Case 2” negative ngram.

6

Interpretability

In this section we show two practical implica-
tions of the ﬁndings above: improvements in both
model-level and prediction-level interpretability
of 1D CNNs for text classiﬁcation.

6.1 Model Interpretability
As in computer vision, we can now interpret a
trained CNN model by “visualizing” its ﬁlters and
interpreting the visible shapes—in other words,
deﬁning a high-level description of what the ﬁlter
detects. We propose to associate each ﬁlter with
the following items: 1) The class which this ﬁl-
ter’s strong signals contribute to (in the sentiment
task: positive or negative); 2) The threshold value
for the ﬁlter, together with its purity and cover-
ages percentages (which essentially capture how
informative this ﬁlter is); 3) A list of semantic pat-
terns identiﬁed by this ﬁlter. Each list item corre-
sponds to a slot-activations cluster. For each clus-
ter we present the top-k ngrams activating it, and
for each ngram we specify its total activation, its

62ngram
’m really pleased
’m really not
’m really upset
’m not pleased
is extremely useful
is extremely limited
is extremely noisy
is not useful
is only useful
is surprisingly good
is not good
is only good
is no good
is probably good
am very satisﬁed
am very dissatisﬁed
am very disappointed
am not satisﬁed
not very satisﬁed

slot #1
2.59

slot #2
1.86

-3.4
3.24

-3.4
-2.82
4.32
-3.4
-2.82
-1.88
-1.66
2.17

-3.4

2.3

2.3

2.01

-2.6

slot #3
5.05
-2.49
-1.14

3.96
-2.8
-2.77

2.8

5.09
-1.9
-1.87

sum
9.5
1.96
3.31
4.24
9.5
2.74
2.8
2.86
3.44
9.42
1.7
2.28
3.22
3.44
9.26
2.27
2.3
3.69
4.66

Table 4: Top-scoring ngrams from one ﬁlter from a
model trained on the Elec dataset, and their accompa-
nying lowest-scoring negative ngrams. We selected a
hamming distance of 1 word. Bold ngrams are Case 2
negative ngrams.

slot-activation vector, and its list of bottom-k neg-
ative ngrams with their activations and slot acti-
vations. In particular, by clustering the activated
ngrams according to their slot activation patterns
and showing the top-k in each clusters, we get a
much more reﬁned coverage of the linguistic pat-
terns that are captured by the ﬁlter.

6.2 Prediction Interpretability
Previous prediction-based interpretation attempts
traced back the ngrams from the max-pooling
layer. Here we improve these previous attempts
by considering only ngrams that pass the threshold
for their ﬁlter. This results in a more concise and
relevant explanation (Figure 1). Figure 3 shows
two examples. Note that in example #1, many
negative-class ﬁlters were “forced” to choose an
ngram in max-pooling despite there not being
strongly negative phrases—but those ngrams do
not pass the threshold and are thus cleaned from
the explanation.

Additionally we can use the individual slot ac-
tivations to tease-apart the contribution of each
word in the ngram. Finally, we can also mark
cases of negative-ngrams (Section 5.4), where an
ngram has high slot activations for some words,
but these are negated by a highly-negative slot and

f-class

slot scores

my UNK ﬁts perfectly . very well made . nice looking and
offers good protection
ngram
ﬁlter
0.7
PAD PAD my
0.98
. very well
1.31
PAD my UNK
0.28
UNK ﬁts perfectly
looking and offers
0.6
good protection PAD 0.52
-0.06
UNK ﬁts perfectly
ﬁts perfectly .
1.34
-0.01
. very well
4.13
perfectly . very

1.65
2.17
-0.07
0.61
0.12
1.6
2.36
-0.71
1.97
0.45

pos
pos
neg
neg
neg
neg
pos
neg
neg
pos

0.16
2.63
0.21
0.03
0.5
-0.01
1.82
1.47
-0.55
-0.01

0
1
2
3
4
5
6
7
8
9

this product sucked was not loud at all lights did n’t work
overall a bad product that ’s UNK taking up space
ﬁlter

slot scores

f-class

0
1
2
3
4
5
6
7
8
9

pos
pos
neg
neg
neg
neg
pos
neg
neg
pos

ngram
product sucked was
overall a bad
lights did n’t
PAD this product
did n’t work
sucked was not
work overall a
was not loud
a bad product
PAD PAD this

0.12
2.53
-0.33
-0.2
1.21
0.98
-0.25
-0.33
-0.45
0.38

2.05
1.4
1.12
1.43
0.97
0.59
4.05
2.85
3.08
0.15

0.1
-1.16
1.63
0.51
2.65
1.32
-0.21
0.52
1.32
1.66

Figure 3: Examples predicted positive and negative
respectively by a model trained on the Elec dataset,
along with their explanations. Ngrams which passed
the threshold are in bold, and case 2 negative ngrams
are in italics. For clarity’s sake we trained a small
model which uses ten ﬁlters.

as a consequence are not selected by max-pooling,
or are selected but do not pass the ﬁlter’s thresh-
old.

7 Conclusion

We have reﬁned several common wisdom assump-
tions regarding the way in which CNNs process
and classify text. First, we have shown that max-
pooling over time induces a thresholding behavior
on the convolution layer’s output, essentially sep-
arating between features that are relevant to the
ﬁnal classiﬁcation and features that are not. We
used this information to identify which ngrams are
important to the classiﬁcation. We also associate
each ﬁlter with the class it contributes to. We de-
compose the ngram score into word-level scores
by treating the convolution of a ﬁlter as a sum
of word-level convolutions, allowing us to exam-
ine the word-level composition of the activation.
Speciﬁcally, by maximizing the word-level acti-
vations by iterating over the vocabulary, we ob-
served that ﬁlters do not maximize activations at

63the word-level, but instead form slot activation
patterns that give different types of ngrams similar
activation strengths. This provides empirical evi-
dence that ﬁlters are not homogeneous. By clus-
tering high-scoring ngrams according to their slot-
activation patterns we can identify the groups of
linguistic patterns captured by a ﬁlter. We also
show that ﬁlters sometimes opt to assign nega-
tive values to certain word activations in order to
cause the ngrams which contain them to receive a
low score despite having otherwise highly activat-
ing words. Finally, we use these ﬁndings to sug-
gest improvements to model-based and prediction-
based interpretability of CNNs for text.

References
David Alvarez-Melis and Tommi S. Jaakkola. 2017.
A causal framework for explaining the predic-
tions of black-box sequence-to-sequence models.
In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017, pages 412–421. Association for Com-
putational Linguistics.

Shaojie Bai, J. Zico Kolter, and Vladlen Koltun.
2018. An empirical evaluation of generic convolu-
tional and recurrent networks for sequence model-
ing. CoRR, abs/1803.01271.

Yizong Cheng. 1995. Mean shift, mode seeking, and
clustering. IEEE Trans. Pattern Anal. Mach. Intell.,
17(8):790–799.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12:2493–2537.

Keinosuke Fukunaga and Larry D. Hostetler. 1975.
The estimation of the gradient of a density func-
tion, with applications in pattern recognition. IEEE
Trans. Information Theory, 21(1):32–40.

Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. J. Artif. In-
tell. Res., 57:345–420.

Pei Guo, Connor Anderson, Kolten Pearson, and
Ryan Farrell. 2018. Neural network interpreta-
tion via ﬁne grained textual summarization. CoRR,
abs/1805.08969.

Mohit Iyyer, Varun Manjunatha, Jordan L. Boyd-
Graber, and Hal Daum´e III. 2015. Deep unordered
composition rivals syntactic methods for text clas-
siﬁcation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics

and the 7th International Joint Conference on Nat-
ural Language Processing of the Asian Federation
of Natural Language Processing, ACL 2015, July
26-31, 2015, Beijing, China, Volume 1: Long Pa-
pers, pages 1681–1691. The Association for Com-
puter Linguistics.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In NAACL HLT 2015, The
2015 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Denver, Colorado,
USA, May 31 - June 5, 2015, pages 103–112. The
Association for Computational Linguistics.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
In Proceedings of the 52nd
modelling sentences.
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
655–665. The Association for Computer Linguis-
tics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751. ACL.

Yann LeCun, Y Bengio, and Geoffrey Hinton. 2015.

Deep learning. 521:436–44.

Yann LeCun, Leon Bottou, Y Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. 86:2278 – 2324.

Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
In Pro-
2016. Rationalizing neural predictions.
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
107–117. The Association for Computational Lin-
guistics.

Julian J. McAuley and Jure Leskovec. 2013. Hidden
factors and hidden topics: understanding rating di-
mensions with review text. In Seventh ACM Confer-
ence on Recommender Systems, RecSys ’13, Hong
Kong, China, October 12-16, 2013, pages 165–172.
ACM.

Julian J. McAuley, Christopher Targett, Qinfeng Shi,
and Anton van den Hengel. 2015. Image-based rec-
In Pro-
ommendations on styles and substitutes.
ceedings of the 38th International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, Santiago, Chile, August 9-13, 2015,
pages 43–52. ACM.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. CoRR, abs/cs/0506075.

64Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Marco T´ulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. ”why should I trust you?”: Explain-
In Proceed-
ing the predictions of any classiﬁer.
ings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
San Francisco, CA, USA, August 13-17, 2016, pages
1135–1144. ACM.

Avraham Ruderman, Neil C. Rabinowitz, Ari S. Mor-
cos, and Daniel Zoran. 2018. Learned deformation
stability in convolutional neural networks. CoRR,
abs/1804.04438.

Peng Wang, Jiaming Xu, Bo Xu, Cheng-Lin Liu, Heng
Zhang, Fangyuan Wang, and Hongwei Hao. 2015.
Semantic clustering and convolutional neural net-
In Proceed-
work for short text categorization.
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers, pages 352–357. The
Association for Computer Linguistics.

Matthew D. Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Com-
puter Vision - ECCV 2014 - 13th European Con-
ference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part I, volume 8689 of Lecture Notes
in Computer Science, pages 818–833. Springer.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun.
2015. Character-level convolutional networks for
In Advances in Neural Infor-
text classiﬁcation.
mation Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada,
pages 649–657.

65
Natural Language Inference with Deﬁnition Embedding

Considering Context On the Fly

Kosuke Nishida, Kyosuke Nishida, Hisako Asano,

Junji Tomita

NTT Media Intelligence Laboratories, NTT Corporation

1-1 Hikarinooka Yokosuka, Kanagawa, Japan

nishida.kosuke@lab.ntt.co.jp

Abstract

Natural language inference (NLI) is one
of the most important tasks in NLP. In
this study, we propose a novel method us-
ing word dictionaries, which are pairs of a
word and its deﬁnition, as external knowl-
edge. Our neural deﬁnition embedding
mechanism encodes input sentences with
the deﬁnitions of each word of the sen-
tences on the ﬂy. It can encode deﬁnitions
of words considering the context of the in-
put sentences by using an attention mech-
anism. We evaluated our method using
WordNet as a dictionary and conﬁrmed
that it performed better than baseline mod-
els when using the full or a subset of 100d
GloVe as word embeddings.

1

Introduction

Recognition of the entailment relationship be-
tween two sentences is one of the most impor-
tant tasks in the ﬁeld of natural language process-
ing. An understanding of entailment relationships
among sentences is useful for performing tasks
such as question answering, information retrieval,
and summarization.

The task of recognizing the entailment relation-
ship between two sentences is called recognizing
textual entailment (RTE) or natural language in-
ference (NLI). NLI has recently been getting more
attention from researchers, owing to the release of
large-scale corpora such as SNLI (Bowman et al.,
2015) and MNLI (Williams et al., 2018).

These corpora consist of pairs of sentences,
such as ‘A soccer game with multiple males
playing.’ and ‘Some men are playing a sport.’, and
ground-truth labels. Each label is a judgment of
whether the latter sentence, which is the premise,

is inferred from the former one, which is the hy-
pothesis. In this example, the label is ‘entailment’.
In this study, we propose a novel method that
uses word dictionaries as external knowledge.
Word dictionaries are useful for domain adapta-
tion, where we need to understand rare or novel
words in which we do not have good embedding
representations. For NLI, there is related work
that does use dictionaries (Bahdanau et al., 2017).
In it, a deﬁnition embedding method is proposed
that obtains representations of out-of-vocabulary
(OOV) words from dictionaries on the ﬂy. In this
method, however, the description of a word is con-
verted into the same embedding anytime without
considering the context of the input sentences.

On the other hand, we consider that word repre-
sentation from dictionaries should reﬂect the con-
text of the input sentences. In the dictionary, we
can explain the meaning of a word from many as-
pects. However, the required information varies
depending on the context of the input sentences.
This problem also occurs for pre-trained word em-
beddings, which are usually ﬁxed for all contexts
in the previous studies.

The proposed method can obtain different rep-
resentations of words according to the contexts
of the input sentences. It introduces an attention
mechanism that improves the encoded representa-
tions of each word in input sentences, by using the
encoded word deﬁnitions of each word in the input
sentences. Moreover, unlike Bahdanau’s method,
it obtains the representation of all words from dic-
tionaries on the ﬂy in order to improve the repre-
sentations of non-OOV words.

2 Task Deﬁnition

We follow the task deﬁnition of SNLI (Bowman
et al., 2015) and MNLI (Williams et al., 2018). We
deﬁne a dictionary as follows.

Proceedingsofthe3rdWorkshoponRepresentationLearningforNLP,pages58–63Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics58Def. 1 (Dictionary). A dictionary D has the fol-
lowing components.

Headword y is an arbitrary token. Deﬁnition
Dy is represented as a token sequence that de-
ﬁnes the headword y. This study assumes that
each headword has only one deﬁnition. For a poly-
semic headword with multiple deﬁnitions, we use
the concatenation of the deﬁnitions. Vocabulary
VD is the set of all headwords in the dictionary.

3 Related Work

Bahdanau et al. (2017) proposed a method that en-
ables dictionary information to be used in NLP
tasks, such as NLI, reading comprehension, and
language modeling. Their method can obtain the
embeddings of OOV words efﬁciently, because
they obtain the deﬁnition embeddings for only
OOV words instead of the random embeddings of
the words. Our method is similar to theirs, but our
purpose is different; we reﬁne word embeddings
considering the contexts of input sentences for all
words.

The deﬁnition embedding is also useful for
other tasks. Hill et al. (2016) used the deﬁni-
tion embedding to understand the phrases. They
presented two applications:
reverse dictionaries
and crossword question answering. They tack-
led these applications with phrase embeddings ob-
tained from their deﬁnitions. Long et al. (2016)
used the encoding of the word deﬁnition for the
initialization of TransE (Bordes et al., 2013),
which obtains the embedding of the relationship
between two entities.

There is related work that uses other external
resources for reﬁning word representations. For
NLI, Chen et al. (2017a) proposed a model that
uses a knowledge graph to reﬂect word relation-
ships (e.g., synonymy, hypernymy). Their method
achieved state-of-the-art performance on SNLI;
however, it cannot handle the deﬁnition descrip-
tion of each word.

Moreover, there are general frameworks to re-
ﬁne word embeddings by using external knowl-
edge. Weissenborn et al. (2017) proposed a
method that reﬁnes the word embedding by encod-
ing the text transformation of ConceptNet (Speer
and Havasi, 2012). McCann et al. (2017) pro-
posed context vectors (CoVe), which uses a RNN
encoder trained on machine translation datasets to
introduce context information to the word embed-
ding. Peters et al. (2018) proposed the Embed-

dings from Language Models (ELMo), which ob-
tains contextualized word representations. They
used the states of the middle layers in the deep lan-
guage model. These methods are also effective at
the NLI task.

4 Existing Methods
This section outlines the existing NLI models and
describes the conventional model that uses a dic-
tionary as external knowledge.

4.1 NLI model
In the architecture of a general NLI model (Bow-
man et al., 2015; Rockt¨aschel et al., 2016; Chen
et al., 2017b), the input of the model is a pair
of token sequences {X s = (xs
) : s ∈
{P, H}}, where ls is the length of X s. s ∈ {P, H}
means a premise or hypothesis.

1,··· , xs

ls

We call the following two layers together the

Encoder.
Encoder Word Embedding Layer (WEL)
This layer takes X s as input. Let e(y) ∈ Rne be
the embedding of token y. It outputs a vector se-
quence Es = (e(xs
Encoder Context Embedding Layer (CEL)
This layer converts the vector sequence Es into
a contextualized vector sequence, Cs = f (Es) ∈
Rnc×ls. The most common approach is to use an
RNN as f.

1),··· , e(xs

)) ∈ Rne×ls.

ls

The encoder outputs CP and CH for the

premise and hypothesis sentences, respectively.
Decoder The input of the decoder is a pair of
vector sequences {CP , CH}. The decoder outputs
the score vector of the classiﬁcation labels.

4.2 Deﬁnition Embedding Mechanism
We summarize the deﬁnition embedding mecha-
nism (DEM) (Bahdanau et al., 2017) as it relates to
NLI. They proposed dictionary embedding mech-
anisms with many variations, such as mean pool-
ing or an RNN. We select one of their models with
an RNN, because we also use an RNN for the def-
inition embedding.

The DEM acts on each premise and hypothesis.
Its input is a token sequence X s and the encoder
word embedding sequence Es. The output is E(cid:48)s,
and E(cid:48)s is passed to the encoder CEL instead of
Es. E(cid:48)s is obtained by adding Es to the ﬁnal state
of the RNN encoding of the deﬁnition. The sizes
of Es and E(cid:48)s are each ne × ls.

595 Proposed Method

We propose a novel DEM considering the con-
texts of the input sentences. Our contributions are
threefold. First, we introduce an attention mecha-
nism. Second, we implement the mechanism after
the encoder. Third, we consider deﬁnition embed-
dings of words including non-OOV ones.

The input is a token sequence X s together
with the encoder word and context embedding se-
quence Es and Cs, and the output is C(cid:48)s. C(cid:48)s is
passed to the decoder instead of Cs, where the
sizes of Cs and C(cid:48)s are each nc×ls. The proposed
mechanism has the following layers.

Deﬁnition Extracting Layer Let V s be the set
of target tokens of the deﬁnition embedding which
are in both the token sequence X s and the vocab-
ulary of the dictionary VD. The deﬁnition Dy of
token y ∈ V s is obtained from the dictionary D.
Let my be the length of Dy. This layer outputs
a set of target tokens V s and a set of deﬁnitions
{Dy : y ∈ V s}.
Deﬁnition WEL This layer has the same pa-
rameters as the encoder WEL. For each ele-
ment of Dy, it outputs a vector sequence Ey =
(e(dy

my )) ∈ Rne×my .

1),··· , e(dy

Deﬁnition CEL This layer has the same model
as the encoder CEL. Parameters are not shared
with the encoder CEL. It converts the vector se-
quence Ey into the output of this layer Cy =
f (Ey) ∈ Rnc×my .
Deﬁnition Attention Layer This layer obtains a
ﬁxed-length vector representation of deﬁnition Dy
with an attention mechanism. It takes the outputs
of the previous layers Ey, Cy, Cs, and C ¯s as input,
where ¯s ∈ {P, H} indicates that either the premise
or hypothesis is different from s.
For Cy ∈ Rnc×my , Cs ∈ Rnc×ls, we deﬁne an
Cs(cid:62)Cy, and an atten-
attention matrix Ay,s = 1√
nc
∈ Rmy .
i Ay,s
tion vector ay,s =
The attention vector ay,s represents the extent that
each token in deﬁnition Dy is related with the in-
put sentence X s. The attended deﬁnition vector to
the input sentence X s is

(cid:16) 1

j=1,··· ,my

(cid:80)

(cid:17)

ls

ij

hy,s =

softmaxi(ay,s)cy

i ∈ Rnc,

i

(cid:88)

where cy
embedding Cy.

i is the i-th state of the deﬁnition context

(cid:40)

The last state of the deﬁnition context embed-
my ∈ Rnc. The output of this layer is
ding is cy
a linear combination of the enhancements (Chen
et al., 2017b) of the attended deﬁnition vectors,

zy = [cy

my , hs,y,a, hs,y,a − cy
h¯s,y,a, h¯s,y,a − cy

my , hs,y,a (cid:12) cy
my ]w,

my , h¯s,y,a (cid:12) cy

my ,

(1)

where w ∈ R7 is a trainable parameter and (cid:12) is
the element-wise product. nc is the size of zy.
Output Layer The output of
mechanism is expressed as

the proposed

c(cid:48)s
i =

i ∈ V s)
(xs
otherwise
The decoder receives C(cid:48)s instead of Cs.

i + zxs
cs
cs
i

i

.

(2)

Algorithm 1 is the pseudo code of the deﬁnition

embedding mechanism.

The above explanation only covers the case of
NLI. However, the proposed method can be ap-
plied to any number of input sentences, because
Equation (1) can take an arbitrary number of ar-
guments. Therefore, it is applicable to other tasks
that have text inputs, such as question answering
and machine translation.
Algorithm 1 Deﬁnition Embedding
Input: X s, Es, Cs, C ¯s
Output: C(cid:48)s
1: V s,{Dy : y ∈ V s} ← Def. Ext.(X s)
2: for all y in V s do
3:
4:
5:
6: end for
7: C(cid:48)s ← Output(Es, Cs, V s,{zy : y ∈ V s})

Ey ← Def. Word Emb.(Dy)
Cy ← Def. Context Emb.(Ey)
zy ← Def. Att.(Ey, Cy, Cs, C ¯s)

6 Experiments
This section describes the results of the evaluation
of the proposed method.

6.1 Experimental Setup
We chose ESIM (Chen et al., 2017b) and one of
the methods in Bahdanau et al. (2017) (BDN) as
the baseline models. ESIM is based on the model
in Section 4.1. BDN and our method each add
a DEM to ESIM. In BDN, the target tokens of
the deﬁnition embedding are not contained in the
pre-trained word embedding vocabulary, because

60[a] SNLI, de = 100

[b] MNLI, de = 100

[c] SNLI, de = 300

[d] MNLI, de = 300

Figure 1: Classiﬁcation accuracy of each model in the not-many-OOV setting. The vertical axis is accu-
racy, and the horizontal axis is the number of the vocabulary entries of the dictionary. The performance
of ESIM and BDN was constant because their dictionary size is less than 1000.

BDN intends to supplement the embeddings of
OOV words. However, in our method, the target
tokens do not depend on a pre-trained word em-
bedding vocabulary, because we intend to improve
the representation of all the words by considering
the context.

Our experiments were on the SNLI and MNLI
benchmarks. For MNLI, we used a matched do-
main development dataset as our development data
and a mismatched domain development dataset as
our test data. The tokenizer was spaCy (Honni-
bal and Montani, 2018). The word embeddings
were pre-trained 100d GloVe 6B vectors and 300d
GloVe 840B vectors (Jeffrey Pennington and Man-
ning, 2014). The embeddings were ﬁxed during
training, because we were interested in the differ-
ence in representation between pre-trained embed-
dings with and without dictionary information.

We used the vocabulary and deﬁnitions in
WordNet (Miller, 1995) as dictionaries. For pol-
ysemic words with multiple deﬁnitions, we used
the top-5 deﬁnitions connected in descending or-
der of frequency of synsets, which are provided
by WordNet. The number of headwords that ap-
pear in SNLI is 24103, and 45225 in MNLI.

The other settings are described in Appendix A.

6.2 Results
Does the proposed method reﬁne the OOV
word embedding? In order to investigate the ef-
fectiveness of our method against OOV words, we
restricted the vocabulary of the 100d GloVe em-
bedding to the most common 3000 words in each
dataset and considered the other words as OOV
(many-OOV setting). The word embeddings of the
OOV words were randomly initialized according
to a Gaussian distribution and ﬁxed during train-
ing.

Table 1 shows the results. When there were
many OOV words, our method improved test ac-

SNLI MNLI

ESIM 82.5
83.7
BDN
83.9

Proposed

69.8
69.7
71.3

Table 1: Test accuracy in the many-OOV setting

curacy by 1.4% in SNLI and 1.5% in MNLI. In
contrast, BDN did not improve accuracy in MNLI.
Does the larger dictionary bring higher accu-
racy? We also evaluated our method with the
whole 100d GloVe embedding (not-many-OOV
setting).
In this experiment, we used the whole
vocabulary of WordNet or restricted the WordNet
vocabulary to the 1000 and 10000 most common
words in the each dataset.

Figures 1a and 1b show the results when using
100d GloVe. We conﬁrmed that the larger dictio-
nary raises accuracy. We think that the pre-trained
GloVe embeddings for the frequent words were
more appropriate than those for the rare words.
This means that our method was effective for
words that had relatively poor embeddings and oc-
cur sufﬁciently often in the training data.

We conﬁrmed that the threefold originality of
our method contributed to the improvement in the
whole WordNet setting. The proposed method us-
ing the whole WordNet achieved the higher test
accuracy on each dataset. The improvement from
ESIM was 1.0% in SNLI and 0.8% in MNLI.
Moreover, our method without the deﬁnition at-
tention mechanism performed worse by 0.4% in
SNLI and 0.5% in MNLI in comparison with the
method with it. This implies that our deﬁnition
embedding layer plays an important role in the
deﬁnition embedding.
In particular, the imple-
mentation of the attention mechanism after the en-
coder, which is essential to reﬂecting the context
of input sentences, contributes to a reﬁned repre-
sentation.

BDN did not perform well. The number of

61OOV words in SNLI (MNLI) is 415 (913); there-
fore, BDN could not sufﬁciently train the repre-
sentations of the words with the sentences in the
datasets.

Does the improvement depend on the quality of
the word embedding? Figures 1c and 1d show
the results when using 300d GloVe. In this setting,
our method provided no signiﬁcant improvement.
It performed slightly better (worse) than ESIM in
SNLI (MNLI). BDN, as well, did not perform bet-
ter than ESIM. We think the 300d GloVe has sufﬁ-
ciently correct embeddings for most of the words
in SNLI and MNLI, because it was created from a
much larger corpora (340 billion tokens) than that
of the 100d one (eight billion tokens).

To summarize the experimental results for the
ﬁrst and third research questions, the effectiveness
of our method is dependent on the quality and cov-
erage of word embeddings. That is, our method is
effective for rare or novel words.

7 Conclusion

We proposed a novel deﬁnition embedding
method. The method considers the contexts of the
input sentences with an attention mechanism for
the deﬁnition embeddings. It considers the deﬁ-
nition embeddings of words including non-OOV
words. Experimental results showed that it is ef-
fective for rare or novel words that do not have
good pre-trained word embeddings.

References
Dzmitry Bahdanau, Tom Bosc, Stanislaw Jastrzebski,
Edward Grefenstette, Pascal Vincent, and Yoshua
Bengio. 2017. Learning to compute word embed-
dings on the ﬂy. arXiv preprint arXiv:1706.00286
.

Steven Bird, Ewan Klein,

and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In NIPS. pages 2787–2795.

Samuel R. Bowman, Gabor Angeli, Christopher
A
lan-
In EMNLP. pages 632–642.

Potts, and Christopher D. Manning. 2015.
large annotated corpus for learning natural
guage inference.
https://doi.org/10.18653/v1/D15-1075.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana
Inkpen, and Si Wei. 2017a. Natural language in-
arXiv preprint
ference with external knowledge.
arXiv:1711.04289 .

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017b. Enhanced LSTM
for natural language inference. In ACL. pages 1657–
1668. https://doi.org/10.18653/v1/P17-1152.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In AISTATS. pages 249–256.

Felix Hill, Kyunghyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016.
Learning to understand
phrases by embedding the dictionary. Transactions
of ACL 4:17–30.

Matthew Honnibal and Ines Montani. 2018. spaCy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear .

Richard Socher Jeffrey Pennington and Christopher D.
Manning. 2014. GloVe: Global vectors for word
In EMNLP. pages 1532–1543.
representation.
https://doi.org/10.3115/v1/D14-1162.

Tao Lei and Yu Zhang. 2017. Training RNNs as fast as

CNNs. arXiv preprint arXiv:1709.02755 .

Teng Long, Ryan Lowe,

and Doina Precup. 2016.

ung,
lexical
dings in multi-relational data.
https://doi.org/10.18653/v1/P16-2019.

resources

for

Jackie Chi Kit Che-
Leveraging
learning entity embed-
pages 112–117.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.

George A. Miller. 1995. WordNet: A lexical
database for english. Commun. ACM 38(11):39–41.
https://doi.org/10.1145/219717.219748.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS Workshop Autodiff .

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In ICLR.

Robert Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in conceptnet 5. In
LREC. pages 3679–3686.

62Dirk Weissenborn, Tomas Kocisky, and Chris Dyer.
2017. Dynamic integration of background knowl-
arXiv preprint
edge in neural NLU systems.
arXiv:1706.02596 .

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2018. A broad-coverage challenge corpus
for sentence understanding through inference.
In
NAACL.

Matthew D. Zeiler. 2012. ADADELTA: an adap-
arXiv preprint

tive learning rate method.
arXiv:1212.5701 .

A Details of the Implementation
The section describes our implementation so that
our experiments can be reproduced.

We implemented our method in PyTorch
(Paszke et al., 2017) and trained it on one Nvidia
GeForce GTX 1080 GPU. The RNNs in the en-
coder, decoder, and deﬁnition embedding mecha-
nism were two-layer bi-directional simple recur-
rent units (SRUs) (Lei and Zhang, 2017). The size
of the output of the RNN was nc = 2ne. The acti-
vation function in the RNN was the tanh function.
Dropout with a keep ratio of 0.8 was applied to the
same layer as ESIM and the deﬁnition embedding
layer.

The parameters of the weights were initialized
using the Xavier normal initializer (Glorot and
Bengio, 2010), and the parameters of the biases
were initialized as zero vectors. Word embeddings
not contained in pre-trained GloVe were random-
ized according to a Gaussian distribution.

The mini-batch size was set to 16. The opti-
mizer was Adadelta (Zeiler, 2012) with an initial
learning rate of 0.075 and ρ of 0.9. Early stopping
with a patience of 7 was used to avoid overﬁtting.
We removed words whose deﬁnition length
was one and stop words in the Natural Language
Toolkit (Bird et al., 2009) from the vocabulary of
the dictionary.

63
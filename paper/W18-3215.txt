Named Entity Recognition on Code-Switched Data

Using Conditional Random Fields

Utpal Kumar Sikdar1, Biswanath Barik2 and Bj¨orn Gamb¨ack2

1Flytxt, Thiruvananthapuram, India
utpal.sikdar@gmail.com

2Dpt. of Computer Science, Norwegian University of Science and Technology

{biswanath.barik,gamback}@ntnu.no

Abstract

Named Entity Recognition is an impor-
tant information extraction task that iden-
tiﬁes proper names in unstructured texts
and classiﬁes them into some pre-deﬁned
categories.
Identiﬁcation of named enti-
ties in code-mixed social media texts is
a more difﬁcult and challenging task as
the contexts are short, ambiguous and of-
ten noisy. This work proposes a Con-
ditional Random Fields based named en-
tity recognition system to identify proper
names in code-switched data and classify
them into nine categories. The system
ranked ﬁfth among nine participant sys-
tems and achieved a 59.25% F1-score.

Introduction

1
With the increasing usage of social media, mi-
cro blogs and chats in various socio-economical
classes, ethnicities and genres in the global so-
ciety, a new category of informal short texts has
evolved in recent years. One of the important
phenomena that can appear in such texts is code-
mixing or code-switching (CS), where bi-lingual
users often switch back and forth between their
common languages during interactions. Process-
ing of such texts by automatic means encounters
several challenges due to the usage of mixed vo-
cabulary, misspellings, abbreviations, translitera-
tions, emojis, and many more. Furthermore, it is in
many cases difﬁcult to interpret the texts because
of the short contexts.

The Natural Language Processing and text min-
ing communities have taken necessary initiatives
to encourage researchers through organizing var-
ious workshops and shared-tasks, and by open-
ing mainstream research tracks to develop re-
sources and novel approaches to processing code-

mixed texts efﬁciently and for extracting valu-
able information from such messy contents.
In
this direction,
the CALCS 2018 Shared Task
(Aguilar et al., 2018) focused on identifying a
predeﬁned set of nine Named Entity (NE) types:
Person, Location, Organization, Group, Title,
Product, Event, Time, and Other.
The NE
identiﬁcation task addressed code-mixed texts of
Spanish-English (SPA-ENG) and Modern Stan-
dard Arabic-Egyptian (MSA-EGY); here we will
look at the ﬁrst pair (SPA-ENG) only.

Previously, several machine learning techniques
have been applied to the NE recognition problem
such as Hidden Markov Models (HMM) (Bikel
et al., 1997), Maximum Entropy models (Borth-
wick, 1999), Conditional Random Fields (CRF)
(Lafferty et al., 2001), and Support Vector Ma-
chines (SVM) (Isozaki and Kazawa, 2002), as well
as deep neural network-based Long Short-Term
Memories (LSTM) (Limsopatham and Collier,
2016), Convolutional Neural Networks (CNN)
(Santos and Guimaraes, 2015), or hybrid combi-
nations (Chiu and Nichols, 2016).

In this work, the named entity recognition task
is considered as a sequence labeling problem, for
which CRF is a natural choice to identify entity
mentions from code-switched data and classify
them to one of the nine aforementioned NE cat-
egories. With initial named entity token and lan-
guage identiﬁcation, a wide range of features (de-
scribed in Section 3) are explored for this purpose.
As per the overall ranking of the submitted sys-
tems under the shared task, our approach is rea-
sonably effective.

The paper is organized as follows: The shared
task datasets are presented in Section 2. The
named entity recognition system is described in
Section 3. Results are presented in Section 4, with
error analysis reported in Section 5. Section 6 ad-
dresses future work and concludes.

ProceedingsofTheThirdWorkshoponComputationalApproachestoCode-Switching,pages115–119Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics115Dataset
Training
Development
Test

#Tweets
50,238
828
15,634

#Named Entities

12,365
151
-

Table 1: Code-switched dataset statistics

2 Datasets

The shared task organizers provided three differ-
ent datasets: training, development and test sets.
The statistics of the datasets are reported in Ta-
ble 1, with the total number of tweets and total
number of named entities. No gold standard anno-
tation of the test data was made available.

3 Named Entity Recognition

To identify and classify each token from the code-
switched data into nine categories (Person, Loca-
tion, Organization, Group, Title, Product, Event,
Time and Other), a supervised CRF-based (Laf-
ferty et al., 2001) approach was used. Different
features were extracted from external sources and
applied to recognize the target entities.

In a ﬁrst step, each token was identiﬁed as ei-
ther being a named entity (called a mention) or
not. All the beginning and intermediate parts of
named entities (for all nine entity categories) were
converted into ‘B-mention’ and ‘I-mention’, re-
spectively, and a CRF-based model was applied to
identify the mentions.

In the next step, the identiﬁed mentions (‘B-
mention’ and ‘I-mention’) were used as features
along with other features described in subsections
3.1 and 3.2 to classify each token into one of the
nine categories. The ‘BIO’1 notation was used to
represent the named entities.

The CRF-based mention and named entity iden-
tiﬁcation models were implemented using CRF-
suite (python-crfsuite),2 which allows for fast
training by utilizing L-BFGS (Liu and Nocedal,
1989), a limited memory quasi-Newton algorithm
for large scale numerical optimization. The classi-
ﬁer was trained both on features retrieved from ex-
ternal resources and on features directly extracted
from the training data, as detailed in the following
two subsections.

1Here ‘B’ represents the beginning of, ‘I’ inside, and ‘O’ out-
side of a named entity.
2www.chokkan.org/software/crfsuite/

3.1 Features from external sources
The following features were extracted from other
external resources:

3.1.1 Language identiﬁcation
The language identiﬁcation data from the previous
code-switching workshop (Diab et al., 2016) was
collected and converted into ‘lang1’, ‘lang2’ and
‘other’ (with ‘other’ grouping the labels ‘mixed’,
‘ne’, ‘fw’ and ‘unknown’).
If any token of the
‘other’ categories was followed by ‘lang1’, it was
assigned to ‘lang1’. If the token was followed by
‘lang2’, it was assigned to ‘lang2’. A model de-
scribed by Sikdar and Gamb¨ack (2016) was built
using the converted language identiﬁcation data
and applied to the current shared task’s (Aguilar
et al., 2018) training and development sets to
get language information (‘lang1’, ‘lang2’ and
‘other’) for each token. This language informa-
tion was then used as a feature for named entity
identiﬁcation in the current shared task.

3.1.2 Named entity token identiﬁcation
Only the tweets containing named entities were
extracted from the data from the previous code-
switched workshop, and a CRF based model was
built using these tweets with different features (lo-
cal context, sufﬁx, preﬁx, all-upper-case, starts-
with-upper-case, and hash symbol) and applied
to the current shared task’s training, development
and test data to get named entity information for
each token.

3.1.3 Part-of-speech information
The Stanford tagger3 was used to extract part-of-
speech (POS) information for training, develop-
ment and test data. First, the English version of the
Stanford tagger was applied to get English POS
tags, and then the Spanish version of the tagger
was applied. For tokens belonging to ‘lang1’ or
‘other’, the English POS tag was considered. For
tokens belonging to ‘lang2’, the Spanish POS was
picked. The POS information for a word together
with its two preceding and two following tokens’
part-of-speech tags (i.e., a -2 to +2 window) were
used as features.

In addition, the ﬁrst two characters of the cur-
rent word’s POS tag and those of the previous and
next two words’ POS tags (-2 to +2 tokens) were
used as features.
3https://nlp.stanford.edu/software/
tagger.shtml

1163.1.4 Stem
The stem of each token was identiﬁed using the
Stanford parser.4

3.1.5 Noisy data named entity recognizer
The named entities of the current workshop’s
datasets were identiﬁed using the model for named
entity recognition in noisy user generated texts de-
scribed by Sikdar and Gamb¨ack (2017).

3.2 Features from training data
The following features were extracted from the
training data.

in the word converted to lower-case.

• word itself: the current word.
• word in lower case: all alphabetic characters
• local context of word in lower-case (with a -2
to +2 window, i.e., from two preceding to two
following tokens).

• all-upper-case:

3 characters.

current word contains any digit or not.

of a word is greater than a threshold (> 5).

binary feature checking
whether the current token only has upper-
case letters or not.
• starts-with-upper-case: binary feature check-
ing whether the current token starts with a
capital letter or not.
• word-length: binary feature set if the length
• sufﬁx: n-grams of the last 1, 2 or 3 characters.
• preﬁx characters: n-grams of the ﬁrst 1, 2 or
• is-digit: binary feature checking whether the
• two-digit: binary feature set if the current
• is-alphanumeric: current word contains both
• is-special-characters: binary feature set if the
• is-stop-word: the current word is on NLTK’s5
• most-frequent-word: after removing all stop
words, a list was prepared based on high fre-
quency of words (1000 words from the train-
ing data). The feature is set if the current
word belongs to this high frequency word list.
the current word with
all lower-case letters replaced with ‘a’, all

current word contains either ‘#’ or ‘@’.

• word-normalization:

word contains two digits.

digits and letters.

stop word list.

4https://nlp.stanford.edu/
IR-book/html/htmledition/
stemming-and-lemmatization-1.html
5https://www.nltk.org/

Data
5-fold
Dev Data

Precision Recall F-score
75.95
62.00

80.64
81.10

71.82
50.20

Table 2: Mention identiﬁcation results (%)

Data
5-fold
Dev Data
Test

F-score
59.19
41.70
59.25

Table 3: Named entity recognition results (%)

Team

IIT BHU
CAiRE++
FAIR
Linguists
Flytxt
semantic
WallyGuzman
Fraunhofer FKIE
Baseline

F-score
63.76
62.76
62.66
62.13
59.25
56.72
54.16
53.65
53.28

Table 4: Comparison with other systems (%)

upper-case letters replaced with ‘A’, all dig-
its replaced with ‘0’, and all other characters
left unaltered.

• Pair-wise-mutual-information-score:

PMI
calculated based on the number of times the
current word belongs to each NE category di-
vided by the word’s total number of occur-
rences in training data.
• beginning-of-the-word:

feature
checking whether the current token belongs
to beginning of the sentence or not.
• ending-of-the-word: binary feature checking
whether the current token belongs to end of
the sentence or not.

binary

To identify the mentions, the above features
were used together. To identify named entities, the
predicted mentions along with contexts consisting
of the previous two and the next two tokens were
used as features, in addition to the other features
described in subsections 3.1 and 3.2.

117EVENT GROUP LOC ORG OTHER PER PROD TIME TITLE

EVENT
GROUP
LOC
ORG
OTHER
PER
PROD
TIME
TITLE
O

1
0
2
0
0
0
0
0
0
0

0
2
0
0
0
0
0
0
0
0

0
1
7
0
0
1
0
0
2
6

0
0
0
0
0
0
0
0
0
2

0
0
0
0
1
0
0
0
0
1

3
0
1
4
0
52
2
0
6
4

0
0
0
5
0
0
11
0
0
1

0
0
0
0
0
0
0
6
0
2

0
0
0
0
0
0
0
0
2
0

O
2
2
6
1
6
42
8
3
40
9348

Table 5: Confusion matrix for NER on the development data

4 Results

The supervised learning approach was applied to
identify mentions. Identiﬁed mentions were taken
as features along with the other features men-
tioned in Section 3 to recognize named entities.
The classiﬁers were learned from the training data
and tested on the development data. 5-fold cross-
validation (CV) was applied to the training data.

The mention identiﬁcation results are shown in
Table 2. The average precision, recall and F1-
score values of 5-fold CV on the training data were
80.64%, 71.82% and 75.95%, respectively. The
F1-score on the development data was 62.00% due
to a signiﬁcant drop in recall.

The system was applied to named entity recog-
nition and results are shown in Table 3. The
average F1-score of 5-fold cross-validation was
59.19%. When tested on the development data,
the system achieved an F-score of 41.70%.

The system was then applied to the unseen test
data and achieved an F1-score of 59.25%, which
is similar to the 5-fold CV F1-score.

Comparing our system (‘Flytxt’) to the other
systems participating in the shared task, Table 4
reports the results and shows that the system se-
cured ﬁfth position and achieved clearly better
scores than the baseline system (‘Baseline’).

5 Error Analysis

When analyzing the output on the development
data for named entity recognition, it is clear that
many of the named entities are not identiﬁed at all
by the system. This might be due to the word it-
self and/or some the contexts word not occurring
in the training data.

Furthermore, some named entities are misclas-

siﬁed into other categories, plausibly since those
words occur in both named entity categories.

The confusion matrix for named entity recog-
nition is reported in Table 5, for each of the
nine classes (‘EVENT’, ‘GROUP’,‘LOC’, ‘ORG’,
‘OTHER’, ‘PER’, ‘PROD’, ‘TIME’, ‘TITLE’).
The matrix was built using relaxed match, with the
‘B-’ and ‘I-’ distinctions ignored for each named
entity class.

6 Conclusion
This paper proposed a Conditional Random Field
based approach to identifying and classifying
named entities. Compared to the baseline, the pro-
posed system achieved better results.

To investigate the effectiveness of the external
features, a feature ablation study should be the
next step. Most of the features have been extracted
directly from training data, but the features could
have been further optimized using grid search and
evolutionary approaches.

As an alternative to the feature-based classiﬁer,
deep learning-based approaches such as LSTM
(Long Short-Term Memory), stack-based LSTM
and CNN (Convolution Neural Network) can be
explored to classify the proper names into the nine
categories.

Acknowledgements
Thanks to the organizers of the 2016 and 2018
code-switching workshops for providing and an-
notating the training and test data. Thanks also to
the three anonymous reviewers for comments that
helped improve the paper and for suggestions that
can be useful in the future.

118References
Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona
Diab, Julia Hirschberg, and Thamar Solorio. 2018.
Overview of the CALCS 2018 shared task: Named
In Pro-
entity recognition on code-switched data.
ceedings of the 3rd Workshop on Computational Ap-
proaches to Linguistic Code-Switching, Melbourne,
Australia. ACL.

Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-ﬁnder. In Proceedings
of the 5th Conference on Applied Natural Language
Processing, pages 194–201, Washington, DC, USA.
ACL.

Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
Computer Science Department, New York Univer-
sity, New York, NY, USA.

Jason P.C. Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional LSTM-CNNs. Trans-
actions of the Association for Computational Lin-
guistics, 4:357–370.

Mona Diab, Pascale Fung, Mahmoud Ghoneim, Julia
Hirschberg, and Thamar Solorio, editors. 2016. Pro-
ceedings of the 2nd Workshop on Computational Ap-
proaches to Code Switching@EMNLP 2016. ACL,
Austin, Texas, USA.

Hideki Isozaki and Hideto Kazawa. 2002. Efﬁcient
support vector classiﬁers for named entity recogni-
tion. In Proceedings of the 19th International Con-
ference on Computational Linguistics, volume 1,
pages 1–7, Taipei, Taiwan. ACL.

John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
In Proceedings of the 18th Interna-
quence data.
tional Conference on Machine Learning, pages 282–
289, Williamstown, MA, USA. IMIS.

Nut Limsopatham and Nigel Henry Collier. 2016.
Bidirectional LSTM for named entity recogni-
In Proceedings of
tion in Twitter messages.
the 2nd Workshop on Noisy User-generated Text,
WNUT@COLING2016, pages 145–152, Osaka,
Japan. ACL.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503–528.

Cicero Nogueira dos Santos and Victor Guimaraes.
2015. Boosting named entity recognition with neu-
In Proceedings of the
ral character embeddings.
5th Named Entity Workshop (NEWS 2015), Beijing,
China. ACL.

Utpal Kumar Sikdar and Bj¨orn Gamb¨ack. 2016. Lan-
guage identiﬁcation in code-switched text using
Conditional Random Fields and Babelnet. In Pro-
ceedings of the 2nd Workshop on Computational Ap-
proaches to Code Switching@EMNLP 2016, pages
127–131, Austin, Texas, USA. ACL.

Utpal Kumar Sikdar and Bj¨orn Gamb¨ack. 2017. A
feature-based ensemble approach to recognition of
In Proceedings
emerging and rare named entities.
of the 3rd Workshop on Noisy User-generated Text,
NUT@EMNLP, pages 177–181, Copenhagen, Den-
mark. ACL.

119
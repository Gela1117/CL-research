On hapax legomena and morphological productivity

Janet B. Pierrehumbert

Dept. of Engineering Science

University of Oxford

janet.pierrehumbert@

oerc.ox.ac.uk

Abstract

Quantifying and predicting morphological
productivity is a long-standing challenge in
corpus linguistics and psycholinguistics. The
same challenge reappears in natural language
processing in the context of handling words
that were not seen in the training set (out-of-
vocabulary, or OOV, words). Prior research
showed that a good indicator of the produc-
tivity of a morpheme is the number of words
involving it that occur exactly once (the ha-
pax legomena). A technical connection was
adduced between this result and Good-Turing
smoothing, which assigns probability mass to
unseen events on the basis of the simplifying
assumption that word frequencies are station-
ary.
In a large-scale study of 133 afﬁxes in
Wikipedia, we develop evidence that success
in fact depends on tapping the frequency range
in which the assumptions of Good-Turing are
violated.

1

Introduction

The productivity of a morpheme is understood as
the extent to which a language uses it actively in
novel combinations. This vexed concept has mul-
tiple interpretations, of which two will concern
us here. One views productivity as the cogni-
tive propensity to create a new word involving a
morpheme. The other infers productivity from the
likelihood that new word types with a morpheme
will be found when a corpus is expanded. The two
can differ because the likelihood of ﬁnding a word
depends not only on its creation, but also on the
extent to which the word is learned and reused
by others, and ultimately noted by an observer.
One might suppose that a morpheme found in
many different combinations would be more ﬂex-
ible in entering into novel ones, as in the rationale
for Witten-Bell smoothing,
(Jurafsky and Mar-
tin, 2000); if so, the type count of the morpheme

Ramon Granell

Dept. of Engineering Science

University of Oxford
ramon.granell@
oerc.ox.ac.uk

This paper

would be a good index of productivity. However,
the type count correlates poorly with human in-
tuitions about productivity and with the number
of OOV words found in test sets
(Baayen and
Lieber, 1991; Baayen and Renouf, 1996; Anshen
and Aronoff, 1999). Working with corpora that are
small by current standards, corpus linguists in the
1990s observed that the number of hapax legom-
ena (or hapaxes) that contain a given morpheme is
a much better predictor (Baayen and Lieber, 1991;
Baayen and Renouf, 1996). This ﬁnding is argued
to follow from assumptions about the cognitive
system that make Good-Turing smoothing appli-
cable, which we explain in the following section.
systematically explores hapax
counts as an indicator of productivity for a set
of 133 morphemes that meet objective inclusion
criteria for a much larger corpus than was used
previously. This is the August 2013 download
of Wikipedia that has 1.24 billion word tokens.
We address several questions: Is the measure suc-
cessful when exercised at a larger scale? Are the
simplifying assumptions put forward to justify the
measure valid? What does the behaviour of the
measure tell us about the lexical system? We ad-
dress these questions with numerical experiments.
We deﬁne “pseudohapax” sets as sets of words
in the full corpus that would be expected to oc-
cur exactly once in ﬁve nominal corpora having
sizes used in classic studies. We explore how
well the pseudohapax sets predict the distribu-
tion of morphemes amongst extremely rare words.
We also downsample the corpus to create hapax
sets from subcorpora matching the nominal cor-
pus sizes. This approach allows us to separate the
inﬂuence of several factors: sparse sampling, vari-
ation across morphological families in the shape
of the rank-frequency distribution, and the actual
frequencies of words that appear as hapaxes in cor-
pora of classic size.

Proceedingsofthe15thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages125–130Brussels,Belgium,October31,2018.c(cid:13)2018TheSpecialInterestGrouponComputationalMorphologyandPhonologyhttps://doi.org/10.18653/v1/P171252 Hapax Legomena and smoothing

Hapax counts are advanced as an indicator of
productivity in Baayen & Lieber (1991). The
article describes as “large” the 18-million word
Cobuild corpus (Renouf, 1987) on which the study
was based. Hay and Baayen (2002) used the
same corpus. The indicator has been widely
used for more than 25 years (Baayen and Re-
nouf, 1996; Chitashvili and Baayen, 1993; Plag
et al., 1999; Popescu and Altmann, 2008; Kenny,
2014; Aronoff and Lindsay, 2014; Stump, 2017,
in press). Several different measures can be de-
ﬁned from the hapax count. Of particular interest
is the hapax percentage P∗ (the percentage of all
hapaxes that contain the morpheme). Using P∗,
the number of unseen word types with any given
morpheme is estimated as proportional to its rep-
resentation amongst the hapaxes, and this supports
predictions about the distributions of morphemes
in OOV sets much bigger than the hapax sets.

A justiﬁcation for the hapax-based measures is
proposed in Baayen & Renouf (1996) and Hay
and Baayen (2002). They assume that the text
meets the assumptions for Good-Turing smooth-
ing: each word is produced with a constant prob-
ability, or put differently, the word frequencies are
stationary and the text results from a Poisson pro-
cess (Church and Gale, 1991). They also assume
that the hapaxes are so rare that they are very
likely to have been created on the spot. Integrat-
ing these assumptions, the idea is that the proba-
bility of creating a rare word form like zeitgeist+y
or post+Sumerian is constant and based on the
mental representation of the parts. The hapax set
is by deﬁnition a subset of the word types with
a morpheme, but nonetheless supports better pre-
dictions. The authors suggest the hapax measure
works well because it eliminates complex words
that are not decomposed during lexical access, and
therefore do not contribute to productivity. Our
numerical experiments were designed to include
only words that are decomposable.

It is well-known that word frequencies ﬂuctuate
with the topic of discussion. Such deviations from
a Poisson process provide the foundation for mod-
ern document retrieval algorithms (Sparck Jones,
1972; Church and Gale, 1995). The effects can be
as large as orders of magnitude in frequency, and
impact all parts of speech, although the impact on
proper nouns is generally greatest (Church, 2000;
Altmann et al., 2009). Different corpora can thus

yield different P∗ values for the same afﬁx, pro-
viding the grist for post-hoc interpretation as in
Plagg et al. (1999). In interpreting our results, we
will also be concerned with the possibility of vari-
ability across speakers, which is of similar magni-
tude (Altmann et al., 2011).

3 Materials

In selecting our materials, one goal was to com-
pare all morphemes that met objective inclusion
criteria (as opposed to using subjective judgment
to make a selection). The inclusion criteria were
designed to identify morphemes that are reason-
ably familiar and that are reliably identiﬁable
within complex words with a minimum of false
positives. We considered words to be potentially
decomposable into Preﬁx+Stem or Stem+Sufﬁx if
removing the afﬁx yielded a stem that also occurs
independently as a word of higher frequency. This
criterion is needed to eliminate many spurious de-
compositions, such as season = sea+son, as well
as words that are probably not decomposed into
their parts in lexical processing (Hay, 2001). To
select the target afﬁxes, we began by considering
all 184,499 words in Wikipedia that occur at least
100 times. Initial and ﬁnal substrings of three or
more letters were considered as potential afﬁxes.
The selected afﬁxes occurred at least 50 times in
the candidate list, and we also required that re-
moving them reliably yield a valid stem. 68 pre-
ﬁxes and 65 sufﬁxes met the criteria. The total
set excludes many productive morphemes that co-
incidentally occur in many simplex words. It in-
cludes many true preﬁxes and sufﬁxes, including
combinations such as +ingly, +ization, +ers. Jus-
tiﬁcation for treating these as units can be found in
Stump (2017; in press). It also includes words that
are used productively in compounding; the distinc-
tion between derivational morphology and com-
pounding is a fuzzy one (Bauer, 2005). Detailed
inclusion criteria and descriptive statistics, and a
complete word list, are in the supplement (posted
on the ﬁrst author’s web site).

Our outcome measure for productivity is the
type frequencies for each morpheme family in the
“far tail” of the distribution, deﬁned as the set of
words occurring 2 to 11 times. As is common in
work on very large corpora, forms occurring only
once are not considered because of problematic
text normalization artifacts. The upper cutoff of
11 was selected to provide a large test set of words

126Preﬁx
non+
anti+
sub+
post+
north+
fore+
south+
second+
Sufﬁx
+ers
+man
+based
+like
+ful
+water
+american
+shire

T
S
O
M

T
S
A
E
L

T
S
O
M

T
S
A
E
L

Example
non-threat
anti-badware
subcritical
post-vulgate
northlands
forebrain
southback
second-degree
Example
adopters
beckman
rock-based
garlic-like
needful
ﬂoodwater
austral-american
dorsetshire

Types
11360
4933
3520
2854
428
410
400
365
Types
6881
6639
6169
5518
430
399
335
246

Table 1:
least types in the far tail. Total types in the dataset.

Preﬁxes and sufﬁxes having the most and

with low frequencies (under 0.01 per million) that
would be novel to many or most readers. The test
set provides a much stronger mirror of underlying
productivity than modest extensions of small cor-
pora could. The far tail contains 129,714 complex
word types that are relevant, in that they begin or
end in one of the target afﬁxes, and can be parsed
into the afﬁx plus a stem. Table 1 shows the most
and least productive morphemes, as indicated by
their counts in the far tail.

4 Frequency bands
We frame the calculations by considering nominal
corpus sizes of 0.25%, 0.5%, 1.0%, 2.0%, 4.0% of
the actual corpus size. Compared to classic corpus
sizes, these range from rather small (3M words) to
rather large (50M words). For each size, we take
the “pseudohapaxes” to be words whose expected
frequency in the nominal corpus would be 1.0, tak-
ing rounding into account. For example, for the
1.0% condition, the band is centered on words oc-
curring 100 times. The 4.0% condition provides
the largest possible pseudohapax set that has no
overlap with the far tail (the test set).

The number of pseudohapaxes grows with the
nominal corpus size. To evaluate the importance
of sample size versus the absolute location in
the frequency range, we also deﬁne down-bands,
which have the same number of word types as a

PBand

Size
0.25% [200,600]
0.5% [100,300]
[50, 150]
1.0%
2.0%
[25,75]
[12, 37]
4.0%

PTypes DB HTypes
5734
8196
11769
8463
16550
12709
19051
22886
30650
29131

8
5
2
1
0

Table 2:
Banding scheme for ﬁve conditions, ex-
pressed as a percentage of the total corpus size. Fre-
quency band for the pseudohapaxes (PBand),
total
number of pseudohapax types containing any of the
preﬁxes or sufﬁxes (PTypes), number of down-bands
available before reaching the far tail (DB). Average
number of hapaxes (HTypes).

pseudohapax band but are simply shifted down-
wards in the frequency range by an integer multi-
ple of the size of that band. Table 2 summarizes
the banding scheme.

For the 4.0% condition, there is no down-band,
because the pseudohapax band falls just above the
far tail. It is also important to look at the set of
words with higher frequencies than the pseudoha-
paxes. In this ”up-band” we include all words up
to the most frequent; the size of the up-band is al-
ways within 15% of the size of the pseudohapax
set. It is never possible to deﬁne more than one
up-band from each set of pseudohapaxes.

A real hapax set corresponding to one of our
nominal sizes would have only a sparse sample of
the pseudohapaxes, but would also include words
of higher and lower frequency. For each of the
ﬁve pseudohapax bands, we simulate a real hapax
set by taking a random sample of sentences in the
corpus and collating the hapaxes. For each corpus
size, 10 different subcorpora were created. If the
hapax set happened to include words from the far
tail, these were removed from the far tail for test-
ing.

5 Evaluating predictions
We use ordinary least squares regression (OLS)
to predict the logarithm of type count for each
morpheme in the tail as a function of the loga-
rithm of type count in a pseudohapax band, treat-
ing preﬁxes and sufﬁxes separately. To ensure that
the observations are robust, we use a hold-one-out
method. Each preﬁx (or sufﬁx) is held out and the
remaining preﬁxes (or sufﬁxes) are used to pre-
dict its value. We make the same calculation for
all conditions, all down-bands and up-bands, and
all hapax sets. Predicted R2 values are adjusted

127as described in Draper et al. (1998), yielding the
measure ¯R2

pred.

Table 3 summarizes the regression parameters.
The slope is close to 1.0 for all ﬁts (and closes in
on 1.0 as the nominal corpus size increases), while
the intercept varies. This means that the number of
types in the far tail is approximately proportional
to the number of types in the pseudohapax or ha-
pax set, with the proportion decreasing as the nom-
inal corpus size increases.

Figure 1 shows the results for ¯R2

pred. For all
conditions, the prediction from the pseudohapax
band is better than the prediction from the up-
band. Attempting predictions from words with
frequencies over 600 (leftmost points in the ﬁgure)
yields poor ¯R2
pred values of 0.4 and below. The
pseudohapax bands for the 2.0%, and 4.0% condi-
tions provide very good predictions with ¯R2
pred >
0.85. This outcome is not chieﬂy due to the large
size of these two pseudohapax sets. Predictions
are nearly as good from the down-bands falling in
the same frequency range. Figure 1 also shows re-
sults for the same calculation for the hapax sets;
the reported ¯R2
pred averages over the results for
the 10 subcorpora of each size. For the small-
est corpora, the prediction from the hapax set is
better than the prediction from the corresponding
pseudohapax set. The hapax set is dominated by
rare words, because lexical rank-frequency distri-
butions are heavy-tailed. The median word fre-
quency for the 0.25% case is 88 (or 0.07 per mil-
lion). As we go towards larger corpora, the differ-
ence between the hapax value and the pseudoha-
pax value dwindles.

6

Interpretation

No matter whether the sample is obtained as a ha-
pax set or a pseudohapax set, success in predict-
ing word types in the far tail depends on having
a sample that is dominated by rare words. Psy-
cholinguists view words with frequencies of 1 to
3 per million as low-frequency words, as in Car-
reiras et al. (2006), but a median of 0.07 per mil-
lion was needed to achieve ¯R2
pred > 0.8. Why did
this outcome occur? Figure 2 sheds light on this
question.
It shows a frequency-rank distribution
on a log-log scale for the 5 most productive, and
the 5 least productive, sufﬁxes as measured by the
count of word types in the far tail. This is a rota-
tion of a Zipﬁan rank-frequency distribution, with
a separate sub-lexicon for each morpheme.

¯R2

Figure 1:
pred for using type counts of the afﬁxes
in the indicated band to predict type counts in the far
tail. The pseudohapax set for each condition is indi-
cated with an enlarged plotting character. The hapax
set for each condition is indicated with a ﬁlled plotting
character.

If a frequency spectrum obeyed a power law (as
proposed by Zipf) it would appear as a straight
line on a log-log plot. All curves are concave
downwards, as typically observed (Baayen, 2001).
There are marked differences in how the spectra
roll off. Words with frequencies above 600 (0.5
per million) provide little information about pro-
ductivity, and two of the most productive sufﬁxes
(+like, +related) still have not pulled out of the
bottom group by 600. The slope around 100 is,
however, very indicative of the slope around 10.

With a frequency of 88, the median hapax in
the 0.25% case has a rank of 187,474 in the rank-
frequency distribution for the entire Wikipedia vo-
cabulary (not shown). This number can be inter-
preted in the light of results on adult vocabular-
ies. Based on a large crowdsourcing experiment,
Brysbaert et al. (2016) estimate that a 60-year-
old at the 95th percentile of vocabulary knowledge
knows 56,400 lemmas, or 95,880 words including
inﬂected forms. Thus, it seems that unlikely that
even such a person knows all of the hapaxes. Brys-
baert et al. (2016) however omit proper names. So
it is also relevant to consider “alphabetic words”,
which are words spelled with alphabetic charac-
ters regardless of their morphological status. Brys-

128Slope

Intercept

s
e
x
a
p
a
h
o
d
u
e
s
P

s
e
x
a
p
a
H

Size Mean Min
0.25% 0.92
0.5% 1.03
1.0% 1.05
2.0% 1.03
4.0% 1.01
0.25% 1.05
0.5% 1.06
1.0% 1.04
2.0% 1.03
4.0% 1.01

0.78 +based
0.90, +based
0.93 +based
0.95 +like
0.96 +based
0.95 +like
0.96 +like
0.96 +like
0.96 +based
0.96 +based

Max
1.05 over+
1.15 wiki+
1.15 side+
1.10 second+
1.06 home+
1.16 self+
1.15 side+
1.11 news+
1.11 non+
1.06 head+

Mean Min
3.32
2.53
2.06
1.73
1.41
2.49
2.06
1.78
1.45
1.20

Max
3.70 +based
2.92 +based
2.47 +based

2.96 over+
2.15 wiki+
1.69 side+
1.50 second+ 1.99 +like
1.22 home+
2.14 home+
1.68 side+
1.49 news+
1.09 non+
0.99 +ful

1.60 +based
2.78 non+
2.42 +like
2.11 +like
1.76 +based
1.43 +based

Table 3: Summary of regression parameters. For minimum and maximum values, the indicated afﬁx is the one
that was held out.

While this may be true for some words, it ap-
pears highly implausible for others. This fre-
quency range includes many words that are not
fully transparent and that recur many times within
individual articles on specialized topics. Technical
terms like interaural (audiology), piquette (oenol-
ogy), demand-side (economics) are prototypical
examples of words with non-stationary probabili-
ties (Church and Gale, 1995; Curran and Osborne,
2002). For proper names, the sufﬁx +ville is 17
times as productive as the sufﬁx +shire. Given
that Wikipedia asks all articles to be supported by
secondary sources, few if any proper names would
have been created on the spot.

We have seen that the hapaxes in a random sam-
ple of merely 3M words succeeded well in predict-
ing the morphological proﬁle in the tail of a cor-
pus 400 times larger. The success seems to have
occurred because the hapaxes provided a good
slice of rare words that are not known to every-
one, and that were not necessarily created on the
spot. Pseudohapax sets that obtained a slice of
similarly rare words worked just as well. Why
are such rare words better indicators of produc-
tivity than more frequent words, even when these
have been -ﬁltered to be decomposable, as in this
study? Possibly, rare words have a higher impact
in ongoing learning of morphology because they
are unexpected and salient. An alternative pos-
sibility brings in a social component. Different
groups of editors in Wikipedia work on different
topics. They may extend the morphological pat-
terns that typify their ﬁeld and distinguish it from
other ﬁelds. In future research, we will evaluate
such possibilities.

Figure 2: Frequency-rank distributions (on a log-log
scale) for the most and least productive sufﬁxes. Far
tail to the right of the solid line at 11. 0.25% up-band
to the left of the dashed line at 600. A comparable plot
for preﬁxes is similar.

baert et al. (2016) apply the model ﬁts in Gerlach
& Altmann (2013) to estimate the number of dis-
tinct alphabetic words that a person has encoun-
tered, as a function of the total hours spent read-
ing in their lifetime. For the Wikipedia editors,
who had a median age of 25 in 2010 (Glott et al.,
2010), reading 8 hours a day from age 5 yields a
median estimated exposure to 146,000 alphabetic
word types, which is still fewer than the median
hapax rank. In short, the success of a hapax set
as a predictor for words in the far tail depends on
having words that are too rare to be known by ev-
eryone, and are therefore not constant in frequency
across speakers.

We now consider the assumption that each
word in the 0.25% hapax set was independently
(and repeatedly) created with some probability.

129References
Eduardo G Altmann, Janet B Pierrehumbert, and Adil-
son E Motter. 2009.
Beyond word frequency:
Bursts, lulls, and scaling in the temporal distribu-
tions of words. PLOS one, 4(11):e7678.

Eduardo G Altmann, Janet B Pierrehumbert, and Adil-
son E Motter. 2011. Niche as a determinant of word
fate in online groups. PloS one, 6(5):e19009.

Frank Anshen and Mark Aronoff. 1999). Using dictio-
naries to study the mental lexicon. Brain and Lan-
guage, 68:16– 26.

Kenneth W Church and William A Gale. 1995. Pois-
Natural Language Engineering,

son mixtures.
1(2):163–190.

James R Curran and Miles Osborne. 2002. A very
very large corpus doesn’t always yield reliable es-
In Proceedings of the 6th conference on
timates.
Natural Language Learning-Volume 20, pages 1–6.
Association for Computational Linguistics.

Norman R Draper and Harry Smith. 1998. Applied re-
gression analysis. Wiley series in probability and
statistics: Texts and references section. Wiley, New
York, NY.

Mark Aronoff and Mark Lindsay. 2014. Productivity,
blocking and lexicalization. The Oxford handbook
of derivational morphology, pages 67–83.

Martin Gerlach and Eduardo G Altmann. 2013.
Stochastic model for the vocabulary growth in natu-
ral languages. Physical Review X, 3(2):021006.

Harald Baayen and Rochelle Lieber. 1991. Produc-
tivity and english derivation: A corpus-based study.
Linguistics, 29(5):801–844.

Ruediger Glott, Philipp Schmidt, and Rishab Ghosh.
2010. Wikipedia survey ? overview of results.
United Nations University.

R Harald Baayen. 2001. Word frequency distributions,

volume 18. Springer Science & Business Media.

R Harald Baayen and Antoinette Renouf. 1996. Chron-
icling the Times: Productive lexical innovations in
an English newspaper. Language, pages 69–96.

Laurie Bauer. 2005. The borderline between deriva-
tion and compounding. In Morphology and its de-
marcations: Selected papers from the 11th Morphol-
ogy meeting, Vienna, February 2004, volume 264,
page 97. John Benjamins Publishing.

Marc Brysbaert, Micha¨el Stevens, Paweł Mandera, and
Emmanuel Keuleers. 2016. How many words do we
know? practical estimates of vocabulary size depen-
dent on word deﬁnition, the degree of language input
and the participant?s age. Frontiers in psychology,
7.

Manuel Carreiras, Andrea Mechelli, and Cathy J Price.
2006. Effect of word and syllable frequency on ac-
tivation during lexical decision and reading aloud.
Human Brain Mapping, 27(12):963–972.

Revas J Chitashvili and R Harald Baayen. 1993. Word
frequency distributions of texts and corpora as large
number of rare event distributions. In Quantitative
text analysis, pages 54–135. Wissenschaftlicher Ver-
lag.

Kenneth W Church. 2000. Empirical estimates of
adaptation: the chance of two noriegas is closer to
p/2 than p 2. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 180–
186. Association for Computational Linguistics.

Kenneth W Church and William A Gale. 1991. A com-
parison of the enhanced Good-Turing and deleted
estimation methods for estimating probabilities of
English bigrams. Computer Speech & Language,
5(1):19–54.

Jennifer Hay. 2001. Lexical frequency in morphol-
ogy: Is everything relative? Linguistics, 39(6; ISSU
376):1041–1070.

Jennifer Hay and Harald Baayen. 2002. Parsing and
In Yearbook of Morphology 2001,

productivity.
pages 203–235. Springer.

Daniel Jurafsky and James H Martin. 2000. Speech and
Language Processing. Prentice-Hall, Upper Saddle
River, NJ.

Dorothy Kenny. 2014. Lexis and Creativity in Transla-

tion: A corpus-based approach. Routledge.

Ingo Plag, Christiane Dalton-Puffer, and Harald
Baayen. 1999. Morphological productivity across
speech and writing. English Language & Linguis-
tics, 3(2):209–228.

Ioan-Iovitz Popescu and Gabriel Altmann. 2008. Ha-
pax legomena and language typology. Journal of
Quantitative Linguistics, 15(4):370–378.

Antoinette Renouf. 1987. Corpus development.

In
John M. Sinclair, editor, Looking up: An account of
the COBUILD Project in lexical computing, pages
1–40. William Collins Sons and Co. Ltd. London,
England.

Karen Sparck Jones. 1972. A statistical interpretation
of term speciﬁcity and its application in retrieval.
Journal of Documentation, 28(1):11–21.

Gregory Stump. 2017.
inferential-realizational
Acta Linguistica Academica, 64(1):79?124.

Rule conﬂation in an
theory of morphotactics.

Gregory Stump. in press. Some sources of apparent

gaps in derivational paradigms. Morphology.

130
Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction
with Hierarchical Tweet Representation and Multi-Head Self-Attention

Chuhan Wu1, Fangzhao Wu2, Junxin Liu1, Sixing Wu1, Yongfeng Huang1 and Xing Xie2

1 Department of Electronic Engineering, Tsinghua University, Beijing 100084, China

{wuch15,ljx16,wu-sx15,yfhuang}@mails.tsinghua.edu.cn

2Microsoft Research Asia

fangzwu,xing.xie@microsoft.com

Abstract

This paper describes our system for the ﬁrst
and third shared tasks of the third Social Me-
dia Mining for Health Applications (SMM4H)
workshop, which aims to detect the tweets
mentioning drug names and adverse drug re-
actions. In our system we propose a neural ap-
proach with hierarchical tweet representation
and multi-head self-attention (HTR-MSA) for
both tasks. Our system achieved the ﬁrst
place in both the ﬁrst and third shared tasks
of SMM4H with an F-score of 91.83% and
52.20% respectively.

Introduction

1
Social media services such as Twitter have become
important platforms for information sharing and
dissemination. Automatically detecting tweets
which mentions drug names (DNs) and adverse
drug reactions (ADRs) at a large scale is an inter-
esting research topic and has many important ap-
plications such as pharmacovigilance (Sarker and
Gonzalez, 2015; Han et al., 2017; Weissenbacher
et al., 2018). However, tweets are very noisy and
informal, and full of misspellings (e.g., “aspirn”
for “aspirin”) and user-created abbreviations (e.g.,
“COC” for “Cocaine”). In addition, many DN and
ADR mentions are context-dependent. For exam-
ple, “I take Vitamin C after meals” is a tweet men-
tioning drug name, but the tweet “Vitamin C is
good for health” is not. Thus, the detection of DN
and ADR mentioning tweets is very challenging.
In order to facilitate the research on automatic
detection of tweets mentioning DN and ADR,
two related shared tasks were released by the
third Social Media Mining for Health Applica-
tions (SMM4H) workshop1 (Weissenbacher et al.,
2018). Task 1 aims to classify whether a tweet
mentions any drug names or dietary supplement.

Task 3 aims to classify whether a tweet contains
adverse drug reaction mention. We designed a
neural approach with hierarchical tweet represen-
tation and multi-head self-attention (HTR-MSA)
to participate in these two tasks. Our hierarchical
tweet representation model ﬁrst learns word rep-
resentations from characters using convolutional
neural network (CNN) and then learns tweet repre-
sentations from words using a combination of Bi-
directional long-short term memory (Bi-LSTM)
network and CNN. In addition, we incorporated
additional features to enhance the word repre-
sentations, including pre-trained word embedding,
part-of-speech (POS) tag embedding, sentiment
features based on sentiment lexicons and lexicon
features extracted from medical lexicons. Besides,
we applied multi-head self-attention mechanism to
our approach to enhance the contextual represen-
tations of words by capturing the interactions be-
tween all words in tweets. Our system achieved
91.83% F-score in Task 1 and 52.20% F-score in
Task 3, and ranked 1st in both task. The codes of
our system are publicly available2.
2 Our Approach
The architecture of our HTR-MSA model
is
shown in Fig. 1. It contains three major modules,
i.e., word representation, tweet representation and
tweet classiﬁcation.
2.1 Word Representation
In order
to handle the massive misspellings
and user-created abbreviations of drug names in
tweets, we propose to learn word representations
from characters. There are three sub-modules in
the word representation module.

The ﬁrst one is character embedding, which
converts each word from a sequence of characters
into a sequence of low-dimensional dense vectors

1https://healthlanguageprocessing.org/smm4h/

2https://github.com/wuch15/SMM4H THU NGN

Proceedingsofthe3rdSocialMediaMiningforHealthApplications(SMM4H)Workshop&SharedTask,pages34–37Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics34Figure 1: Architecture of our HTR-MSA model.

using a character embedding matrix. The second
one is a character-level CNN network to learn con-
textual representations of characters. CNN is ef-
fective to capture local context information. Since
many drug names contain speciﬁc character com-
binations (e.g., “benz” and “acid”), we apply CNN
to learn contextual character representations by
capturing the local information of neighbor char-
acters. We use max-pooling operation to the fea-
ture maps generated by multiple ﬁlters in CNN to
select the most signiﬁcant features to build word
representations based on characters.

The third one is feature concatenation, where
the word representation learned from characters
is concatenated with additional word features to
build the ﬁnal word representation vector. The ﬁrst
additional feature is word embeddings which are
pre-trained on a large corpus and contain rich se-
mantic information of words. According to pre-
vious studies (Sarker and Gonzalez, 2015), sen-
timent information and medical lexicons are very
important for DN and ADR detection. Therefore,
we incorporate words’ sentiment scores extracted
from SentiWordNet 3.0 sentiment lexicon 3 and
their appearance in the SIDER 4.1 medical lexi-
con4 into their representation vectors. In addition,
since DN and ADR mentions usually have speciﬁc
POS tags (e.g., nouns), we also incorporate the
embeddings of their POS tags . The ﬁnal repre-
sentation vector of a word is a concatenation of its
character-based representation, word embedding,
POS tag embedding, sentiment scores and lexicon
appearance.
2.2 Tweet Representation
The tweet representation module aims to learn the
representation vectors of tweets from their words.

3http://sentiwordnet.isti.cnr.it/
4http://sideeffects.embl.de/

(last access: Jul 19.)

(last access: Jul 20.)

It also contains three sub-modules.

The ﬁrst one is a Bi-LSTM network (Graves and
Schmidhuber, 2005). Long-distance information
is very important for the detection of tweets men-
tioning DN and ADR. For example, the tweet “I
took amoxicillin last night, but I ﬁnd I’m so tired
today” contains an ADR mention “tired”, which
has a long distance to the drug name “amoxi-
cillin”. LSTM is an effective network to capture
long-distance information. We use Bi-LSTM net-
work in our approach. It can capture the context
information from both directions and output the
hidden states at each position. Denote the hidden
states of words in a tweet as H = [h1, ..., hM ],
where M is sentence length.

The second sub-module is multi-head self-
attention network.
In most of existing attention
mechanisms the attention weight of a word is com-
puted only based on its hidden representation, and
the relationships between different words in a text
cannot be modeled. Usually, many DN and ADR
mentions are context-dependent and the interac-
tions between words are very important to de-
tect the DN and ADR mentioning tweets. Self-
attention is an effective way to capture the use-
ful interactions between words in texts (Vaswani
et al., 2017). In addition, a word may interact with
multiple words. For example, in the tweet “I for-
got to take aspirin and I’m in huge pain”, the in-
teraction of “aspirin” with “forgot” and the inter-
action of “aspirin” with“pain” are both important
for ADR mention detection. Thus, we propose to
use multi-head self-attention mechanism (Vaswani
et al., 2017) to learn better hidden representations
of words by modelling their interactions with mul-
tiple words. In this layer, the representation vector
mi,j of the jth word learned by the ith attention
head is computed by a weighted summation of H
as follows:

35…………………………………………�𝒚𝒚⋯⋯Char EmbCNN𝒆𝒆𝑖𝑖𝑗𝑗𝒆𝒆𝑗𝑗𝑗𝑗𝒆𝒆𝑗𝑗𝑗𝒑𝒑𝑗Char Emb𝑐𝑐𝑖𝑖𝑗⋯𝑐𝑐𝑖𝑖𝑗𝑐𝑐𝑖𝑖𝑗𝑗𝑐𝑐𝑗𝑗𝑗⋯𝑐𝑐𝑗𝑗𝑗𝑐𝑐𝑗𝑗𝑗𝑗𝒆𝒆𝑖𝑖𝑗⋯⋯⋯𝒑𝒑𝑖𝑖𝒑𝒑𝑗𝑗𝒑𝒑𝑀𝑀𝑤𝑤𝑖𝑖𝑤𝑤𝑗𝑗LSTMCNN𝒓𝒓𝒆𝒆𝑗𝒆𝒆𝑖𝑖𝒆𝒆𝑗𝑗𝒆𝒆𝑀𝑀Character Features⋯⋯CNN𝒎𝒎𝑗𝒎𝒎𝑖𝑖𝒎𝒎𝑗𝑗𝒎𝒎𝑀𝑀ReLUSoftmax𝒉𝒉𝑗𝒉𝒉𝑖𝑖𝒉𝒉𝑗𝑗𝒉𝒉𝑀𝑀⋯⋯⋯DenseLayers𝒉𝒉𝑀𝑀𝒉𝒉𝑗𝑗𝒉𝒉i𝒉𝒉𝑗⋯……………………Word EmbeddingPOS TagEmbeddingSentimentFeatureLexiconFeatureWord RepresentationTweet RepresentationTweet Classificationˆαi
j,k = hT

j Uihk,

αi

j,k =

exp( ˆαi
j,k)
m=1 ˆαi

j,m

,

(cid:80)M
M(cid:88)

m=1

mi,j = Wi(

αi

j,mhm),

(1)

(2)

(3)

where Ui and Wi are the parameters of the ith
self-attention head, and αi
j,k represents the rel-
ative importance of the interaction between the
jth and kth words.
In this way, the representa-
tion of each word is learned by utilizing the hid-
den representations of all words in the same text
and modeling the interactions between this word
with all other words. The multi-head representa-
tion mj of the jth word is the concatenation of the
outputs from h different self-attention heads, i.e.,
mj = [m1,j; m2,j; ...; mh,j].

The third sub-module is a word-level CNN net-
work with max-pooling operation. Since many
drug names contain speciﬁc word combinations
(e.g., salicylic acid and acetic acid), local contex-
tual information between words is important for
DN and ADR detection. We apply CNN to the se-
quence of hidden representations of words in each
tweet, and the ﬁnal representation vector of a tweet
r is obtained from the results of max-pooling on
the CNN feature maps.
2.3 Tweet Classiﬁcation
The tweet classiﬁcation module is used to classify
whether a tweet mentions DN or ADR. It contains
two dense layers with ReLU and softmax activa-
tion functions respectively. The predicted label ˆy
of a tweet is computed as:

(cid:48)
r

= ReLU (U1r + b1),

(4)

(cid:48)

ˆy = sof tmax(U2r

(5)
where U1, U2, b1, b2 are the parameters for DN
and ADR mention classiﬁcation. The loss function
L used for model training is crossentropy:

+ b2),

yk log(ˆyk),

(6)

i=1

k=1

where yi,k and ˆyi,k are gold label and predicted
label for the ith tweet in the kth label category. N
is the number of labeled tweets.

3 Experiments
3.1 Datasets and Experimental Settings
The datasets provided by Task 1 and Task 3
in the shared tasks of the third SMM4H work-
shop (Weissenbacher et al., 2018) were used in

L = − N(cid:88)

K(cid:88)

our experiments. The ﬁrst one is for detection of
tweets mentioning DNs (denoted as DN). It con-
tains 9,622 tweet IDs (4,975 positive and 4,647
negative samples) for training, and 5,382 tweet
for test. The third one is for detection of tweets
mentioning ADRs (denoted as ADR). It contains
25,598 tweet IDs (2,223 positive and 23,375 neg-
ative samples) for training, and 5,000 tweets for
test. Since many tweets are not available now, we
only crawled 9,065 and 16,694 tweets for training
in DN and ADR respectively using these IDs.

In our experiments, we use the 400-dim pre-
trained word embeddings released by Godin et
al. (2015). The Bi-LSTM network has 2 × 200
units. The CNN network has 400 ﬁlters with win-
dow size of 3. There are 16 heads in the multi-head
self-attention network, and the output dimension
of each head is 16. RMSProp is selected as the
optimizer. Since the negative samples are domi-
nant in the ADR dataset, we use the over-sampling
strategy (Weiss et al., 2007) to balance the number
of positive and negative samples. Besides, in or-
der to further improve the performance of our ap-
proach, we incorporate the ensemble strategy by
independently training our model for 10 times and
using the average prediction results. The perfor-
mance metric is F-score on positive samples.

3.2 Performance Evaluation
In this section, we evaluate the performance of
our approach by comparing it with baseline meth-
ods, including: (1) SVM, support vector machine
with word unigram features (Sarker and Gonza-
lez, 2015); (2) CNN, convolutional neural net-
work (Huynh et al., 2016); (3) LSTM, Bi-LSTM
network (Huynh et al., 2016); (4) CRNN, com-
bining CNN and LSTM (Huynh et al., 2016);
(5) RCNN, combining LSTM and CNN (Huynh
et al., 2016); (6) HTR, our basic hierarchical tweet
representation model without self-attention; (7)
HTR-MSA, our hierarchical tweet representation
model with multi-head self-attention; (8) HTR-
MSA-ens, using an ensemble of our HTR-MSA
models. For fair comparisons, we use the same
additional word features with our approach in all
baseline methods. We conducted 10-fold cross-
validation on the labeled tweets and the results
are summarized in Table 1.
According to Ta-
ble 1, our approach can outperform all the baseline
methods. This may be because in our approach
we learn word representations from not only the
word embeddings but also the characters in words.

36Method
SVM
CNN
LSTM
CRNN
RCNN
HTR

HTR-MSA

HTR-MSA-ens
HTR-MSA-ens*

DN
88.20
89.16
88.78
89.10
89.31
89.80
90.57
91.85
91.83

ADR
47.20
48.56
48.28
48.44
48.75
49.49
50.55
52.48
52.20

Table 1: The performance of different methods in the
DN and ADR detection task. *Results on the test set.

Thus, our approach can be more robust to the mas-
sive misspellings of drug names in tweets and can
mitigate the inﬂuence of out-of-vocabulary words.
In addition, by comparing the results of HTR-
MSA and HTR, we ﬁnd that the multi-head self-
attention network is helpful to improve the per-
formance of our approach. This may be because
the global context information is very important
for detecting tweets mentioning DNs and ADRs
and the multi-head self-attention network can ef-
fectively capture the interactions between words
within a tweet. Besides, ensemble strategy can
further improve the performance of our approach.
It indicates that a more robust system can be built
for detecting tweets mentioning drug names and
adverse drug reactions using the ensemble of mul-
tiple models independently trained using our ap-
proach.
3.3
In this section, we conducted experiments to ex-
plore the effectiveness of additional word features
and the results are shown in Table 2. Accord-
ing to Table 2, each kind of additional word fea-
ture, such as word embedding, POS tag embed-
ding, sentiment score and medial lexicon features,
is effective to improve the performance of our ap-
proach. In addition, among these additional word
features word embedding seems to be most useful.
This is probably because that pre-trained word em-
beddings can provide rich semantic information
of words, which is important for detecting tweets
mentioning DNs and ADRs.

Inﬂuence of Additional Word Features

Feature

All

-Word embedding
-POS tag embedding
-Sentiment scores
-Lexicon feature

DN
90.57
86.45
90.26
90.33
89.94

ADR
50.55
46.29
50.31
50.29
50.10

Table 2: Effectiveness of additional word features.

4 Conclusion
In this paper, we introduce our system participat-
ing in the ﬁrst and the third shared tasks in the
3rd SMM4H workshop. We propose a neural ap-
proach with hierarchical tweet representation and
multi-head self-attention to detect tweets mention-
ing DNs and ADRs. Our system achieved the ﬁrst
place in both tasks.
Acknowledgments
This work was supported in part by the National
Key Research and Development Program of China
under Grant 2016YFB0800402 and the National
Natural Science Foundation of China under Grant
U1705261, U1536207, U1536201 and U1636113.

References
Fr´ederic Godin, Baptist Vandersmissen, Wesley
De Neve, and Rik Van de Walle. 2015. Multime-
dia lab @ acl wnut ner shared task: Named entity
recognition for twitter microposts using distributed
word representations. In WNUT, pages 146–153.

Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classiﬁcation with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602–610.

Sifei Han, Tung Tran, Anthony Rios, and Ramakanth
Kavuluru. 2017. Team uknlp: Detecting adrs, clas-
sifying medication intake messages, and normaliz-
In SMM4H@ AMIA,
ing adr mentions on twitter.
pages 49–53.

Trung Huynh, Yulan He, Alistair Willis, and Stefan
Rueger. 2016. Adverse drug reaction classiﬁcation
In COLING Technical
with deep neural networks.
Papers, pages 877–887.

Abeed Sarker and Graciela Gonzalez. 2015. Portable
automatic text classiﬁcation for adverse drug reac-
tion detection via multi-corpus training. Journal of
biomedical informatics, 53:196–207.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS, pages 5998–6008.

Gary M Weiss, Kate McCarthy, and Bibi Zabar. 2007.
Cost-sensitive learning vs. sampling: Which is best
for handling unbalanced classes with unequal error
costs? DMIN, 7:35–41.

Davy Weissenbacher, Abeed Sarker, Michael Paul, and
Graciela Gonzalez-Hernandez. 2018. Overview of
the third social media mining for health (smm4h)
shared tasks at emnlp 2018. In EMNLP.

37
IEST: WASSA-2018 Implicit Emotions Shared Task

Roman Klinger1, Orph´ee De Clercq2, Saif M. Mohammad3, and Alexandra Balahur4

1 Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart, Germany

klinger@ims.uni-stuttgart.de

2 LT3, Language and Translation Technology Team, Ghent University, Belgium

orphee.declercq@ugent.be

3 National Research Council Canada, Ottawa, Ontario, Canada

saif.mohammad@nrc-cnrc.gc.ca

4 Text and Data Mining Unit, European Commission Joint Research Centre, Ispra, Italy

alexandra.balahur@ec.europa.eu

Abstract

Past shared tasks on emotions use data with
both overt expressions of emotions (I am so
happy to see you!) as well as subtle expres-
sions where the emotions have to be inferred,
for instance from event descriptions. Further,
most datasets do not focus on the cause or the
stimulus of the emotion. Here, for the ﬁrst
time, we propose a shared task where systems
have to predict the emotions in a large auto-
matically labeled dataset of tweets without ac-
cess to words denoting emotions. Based on
this intention, we call this the Implicit Emotion
Shared Task (IEST) because the systems have
to infer the emotion mostly from the context.
Every tweet has an occurrence of an explicit
emotion word that is masked. The tweets are
collected in a manner such that they are likely
to include a description of the cause of the
emotion – the stimulus. Altogether, 30 teams
submitted results which range from macro F1
scores of 21 % to 71 %. The baseline (Max-
Ent bag of words and bigrams) obtains an F1
score of 60 % which was available to the partic-
ipants during the development phase. A study
with human annotators suggests that automatic
methods outperform human predictions, pos-
sibly by honing into subtle textual clues not
used by humans. Corpora, resources, and re-
sults are available at the shared task website at
http://implicitemotions.wassa2018.com.

1

Introduction

The deﬁnition of emotion has long been debated.
The main subjects of discussion are the origin of
the emotion (physiological or cognitive), the com-
ponents it has (cognition, feeling, behaviour) and

the manner in which it can be measured (categori-
cally or with continuous dimensions). The Implicit
Emotion Shared Task (IEST) is based on Scherer
(2005), who considers emotion as “an episode of
interrelated, synchronized changes in the states of
all or most of the ﬁve organismic subsystems (in-
formation processing, support, executive, action,
monitor) in response to the evaluation of an exter-
nal or internal stimulus event as relevant to major
concerns of the organism”.

This deﬁnition suggests that emotion is triggered
by the interpretation of a stimulus event (i. e., a sit-
uation) according to its meaning, the criteria of
relevance to the personal goals, needs, values and
the capacity to react. As such, while most situations
will trigger the same emotional reaction in most
people, there are situations that may trigger differ-
ent affective responses in different people. This is
explained more in detail by the psychological the-
ories of emotion known as the “appraisal theories”
(Scherer, 2005).

Emotion recognition from text is a research area
in natural language processing (NLP) concerned
with the classiﬁcation of words, phrases, or doc-
uments into predeﬁned emotion categories or di-
mensions. Most research focuses on discrete emo-
tion recognition, which assigns categorical emo-
tion labels (Ekman, 1992; Plutchik, 2001), e. g.,
Anger, Anticipation, Disgust, Fear, Joy, Sadness,
Surprise and Trust.1 Previous research developed
statistical, dictionary, and rule-based models for

1Some shared tasks on ﬁne emotion intensity include the
SemEval-2007 Task 14, WASSA-2017 shared task EmoInt
(Mohammad and Bravo-Marquez, 2017), and SemEval-2018
Task 1 (Mohammad et al., 2018).

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages31–42Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P1731several domains, including fairy tales (Alm et al.,
2005), blogs (Aman and Szpakowicz, 2007) and
microblogs (Dodds et al., 2011). Presumably, most
models built on such datasets rely on emotion
words (or their representations) whenever acces-
sible and are therefore not forced to learn associ-
ations for more subtle descriptions. Such models
might fail to predict the correct emotion when such
overt words are not accessible. Consider the in-
stance “when my child was born” from the ISEAR
corpus, a resource in which people have been asked
to report on events when they felt a speciﬁc prede-
ﬁned emotion. This example does not contain any
emotion word itself, though one might argue that
the words “child” and “born” have a positive prior
connotation.

Balahur et al. (2012b) showed that the inference
of affect from text often results from the interpreta-
tion of the situation presented therein. Therefore,
speciﬁc approaches have to be designed to under-
stand the emotion that is generally triggered by situ-
ations. Such approaches require common sense and
world knowledge (Liu et al., 2003; Cambria et al.,
2009). Gathering world knowledge to support NLP
is challenging, although different resources have
been built to this aim – e. g., Cyc2 and ConceptNet
(Liu and Singh, 2004).

On a different research branch, the ﬁeld of dis-
tant supervision and weak supervision addresses
the challenge that manually annotating data is te-
dious and expensive. Distant supervision tackles
this by making use of structured resources to au-
tomatically label data (Mintz et al., 2009; Riedel
et al., 2010; Mohammad, 2012). This approach has
been adapted in emotion analysis by using informa-
tion assigned by authors to their own text, with the
use of hashtags and emoticons (Wang et al., 2012).
With the Implicit Emotion Shared Task (IEST),
we aim at combining these two research branches:
On the one hand, we use distant supervision to
compile a corpus of substantial size. On the other
hand, we limit the corpus to those texts which are
likely to contain descriptions of the cause of the
emotion – the stimulus. Due to the ease of access
and the variability and data richness on Twitter,
we opt for compiling a corpus of microposts, from
which we sample tweets that contain an emotion
word followed by ‘that’, ‘when’, or ‘because’. We
then mask the emotion word and ask systems to
predict the emotion category associated with that

2http://www.cyc.com

word.3 The emotion category can be one of six
classes: anger, disgust, fear, joy, sadness, and sur-
prise. Examples from the data are:

(1) “It’s [#TARGETWORD#] when you
feel like you are invisible to others.”
(2) “My step mom got so [#TARGET-
WORD#] when she came home from
work and saw that the boys didn’t come
to Austin with me.”

In Example 1, the inference is that feeling invisible
typically makes us sad. In Example 2, the context
is presumably that a person (mom) expected some-
thing else than what was expected. This in isolation
might cause anger or sadness, however, since “the
boys are home” the mother is likely happy. Note
that such examples can be used as source of com-
monsense or world knowledge to detect emotions
from contexts where the emotion is not explicitly
implied.

The shared task was conducted between 15
March 2018 (publication of train and trial data)
and the evaluation phase, which ran from 2 to 9
July. Submissions were managed on CodaLab4.
The best performing systems are all ensembles of
deep learning approaches. Several systems make
use of external additional resources such as pre-
trained word vectors, affect lexicons, and language
models ﬁne-tuned to the task.

The rest of the paper is organized as follows: we
ﬁrst review related work (Section 2). Section 3
introduces the shared task, the data used, and the
setup. The results are presented in Section 4, in-
cluding the ofﬁcial results and a discussion of dif-
ferent submissions. The automatic system’s pre-
dictions are then compared to human performance
in Section 5, where we report on a crowdsourcing
study with the data used for the shared task. We
conclude in Section 6.

2 Related Work

Related work is found in different directions of
research on emotion detection in NLP: resource
creation and emotion classiﬁcation, as well as mod-
eling the emotion itself.

Modeling the emotion computationally has been
approached from the perspective of humans needs

3This gives the shared task a mixed ﬂavor of both text
classiﬁcation and word prediction, in the spirit of distributional
semantics.

4https://competitions.codalab.org/competitions/19214

32and desires with the goal of simulating human re-
actions. Dyer (1987) presents three models which
take into account characters, arguments, emotion
experiencers, and events. These aspects are mod-
eled with ﬁrst order logic in a procedural manner.
Similarly, Subasic and Huettner (2001) use fuzzy
logic for such modeling in order to consider grad-
ual differences. A similar approach is followed by
the OCC model (Ortony et al., 1990), for which
Udochukwu and He (2015) show how to connect it
to text in a rule-based manner for implicit emotion
detection. Despite of this early work on holistic
computational models of emotions, NLP focused
mostly on a more coarse-grained level.

One of the ﬁrst corpora annotated for emotions
is that by Alm et al. (2005) who analyze sentences
from fairy tales. Strapparava and Mihalcea (2007)
annotate news headlines with emotions and valence,
Mohammad et al. (2015) annotate tweets on elec-
tions, and Schuff et al. (2017) tweets of a stance
dataset (Mohammad et al., 2017). The SemEval-
2018 Task 1: Affect in Tweets (Mohammad et al.,
2018) includes several subtasks on inferring the af-
fectual state of a person from their tweet: emotion
intensity regression, emotion intensity ordinal clas-
siﬁcation, valence (sentiment) regression, valence
ordinal classiﬁcation, and multi-label emotion clas-
siﬁcation. In all of these prior shared tasks and
datasets, no distinction is made between implicit
or explicit mentions of the emotions. We refer the
reader to Bostan and Klinger (2018) for a more de-
tailed overview of emotion classiﬁcation datasets.
Few authors speciﬁcally analyze which phrase
triggers the perception of an emotion. Aman and
Szpakowicz (2007) focus on the annotation on doc-
ument level but also mark emotion indicators. Mo-
hammad et al. (2014) annotate electoral tweets
for semantic roles such as emotion and stimulus
(from FrameNet). Ghazi et al. (2015) annotate
a subset of Aman and Szpakowicz (2007) with
causes (inspired by the FrameNet structure). Kim
and Klinger (2018) and Neviarouskaya and Aono
(2013) similarly annotate emotion holders, targets,
and causes as well as the trigger words.

One of the oldest resources nowadays used for
emotion recognition is the ISEAR set (Scherer,
1997) which consists of self-reports of emotional
events. As the task of participants in a psycho-
logical study was not to express an emotion but
to report on an event in which they experienced
a given emotion, this resource can be considered

similar to our goal of focusing on implicit emotion
expressions.

With the aim to extend the coverage of ISEAR,
Balahur et al. (2011, 2012a) build EmotiNet, a
knowledge base to store situations and the affective
reactions they have the potential to trigger. They
show how the knowledge stored can be expanded
using lexical and semantic similarity, as well as
through the use of Web-extracted knowledge (Bal-
ahur et al., 2013). The patterns used to populate
the database are of the type “I feel [emotion] when
[situation]”, which was also a starting point for our
task.

Finally, several approaches take into consider-
ation distant supervision (Mohammad and Kir-
itchenko, 2015; Abdul-Mageed and Ungar, 2017;
De Choudhury et al., 2012; Liu et al., 2017, i. a.).
This is motivated by the high availability of user-
generated text and by the challenge that manual
annotation is typically tedious or expensive. This
contrasts with the current data demand of machine
learning, and especially, deep learning approaches.
With our work in IEST, we combine the goal of
the development of models which are able to recog-
nize emotions from implicit descriptions without
having access to explicit emotion words, with the
paradigm of distant supervision.

3 Shared Task
3.1 Data
The aim of the Implicit Emotion Shared Task
is to force models to infer emotions from the
context of emotion words without having access
to them. Speciﬁcally, the aim is that models infer
the emotion through the causes mentioned in the
text. Thus, we build the corpus of Twitter posts
by polling the Twitter API5 for the expression
‘EMOTION-WORD (that|because|when)’,
where EMOTION-WORD contains a synonym for
one out of six emotions.6 The synonyms are shown
in Table 1. The requirement of tweets to have
either ‘that’, ‘because’, or ‘when’ immediately
after the emotion word means that the tweet likely
describes the cause of the emotion.

The initially retrieved large dataset has a distribu-
tion of 25 % surprise, 23 % sadness, 18 % joy, 16 %
fear, 10 % anger, 8 % disgust. We discard tweets

5https://developer.twitter.com/en/docs.html
6Note that we do not check that there is a white space
before the emotion word, which leads to tweets containing
. . . “unEMOTION-word. . . ”.

33Emotion Abbr.
Anger
Fear
Disgust
Joy
Sadness
Surprise

A
F
D
J
Sa
Su

Synonyms
angry, furious
afraid, frightened, scared, fearful
disgusted, disgusting
cheerful, happy, joyful
sad, depressed, sorrowful
surprising,
astonished,
shocked, startled, astounded, stunned

surprised,

Predicted Labels

A
2431
426
430
378
450
411

D
476
2991
249
169
455
508

F
496
245
3016
290
313
454

J
390
213
327
3698
458
310

Sa
410
397
251
366
2335
279

Su
426
522
518
345
329
2930

s A
D
F
J
Sa
Su

l
e
b
a
L
d
l
o
G

Table 1: Emotion synonyms used when polling Twitter.

Table 3: Confusion Matrix on Test Data for Baseline.

Emotion
Anger
Disgust
Fear
Joy
Sadness
Surprise
Sum

Train
25562
25558
25575
27958
23165
25565
153383

Trial
1600
1597
1598
1736
1460
1600
9591

Test
4794
4794
4791
5246
4340
4792
28757

Table 2: Distribution of IEST data.

with more than one emotion word, as well as exact
duplicates, and mask usernames and URLs. From
this set, we randomly sample 80 % of the tweets
to form the training set (153,600 instances), 5 %
as trial set (9,600 instances), and 15 % as test set
(28,800 instances). We perform stratiﬁed sampling
to obtain a balanced dataset. While the shared task
took place, two errors in the data preprocessing
were discovered by participants (the use of the word
unhappy as synonym for sadness, which lead to in-
consistent preprocessing in the context of negated
expressions, and the occurrence of instances with-
out emotion words). To keep the change of the data
at a minimum, the erroneous instances were only
removed, which leads to a distribution of the data
as shown in Table 2.

3.2 Task Setup
The shared task was announced through a dedi-
cated website (http://implicitemotions.wassa2018.
com/) and computational-linguistics-speciﬁc mail-
ing lists. The organizers published an evalua-
tion script which calculates precision, recall, and
F1 measure for each emotion class as well as micro
and macro average. Due to the nearly balanced
dataset, the chosen ofﬁcial metric for ranking sub-
mitted systems is the macro-F1 measure.

In addition to the data, the participants were
provided a list of resources they might want to
use7 (and they were allowed to use any other
resources they have access to or create them-

Predicted Labels

A
3182
407
403
297
443
411

D
313
3344
129
67
340
367

F
293
134
3490
161
171
293

J
224
102
196
4284
240
209

Sa
329
336
190
220
2947
176

Su
453
471
383
217
199
3336

s A
D
F
J
Sa
Su

l
e
b
a
L
d
l
o
G

Table 4: Confusion Matrix on Test Data of Best Sub-
mitted System

selves). We also provided access to a baseline
system.8 This baseline is a maximum entropy
classiﬁer with L2 regularization. Strings which
match [#a-zA-Z0-9_=]+|[ˆ ] form tokens.
As preprocessing, all symbols which are not al-
phanumeric or contain the # sign are removed.
Based on that, unigrams and bigrams form the
Boolean features as a set of words for the classiﬁer.

4 Results
4.1 Baseline
The intention of the baseline implementation was
to provide participants with an intuition of the dif-
ﬁculty of the task. It reaches 59.88 % macro F1
on the test data, which is very similar to the trial
data result (60.1 % F1). The confusion matrix for
the baseline is presented in Table 3; the confusion
matrix for the best submitted system is shown in
Table 4.

4.2 Submission Results
Table 5 shows the main results of the shared task.
We received submissions through CodaLab from
thirty participants. Twenty-six teams responded
to a post-competition survey providing additional
information regarding team members (56 people in
total) and the systems that were developed. For the
remaining analyses and the ranking, we only report
on these twenty-six teams.

7http://implicitemotions.wassa2018.com/resources/

8https://bitbucket.org/rklinger/simpletextclassiﬁer

34id
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31

Team
Amobee
IIIDYT
NTUA-SLP
UBC-NLP
Sentylic
HUMIR
nlp
DataSEARCH
YNU1510
EmotiKLUE
wojtek.pierre
hgsgnlp
UWB
NL-FIIT
TubOslo
YNU Lab
Braint
EmoNLP
RW
Baseline
USI-IR
THU NGN
SINAI
UTFPR
CNHZ2017
lyb3b
Adobe Research
Anonymous
dinel
CHANDA
NLP LDW

F1

71.45
71.05
70.29
69.28
69.20
68.64
68.48
68.04
67.63
67.13
66.15
65.80
65.70
65.52
64.63
64.10
62.61
62.11
60.97
59.88
58.37
58.01
57.94
56.92
56.40
55.87
53.08
50.38
49.99
41.89
21.03

Rank
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
(19)

(20)
(21)
(22)
(23)

(24)

(25)
(26)

B
3
3
4
6
7
8
9
10
11
11
15
15
15
15
17
17
19
19
20
21
22
23
24
26
27
27
28
29
30
31

w
o
ﬂ
r
o
s
n
e
T

s
a
d
n
a
P

n
r
a
e
L

t
i

K
i
c
S

k
n
a
R

s
a
r
e
K

K
T
L
N

e
V
o
l
G

m
i
s
n
e
G

h
c
r
o
T
y
P

t
x
e
T
t
s
a
F

y
C
a
p
S

a
k
e
W

o
M
L
E

r
a
e
n
i
L
b
i
L

o
n
a
e
h
T

(cid:88)

(cid:88)
(cid:88)

(cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88)

1 (cid:88) (cid:88) (cid:88)
2
(cid:88) (cid:88)
3
4 (cid:88)
(cid:88) (cid:88)
5 (cid:88) (cid:88) (cid:88) (cid:88)
6 (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)
7 (cid:88) (cid:88)
8 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
9 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)
10 (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)
11
12 (cid:88)
(cid:88)
(cid:88) (cid:88)
13 (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)
14 (cid:88) (cid:88) (cid:88)
(cid:88)
15
16 (cid:88) (cid:88) (cid:88)
17 (cid:88)
18 (cid:88) (cid:88)
19
20
21 (cid:88) (cid:88)
22 (cid:88) (cid:88) (cid:88)
23
24
25
26

(cid:88) (cid:88)
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88) (cid:88) (cid:88) (cid:88)
(cid:88) (cid:88)

(cid:88) (cid:88)
(cid:88)

(cid:88) (cid:88)

(cid:88) (cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

16 14 14 13 13 11 11

5 5

3 3 3 1 1

Table 6: Overview of tools employed by different
teams (sorted by popularity from left to right).

Table 5: Ofﬁcial results of IEST 2018. Participants
who did not report on the system details did not get
assigned a rank and are reported in gray. Column B
provides the ﬁrst row in the results table to which the re-
spective row is signiﬁcantly different (conﬁdence level
0.99), tested with bootstrap resampling.

The table shows results from 31 systems, includ-
ing the baseline results which have been made avail-
able to participants during the shared task started.
From all submissions, 19 submissions scored above
the baseline. The best scoring system is from
team Amobee, followed by IIDYT and NTUA-SLP.
The ﬁrst two results are not signiﬁcantly differ-
ent, as tested with the Wilcoxon (1945) sign test
(p < 0.01) and with bootstrap resampling (conﬁ-
dence level 0.99).

Table 10 in the Appendix shows a breakdown
of the results by emotion class. Though the data
was nearly balanced, joy is mostly predicted with
highest performance, followed by fear and disgust.
The prediction of surprise and anger shows a lower
performance.

Note that the macro F1 evaluation took into ac-
count all classes which were either predicted or in
the gold data. Two teams submitted results which

contain labels not present in the gold data, which
reduced the macro-F1 dramatically. With an evalu-
ation only taking into account 6 labels, id 22 would
be on rank 9 and id 28 would be on rank 10.

4.3 Review of Methods

Table 6 shows that many participants use high-level
libraries like Keras or NLTK. Tensorﬂow is only
of medium popularity and Theano is only used
by one participant. Table 7 shows a summary of
machine learning methods used by the teams, as
reported by themselves. Nearly every team uses
embeddings and neural networks; many teams use
an ensemble of architectures. Several teams use
language models showing a current trend in NLP to
ﬁne-tune those to speciﬁc tasks (Howard and Ruder,
2018). Presumably, those are speciﬁcally helpful
in our task due to its word-prediction aspect.

Finally, Table 8 summarizes the different kinds
of information sources taken into account by the
teams. Several teams use affect lexicons in addi-
tion to word information and emoji-speciﬁc infor-
mation. The incorporation of statistical knowledge
from unlabeled corpora is also popular.

35P
L
M

(cid:88)

(cid:88)

t
s
e
r
r
o
F
m
o
d
n
a
R

r
e
d
o
c
n
e
o
t
u
A

s
n
a
e

M
-
k

g
n
i
g
g
a
B

A
D
L

(cid:88)

(cid:88)

(cid:88)

(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88) (cid:88)
(cid:88)

(cid:88)

/

U
R
G
N
N
R
M
T
S
L

/

s
e
l
u
s
p
a
C
N
N
C

/

e
l
b
m
e
s
n
E

n
o
i
t
n
e
t
t

A

g
n
i
n
r
a
e
L
r
e
f
s
n
a
r
T

r
e
ﬁ
i
s
s
a
l
C

r
a
e
n
i
L

l
e
d
o
m
e
g
a
u
g
n
a
L

s
g
n
i
d
d
e
b
m
E

k
n
a
R

(cid:88)

(cid:88)
(cid:88) (cid:88)

1 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
2 (cid:88) (cid:88) (cid:88)
3 (cid:88) (cid:88) (cid:88)
(cid:88)
4 (cid:88) (cid:88) (cid:88)
(cid:88)
5 (cid:88) (cid:88) (cid:88) (cid:88)
6 (cid:88) (cid:88) (cid:88)
7 (cid:88) (cid:88) (cid:88)
(cid:88)
8 (cid:88) (cid:88)
9 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
10 (cid:88) (cid:88)
11 (cid:88) (cid:88)
12 (cid:88)
13 (cid:88) (cid:88)
14 (cid:88) (cid:88)
15
16 (cid:88) (cid:88)
17 (cid:88) (cid:88)
18 (cid:88) (cid:88) (cid:88) (cid:88)
19
20 (cid:88) (cid:88)
21 (cid:88) (cid:88) (cid:88) (cid:88)
22 (cid:88) (cid:88)
23 (cid:88) (cid:88)
24 (cid:88)
25 (cid:88)
26

(cid:88) (cid:88)

(cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

23 20 12 9 7 5 5 3 2 1 1 1 1 1

Table 7: Overview of methods employed by different
teams (sorted by popularity from left to right).

4.4 Top 3 Submissions
In the following, we brieﬂy summarize the ap-
proaches used by the top three teams: Amobee,
IIIDYT, and NTUA-SLP. For more information on
these approaches and those of the other teams, we
refer the reader to the individual system description
papers. The three best performing systems are all
ensemble approaches. However, they make use of
different underlying machine learning architectures
and rely on different kinds of information.

4.4.1 Amobee
The top-ranking system, Amobee, is an ensemble
approach of several models (Rozental et al., 2018).
First, the team trains a Twitter-speciﬁc language
model based on the transformer decoder architec-
ture using 5B tweets as training data. This model
is used to ﬁnd the probabilities of potential miss-
ing words, conditional upon the missing word de-
scribing one of the six emotions. Next, the team
applies transfer learning from the trained models
they developed for SemEval 2018 Task 1: Affect
in Tweets (Rozental and Fleischer, 2018). Finally,
they directly train on the data provided in the shared
task while incorporating outputs from DeepMoji

a
r
o
p
r
o
C
d
e
l
e
b
a
l
n
U

t
n
e
m
u
c
o
D
/
e
c
n
e
t
n
e
S

.

b
m
E
n
o
i
t
o
m
E

l
a
v
E
m
e
S

s
n
o
c
i
x
e
L

s
r
e
t
c
a
r
a
h
C

i
j
o
m
E

k
n
a
R

s
d
r
o
W

(cid:88) (cid:88) (cid:88) (cid:88)
(cid:88)
(cid:88) (cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88) (cid:88)

(cid:88) (cid:88)

1 (cid:88) (cid:88)
2 (cid:88)
3 (cid:88)
4 (cid:88)
5 (cid:88)
6 (cid:88) (cid:88)
7 (cid:88)
8 (cid:88)
9 (cid:88)
10 (cid:88)
11 (cid:88) (cid:88) (cid:88)
12 (cid:88) (cid:88) (cid:88)
13 (cid:88)
14 (cid:88)
15 (cid:88)
16 (cid:88)
17 (cid:88)
18 (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
19 (cid:88)
20 (cid:88) (cid:88)
21 (cid:88) (cid:88) (cid:88)
22 (cid:88) (cid:88)
23 (cid:88)
(cid:88)
24 (cid:88) (cid:88)
25 (cid:88)
26 (cid:88)
26

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

9

8

7

6

4

(cid:88)

(cid:88)

(cid:88)

4

3

Table 8: Overview of information sources employed by
different teams (sorted by popularity from left to right).

(Felbo et al., 2017) and “Universal Sentence En-
coder” (Cer et al., 2018) as features.

IIIDYT

4.4.2
The second-ranking system, IIIDYT (Balazs et al.,
2018), preprocesses the dataset by tokenizing the
sentences (including emojis), and normalizing the
USERNAME, NEWLINE, URL and TRIGGER-
WORD indicators. Then, it feeds word-level rep-
resentations returned by a pretrained ELMo layer
into a Bi-LSTM with 1 layer of 2048 hidden units
for each direction. The Bi-LSTM output word rep-
resentations are max-pooled to generate sentence-
level representations, followed by a single hidden
layer of 512 units and output size of 6. The team
trains six models with different random initializa-
tions, obtains the probability distributions for each
example, and then averages these to obtain the ﬁnal
label prediction.

4.4.3 NTUA-SLP
The NTUA-SLP system (Chronopoulou et al.,
2018) is an ensemble of three different generic
models. For the ﬁrst model, the team pretrains
Twitter embeddings with the word2vec skip-gram

36model using a large Twitter corpus. Then, these
pretrained embeddings are fed to a neural classi-
ﬁer with 2 layers, each consisting of 400 bi-LSTM
units with attention. For the second model, they
use transfer learning of a pretrained classiﬁer on
a 3-class sentiment classiﬁcation task (Semeval17
Task4A) and then apply ﬁne-tuning to the IEST
dataset. Finally, for the third model the team uses
transfer learning of a pretrained language model,
according to Howard and Ruder (2018). They ﬁrst
train 3 language models on 3 different Twitter cor-
pora (2M, 3M, 5M) and then they ﬁne-tune them
to the IEST dataset with gradual unfreezing.

4.5 Error Analysis
Table 11 in the Appendix shows a subsample of
instances which are predicted correctly by all teams
(marked as +, including the baseline system and
those who did not report on system details) and that
were not predicted correctly by any team (marked
as −), separated by correct emotion label.

For the positive examples which are correctly
predicted by all teams, speciﬁc patterns reoccur.
For anger, the author of the ﬁrst example encour-
ages the reader not to be afraid – a prompt which
might be less likely for other emotions. For several
emotions, single words or phrases are presumably
associated with such emotions, e. g., “hungry” with
anger, “underwear”, “sweat”, “ewww” with dis-
gust, “leaving”, “depression” for sadness, “why am
i not” for surprise.

Several examples which are all correctly pre-
dicted by all teams for joy include the syllable “un”
preceding the triggerword – a pattern more frequent
for this emotion than for others. Another pattern
is the phrase “fast and furious” (with furious for
anger) which should be considered a mistake in the
sampling procedure, as it refers to a movie instead
of an emotion expression.

Negative examples appear to be reasonable when
the emotion is given but may also be valid with
other labels than the gold. For disgust, respec-
tive emotion synonyms are often used as a strong
expression actually referring to other negative emo-
tions. Especially for sadness, the negative exam-
ples include comparably long event descriptions.

5 Comparison to Human Performance

An interesting research question is how accurately
native speakers of a language can predict the emo-
tion class when the emotion word is removed from

Predicted Labels

A
349
195
94
39
88
123

D
40
92
20
6
37
25

F
34
30
265
22
23
29

J
55
84
92
398
89
132

Sa
95
157
120
36
401
53

Su
43
69
42
13
46
183

s A
D
F
J
Sa
Su

l
e
b
a
L
d
l
o
G

Table 9: Confusion Matrix Sample Annotated by Hu-
mans in Crowdsourcing

a tweet. Thus we conducted a crowdsourced study
asking humans to perform the same task as pro-
posed for automatic systems in this shared task.

We sampled 900 instances from the IEST data:
50 tweets for each of the six emotions in 18
pair-wise combinations with ‘because’, ‘that’, and
‘when’. The tweets and annotation questionnaires
were uploaded on a crowdsourcing platform, Figure
Eight (earlier called CrowdFlower).9 The question-
naire asked for the best guess for the emotion (Q1)
as well as any other emotion that they think might
apply (Q2).

About 5 % of the tweets were annotated inter-
nally beforehand for Q1 (by one of the authors of
this paper). These tweets are referred to as gold
tweets. The gold tweets were interspersed with
other tweets. If a crowd-worker got a gold tweet
question wrong, they were immediately notiﬁed
of the error. If the worker’s accuracy on the gold
tweet questions fell below 70 %, they were refused
further annotation, and all of their annotations were
discarded. This served as a mechanism to avoid
malicious annotations.

Each tweet is annotated by at least three people.
A total of 3,619 human judgments of emotion asso-
ciated with the trigger word were obtained. Each
judgment included the best guess for the emotion
(response to Q1) as well as any other emotion that
they think might apply (response to Q2). The an-
swer to Q1 corresponds to the shared task setting.
However, automatic systems were not given the
option of providing additional emotions that might
apply (Q2).

The macro F1 for predicting the emotion is 45 %
(Q1, micro F1 of 0.47). Observe that human perfor-
mance is lower than what automatic systems reach
in the shared task. The correct emotion was present
in the top two guessed emotions in 57 % of the
cases. Perhaps, the automatic systems are honing

9https://www.ﬁgure-eight.com

37in to some subtle systematic regularities in hope
that particular emotion words are used (for exam-
ple, the function words in the immediate neighbor-
hood of the target word). It should also be noted,
however, that the data used for human annotations
was only a subsample of the IEST data.

An analysis of subsets of Tweets containing the
words because, that, and when after the emotion
word shows that Tweets with “that” are more dif-
ﬁcult (41 % accuracy) than with “when” (49 %)
and “because” (51 %). This relationship between
performance and query string is not observed in
the baseline system – here, accuracy on the test
data (on the data used for human evaluation) for
the “that” subset is 61 % (60 %), for “when” 62 %
(53 %), and for “because” 55 % (50 %) – there-
fore, the automatic system is most challenged by
“because”, while humans are more challenged by
“that”. Please note that this comparison on the test
data is somewhat unfair since for the human anal-
ysis, the data was sampled in a stratiﬁed manner,
but not for the automatic prediction. The test data
contains 5635 “because” tweets, 13649 with “that”
and 9474 with “when”.

There are differences in the difﬁculty of the task
for different emotions: The accuracy (F1) by emo-
tion is 57 % (46 %) for anger, 15 % (21 %) for dis-
gust, 42 % (51 %) for fear, 77 % (58 %) for joy,
59 % (52 %) for sadness and 34 % (39 %) for sur-
prise. The confusion matrix is depicted in Table 9.
Disgust is often confused with anger, followed by
fear being confused with sadness. Surprise is often
confused with anger and joy.

6 Conclusions & Future Work

With this paper and the Implicit Emotion Shared
Task, we presented the ﬁrst dataset and joint effort
to focus on causal descriptions to infer emotions
that are triggered by speciﬁc life situations on a
large scale. A substantial number of participating
systems presented the current state of the art in text
classiﬁcation in general and transferred it to the
task of emotion classiﬁcation.

Based on the experiences during the organiza-
tion and preparation of this shared task, we plan
the following steps for a potential second iteration.
The dataset was now constructed via distant super-
vision, which might be a cause for inconsistencies
in the dataset. We plan to use crowdsourcing as
applied for the estimation of human performance
to improve preprocessing of the data. In addition,

as one participant noted, the emotion words which
were used to retrieve the data were removed, but,
in a subset of the data, other emotion words were
retained.

The next step, which we suggest to the partici-
pants and future researchers is introspection of the
models – carefully analyse them to prove that the
models actually learn to infer emotions from subtle
descriptions of situations, instead of purely associ-
ating emotion words with emotion labels. Similarly,
an open research question is how models developed
on the IEST data perform on other data sets. Bostan
and Klinger (2018) showed that transferring mod-
els from one corpus to another in emotion analysis
leads to drops in performance. Therefore, an inter-
esting option is to use transfer learning from estab-
lished corpora (which do not distinguish explicit
and implicit emotion statements) to the IEST data
and compare the models to those directly trained
on the IEST and vice versa.

Finally, another line of future research is the
application of the knowledge inferred to other tasks,
such as argument mining and sentiment analysis.

Acknowledgments

This work has been partially supported by the
German Research Council (DFG), project SEAT
(Structured Multi-Domain Emotion Analysis from
Text, KL 2869/1-1). We thank Evgeny Kim, Laura
Bostan, Jeremy Barnes, and Veronique Hoste for
fruitful discussions.

References
Muhammad Abdul-Mageed and Lyle Ungar. 2017.
Emonet: Fine-grained emotion detection with gated
In Proceedings of the
recurrent neural networks.
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
718–728, Vancouver, Canada. Association for Com-
putational Linguistics.

Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
Emotions from text: Machine learning
2005.
In Proceed-
for text-based emotion prediction.
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 579–586, Vancouver,
British Columbia, Canada. Association for Compu-
tational Linguistics.

Saima Aman and Stan Szpakowicz. 2007.

Identify-
ing expressions of emotion in text. In Text, Speech
and Dialogue, pages 196–205, Berlin, Heidelberg.
Springer Berlin Heidelberg.

38Alexandra Balahur, Jesus Hermida, and Andres Mon-
toyo. 2012a. Building and exploiting emotinet, a
knowledge base for emotion detection based on the
IEEE Transactions on Af-
appraisal theory model.
fective Computing, 3:88–101.

Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012. Not all moods are created equal! ex-
ploring human emotional states in social media. In
Sixth international AAAI conference on weblogs and
social media, pages 66–73.

Alexandra Balahur, Jes´us M. Hermida, Andr´es Mon-
toyo, and Rafael Mu˜noz. 2011. EmotiNet: A knowl-
edge base for emotion detection in text built on the
appraisal theories. In Natural Language Processing
and Information Systems, pages 27–39, Berlin, Hei-
delberg. Springer Berlin Heidelberg.

Alexandra Balahur, Jes´us M. Hermida, and Hristo
Tanev. 2013. Detecting implicit emotion expres-
sions from text using ontological resources and lex-
ical learning. In New Trends of Research in Ontolo-
gies and Lexical Resources:
Ideas, Projects, Sys-
tems, pages 235–255, Berlin, Heidelberg. Springer
Berlin Heidelberg.

Alexandra Balahur, Jess Hermida, and Andrs Montoyo.
2012b. Detecting implicit expressions of emotion
in text: A comparative analysis. Decision Support
Systems, 53(4):742753.

Jorge A. Balazs, Edison Marrese-Taylor, and Yutaka
Matsuo. 2018. IIIDYT at IEST 2018: Implicit Emo-
tion Classiﬁcation with Deep Contextualized Word
In Proceedings of the 9th Work-
Representations.
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, Brussels, Bel-
gium. Association for Computational Linguistics.

Laura Ana Maria Bostan and Roman Klinger. 2018. A
survey on annotated data sets for emotion classiﬁ-
In Proceedings of COLING 2018,
cation in text.
the 27th International Conference on Computational
Linguistics, Santa Fe, USA.

Erik Cambria, Amir Hussain, Catherine Havasi, and
Chris Eckl. 2009. Affectivespace: Blending com-
mon sense and affective knowledge to perform
1st Work-
emotive reasoning.
shop on Opinion Mining and Sentiment Analysis.
WOMSA’09: 1st Workshop on Opinion Mining and
Sentiment Analysis, pages 32–41, Seville, Spain.

In WOMSA09:

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris
Tar, Yun-Hsuan Sung, Brian Strope, and Ray
Kurzweil. 2018. Universal sentence encoder. CoRR,
abs/1803.11175.

Alexandra Chronopoulou, Aikaterini Margatina, Chris-
tos Baziotis, and Alexandros Potamianos. 2018.
NTUA-SLP at IEST 2018: Ensemble of neural trans-
fer methods for implicit emotion classiﬁcation.
In
Proceedings of the 9th Workshop on Computational
Approaches to Subjectivity, Sentiment and Social
Media Analysis, Brussels, Belgium. Association for
Computational Linguistics.

Peter S. Dodds, Kameron D. Harris,

Isabel M.
Kloumann, Catherine A. Bliss, and Christopher M.
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. PloS one, 6(12).

Michael G. Dyer. 1987. Emotions and their computa-
tions: Three computer models. Cognition and Emo-
tion, 1(3):323–347.

Paul Ekman. 1992. An argument for basic emotions.

Cognition & emotion, 6(3-4):169–200.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sarcasm.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1615–1625, Copenhagen, Denmark. Association for
Computational Linguistics.

Diman Ghazi, Diana Inkpen, and Stan Szpakowicz.
2015. Detecting emotion stimuli in emotion-bearing
In Computational Linguistics and In-
sentences.
telligent Text Processing, pages 152–165, Cham.
Springer International Publishing.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 328–339, Melbourne, Australia.
Association for Computational Linguistics.

Evgeny Kim and Roman Klinger. 2018. Who feels
what and why? annotation of a literature corpus
In Proceedings
with semantic roles of emotions.
of COLING 2018, the 27th International Conference
on Computational Linguistics, Santa Fe, USA.

Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 8th International
Conference on Intelligent User Interfaces, IUI ’03,
pages 125–132, New York, NY, USA. ACM.

Hugo Liu and Push Singh. 2004. Conceptnet – a practi-
cal commonsense reasoning tool-kit. BT Technology
Journal, 22(4):211–226.

Vicki Liu, Carmen Banea, and Rada Mihalcea. 2017.
Grounded emotions. In 2017 Seventh International
Conference on Affective Computing and Intelligent
Interaction (ACII), pages 477–483, San Antonio,
Texas.

39Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Conference of the As-
sociation for Computational Linguistics and the In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing.

Saif Mohammad. 2012. #emotional tweets. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
246–255, Montr´eal, Canada. Association for Com-
putational Linguistics.

Saif M. Mohammad and Felipe Bravo-Marquez. 2017.
WASSA-2017 shared task on emotion intensity. In
Proceedings of the Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis (WASSA), Copenhagen, Denmark.

Saif M. Mohammad, Felipe Bravo-Marquez, Mo-
hammad Salameh, and Svetlana Kiritchenko. 2018.
Semeval-2018 Task 1: Affect in tweets. In Proceed-
ings of International Workshop on Semantic Evalua-
tion (SemEval-2018), New Orleans, LA, USA.

Saif M. Mohammad and Svetlana Kiritchenko. 2015.
Using hashtags
to capture ﬁne emotion cate-
gories from tweets. Computational Intelligence,
31(2):301–326.

Saif M. Mohammad, Parinaz Sobhani, and Svetlana
Kiritchenko. 2017. Stance and sentiment in tweets.
ACM Trans. Internet Technol., 17(3):26:1–26:23.

Saif M. Mohammad, Xiaodan Zhu, Svetlana Kir-
itchenko, and Joel Martin. 2015. Sentiment, emo-
tion, purpose, and style in electoral tweets. Informa-
tion Processing & Management, 51(4):480–499.

Saif M. Mohammad, Xiaodan Zhu, and Joel Martin.
2014. Semantic role labeling of emotions in tweets.
In Proceedings of the 5th Workshop on Computa-
tional Approaches to Subjectivity, Sentiment and So-
cial Media Analysis, pages 32–41, Baltimore, Mary-
land. Association for Computational Linguistics.

Alena Neviarouskaya and Masaki Aono. 2013. Extract-
ing causes of emotions from text. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 932–936.

Andrew Ortony, Gerald L. Clore, and Allan Collins.
1990. The cognitive structure of emotions. Cam-
bridge University Press.

Robert Plutchik. 2001. The nature of emotions hu-
man emotions have deep evolutionary roots, a fact
that may explain their complexity and provide tools
for clinical practice. American Scientist, 89(4):344–
350.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
In Proceedings of the Euro-
out Labeled Text.
pean Conference on Machine Learning and Prin-
ciples and Practice in Knowledge Discovery from
Databases.

Alon Rozental and Daniel Fleischer. 2018. Amobee at
semeval-2018 task 1: GRU neural network with a
CNN attention mechanism for sentiment classiﬁca-
tion. CoRR, abs/1804.04380.

Alon Rozental, Daniel Fleischer, and Zohar Kelrich.
2018. Amobee at IEST 2018: Transfer Learning
from Language Models. In Proceedings of the 9th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Klaus R Scherer. 1997. Proﬁles of emotion-antecedent
appraisal: Testing theoretical predictions across cul-
tures. Cognition & Emotion, 11(2):113–150.

Klaus. R. Scherer. 2005. What are emotions? and how
can they be measured? Social Science Information,
44(4):695–729.

Hendrik Schuff, Jeremy Barnes, Julian Mohme, Sebas-
tian Pad´o, and Roman Klinger. 2017. Annotation,
modelling and analysis of ﬁne-grained emotions on
In Pro-
a stance and sentiment detection corpus.
ceedings of the 8th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis, Copenhagen, Denmark. Workshop at
Conference on Empirical Methods in Natural Lan-
guage Processing, Association for Computational
Linguistics.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 70–74, Prague, Czech
Republic. Association for Computational Linguis-
tics.

Pero Subasic and Alison Huettner. 2001. Affect analy-
sis of text using fuzzy semantic typing. IEEE Trans-
actions on Fuzzy Systems, 9(4):483–496.

Orizu Udochukwu and Yulan He. 2015. A rule-based
approach to implicit emotion detection in text.
In
Natural Language Processing and Information Sys-
tems, pages 197–203, Cham. Springer International
Publishing.

Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter ”big
In So-
data” for automatic emotion identiﬁcation.
cialCom/PASSAT, pages 587–592. IEEE.

Frank Wilcoxon. 1945.

Individual comparisons by

ranking methods. Biometrics bulletin, 1(6):80–83.

40A Results by emotion class
Table 10 shows breakdown of the results by emotion class.

Team
Amobee
IIIDYT
NTUA-SLP
UBC-NLP
Sentylic
HUMIR
nlp
DataSEARCH
YNU1510
EmotiKLUE
wojtek.pierre
hgsgnlp
UWB
NL-FIIT
TubOslo
YNU Lab
Braint
EmoNLP
RW
Baseline
USI-IR
THU NGN
SINAI
UTFPR
CNHZ2017
lyb3b
AdobeResearch
Anonymous
dinel
CHANDA
NLP LDW

Joy
R F1
82
82
80
81
77
79
79
79
79
77
78
78
78
78
77
77
76
75
77
78
76
75
75
75
77
75
75
74
74
67
74
74
73
70
72
73
72
72
70
71
70
69
78
77
68
68
58
53
67
70
68
64
65
63
76
77
61
61
54
64
38
36

P
82
79
81
79
80
77
77
77
78
77
77
75
74
76
82
74
77
73
71
69
71
77
68
64
65
72
62
76
61
46
33

Sadness

Disgust

Anger

Surprise

P
70
71
71
67
68
70
68
66
64
69
67
66
61
62
62
66
61
62
60
58
58
69
52
54
58
58
52
64
52
39
18

R F1
69
68
69
67
66
69
67
67
67
66
66
64
65
62
64
65
64
64
64
59
64
61
62
59
68
64
63
64
62
63
61
56
60
60
57
60
59
57
56
54
54
51
63
66
52
52
57
60
52
47
52
46
52
52
65
67
43
37
38
36
12
14

P
73
70
72
69
69
70
70
69
68
67
66
67
74
69
62
63
60
63
62
62
59
68
59
59
58
55
52
70
52
54
20

R F1
72
70
71
71
70
71
69
68
69
69
69
68
69
67
68
68
68
68
67
67
67
68
67
66
59
65
66
63
65
68
65
67
64
68
62
63
62
63
62
62
59
59
68
68
59
60
58
58
59
59
58
62
51
52
67
64
50
49
47
42
31
25

P
62
66
63
62
63
61
62
61
60
60
57
59
57
61
59
55
56
55
55
54
49
60
52
50
51
46
48
62
44
38
22

R F1
64
66
64
63
64
63
62
63
62
61
62
63
62
63
62
62
62
63
60
61
58
60
59
59
63
60
59
57
58
56
58
61
55
55
56
56
53
52
52
51
53
58
63
62
52
51
52
53
50
48
50
53
45
46
60
59
47
50
37
37
26
24

P
66
66
62
65
63
61
62
64
64
60
62
59
66
58
57
63
60
56
56
55
57
61
56
51
49
47
49
59
44
51
18

R F1
68
70
68
71
71
67
66
67
66
69
65
69
65
68
65
65
64
65
64
68
62
63
63
67
56
61
61
65
62
66
60
56
59
57
61
58
58
60
57
59
53
50
66
64
55
55
56
62
53
58
49
50
52
50
64
69
48
54
29
20
7
10

Fear
R F1
75
73
74
75
73
74
73
73
73
73
72
70
72
72
71
71
72
71
71
69
69
70
69
69
73
69
69
70
67
66
68
70
65
66
64
64
63
63
63
63
61
62
68
70
61
61
61
56
58
57
59
58
54
55
71
68
50
50
46
58
17
18

P
77
76
75
73
73
74
72
72
73
72
69
69
65
68
68
66
63
64
62
63
59
71
61
66
58
60
56
74
51
39
18

Table 10: Results by emotion class. Note that this table is limited to the six emotion labels of interest in the
data set. However, other labels predicted than these six were taken into account for calculation of the ﬁnal macro
F1 score. Therefore, the macro F1 calculated from this table is different from the results in Table 5 in two cases
(THU NGN and Anonymous, who would be on rank 9 and rank 10, when predictions for classes outside the labels
were ignored.).

41B Examples
Table 11 shows examples which have been correctly or wrongly predicted by all instances. They are discussed in Section 4.5.
Emo. +/− Instance

+ You can’t spend your whole life holding the door open for people and then being TRIGGER when they

r
e
g
n
A

t
s
u
g
s
D

i

r
a
e
F

y
o
J

s
s
e
n
d
a
S

e
s
i
r
p
r
u
S

dont thank you. Nobody asked you to do it.

+ I get impatient and TRIGGER when I’m hungry
+ Anyone have the ﬁrst fast and TRIGGER that I can borrow?
− I’m kinda TRIGGER that I have to work on Father’s Day
− @USERNAME she’ll become TRIGGER that I live close by and she will ﬁnd me and punch me
− This has been such a miserable day and I’m TRIGGER because I wish I could’ve enjoyed myself more
+ I ﬁnd it TRIGGER when I can see your underwear through your leggings
+ @USERNAME ew ew eeww your weird I can’t I would feel so TRIGGER when people touch my hair
+ nyc smells TRIGGER when it’s wet.
− I wanted a cup of coffee for the train ride. Got ignored twice. I left TRIGGER because I can’t afford to
− So this thing where other black people ask where you’re ”really” from then act TRIGGER when you
− I’m so TRIGGER that I have to go to the post ofﬁce to get my jacket that i ordered because delivering it

reply with some US state. STAHP

miss my train. #needcoffee :(

was obviously rocket science

at me bc my dad laughed after he

+ @USERNAME & explain how much the boys mean to me but I’m too TRIGGER that they’ll just laugh

+ I threw up in a parking lot last night. I’m TRIGGER that’s becoming my thing. #illbutmostlymentally
+ When you holding back your emotions and you’re TRIGGER that when someone tries to comfort you

they’ll come spilling out http://url.removed

respond in Portuguese

− It’s so funny how people come up to me at work speaking Portuguese and they get TRIGGER when I
− @USERNAME it seems so fun but i haven’t got to try it yet. my mom and sis are always TRIGGER
− @USERNAME It’s hard to be TRIGGER when your giggle is so cute
+ maybe im so unTRIGGER because i never see the sunlight?
+ @USERNAME you’re so welcome !! i’m super TRIGGER that i’ve discovered ur work ! cant wait to see

when i try do something new with food.

segments is a rerun.

+ @USERNAME Im so TRIGGER that you guys had fun love you
− @USERNAME Not TRIGGER that your show is a rerun.
It seems every week one or more your
− I am actually TRIGGER when not invited to certain things. I don’t have the time and patience to pretend.
− This has been such a miserable day and I’m TRIGGER because I wish I could’ve enjoyed myself more
+ this award honestly made me so TRIGGER because my teacher is leaving http://url.removed
+ It is very TRIGGER that people think depression actually does work like that... http://url.removed
+ @USERNAME @USERNAME @USERNAME It’s also TRIGGER that you so hurt about it :’(
− Some bitch stole my seat then I had to steal the seat next to me. The boy looked TRIGGER when he
− I was so TRIGGER because I was having fun lol then i slipped cus I wasn’t wearing shoes
− @USERNAME I wipe at my eyes next, then swim a bit. ”I’m sorry.” I repeat, TRIGGER that I made him

saw me, and he was smart! #iwasgonnapass

more !!

worry.

+ why am i not TRIGGER that cal said that
+ @USERNAME why am I not TRIGGER that you’re the founder
+ @USERNAME I’m still TRIGGER when students know my name. I’m usually just ”that guy who wears

bow ties” =) (and there are a few at WC!)

− It’s TRIGGER when I see people that have the same phone as me no has htcs
− There is a little boy in here who is TRIGGER that he has to pay for things and that we won’t just give
− totally TRIGGER that my fams celebrating easter today because my sister goes back to uni sunday

him things

Table 11: Subsample of Tweets that were correctly predicted by all teams and of Tweets that were not
correctly predicted by any team.

42
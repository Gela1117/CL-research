Word Embeddings-Based Uncertainty Detection in Financial Disclosures

Christoph Kilian Theil, Sanja ˇStajner and Heiner Stuckenschmidt

Data and Web Science Group

University of Mannheim, Germany

{christoph, sanja, heiner}@informatik.uni-mannheim.de

Abstract

In this paper, we use NLP techniques
to detect linguistic uncertainty in ﬁnan-
cial disclosures.
Leveraging general-
domain and domain-speciﬁc word embed-
ding models, we automatically expand an
existing dictionary of uncertainty triggers.
We furthermore examine how an expert ﬁl-
tering affects the quality of such an ex-
pansion. We show that the dictionary
expansions signiﬁcantly improve regres-
sions on stock return volatility. Lastly,
we prove that the expansions signiﬁcantly
boost the automatic detection of uncertain
sentences.

1

Introduction

Despite its real world impact in tasks like volatil-
ity prediction, the automatic detection of linguis-
tic uncertainty has been left relatively untouched
in ﬁnance. Motivated by this research gap, we cre-
ated the ﬁrst classiﬁer capable of detecting uncer-
tain sentences in so-called 10-Ks. These annual
reports are required by the U.S. Securities and Ex-
change Commission (SEC) and give a comprehen-
sive overview of a company’s business activities.
We selected this disclosure type since it has to be
ﬁled by all public companies in the U.S., thus en-
suring a large sample size. Furthermore, it is the
only disclosure type for which a tailored dictio-
nary resource exists.

1.1 Loughran & McDonald’s Dictionary
As basis for our experiments, we took an exist-
ing ﬁnancial domain dictionary containing 297 un-
certain terms. This dictionary has been shown
to possess explanatory power of future stock re-
turn volatility (Loughran and McDonald, 2011)
and is the only of its kind speciﬁcally designed for

10-Ks.
Its creators developed it “with emphasis
on the general notion of imprecision rather than
exclusively focusing on risk” (Loughran and Mc-
Donald, 2011, p. 45). As this quote indicates,
on one hand, the dictionary contains terms mark-
ing imprecision (e.g. “could”, “may”, “probably”,
“somewhat”). On the other hand, it contains terms
referring to real-worldly risk and uncertainty (e.g.
“anomaly”, “risk”, “uncertainty”, “volatility”).

1.2 Contributions
We automatically expanded Loughran and Mc-
Donald’s (2011) uncertainty dictionary by adding
semantically close candidate terms according to
word embeddings. Apart from training our own
domain-speciﬁc embedding model, we compared
such an expansion to one using a general-domain
embedding model. Moreover, we investigated
whether manual ﬁltering of candidate terms by a
domain expert can further improve the results. We
evaluated the quality of our expansions in both a
set of regressions on stock return volatility and a
binary sentence classiﬁcation task by posing two
research questions:

– RQ1 How do a general-domain and a

domain-speciﬁc expansion compare?

– RQ2 How do an automatic and a semi-
automatic, expert-ﬁltered expansion com-
pare?

We show that our unﬁltered domain-speciﬁc ex-
pansion signiﬁcantly increases the explanatory
power of regressions on stock return volatility over
the plain dictionary. We furthermore introduce
a dataset of annual reports newly annotated for
this study and train a binary classiﬁer distinguish-
ing uncertain from certain sentences. Again, the
domain-speciﬁc expansion signiﬁcantly improves
the classiﬁcation performance over the plain dic-
tionary. In this case, however, the expert-ﬁltering

ProceedingsoftheFirstWorkshoponEconomicsandNaturalLanguageProcessing,pages32–37Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics32provides a small performance increase over the
fully automatic expansion.

2 Related Work
Loughran and McDonald (2011) introduced ﬁ-
nancial dictionaries spanning the categories of
positive, negative, litigious, strong modal, weak
modal, and—most important for us—uncertain
words. Perhaps not surprisingly, they ﬁnd that the
cumulative tf–idf of uncertain terms in a set of
10-Ks shares a positive and highly signiﬁcant re-
lation with future stock return volatility. To quan-
tify the improvement of our new expansions over
this dictionary, we use a regression setup similar to
their subsequent paper (Loughran and McDonald,
2014).

Tsai and Wang (2014) automatically expanded
said dictionaries by training word embeddings and
adding the 20 most cosine similar terms to each
original dictionary term. Using a dataset of 10-Ks,
they show that this expansion improves a predic-
tion of future stock return volatility.
In contrast
to them, we provide a systematic analysis how
a domain-speciﬁc vs. a general-domain (RQ1)
and an automatic vs. a semi-automatic expansion
(RQ2) perform in a set of regressions. Further-
more, for the ﬁrst time in the community, we per-
form a binary sentence classiﬁcation task on 10-Ks
to assess directly whether our models are indeed
suitable to detect linguistic uncertainty.

Theil et al. (2017) created the ﬁrst classiﬁer ca-
pable to detect uncertain sentences in the ﬁnan-
cial domain. Yet, they sample their sentences from
earnings call transcripts, a largely different disclo-
sure type than 10-Ks. Apart from typical charac-
teristics of spoken language such as less structure
and more spontaneity, these disclosures are volun-
tary and thus usually less available. As previous
studies have hinted that analyzing the language of
10-Ks can help to explain uncertainty of the in-
formation environment (Loughran and McDonald,
2011, 2014), we were further motivated to create
the ﬁrst sentence classiﬁer for 10-Ks.

downloaded

3 Data
We
and McDon-
ald’s (2011) dictionary1 of 297 ﬁnancial un-
certainty triggers such as “may”, “probably”,
or “volatility”. From now on, we refer to this
dictionary as Unc. We further downloaded all

Loughran

1https://sraf.nd.edu/textual-analysis/resources

220,565 10-Ks during 1994 to 2015 from the
SEC’s database EDGAR2. We removed duplicates
and ﬁlings shorter than 250 words, thus leaving
203,321 ﬁles. We divided this set into three
non-overlapping subsets: First, using word2vec
(Mikolov et al., 2013) with standard parameters,
we deployed 124,830 10-Ks (approximately 2.3
billion words) to train a domain-speciﬁc embed-
ding model. As benchmark, we also retrieved
Google’s generic word2vec embedding model,3
which was trained on approximately 100 billion
words from the Google News dataset.

Second, we used 76,991 10-Ks in our regres-
sions.
For each instance, we retrieved stock
pricing data from the databases CRSP4 and
CRSP/Compustat Merged. To facilitate replica-
tion, our data screening and parsing procedures
are described in greater detail in our Online Ap-
pendix.5 It further contains all textual and ﬁnan-
cial data needed to replicate our regressions.

Third, we used a random sample of 1,500 10-Ks
for the classiﬁcation task. Out of these, we ran-
domly sampled 100 sentences and let two anno-
tators of ﬁnancial and linguistic knowledge co-
annotate them as either certain or uncertain. The
guidelines which we gave to our annotators can be
found in the Online Appendix.

It has to be noted that the task of evaluating un-
certainty as an inherently subjective semantic con-
cept—especially in such a specialized domain as
ﬁnance—is of particular intricacy. First, consider
the following sentence, which both annotators la-
beled uncertain:
Example 3.1. “These factors raise substantial
doubt regarding the Company’s ability to continue
as a going concern.”

In contrast, consider the following sentence, on
which the annotators disagreed; words and phrases
considered to be uncertainty triggers by the anno-
tator proposing an uncertain label are underlined:
Example 3.2. “Fidelity is subject to interest rate
risk to the degree that its interest-bearing liabil-
ities, primarily deposits with short and medium
term maturities, mature or reprice at different rates
than its interest-earning assets.”

This sentence references “risk” and contains
2https://www.sec.gov/edgar.shtml
3https://code.google.com/archive/p/word2vec
4http://www.crsp.com
5http://dws.informatik.uni-mannheim.de/en/people/

researchers/christoph-kilian-theil/

33additional imprecisions, which speaks in favor
of an uncertain label. Yet, the referenced risk
is nonopaque and said imprecisions could be at-
tributed to legal requirements as inherent to any
regulated corporate disclosure; hence, a case could
also be made for an certain label.

Nevertheless, the IAA measured as κ (Cohen,
1960) was 0.73, which can be considered “sub-
stantial” (Landis and Koch, 1977). Notably, Gan-
ter and Strube (2009) report an even lower pair-
wise IAA with 0.45 ≤ κ ≤ 0.80, ¯xκ = 0.56 for
an annotation of Wikipedia sentences as certain or
uncertain. Despite making use of highly trained
domain experts, ˇStajner et al. (2017) also obtained
a lower IAA with 0.47 ≤ κ ≤ 0.70, ¯xκ = 0.61
for a comparable annotation task. They sampled
their sentences from transcribed debates held by
the U.S. central bank’s monetary policy commit-
tee (FOMC).

Given our comparably high IAA, we were con-
ﬁdent of our annotation quality and let the ﬁrst an-
notator annotate an additional 900 sentences, thus
forming our newly created dataset REPORTS. Out
of its 1,000 sentences, 870 were labeled certain
and 130 were labeled uncertain. This new dataset
can also be found in our Online Appendix as use-
ful resource for others to advance the ﬁeld.

4 Methodology
4.1 Expanding the Dictionary
To answer RQ1, we ﬁrst determined the 20 most
cosine similar terms according to the generic em-
bedding model for each of the 297 terms of Unc.
We chose 20 as the number of added terms since
this is the value suggested by Tsai and Wang
(2014). After lowercasing, we removed 28 anoma-
lous tokens (e.g. “##.million”), 1,657 n-grams,
and 2,139 duplicates. We excluded n-grams, since
Unc contains only unigrams and we wanted to
keep its expansions comparable. We added the re-
maining 2,036 terms to Unc and thus created Unc-
Gen with 2,333 terms.

For our domain-speciﬁc model, we derived a
list of 5,820 candidate terms and removed 1,947
duplicates. We did not lowercase in this case, as
this was already part of our preprocessing. We
again added the remaining 3,873 terms to Unc and
thus created UncSpec with 4,170 terms. Remark-
ably, UncGen and UncSpec share an overlap of
458 (23% and 12%) of the newly added terms,
which indicates that they employ a largely differ-

Figure 1: Candidate terms for “probably” in our
domain-speciﬁc embedding model. Terms re-
tained/removed by the annotator are marked by
dots/triangles. Dimensionality is reduced through
t-SNE (van der Maaten and Hinton, 2008).

ent vocabulary. An exemplary overview of added
terms according to both models can be found in
our Online Appendix.

We found that antonyms—despite their oppo-
site meaning—were frequently embedded in sim-
ilar semantic spaces. Coalescing relations of syn-
onymy and antonymy is a well-known and often
undesired property of distributional models (Mo-
hammed et al., 2008). Hence, it can be explained
why both UncGen as well as UncSpec contain the
token “certainly” as cosine similar term to “proba-
bly” (similarity of 0.68 and 0.45, respectively). In
addition, other irrelevant terms (e.g. “event”, “sig-
niﬁcance”) appeared frequently in close proximity
to uncertain terms.

This motivated us to explore how manual ﬁl-
tering could improve the expanded dictionaries
(RQ2). Therefore, we let an annotator of both
ﬁnancial and linguistic domain knowledge eval-
uate and remove such terms he deemed inappro-
priate to cover uncertainty. Figure 1 provides an
exemplary visualization of this procedure. Thus,
we created the dictionaries UncGenexp with 538
and UncSpecexp with 475 terms. Notably,
the
ﬁltering caused the vocabulary of both lists to
converge, as they now shared an overlap of 241
(45% and 51%) of the added terms. Finally, we
created the combinations UncGen+UncSpec and
UncGenexp+UncSpecexp. An overview of all dic-
tionaries can be found in Table 1.

34Table 1: Number of uncertainty triggers per dic-
tionary.

Dictionary
Unc
Automatic Expansions:
UncGen
UncSpec
UncGen+UncSpec
Expert-Aided Expansions:
UncGenexp
UncSpecexp
UncGenexp+UncSpecexp

# of Triggers

297

2,333
4,170
5,748

538
475
652

4.2 Regressing Uncertainty on Volatility
To assess the real world impact of our problem,
we performed event studies measuring the associ-
ation of linguistic uncertainty in our set of 76,991
10-Ks with volatility, a common measure of ﬁ-
nancial uncertainty. To be comparable with previ-
ous work (Loughran and McDonald, 2011, 2014),
we measure volatility as the deviation between
the expected and the actual returns after the re-
port’s ﬁling date in terms of root mean square error
(RMSE). We calculate the expected returns esti-
mating market models (Sharpe, 1963) using trad-
ing days [6, 28] relative to the ﬁling date.

In addition, following Loughran and McDonald
(2014), we used an extensive set of control vari-
ables: the intercepts α and the RMSE from market
models using trading days [−252,−6] as indica-
tors of historic performance and historic volatil-
ity. The ﬁling period abnormal return as absolute
value of the buy-and-hold return in trading days
[0, 1] minus the buy-and-hold return of the market
index. The ﬁrm size as current stock price mul-
tiplied by the number of outstanding shares. The
book-to-market ratio, a valuation measure, calcu-
lated as the book value of equity divided by the
market value of equity. Here, we only considered
ﬁrms with a positive book value and winsorized at
the 1% level. Lastly, we used a NASDAQ dummy
variable set to one if the ﬁrm is listed on the NAS-
DAQ at the time of the 10-K ﬁling, otherwise zero.
We calculated the cumulative tf–idf of all uncer-
tain terms according to the dictionary Unc and its
six expansions (see Table 1). Then, we conducted
seven regressions with each containing uncertainty
gauged via the respective dictionary as main inde-
pendent variable, the control variables, and post-
ﬁling volatility as dependent variable. This setup
allows us to quantify ﬁnancial uncertainty likely

Table 2: Results of the regression on volatility.
Standard errors are clustered by year and indus-
try. Coefﬁcients are standardized with a mean of
zero and a standard deviation of one.

Dictionary
Unc
UncGen
UncGenexp
UncSpec
UncSpecexp
UncGen+UncSpec
UncGenexp+UncSpecexp

Coefﬁcient
0.016∗
0.014∗
0.017∗
0.034∗∗∗
0.020∗
0.032∗∗∗
0.017∗
∗p ≤ 0.05, ∗∗p ≤ 0.01, ∗∗∗p ≤ 0.001

t

2.28
2.20
2.56
3.90
2.68
3.67
2.59

R2

47.91%
47.90%
47.91%
47.96%
47.91%
47.95%
47.91%

induced by the ﬁling event. All regressions in-
clude an intercept, calendar year dummies, and
Fama and French (1997) 48-industry dummies for
time- and industry-ﬁxed effects. For brevity, we
only report the key statistics for our main indepen-
dent variables—a more detailed overview with all
control variables can be found in our Online Ap-
pendix.

4.3 Creating a Classiﬁer of Uncertainty
We used the seven dictionaries (Table 1) as fea-
ture sets in the binary classiﬁcation task. As fea-
ture representation, we tried both relative term
frequency (tf) as well as term frequency–inverse
document frequency (tf–idf). Next, using Weka
Experimenter (Hall et al., 2009), we applied six
machine algorithms in a 10-fold cross-validation
setup with ten repetitions: Logistic Regression
(le Cessie and van Houwelingen, 1992); SMO, the
Weka implementation of Support Vector Machine
(Platt, 1999); JRip, the Weka implementation of
RIPPER (Cohen, 1995); J48, the Weka implemen-
tation of C4.5 decision tree (Quinlan, 1993); Ran-
dom Forest (Breiman, 2001); and a Convolutional
Neural Network (Amt´en, 2014). As a JRip classi-
ﬁer outperformed the other ﬁve, we only report its
performance—for the same reason, we only report
the results for tf–idf weighting. The full results
according to all algorithms and both feature repre-
sentations can be found in our Online Appendix.

5 Results and Discussion
5.1 Regression
The results of our regressions are depicted in Ta-
ble 2.
In all regressions, uncertainty and post-
ﬁling volatility have a positive statistical rela-
tion. This relation is signiﬁcant (p ≤ 0.05) for

35Table 3: Results of the classiﬁcation task for the
uncertain class of REPORTS. The best results are
boldfaced and signiﬁcant performance increases
(α = 0.05) over Unc are marked with asterisks.

Dictionary
Unc
UncGen
UncGenexp
UncSpec
UncSpecexp
UncGen+UncSpec
UncGenexp+UncSpecexp
Majority Class (certain)

P
0.65
0.66
0.67
0.69∗
0.67
0.69∗
0.66
0.00

R
0.49
0.51
0.52
0.54∗
0.56∗
0.52
0.54
0.00

F
0.54
0.56
0.56
0.58∗
0.59∗
0.57
0.58
0.00

A

89.66%
89.86%
90.05%
90.31%
90.37%
90.30%
90.07%
87.00%

Unc, UncGen, UncGenexp, UncSpecexp, as well
as UncGenexp+UncSpecexp, and highly signiﬁcant
(p ≤ 0.001) for UncSpec and UncGen+UncSpec.
The strength of this association is also the
highest for both UncSpec and UncGen+UncSpec
(0.034 and 0.032), twice as high than that of Unc
(0.016). This shows that these expansions have
a decisively higher explanatory power of volatil-
ity. Furthermore, concerning RQ1, the regressions
seem to beneﬁt most from a speciﬁc instead of a
generic dictionary expansion. Additionally, with
regard to RQ2, the expert ﬁltering does not im-
prove the results—in some cases, it even worsens
them. As shown in Table 1, our expert annota-
tor retained a relatively small subset of the candi-
date terms (23% of UncGen and 11% of UncSpec).
Such a rigid ﬁltering causes a smaller coverage of
the expansions and furthermore carries the dan-
ger of false negative errors. We hypothesize that
the effect of erroneously added terms is already
mitigated through tf–idf weighting, thus rendering
manual work unnecessary.

Above discussed coefﬁcients might appear
small, as a one standard deviation increase of Unc-
Spec explains only a 3.4% of such an increase
in volatility. However, this is in line with previ-
ous research attesting that the “economic magni-
tude of the soft information is somewhat limited”
(Loughran and McDonald, 2016, p. 1202).

5.2 Classiﬁcation
Table 3 shows the results of the classiﬁcation task
on the newly created dataset REPORTS. Perfor-
mance is evaluated in terms of precision (P), re-
call (R), F1 score (F) on the uncertain class, and
in terms of overall accuracy (A). Additionally, sig-
niﬁcant performance increases over Unc were de-
termined through paired t-tests with α = 0.05.

The highest precision (0.69) is obtained through
UncSpec and UncGen+UncSpec, which signiﬁ-
cantly outperform Unc. UncSpecexp scores high-
est in terms of recall (0.56), which again is sig-
niﬁcantly higher than that of Unc. This value in
combination with a relatively high precision (0.67)
makes the former feature set the strongest overall
in terms of an F1 score of 0.59, thus signiﬁcantly
exceeding Unc and UncGenexp.

Overall,

the high precision of UncSpec and
UncGen+UncSpec coincides with the regressions
(see Section 5.1), where both already yielded the
highest coefﬁcients. Another similarity are the im-
plications for our research questions: Again, the
domain-speciﬁc expansion performs best (RQ1),
while the expert ﬁltering does not provide visible
improvements (RQ2).

Out of the newly added terms of UncGenexp,
24 are contained in the 130 sentences labeled as
uncertain. This stands in contrast to 29 matches
with the terms of UncSpecexp, which again indi-
cates an advantage of the domain-speciﬁc model.
The domain-dependent and legalese language of
the latter reﬂects in matching terms such as
“uninsured”, “more-likely-than-not”, and “inter-
pretive”.

In summary, our results show that for the given
task, training own domain-speciﬁc word embed-
ding models gives an advantage over relying on
generic, off-the-shelf solutions. Lastly, the results
reveal that the manual ﬁltering of candidate terms
has only a negligible impact on performance.

6 Conclusion
In this paper, we expanded a dictionary of ﬁnancial
uncertainty triggers through both a generic and a
domain-speciﬁc embedding model. In a set of ﬁ-
nancial regressions, we showed that our domain-
speciﬁc expansion shares a two times greater
and highly signiﬁcant association with subsequent
volatility than the plain dictionary. Furthermore,
we presented a newly annotated dataset of annual
reports and showed that the dictionary expansions
signiﬁcantly boost the performance in a binary
classiﬁcation task of uncertain sentences.

Acknowledgments
This work was supported by the SFB 884 on the
Political Economy of Reforms at the University
of Mannheim (project C4), funded by the German
Research Foundation (DFG).

36J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Fran-
cisco.

William Sharpe. 1963. A simpliﬁed model for portfo-

lio analysis. Management Science, 9:277–293.

Christoph Kilian Theil, Sanja ˇStajner, Heiner Stucken-
schmidt, and Simone Paolo Ponzetto. 2017. Auto-
matic detection of uncertain statements in the ﬁnan-
cial domain. In Lecture Notes in Computer Science:
Proceedings of the CICLing, Berlin. Springer.
In
press.

Ming-Feng Tsai and Chuan-Ju Wang. 2014. Finan-
cial keyword expansion via continuous word vector
representations. Proceedings of the EMNLP, pages
1453–1458.

Sanja ˇStajner, Goran Glavaˇs, Simone Paolo Ponzetto,
and Heiner Stuckenschmidt. 2017. Domain adap-
tation for automatic detection of speculative sen-
In Proceedings of the IEEE ICSC, pages
tences.
164–171.

References
Johannes Amt´en. 2014. Neural network plugin for
https://github.com/amten/NeuralNetwork.

Weka.
Accessed on January 16, 2018.

Leo Breiman. 2001. Random forests. Machine Learn-

ing, 45(1):5–32.

Saskia le Cessie and Hans C. van Houwelingen. 1992.
Ridge estimators in logistic regression. Applied
Statistics, 41(1):191–201.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and Psychological

nominal scales.
Measurement, 20(1):41–48.

William W. Cohen. 1995. Fast effective rule induction.

In Proceedings of the ICML, pages 115–123.

Eugene F. Fama and Kenneth R. French. 1997. Indus-
try costs of equity. Journal of Financial Economics,
43:153–193.

Viola Ganter and Michael Strube. 2009.

Finding
hedges by chasing weasels: Hedge detection us-
ing Wikipedia tags and shallow linguistic features.
In Proceedings of the ACL–IJCNLP: Short Papers,
pages 173–176.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations Newsletter, 11:10–18.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.

Tim Loughran and Bill McDonald. 2011. When is a li-
ability not a liability? Textual analysis, dictionaries,
and 10-Ks. The Journal of Finance, 66(1):35–65.

Tim Loughran and Bill McDonald. 2014. Measuring
readability in ﬁnancial disclosures. The Journal of
Finance, 69(4):1643–1671.

Tim Loughran and Bill McDonald. 2016. Textual anal-
ysis in accounting and ﬁnance: A survey. Journal of
Accounting Research, 54(4):1187–1230.

Laurens J. P. van der Maaten and Geoffrey E. Hin-
ton. 2008. Visualizing high-dimensional data us-
ing t-SNE. Journal of Machine Learning Research,
9:2579–2605.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. ArXiv e-prints.

Saif Mohammed, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. Proceed-
ings of the EMNLP, pages 982–991.

John C. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal optimiza-
tion. In Advances in Kernel Methods—Support Vec-
tor Learning.

37
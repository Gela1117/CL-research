Findings of the E2E NLG Challenge

Ondˇrej Duˇsek, Jekaterina Novikova and Verena Rieser

The Interaction Lab, School of Mathematical and Computer Sciences

Heriot-Watt University
Edinburgh, Scotland, UK

{o.dusek, j.novikova, v.t.rieser}@hw.ac.uk

Abstract

This paper summarises the experimental
setup and results of the ﬁrst shared task
on end-to-end (E2E) natural language gen-
eration (NLG) in spoken dialogue sys-
tems. Recent end-to-end generation sys-
tems are promising since they reduce the
need for data annotation. However, they
are currently limited to small, delexi-
calised datasets. The E2E NLG shared
task aims to assess whether these novel ap-
proaches can generate better-quality out-
put by learning from a dataset contain-
ing higher lexical richness, syntactic com-
plexity and diverse discourse phenom-
ena. We compare 62 systems submitted
by 17 institutions, covering a wide range
of approaches, including machine learn-
ing architectures – with the majority im-
plementing sequence-to-sequence models
(seq2seq) – as well as systems based on
grammatical rules and templates.

Introduction

1
This paper summarises the ﬁrst shared task
on end-to-end (E2E) natural
language genera-
tion (NLG) in spoken dialogue systems (SDSs).
Shared tasks have become an established way of
pushing research boundaries in the ﬁeld of nat-
ural language processing, with NLG benchmark-
ing tasks running since 2007 (Belz and Gatt,
2007). This task is novel in that it poses new
challenges for recent end-to-end, data-driven NLG
systems for SDSs which jointly learn sentence
planning and surface realisation and do not re-
quire costly semantic alignment between mean-
ing representations (MRs) and the corresponding
natural language reference texts, e.g. (Duˇsek and
Jurˇc´ıˇcek, 2015; Wen et al., 2015b; Mei et al.,

2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek
and Jurˇc´ıˇcek, 2016a; Lampouras and Vlachos,
2016).1 So far, end-to-end approaches to NLG
are limited to small, delexicalised datasets, e.g.
BAGEL (Mairesse et al., 2010), SF Hotels/Restau-
rants (Wen et al., 2015b), or RoboCup (Chen and
Mooney, 2008), whereas the E2E shared task is
based on a new crowdsourced dataset of 50k in-
stances in the restaurant domain, which is about
10 times larger and also more complex than pre-
vious datasets. For the shared challenge, we re-
ceived 62 system submissions by 17 institutions
from 11 countries, with about 1/3 of these sub-
missions coming from industry. We assess the
submitted systems by comparing them to a chal-
lenging baseline using automatic as well as human
evaluation. We consider this level of participa-
tion an unexpected success, which underlines the
timeliness of this task.2 While there are previous
studies comparing a limited number of end-to-end
NLG approaches (Novikova et al., 2017a; Wise-
man et al., 2017; Gardent et al., 2017), this is the
ﬁrst research to evaluate novel end-to-end genera-
tion at scale and using human assessment.

2 The E2E NLG dataset

2.1 Data Collection Procedure
In order to maximise the chances for data-driven
end-to-end systems to produce high quality out-
put, we aim to provide training data in high quality
and large quantity. To collect data in large enough
quantity, we use crowdsourcing with automatic

1Note that as opposed to the “classical” deﬁnition of NLG
(Reiter and Dale, 2000; Gatt and Krahmer, 2018), generation
for dialogue systems does not involve content selection and
its sentence planning stage may be less complex.

2In comparison, the well established Conference in Ma-
chine Translation WMT’17 (running since 2006) received
submissions from 31 institutions to a total of 8 tasks (Bojar
et al., 2017a).

ProceedingsofThe11thInternationalNaturalLanguageGenerationConference,pages322–328,Tilburg,TheNetherlands,November5-8,2018.c(cid:13)2018AssociationforComputationalLinguistics322MR

name[The Wrestlers],

priceRange[cheap],
customerRating[low]

Reference

The wrestlers offers competitive prices,

but isn’t highly rated by
customers.

Figure 1: Example of an MR-reference pair.

Figure 2: An example pictorial MR.

Example value

verbatim string The Eagle, ...

restaurant, pub, ...
Yes / No
cheap, expensive, ...
French, Italian, ...

Data Type

Attribute
name
eatType
familyFriendly
priceRange
food
near
area
dictionary
customerRating dictionary

dictionary
boolean
dictionary
dictionary

verbatim string Zizzi, Cafe Adriatic, ...
riverside, city center, ...
1 of 5 (low), 4 of 5 (high), ...

Table 1: Domain ontology of the E2E dataset.

quality checks. We use MRs consisting of an un-
ordered set of attributes and their values and col-
lect multiple corresponding natural language texts
(references) – utterances consisting of one or sev-
eral sentences. An example MR-reference pair is
shown in Figure 1, Table 1 lists all the attributes in
our domain.

In contrast to previous work (Mairesse et al.,
2010; Wen et al., 2015a; Duˇsek and Jurˇc´ıˇcek,
2016), we use different modalities of meaning rep-
resentation for data collection: textual/logical and
pictorial MRs. The textual/logical MRs (see Fig-
ure 1) take the form of a sequence with attribute-
value pairs provided in a random order. The pic-
torial MRs (see Figure 2) are semi-automatically
generated pictures with a combination of icons
corresponding to the appropriate attributes. The
icons are located on a background showing a map
of a city, thus allowing to represent the meaning of
attributes area and near (cf. Table 1).

In a pre-study (Novikova et al., 2016), we
showed that pictorial MRs provide similar col-
lection speed and utterance length, but are less
likely to prime the crowd workers in their lexi-
cal choices. Utterances produced using pictorial
MRs were considered to be more informative, nat-
ural and better phrased. However, while pictorial
MRs provide more variety in the utterances, this
also introduces noise. Therefore, we decided to
use pictorial MRs to collect 20% of the dataset.

Our crowd workers were asked to verbalise all
information from the MR; however, they were not

E2E data part MRs References
training set
development set
test set
full dataset

42,061
4,672
4,693
51,426

4,862
547
630
6,039

Table 2: Total number of MRs and human refer-
ences in the E2E data sections.

penalised for skipping an attribute. This makes the
dataset more challenging, as NLG systems need to
account for noise in training data. On the other
hand, the systems are helped by having multiple
human references per MR at their disposal.

2.2 Data Statistics

The resulting dataset (Novikova et al., 2017b) con-
tains over 50k references for 6k distinct MRs (cf.
Table 2), which is 10 times bigger than previ-
ous sets in comparable domains (BAGEL, SF Ho-
tels/Restaurants, RoboCup). The dataset contains
more human references per MR (8.27 on average),
which should make it more suitable for data-driven
approaches. However, it is also more challenging
as it uses a larger number of sentences in refer-
ences (up to 6 compared to 1–2 in other sets) and
more attributes in MRs.

For the E2E challenge, we split the data into
training, development and test sets (in a roughly
82-9-9 ratio). MRs in the test set are all previously
unseen,
i.e. none of them overlaps with train-
ing/development sets, even if restaurant names are
removed. MRs for the test set were only released
to participants two weeks before the challenge
submission deadline on October 31, 2017. Par-
ticipants had no access to test reference texts. The
whole dataset is now freely available at the E2E
NLG Challenge website at:

http://www.macs.hw.ac.uk/

InteractionLab/E2E/

323System
♥TGEN baseline (Novikova et al., 2017b): seq2seq with MR classiﬁer reranking
♥SLUG (Juraska et al., 2018): seq2seq-based ensemble (LSTM/CNN encoders, LSTM

decoder), heuristic slot aligner reranking, data augmentation

♥TNT1 (Oraby et al., 2018): TGEN with data augmentation
♥NLE (Agarwal et al., 2018): fully lexicalised character-based seq2seq with MR

classiﬁcation reranking

♥TNT2 (Tandon et al., 2018): TGEN with data augmentation
♥HARV (Gehrmann et al., 2018): fully lexicalised seq2seq with copy mechanism,

coverage penalty reranking, diverse ensembling

♥ZHANG (Zhang et al., 2018): fully lexicalised seq2seq over subword units, attention

memory

♥GONG (Gong, 2018): TGEN ﬁne-tuned using reinforcement learning
♥TR1 (Schilder et al., 2018): seq2seq with stronger delexicalization (incl. priceRange

and customerRating)

♦SHEFF1 (Chen et al., 2018): 2-level linear classiﬁers deciding on next slot/token,

trained using LOLS, training data ﬁltering

♣DANGNT (Nguyen and Tran, 2018): rule-based two-step approach, selecting phrases

for each slot + lexicalising

♥SLUG-ALT (late submission, Juraska et al., 2018): SLUG trained only using complex

sentences from the training data

♦ZHAW2 (Deriu and Cieliebak, 2018): semantically conditioned LSTM RNN language

model (Wen et al., 2015b) + controlling the ﬁrst generated word

♠TUDA (Puzikov and Gurevych, 2018): handcrafted templates
♦ZHAW1 (Deriu and Cieliebak, 2018): ZHAW2 with MR classiﬁcation loss + reranking
♥ADAPT (Elder et al., 2018): seq2seq with preprocessing that enriches the MR with

♥CHEN (Chen, 2018): fully lexicalised seq2seq with copy mechanism and attention

desired target words

memory

♠FORGE3 (Mille and Dasiopoulou, 2018): templates mined from training data
♥SHEFF2 (Chen et al., 2018): vanilla seq2seq
♠TR2 (Schilder et al., 2018): templates mined from training data
♣FORGE1 (Mille and Dasiopoulou, 2018): grammar-based

BLEU

NIST METEOR ROUGE-L CIDEr norm. avg.

0.6593

8.6094

0.4483

0.6850

2.2338

0.5754

0.6619

8.6130

0.4454

0.6772

2.2615

0.5744

0.6561

8.5105

0.4517

0.6839

2.2183

0.5729

0.6534

8.5300

0.4435

0.6829

2.1539

0.5696

0.6502

8.5211

0.4396

0.6496

8.5268

0.4386

0.6853

0.6872

2.1670

0.5688

2.0850

0.5673

0.6545

8.1840

0.4392

0.7083

2.1012

0.5661

0.6422

8.3453

0.4469

0.6645

2.2721

0.5631

0.6336

8.1848

0.4322

0.6828

2.1425

0.5563

0.6015

8.3075

0.4405

0.6778

2.1775

0.5537

0.5990

7.9277

0.4346

0.6634

2.0783

0.5395

0.6035

8.3954

0.4369

0.5991

2.1019

0.5378

0.6004

8.1394

0.4388

0.6119

1.9188

0.5314

0.5657
0.5864

7.4544
8.0212

0.4529
0.4322

0.6614
0.5998

1.8206
1.8173

0.5215
0.5205

0.5092

7.1954

0.4025

0.5872

1.5039

0.4738

0.5859

5.4383

0.3836

0.6714

1.5790

0.4685

0.4599
0.5436
0.4202
0.4207

7.1092
5.7462
6.7686
6.5139

0.3858
0.3561
0.3968
0.3685

0.5611
0.6152
0.5481
0.5437

1.5586
1.4130
1.4389
1.3106

0.4547
0.4462
0.4372
0.4231

Table 3: A list of primary systems in the E2E NLG challenge, with word-overlap metric scores.

System architectures are coded with colours and symbols: ♥seq2seq, ♦other data-driven, ♣rule-based, ♠template-based. Un-
less noted otherwise, all data-driven systems use partial delexicalisation (with name and near attributes replaced by placeholders
during generation), template- and rule-based systems delexicalise all attributes. In addition to word-overlap metrics (see Sec-
tion 4.1), we show the average of all metrics’ values normalised into the 0-1 range, and use this to sort the list. Any values
higher than the baseline are marked in bold.

3 Systems in the Competition
The interest in the E2E Challenge has by far ex-
ceeded our expectations. We received a total of
62 submitted systems by 17 institutions (about 1/3
from industry).
In accordance with ethical con-
siderations for NLP shared tasks (Parra Escart´ın
et al., 2017), we allowed researchers to withdraw
or anonymise their results if their system performs
in the lower 50% of submissions. Two groups
from industry withdrew their submissions and one
group asked to be anonymised after obtaining au-
tomatic evaluation results.

We asked each of the remaining teams to iden-
tify 1-2 primary systems, which resulted in 20 sys-
tems by 14 groups. Each primary system is de-
scribed in a short technical paper (available on
the challenge website) and was evaluated both by
automatic metrics and human judges (see Sec-

tion 4). We compare the primary systems to a
baseline based on the TGEN generator (Duˇsek and
Jurˇc´ıˇcek, 2016a). An overview of all primary sys-
tems is given in Table 3, including the main fea-
tures of their architectures. A more detailed de-
scription and comparison of systems will be given
in (Duˇsek et al., 2018).

4 Evaluation Results
4.1 Word-overlap Metrics
Following previous shared tasks in related ﬁelds
(Bojar et al., 2017b; Chen et al., 2015), we se-
lected a range of metrics measuring word-overlap
between system output and references, including
BLEU, NIST, METEOR, ROUGE-L, and CIDEr.
Table 3 summarises the primary system scores.
The TGEN baseline is very strong in terms of
word-overlap metrics: No primary system is able

324y
t
i
l
a
u
Q

#
1

2

3

4

5

TrueSkill

0.300
0.228
0.213
0.184
0.184
0.136
0.117
0.084
0.065
0.048
0.018
0.014
-0.012
-0.012
-0.078
-0.083
-0.152
-0.185
-0.186
-0.426
-0.457

Rank
1–1
2–4
2–5
3–5
3–6
5–7
6–8
7–10
8–10
8–12
10–13
10–14
11–14
11–14
15–16
15–16
17–19
17–19
17–19
20–21
20–21

System
♥SLUG
♠TUDA
♥GONG
♣DANGNT
♥TGEN
♥SLUG-ALT (late)
♦ZHAW2
♥TNT1
♥TNT2
♥NLE
♦ZHAW1
♣FORGE1
♦SHEFF1
♥HARV
♠TR2
♠FORGE3
♥ADAPT
♥TR1
♥ZHANG
♥CHEN
♥SHEFF2

s
s
e
n
l
a
r
u
t
a
N

#
1

2

3

4

5

TrueSkill

0.211
0.171
0.154
0.126
0.105
0.101
0.091
0.077
0.060
0.046
0.027
0.027
-0.053
-0.073
-0.077
-0.083
-0.104
-0.144
-0.164
-0.243
-0.255

Rank
1–1
2–3
2–4
3–6
4–8
4–8
5–8
5–10
7–11
9–12
9–12
10–12
13–16
13–17
13–17
13–17
15–17
18–19
18–19
20–21
20–21

System
♥SHEFF2
♥SLUG
♥CHEN
♥HARV
♥NLE
♥TGEN
♣DANGNT
♠TUDA
♥TNT2
♥GONG
♥TNT1
♥ZHANG
♥TR1
♥SLUG-ALT (late)
♦SHEFF1
♦ZHAW2
♦ZHAW1
♣FORGE1
♥ADAPT
♠TR2
♠FORGE3

Table 4: TrueSkill measurements of quality (left) and naturalness (right).

Signiﬁcance cluster number, TrueSkill value, range of ranks where the system falls in 95% of cases or more, system name.
Signiﬁcance clusters are separated by a dotted line. Systems are colour-coded by architecture as in Table 3.

to beat it in terms of all metrics – only SLUG
comes very close. Several other systems beat
TGEN in one of the metrics but not in others.3
Overall, seq2seq-based systems show the best
word-based metric values, followed by SHEFF1,
a data-driven system based on imitation learning.
Template-based and rule-based systems mostly
score at the bottom of the list.

4.2 Results of Human Evaluation
However, the human evaluation study provides a
different picture. Rank-based Magnitude Estima-
tion (RankME) (Novikova et al., 2018) was used
for evaluation, where crowd workers compared
outputs of 5 systems for the same MR and as-
signed scores on a continuous scale. We evaluated
output naturalness and overall quality in separate
tasks; for naturalness evaluation, the source MR
was not shown to workers. We collected 4,239 5-
way rankings for naturalness and 2,979 for quality,
comparing 9.5 systems per MR on average.

The ﬁnal evaluation results were produced us-
ing the TrueSkill algorithm (Herbrich et al., 2006;
Sakaguchi et al., 2014), with partial ordering into
signiﬁcance clusters computed using bootstrap re-
sampling (Bojar et al., 2013, 2014; Sakaguchi
et al., 2014). For both criteria, this resulted in 5

3Note, however, that several secondary system submis-
sions perform better than the primary ones (and the baseline)
with respect to word-overlap metrics.

clusters of systems with signiﬁcantly different per-
formance and showed a clear winner: SHEFF2 for
naturalness and SLUG for quality. The 2nd clus-
ters are quite large for both criteria – they contain
13 and 11 systems, respectively, and both include
the baseline TGEN system.

The results indicate that seq2seq systems domi-
nate in terms of naturalness of their outputs, while
most systems of other architectures score lower.
The bottom cluster is ﬁlled with template-based
systems. The results for quality are, however,
more mixed in terms of architectures, with none
of them clearly prevailing. Here, seq2seq systems
with reranking based on checking output correct-
ness score high while seq2seq systems with no
such mechanism occupy the bottom two clusters.

5 Conclusion

This paper presents the ﬁrst shared task on end-to-
end NLG. The aim of this challenge was to assess
the capabilities of recent end-to-end, fully data-
driven NLG systems, which can be trained from
pairs of input MRs and texts, without the need for
ﬁne-grained semantic alignments. We created a
novel dataset for the challenge, which is an order-
of-magnitude bigger than any previous publicly
available dataset for task-oriented NLG. We re-
ceived 62 system submissions by 17 participating
institutions, with a wide range of architectures,
from seq2seq-based models to simple templates.

325We evaluated all the entries in terms of ﬁve differ-
ent automatic metrics; 20 primary submissions (as
identiﬁed by the 14 remaining participants) under-
went crowdsourced human evaluation of natural-
ness and overall quality of their outputs.

We consider the SLUG system (Juraska et al.,
2018), a seq2seq-based ensemble system with a
reranker, as the overall winner of the E2E NLG
challenge. SLUG scores best in human evalua-
tions of quality, it is placed in the 2nd-best clus-
ter of systems in terms of naturalness and reaches
high automatic scores. While the SHEFF2 sys-
tem (Chen et al., 2018), a vanilla seq2seq setup,
won in terms of naturalness, it scores poorly on
overall quality – it placed in the last cluster. The
TGEN baseline system turned out hard to beat: It
ranked highest on average in word-overlap-based
automatic metrics and placed in the 2nd cluster in
both quality and naturalness.

The results in general show the seq2seq archi-
tecture as very capable, but requiring reranking
to reach high-quality results. On the other hand,
while rule-based approaches are not able to beat
data-driven systems in terms of automatic metrics,
they often perform comparably or better in human
evaluations.

We are preparing a detailed analysis of the re-
sults (Duˇsek et al., 2018) and a release of all sys-
tem outputs with user ratings on the challenge
website.4 We plan to use this data for experiments
in automatic NLG output quality estimation (Spe-
cia et al., 2010; Duˇsek et al., 2017), where the
large amount of data obtained in this challenge al-
lows a wider range of experiments than previously
possible.

Acknowledgements
This research received funding from the EPSRC
projects DILiGENt (EP/M005429/1) and MaDrI-
gAL (EP/N017536/1). The Titan Xp used for this
research was donated by the NVIDIA Corpora-
tion.

References

Shubham Agarwal, Marc Dymetman,

and ´Eric
Gaussier. 2018. Char2char generation with rerank-
ing for the E2E NLG Challenge. In Proceedings of
INLG.

4http://www.macs.hw.ac.uk/

InteractionLab/E2E

Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proceedings of MT Summit, pages 75–83,
Copenhagen, Denmark.

Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Findings of the 2017
Logacheva, et al. 2017a.
conference on machine translation (WMT17).
In
Proceedings of WMT, pages 169–214, Copenhagen,
Denmark.

Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of WMT. In Proceed-
ings of WMT, pages 1–44, Soﬁa, Bulgaria.

Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 Workshop
on Statistical Machine Translation. In Proceedings
of WMT, pages 12–58, Baltimore, Maryland, USA.

Ondˇrej Bojar, Yvette Graham, and Amir Kamran.
2017b. Results of the WMT17 Metrics Shared Task.
In Proceedings of WMT, pages 489–513, Copen-
hagen, Denmark.

David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language ac-
quisition. In Proceedings of ICML, pages 128–135,
Helsinki, Finland.

Mingje Chen, Gerasimos Lampouras, and Andreas
Vlachos. 2018. Shefﬁeld at E2E: structured predic-
tion approaches to end-to-end language generation.
In E2E NLG Challenge System Descriptions.

Shuang Chen. 2018. A General Model for Neural
Text Generation from Structured Data. In E2E NLG
Challenge System Descriptions.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollar, and
C. Lawrence Zitnick. 2015. Microsoft COCO
Captions: Data Collection and Evaluation Server.
CoRR, abs/1504.00325.

Jan Deriu and Mark Cieliebak. 2018. End-to-End
Trainable System for Enhancing Diversity in Nat-
ural Language Generation. In E2E NLG Challenge
System Descriptions.

Ondˇrej Duˇsek, Jekaterina Novikova, and Verena Rieser.

2018. The E2E NLG Challenge. In (in prep.).

Ondˇrej Duˇsek and Filip Jurˇc´ıˇcek. 2015. Training a
natural language generator from unaligned data. In
Proceedings of ACL-IJCNLP, pages 451–461, Bei-
jing, China.

326Ondˇrej Duˇsek and Filip Jurˇc´ıˇcek. 2016. A Context-
aware Natural Language Generator for Dialogue
Systems. In Proceedings of SIGDIAL, pages 185–
190, Los Angeles, CA, USA.

Ondˇrej Duˇsek and Filip Jurˇc´ıˇcek. 2016a. Sequence-to-
sequence generation for spoken dialogue via deep
syntax trees and strings. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 45–51, Berlin, Germany.
arXiv:1606.05491.

Ondˇrej Duˇsek, Jekaterina Novikova, and Verena Rieser.
2017. Referenceless Quality Estimation for Nat-
In Proceedings of
ural Language Generation.
the 1st Workshop on Learning to Generate Natu-
ral Language (LGNL), Sydney, Australia. ArXiv:
1708.01759.

Henry Elder,

Sebastian Gehrmann, Alexander
O’Connor, and Qun Liu. 2018. E2E NLG Chal-
lenge Submission: Towards Controllable Generation
In Proceedings of
of Diverse Natural Language.
INLG.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The WebNLG
Challenge: Generating text from RDF data.
In
Proceedings of INLG, pages 124–133, Santiago de
Compostela, Spain.

Albert Gatt and Emiel Krahmer. 2018. Survey of the
State of the Art in Natural Language Generation:
Core tasks, applications and evaluation. Journal of
Artiﬁcial Intelligence Research (JAIR), 61:65–170.

Sebastian Gehrmann, Falcon Z. Dai, Henry Elder, and
Alexander M. Rush. 2018. End-to-End Content and
Plan Selection for Natural Language Generation. In
E2E NLG Challenge System Descriptions.

Heng Gong. 2018. Technical Report for E2E NLG
Challenge. In E2E NLG Challenge System Descrip-
tions.

Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
TrueSkillTM: a Bayesian skill rating system.
In
Proceedings of NIPS, pages 569–576, Vancouver,
Canada.

Juraj Juraska, Panagiotis Karagiannis, Kevin K. Bow-
den, and Marilyn A. Walker. 2018. A Deep En-
semble Model with Slot Alignment for Sequence-
to-Sequence Natural Language Generation. In Pro-
ceedings of NAACL-HLT, New Orleans, LA, USA.

Gerasimos Lampouras and Andreas Vlachos. 2016.
Imitation learning for language generation from un-
In Proceedings of COLING, pages
aligned data.
1101–1112, Osaka, Japan.

Franc¸ois Mairesse, Milica Gaˇsi´c, Filip Jurˇc´ıˇcek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation
using graphical models and active learning. In Pro-
ceedings of ACL, pages 1552–1561, Uppsala, Swe-
den.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. What to talk about and how? Selective
generation using LSTMs with coarse-to-ﬁne align-
ment. In Proceedings of NAACL-HLT, San Diego,
CA, USA.

Simon Mille and Stamatia Dasiopoulou. 2018. FORGe
In E2E NLG Challenge System De-

at E2E 2017.
scriptions.

Dang Tuan Nguyen and Trung Tran. 2018. Structure-
based Generation System for E2E NLG Challenge.
In E2E NLG Challenge System Descriptions.

Jekaterina Novikova, Ondˇrej Duˇsek, Amanda Cer-
cas Curry, and Verena Rieser. 2017a. Why we need
new evaluation metrics for NLG. In Proceedings of
EMNLP, pages 2231–2242.

Jekaterina Novikova, Ondrej Duˇsek, and Verena Rieser.
2017b. The E2E Dataset: New Challenges for End-
In Proceedings of SIGDIAL,
to-End Generation.
pages 201–206, Saarbr¨ucken, Germany.

Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser.
2018. RankME: Reliable Human Ratings for Nat-
In Proceedings of
ural Language Generation.
NAACL-HLT, pages 72–78, New Orleans, LA, USA.

Jekaterina Novikova, Oliver Lemon, and Verena
Rieser. 2016. Crowd-sourcing NLG data: Pictures
In Proceedings of INLG, pages
elicit better data.
265–273, Edinburgh, UK. arXiv:1608.00339.

Shereen Oraby, Lena Reed, Shubhangi Tandon,
Sharath T.S., Stephanie Lukin, and Marilyn Walker.
2018. TNT-NLG, System 1: Using a statistical NLG
to massively augment crowd-sourced data for neural
generation. In E2E NLG Challenge System Descrip-
tions.

Carla Parra Escart´ın, Wessel Reijers, Teresa Lynn, Joss
Moorkens, Andy Way, and Chao-Hong Liu. 2017.
Ethical considerations in NLP shared tasks. In Pro-
ceedings of the First ACL Workshop on Ethics in
Natural Language Processing, pages 66–73.

Yevgeniy Puzikov and Iryna Gurevych. 2018. E2E
NLG Challenge: Neural Models vs. Templates. In
Proceedings of INLG.

E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems, studies in natural lan-
guage processing edition. Cambridge University
Press.

Keisuke Sakaguchi, Matt Post,

and Benjamin
Van Durme. 2014. Efﬁcient elicitation of annota-
tions for human evaluation of machine translation.
In Proceedings of WMT, pages 1–11, Baltimore,
MD, USA.

Frank Schilder, Charese Smiley, Elnaz Davoodi, and
Dezhao Song. 2018. The E2E NLG Challenge: A
tale of two systems. In Proceedings of INLG.

327Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2016. Natural lan-
guage generation in dialogue using lexicalized and
delexicalized data. CoRR, abs/1606.03632.

Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine translation evaluation versus quality esti-
mation. Machine translation, 24(1):39–50.

Shubhangi Tandon, Sharath T.S., Shereen Oraby, Lena
Reed, Stephanie Lukin, and Marilyn Walker. 2018.
TNT-NLG, System 2: Data repetition and meaning
representation manipulation to improve neural gen-
In E2E NLG Challenge System Descrip-
eration.
tions.

Tsung-Hsien Wen, Milica Gasi´c, Dongho Kim, Nikola
Mrkˇsi´c, Pei-Hao Su, David Vandyke, and Steve
Young. 2015a. Stochastic language generation in di-
alogue using recurrent neural networks with convo-
lutional sentence reranking. In Proceedings of SIG-
DIAL, pages 275–284, Prague, Czech Republic.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c,
Lina Maria Rojas-Barahona, Pei-hao Su, David
Vandyke, and Steve J. Young. 2016. Multi-domain
neural network language generation for spoken di-
In Proceedings of NAACL-HLT,
alogue systems.
pages 120–129, San Diego, CA, USA.

Tsung-Hsien Wen, Milica Gaˇsi´c, Nikola Mrkˇsi´c, Pei-
Hao Su, David Vandyke, and Steve Young. 2015b.
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems.
In
Proceedings of EMNLP, pages 1711–1721, Lisbon,
Portugal.

Sam Wiseman, Stuart M. Shieber, and Alexander M.
Rush. 2017. Challenges in data-to-document gener-
ation. In Proceedings of EMNLP, pages 2253–2263,
Copenhagen, Denmark.

Biao Zhang, Jing Yang, Qian Lin, and Jinsong Su.
2018. Attention Regularized Sequence-to-Sequence
In E2E NLG
Learning for E2E NLG Challenge.
Challenge System Descriptions.

328
Identiﬁcation of Parallel Sentences in Comparable Monolingual Corpora

from Different Registers

R´emi Cardon

UMR CNRS 8163 – STL

F-59000 Lille, France

Natalia Grabar

UMR CNRS 8163 – STL

F-59000 Lille, France

remi.cardon@univ-lille.fr

natalia.grabar@univ-lille.fr

Abstract

Parallel aligned sentences provide useful in-
formation for different NLP applications. Yet,
this kind of data is seldom available, especially
for languages other than English. We pro-
pose to exploit comparable corpora in French
which are distinguished by their registers (spe-
cialized and simpliﬁed versions) to detect and
align parallel sentences. These corpora are re-
lated to the biomedical area. Our purpose is
to state whether a given pair of specialized
and simpliﬁed sentences is to be aligned or
not. Manually created reference data show
0.76 inter-annotator agreement. We exploit a
set of features and several automatic classi-
ﬁers. The automatic alignment reaches up to
0.93 Precision, Recall and F-measure. In or-
der to better evaluate the method, it is applied
to data in English from the SemEval STS com-
petitions. The same features and models are
applied in monolingual and cross-lingual con-
texts, in which they show up to 0.90 and 0.73
F-measure, respectively.
Introduction

1
The purpose of text simpliﬁcation is to provide
simpliﬁed versions of texts, in order to remove or
replace difﬁcult words or information. Simpliﬁca-
tion can be concerned with different linguistic as-
pects, such as lexicon, syntax, semantics, pragmat-
ics and even document structure. Simpliﬁcation
can address needs of people or NLP applications
(Brunato et al., 2014).
In the ﬁrst case, simpli-
ﬁed documents are typically created for children
(Son et al., 1008; De Belder and Moens, 2010; Vu
et al., 2014), people with low literacy or foreign-
ers (Paetzold and Specia, 2016), people with men-
tal or neurodegenerative disorders (Chen et al.,
2016), or laypeople who face specialized docu-
ments (Arya et al., 2011; Leroy et al., 2013). In
the second case, the purpose of simpliﬁcation is to
transform documents in order to make them easier

to process within other NLP tasks, such as syn-
tactic analysis (Chandrasekar and Srinivas, 1997;
Jonnalagadda et al., 2009), semantic annotation
(Vickrey and Koller, 2008), summarization (Blake
et al., 2007), machine translation (Stymne et al.,
2013; ˇStajner and Popovi´c, 2016), indexing (Wei
et al., 2014), or information retrieval and extrac-
tion (Beigman Klebanov et al., 2004). Hence, par-
allel sentences, which align difﬁcult and simple
information, provide crucial indicators for the text
simpliﬁcation. Indeed such pairs of sentences con-
tain cues on transformations which are suitable for
the simpliﬁcation, such as lexical substitutes and
syntactic modiﬁcations. Yet, this kind of resources
is seldom available, especially in languages other
than English. The purpose of our work is to de-
tect and align parallel sentences from compara-
ble monolingual corpora, that are differentiated by
their registers. Besides, comparable corpora are
easier to obtain. More precisely, we work with
texts written for specialists and their simpliﬁed
versions. We work with corpora in French.

2 Existing Work

In parallel corpora, sentence alignment can rely on
empirical information, such as relative length of
the sentences in each language (Gale and Church,
1993), or lexical information (Chen, 1993).
In
comparable corpora, both monolingual and bilin-
gual, sentences present relatively loose common
semantics and do not necessarily occur in the same
order.
It should also be noted that (1) the de-
gree of parallelism can vary from nearly parallel
corpora, with a lot of parallel sentences, to very-
non-parallel corpora (Fung and Cheung, 2004);
and that (2) such corpora can contain parallel in-
formation at various degrees of granularity, such
as documents, sentences or sub-phrastic segments
(Hewavitharana and Vogel, 2011). Detection of

Proceedingsofthe9thInternationalWorkshoponHealthTextMiningandInformationAnalysis(LOUHI2018),pages83–93Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics83parallel sentences in comparable corpora is thus
a substantial challenge and requires speciﬁc meth-
ods.

Several existing works are related to machine
translation: bilingual comparable corpora are ex-
ploited for creation of parallel and aligned cor-
pora. Usually, these methods rely on three steps:

1. detection of comparable documents using for
instance generative models (Zhao and Vogel,
2002) or similarity scores (Utiyama and Isa-
hara, 2003; Fung and Cheung, 2004);

2. detection of candidate sentences, or sub-
phrastic segments, for the alignment using for
instance cross-lingual information retrieval
(Utiyama and Isahara, 2003; Munteanu and
Marcu, 2006),
trees
(Munteanu and Marcu, 2002), mutual trans-
lations (Munteanu and Marcu, 2005; Ku-
mano et al., 2007; Abdul-Rauf and Schwenk,
2009), or dynamic programming (Yang and
Li, 2003);

sequence alignment

similarity measure of

3. ﬁltering and selection of correct extractions
using classiﬁcation (Munteanu and Marcu,
2005; Tillmann and Xu, 2009; Hewavitha-
rana and Vogel, 2011; S, tef˘anescu et al.,
2012),
translations
(Fung and Cheung, 2004; Hewavitharana and
Vogel, 2011), error rate (Abdul-Rauf and
Schwenk, 2009), generative models (Zhao
and Vogel, 2002; Quirk et al., 2007), or spe-
ciﬁc rules (Munteanu and Marcu, 2002; Yang
and Li, 2003).

In relation with monolingual comparable cor-
pora, the main difﬁculty is that sentences may
show low lexical overlap but be nevertheless paral-
lel. Recently, this task gained in popularity thanks
to the semantic text similarity (STS) initiative.
Dedicated SemEval competitions have been pro-
posed for several years (Agirre et al., 2013, 2015,
2016). The objective, for a given pair of sentences,
is to predict if they are semantically similar and to
assign similarity score going from 0 (independent
semantics) to 5 (semantic equivalence). This task
is usually explored in general-language corpora.
Among the exploited methods, we can notice:

• lexicon-based methods which rely on simi-
larity of subwords or words from the pro-
cessed texts or on machine translation (Mad-
nani et al., 2012). The features exploited can

be:
lexical overlap, sentence length, string
edition distance, numbers, named entities,
the longest common substring (Clough et al.,
2002; Zhang and Patrick, 2005; Qiu et al.,
2006; Zhao et al., 2014; Nelken and Shieber,
2006; Zhu et al., 2010);

• knowledge-based methods which exploit ex-
ternal resources, such as WordNet (Miller
et al., 1993) or PPDB (Ganitkevitch et al.,
2013). The features exploited can be: over-
lap with external resources, distance between
the synsets, intersection of synsets, seman-
tic similarity of resource graphs, presence of
synonyms, hyperonyms or antonyms (Mihal-
cea et al., 2006; Fernando and Stevenson,
2008; Lai and Hockenmaier, 2014);

• syntax-based methods which exploit the syn-
The fea-
tactic modelling of sentences.
syntactic cate-
tures often exploited are:
gories, syntactic overlap, syntactic dependen-
cies and constituents, predicat-argument rela-
tions, edition distance between syntactic trees
(Wan et al., 2006; Severyn et al., 2013; Tai
et al., 2015; Tsubaki et al., 2016);

• corpus-based methods which exploit distri-
butional methods,
latent semantic analysis
(LSA), topics modelling, word embeddings,
etc. (Barzilay and Elhadad, 2003; Guo and
Diab, 2012; Zhao et al., 2014; Kiros et al.,
2015; He et al., 2015; Mueller and Thyagara-
jan, 2016).

These methods and types of features can of course
be combined for optimizing the results (Bjerva
et al., 2014; Lai and Hockenmaier, 2014; Zhao
et al., 2014; Rychalska et al., 2016; Severyn et al.,
2013; Kiros et al., 2015; He et al., 2015; Tsubaki
et al., 2016; Mueller and Thyagarajan, 2016).

Our objective is close to the second type of
works: we want to detect and align parallel sen-
tences from monologual comparable corpora. Yet,
there are some differences: (1) we work with cor-
pora related to the biomedical area and not to the
general language, (2) we have to state if two sen-
tences have to be aligned (binary statement) and
not to compute their similarity score, and (3) we
work with data in French which were not exploited
for this kind of task yet. To our knowledge, the
only work which exploited articles from French
encyclopedia performed manual alignment of sen-
tences (Brouwers et al., 2014).

84In what follows, we ﬁrst present the linguistic
material used, and the methods proposed. We then
present and discuss the results obtained, and con-
clude with directions of future work.

3 Linguistic Material
We use three comparable corpora in French. They
are related to the biomedical domain and are con-
trasted by the technicity of information they con-
tain with typically specialized and simpliﬁed ver-
sions of a given text. These corpora cover three
genres: drug information, summaries of scientiﬁc
articles, and encyclopedia articles (Sec. 3.1). We
also exploit a set of stopwords (Sec. 3.2), and the
reference data with sentences manually aligned by
two annotators (Sec. 3.3).

3.1 Comparable Corpora
Table 1 indicates the size of the source corpora
(number of documents, number of words in spe-
cialized and simpliﬁed versions). The three cor-
pora are built with French data.

The Drug corpus contains drug information
such as provided to health professionals and pa-
tients. Indeed, two distinct sets of documents ex-
ist, each of which contains common and speciﬁc
information. This corpus is built from the pub-
lic drug database1 of the French Health ministry.
These data have been downloaded in June 2017.
We can see that the specialized versions of docu-
ments provide more word occurrences.

The Scientiﬁc corpus contains summaries of
meta-reviews of high evidence health-related ar-
ticles, such as proposed by the Cochrane collabo-
ration (Sackett et al., 1996). These reviews have
been ﬁrst intended for health professionals but re-
cently the collaborators started to create simpli-
ﬁed versions of the reviews (Plain language sum-
mary) so that they can be read and understood by
the whole population. This corpus has been built
from the online library of the Cochrane collabora-
tion2. The data have been downloaded in Novem-
ber 2017. We can see that specialized version of
summaries is also larger than the simpliﬁed ver-
sion, although the difference is not very important.
The Encyclopedia corpus contains encyclo-
pedia articles from Wikipedia3 and Vikidia4.

1http://base-donnees-publique.

medicaments.gouv.fr/

2http://www.cochranelibrary.com/
3https://fr.wikipedia.org
4https://fr.vikidia.org

Wikipedia articles are considered as technical
texts while Vikidia articles are considered as their
simpliﬁed versions (they are created for children
8 to 13 year old). Similarly to the works done
in English, we associate Vikidia with Simple
Wikipedia5. Only articles related to the medical
portal are exploited in this work. These encyclo-
pedia articles have been downloaded in August
and September 2017. From Table 1, we can see
that specialized versions (from Wikipedia) are also
longer than simpliﬁed versions.

These three corpora are more or less parallel:
Wikipedia and Vikidia articles are written inde-
pendently from each other, drug information doc-
uments are related to the same drugs but the types
of information presented for experts and laypeo-
ple vary a lot, while simpliﬁed summaries from
the scientiﬁc corpus are created starting from the
expert summaries.

3.2 Stopwords
We use a set of 83 stopwords in French, which are
mostly grammatical words, like prepositions (de,
et, `a, ou (of, and, in, or)), auxiliary verbs (est, a (is,
has)) or adverbs (tout, plusieurs (all, several)).

3.3 Reference Data
In this section we describe the data that are used
for training and evaluation of the automatic sen-
tence alignments.

The reference data are created manually. We
have randomly selected 2*14 encyclopedia arti-
cles, 2*12 drug documents, and 2*13 scientiﬁc
summaries. The sentence alignment is done by
two annotators following these guidelines:

1. exclude identical sentences or sentences with

only punctuation and stopword difference ;

2. include sentence pairs with morphological
variations (e.g. Ne pas d´epasser la posolo-
gie recommand´ee. and Ne d´epassez pas la
posologie recommand´ee. – both examples
can be translated by Do not take more than
the recommended dose.);

3. exclude sentence pairs with overlapping se-
mantics, when each sentence brings own in-
formation, in addition to the common seman-
tics;

5http://simple.wikipedia.org

85corpus
Drugs
Scient.
Encyc.

# docs
11,800*2
3,815*2
575*2

# occsp
52,313,126
2,840,003
2,293,078

# occsimpl
33,682,889
1,515,051
197,672

# lemmassp
43,515
11,558
19,287

# lemmassimpl
25,725
7,567
3,117

Table 1: Size of the three source corpora. (column headers : number of documents, total of occurrences (specialized
and simple), total of unique words (specialized and simple))

Specialized

Simpliﬁed

source

aligned

source

aligned

corpus
Drugs
Scient.
Encyc.

# doc.
12*2
13*2
14*2

# pairs.
4,416
553
2,494

# occ.
44,709
8,854
36,002

# pairs.
502
112
49

# occ.
5,751
3,166
1,100

# pairs
2,736
263
238

# occ.
27,820
4,688
2,659

# pairs.
502
112
49

# occ.
10,398
3,306
853

Alignment
rate (%)
simp.
sp.
11
18
20
43
21
2

Table 2: Size of the reference data with consensual alignment of sentences. (number of sentence pairs and word
occurrences for each subset)

4. include sentence pairs in which one sentence
is included in the other, which enables many-
to-one matching (e.g. C’est un organe fait de
tissus membraneux et musculaires, d’environ
10 `a 15 mm de long, qui pend `a la partie
moyenne du voile du palais. and Elle est con-
stitu´ee d’ un tissu membraneux et musculaire.
– It is an organ made of membranous and
muscular tissues, approximately 10 to 15 mm
long, that hangs from the medium part of the
soft palate. and It is made of a membranous
and muscular tissue.);

5. include sentence pairs with equivalent se-
mantics – other than semantic intersection
and inclusion (e.g. Les m´edicaments inhibant
le p´eristaltisme sont contre-indiqu´es dans
cette situation. and Dans ce cas, ne prenez
pas de m´edicaments destin´es `a bloquer ou
ralentir le transit intestinal. – Drugs that
inhibit the peristalsis are contraindicated in
that situation. and In that case, do not take
drugs intended for blocking or slowing down
the intestinal transit.)

The judgement on semantic closeness may vary
according to the annotators. For this reason, the
alignments provided by each annotator undergo
consensus discussions. This alignment process
provides a set of 663 aligned sentence pairs. The
inter-annotator agreement is 0.76 (Cohen, 1960).
It is computed within the two sets of sentences
proposed for alignment by the two annotators.

Table 2 indicates the size of the reference
data before (source columns) and after (aligned

columns) the alignment. In the two last columns
(Alignment rate), we indicate the percentage of
sentences aligned in each register and corpus.
We can observe that scientiﬁc corpus is the most
parallel with the highest alignment rate of sen-
tences from specialized and simpliﬁed documents,
while the two other corpora (drugs and encylo-
pedia) contain proportionnally less parallel sen-
tences. Another interesting observation is that
sentences from simpliﬁed documents in the sci-
entiﬁc and drugs corpora are longer than sen-
tences from specialized documents because they
often add explanations for technical notions, like
in this example: We considered studies involv-
ing bulking agents (a ﬁbre supplement), antispas-
modics (smooth muscle relaxants) or antidepres-
sants (drugs used to treat depression that can also
change pain perceptions) that used outcome mea-
sures including improvement of abdominal pain,
global assessment (overall relief of IBS symp-
toms) or symptom score. In the encylopedia cor-
pus such notions are replaced by simpler words,
or removed. Finally, in all corpora, we observe
frequent substitutions by synonyms, like in these
pairs: {nutrition; food}, {enteral; directly in the
stomach}, {hypersensitivity; allergy}, {incidence;
possible complications}. Notice that with such
substitutions, lexical similarity between sentences
is reduced.

864 Automatic Alignment of Parallel

Sentences

As already indicated, our objective is to detect and
align parallel sentences within monologual com-
parable corpora in French. We already have the
information on which documents are comparable.
So, the task is really dedicated to the alignment of
sentences from specialized and simpliﬁed versions
of documents. The method is composed of several
steps: pre-processing of data (Sec. 4.1), genera-
tion of features (Sec. 4.2), automatic alignment of
sentences (Sec. 4.3), and evaluation (Sec. 4.4).

4.1 Pre-processing of Data
The documents are ﬁrst pre-processed:
they are
POS-tagged with TreeTagger (Schmid, 1994),
which permits to obtain their lemmatized versions.
Then, the documents are segmented into sentences
using strong punctuation (i.e.
.?!;:). The same
pre-processing and segmentation have been ap-
plied when creating the reference data.

4.2 Feature Generation
Our goal is to propose features that can work on
textual data in different languages. We use sev-
eral features which are mainly lexicon-based and
corpus-based, so that they can be easily applied
to textual data in other languages or transposed
to data in other languages. The features are com-
puted on word forms and on lemmas:

1. Number of common non-stopwords. This
feature permits to compute the basic lexical
overlap between specialized and simpliﬁed
versions of sentences (Barzilay and Elhadad,
2003). This feature exploits external knowl-
edge (set of stopwords), which are neverthe-
less very common linguistic data;

2. Number of common stopwords. This feature
also exploits external knowledge (set of stop-
words). It concentrates on non-lexical con-
tent of sentences;

3. Percentage of words from one sentence in-
cluded in the other sentence, computed in
both directions. This features represents pos-
sible lexical and semantic inclusion relations
between the sentences;

4. Sentence length difference between special-
ized and simpliﬁed sentences. This feature

assumes that simpliﬁcation may imply stable
association with the sentence length;

5. Average length difference in words between
specialized and simpliﬁed sentences. This
feature is similar to the previous one but takes
into account average difference in sentence
length;

6. Total number of common bigrams and tri-
grams. This feature is computed on charac-
ter ngrams. The assumption is that, at the
sub-word level, some sequences of characters
may be meaningful for the alignment of sen-
tences if they are shared by them;

7. Word-based similarity measure exploits three
scores (cosine, Dice and Jaccard). This fea-
ture provides a more sophisticated indication
on word overlap between the two compared
sentences. Weight assigned to each word is
set to 1;

8. Word-based similarity measure with the
tf*idf weighting of words (Nelken and
Shieber, 2006). This feature is similar to the
previous one but it also exploits information
on context by incorporating the tf*idf weight-
ing (Salton and Buckley, 1988) of words. For
this, sentences are considered as documents
and documents as corpora. This feature per-
mits to weigh words in a sentence with re-
spect to their occurrences in other sentences
of the document;

9. Character-based minimal edit distance (Lev-
enshtein, 1966). This is a classical acception
of edit distance. It takes into account basic
edit operations (insertion, deletion and sub-
stitution) at the level of characters. The cost
of each operation is set to 1;

10. Word-based minimal edit distance (Leven-
shtein, 1966). This feature is computed with
words as units within sentence. It takes into
account the same three edit operations with
the same cost set to 1. This feature permits to
compute the cost of lexical transformation of
one sentence into another.

4.3 Automatic Alignment of Sentences
The task is to ﬁnd parallel sentences within the
whole set of sentences we described in section

873.3. Hence, we have to categorize the pairs of sen-
tences in one of the two categories:

• alignment: the sentences are parallel and can

be aligned;

• non-alignment:

the sentences are non-

parallel and cannot be aligned.

The reference data provide positive examples
(663 parallel sentences), while negative examples
are obtained by randomly pairing some of the
remaining sentences (800 non-parallel sentences)
from the same documents.

We use several linear classiﬁers with their de-
fault parameters if not indicated otherwise: Per-
ceptron (Rosenblatt, 1958), Multilayer Percep-
tron (MLP) (Rosenblatt, 1961), Linear discrim-
inant analysis (LDA) (Fisher, 1936) with the
LSQR solver, Quadratic discriminant analysis
(QDA) (Cover, 1965), Logistic regression (Berk-
son, 1944), Stochastic gradient descent (SGD)
(Ferguson, 1982) with the log loss, Linear SVM
(Vapnik and Lerner, 1963). We also tested hinge
and modified huber as loss functions with
the SGD, and Eigen and SVD solvers with the
LDA, but the results were either lower or very
close to the best parameters and we abandoned the
idea to use them.

4.4 Evaluation
The training of the system is performed on two
thirds of the sentence pairs, and the test is per-
formed on the remaining third. Several classiﬁers
and several combinations of features are tested.
Classical evaluation measures are computed: Pre-
cision, Recall, F-measure, Mean Square Errors,
and True Positives. Our baseline is the combina-
tion of length measures with the common words
(features 1, 2, 4 and 5). These features are indeed
traditionnally exploited in the existing work.

We also evaluate the system on data in English
that were released for STS competitions6: we use
750 sentence pairs from SemEval 2012, 1,500 sen-
tence pairs from SemEval 2013, 3,750 sentence
pairs from SemEval 2014. Each pair of sentences
is associated with the similarity score [0;5]. We
apply our system to these data in two ways: (1)
the system is trained and tested on the STS dataset,
and (2) the system is trained on our dataset in
French and tested on the STS dataset in English.

6http://ixa2.si.ehu.es/stswiki/index.

php/Main_Page

We assume indeed that the features used and even
the models generated can be transposed to data in
other languages. For the experiments with the En-
glish data, we use the same evaluation measures
(Precision, Recall, F-measure, Mean Square Er-
rors, and True Positives). The set of stopwords
in English contains 150 entities.

5 Results and Discussion

Classiﬁer
Perceptron
MLP
LDA
QDA
LogReg
SGD
LinSVM

R
0.87
0.87
0.90
0.89
0.93
0.87
0.81

P
0.84
0.87
0.90
0.89
0.93
0.84
0.81

F1 MSE
0.63
0.53
0.40
0.45
0.30
0.84
0.74

0.84
0.86
0.90
0.89
0.93
0.84
0.81

TP
142
167
175
197
191
210
166

Table 3: Alignment results obtained with different clas-
siﬁers on French data, test set, whole featureset without
tf*idf similarity scores, and non-lemmatized text.

In Table 3, we present the results obtained on
French data using the whole set of features (but
without the tf*idf similarity scores) on test set, and
non-lemmatized texts. The results are indicated
in terms of Recall R, Precision P , F-measure F ,
Mean Square Errors M SE and True positives T P
(out of the 221 positive sentence pairs in the test
set). We can see that all the classiﬁers are com-
petitive with F-measure above 0.80. Overall, sev-
eral classiﬁers (LDA, QDA, LogReg, LinSVM)
provide stable results, for which we indicate the
evaluation scores obtained in one iteration. Other
classiﬁers (Perceptron, MLP, SGD) provide ﬂuc-
tuating results, and we indicate then the average
scores obtained after 20 iterations. Another posi-
tive observation is that Precision and Recall values
are well balanced. Logistic regression seems to be
the best classiﬁer for this task, with Precision, Re-
call and F-measure at 0.93. This classiﬁer is used
for the experiments described in the next sections.
We ﬁrst present and discuss the exploitation of
various featuresets on French data (Sec. 5.1), and
then the exploitation of the features and models on
the STS data in English in monolingual (Sec. 5.2)
and cross-lingual (Sec. 5.3) contexts. As our ﬁnal
objective (text simpliﬁcation in French) and the
data we work on (French texts from the biomed-
ical domain) are different from the STS context,
we believe it should be noted that there are intrin-

88sic limitations as to the comparison we can make.

5.1 Different Featuresets

set of features (BL+L+S+N), which shows 0.93 F-
measure. We use the whole set of features for the
experiments with the STS dataset.

R
Feature set
BL
0.87
S
0.84
L
0.79
N
0.89
L+S
0.88
L+N
0.91
S+N
0.91
BL+L
0.90
BL+S
0.89
BL+N
0.91
BL+L+S
0.90
BL+L+N
0.93
BL+S+N
0.91
L+S+N
0.91
BL+L+S+N 0.93

P
0.87
0.84
0.78
0.88
0.88
0.91
0.91
0.90
0.89
0.91
0.90
0.93
0.91
0.91
0.93

F1 MSE
0.54
0.64
0.86
0.48
0.48
0.37
0.37
0.40
0.46
0.35
0.40
0.29
0.36
0.36
0.29

0.86
0.84
0.78
0.88
0.88
0.91
0.91
0.90
0.89
0.91
0.90
0.93
0.91
0.91
0.93

TP
173
174
146
168
170
187
183
184
180
187
184
191
189
189
191

Table 4: Alignment results obtained with various fea-
turesets, logistic regression, non-lemmatized text.

The purpose of these experiments is to detect
the most suitable combinations of features. We
present the results obtained on our data. We dis-
tinguish four sets of features, which are used in
isolation and in various combinations. We indi-
cate the corresponding numbers from section 4.2
between brackets :

1. BL: baseline (1, 2, 3, 4 5);

2. L: Levenshtein-based features (9, 10);

3. S: similarity-based features (7, 8);

4. N: ngram-based features (6).

Contrary to the previous work (Nelken and
Shieber, 2006; Zhu et al., 2010), the tf*idf weight-
ing of words is not efﬁcient on our data. For this
reason, this set of features was not used in the ex-
periments.

The results are presented in Table 4. The
lowest results are obtained with the Levenshtein-
based features (F-measure 0.78),
they are fol-
lowed by the similarity-based features (F-measure
0.84). We obtain 0.86 F-measure with the base-
line. Other combinations indicate that each set
of features exploited is useful to gain efﬁciency
for this task. Hence, the best results are obtained
with the combination BL+L+N and with the whole

5.2 Classiﬁcation of the STS Sentence Pairs

STSset score
STS2012 2.5
STS2012 3.5
STS2012 4.5
STS2013 2.5
STS2013 3.5
STS2013 4.5
STS2014 2.5
STS2014 3.5
STS2014 4.5

R
0.82
0.74
0.79
0.73
0.78
0.89
0.75
0.70
0.89

P
0.82
0.74
0.81
0.73
0.78
0.93
0.76
0.71
0.93

F1 MSE
0.71
1.04
0.74
1.09
0.87
0.29
0.97
1.17
0.29

0.82
0.74
0.78
0.73
0.78
0.90
0.75
0.71
0.90

TP
477
277
37
176
96
2
653
306
2

Table 5: Alignment results obtained on the STS data in
English, test set, whole featureset, logistic regression,
non-lemmatized text and training on the STS data.

In this set of experiments,

the classiﬁcation
model is trained and tested on the STS reference
data in English. Our assumption is that the fea-
tures exploited are transferable from one language
to another. The reference data and categories in
English and in French differ. One difference is
that the STS pairs of sentences are scored from 0
to 5 according to their similarity, while in French
we do binary classiﬁcation (a given pair of sen-
tences should be aligned or not). To make the two
datasets comparable, we propose to transform the
STS scoring in binary categories. We test sim-
ilarity thresholds within the interval [2.5;4.5] by
step of 0.5, which permits not to consider identi-
cal sentences (scores close to 5) and very distant
sentences (scores lower than 2.5). As indicated
in Table 5, we obtain up to 0.90 F-measure with
the similarity threshold 4.5 on data from 2013 and
2014, while in 2012 the best F-measure (0.82) is
obtained with the similarity score 2.5. It is difﬁcult
to compare our results with those of the participat-
ing teams and already published results because
our categories and evaluation differ from the STS
protocols – we rate sentence pairs as either aligned
or not aligned, while STS offers a scale from 0 to
5. Yet, the MSE rate (0.308) published by one of
the top participants in 2014 (Bjerva et al., 2014)
indicates that our MSE rate is improved, as it is at
0.29 on the 2014 data.

895.3 Cross-lingual Classiﬁcation of the STS

Sentence Pairs

STSset score
STS2012 2.5
STS2012 3.5
STS2012 4.5
STS2013 2.5
STS2013 3.5
STS2013 4.5
STS2014 2.5
STS2014 3.5
STS2014 4.5

R
0.83
0.74
0.81
0.74
0.78
0.92
0.74
0.72
0.88

P
0.81
0.72
0.49
0.74
0.73
0.57
0.72
0.69
0.54

F1 MSE
0.19
0.28
0.51
0.26
0.27
0.43
0.28
0.31
0.46

0.82
0.71
0.52
0.74
0.74
0.67
0.73
0.69
0.61

TP
1378
1035
413
523
396
88
1688
1216
384

Table 6: Alignment results obtained on the STS data in
English, test set, whole featureset, Logistic regression,
non-lemmatized text and training on the French data.

In this set of experiments,

the classiﬁcation
model is trained on French data and tested on the
STS data in English. Here, our assumption is
that the models generated on one language can be
transferable to another language in order to detect
parallel sentences. Here as well, we test several
similarity thresholds. As we can see in Table 6, in
this cross-lingual experiment, the best F-measures
are obtained with the score 2.5 in 2012 (0.82) and
in 2014 (0.73), and with scores 2.5 and 3.0 in 2013
(0.74). These thresholds indicate that the models
generated on our French data can be exploited on
the STS data in English quite efﬁciently and that
the features that are used show cross-lingual rele-
vance for the French-English language pair. These
results also indicate that, for the targeted task of
text simpliﬁcation, we need quite a strong similar-
ity between sentences.
6 Conclusion and Future Work
In this work, we proposed to address the task of
detection and alignment of parallel sentences from
monolingual comparable corpora in French. The
comparable dimension is due to the technicality
of documents, which contrast specialized and sim-
pliﬁed versions of documents and sentences. We
use three corpora which are related to the biomed-
ical area. Several features and classiﬁers and ex-
ploited. Our results reach up to 0.93 F-measure
on the French data, with a very good balance be-
tween Precision and Recall. Linear regression ap-
pears to be the best classiﬁer for this task. Our
approach is then tested on the STS data in En-
glish, such as proposed by several SemEval com-

petitions between 2012 and 2014. We ﬁrst test the
features, with training and testing done on the STS
data. This gives up to 0.90 F-measure with the
4.5 similarity threshold. Then, we test the models:
they are generated on the French data and tested
on the STS data. This gives 0.82 F-measure. We
assume that the proposed approach (features and
classiﬁers) show a good transferability to another
language. This is a good point because it validates
our approach on data from another language.

In future, we plan to exploit the best models
generated in French for enriching the set of par-
allel sentences. This will permit to prepare data
necessary for the developement of simpliﬁcation
methods for French. Parallel sentences may also
be helpful for othe NLP applications. Other di-
rections for future work are concerned with the
exploitation of other features for the alignment
of sentences, such as use of word embeddings to
smooth lexical variation or exploitation of external
knowledge. Besides, our appoach will be further
evaluated on data from other languages.
7 Acknowledgements
This work was funded by the French National
Agency for Research (ANR) as part of the CLEAR
project (Communication, Literacy, Education, Ac-
cessibility, Readability), ANR-17-CE19-0016-01.
The authors would like to thank the reviewers

for their helpful comments.

References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve smt perfor-
mance. In European Chapter of the ACL, pages 16–
23.

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. SemEval-2015 task 2: Semantic tex-
tual similarity, english, spanish and pilot on inter-
pretability. In SemEval 2015, pages 252–263.

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, Rada Mihalcea, German
Rigau, and Janyce Wiebe. 2016. SemEval-2016
task 1: Semantic textual similarity, monolingual and
In SemEval 2016, pages
cross-lingual evaluation.
497–511.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and WeiWei Guo. 2013. *sem 2013 shared
In *SEM, pages
task: Semantic textual similarity.
32–43.

90Diana J. Arya, Elfrieda H. Hiebert, and P. David Pear-
son. 2011. The effects of syntactic and lexical com-
plexity on the comprehension of elementary science
texts. Int Electronic Journal of Elementary Educa-
tion, 4(1):107–125.

Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
EMNLP, pages 25–32.

Beata Beigman Klebanov, Kevin Knight, and Daniel
Marcu. 2004. Text simpliﬁcation for information-
seeking applications. In Robert Meersman and Za-
hir Tari, editors, On the Move to Meaningful In-
ternet Systems 2004: CoopIS, DOA, and ODBASE.
Springer, LNCS vol 3290, Berlin, Heidelberg.

Thomas M. Cover. 1965. Geometrical and statistical
properties of systems of linear inequalities with ap-
plications in pattern recognition. IEEE Transactions
on Electronic Computers, 14(3):326–334.

Jan De Belder and Marie-Francine Moens. 2010. Text
simpliﬁcation for children. In Workshop on Accessi-
ble Search Systems of SIGIR, pages 1–8.

Thomas S. Ferguson. 1982. An inconsistent maximum
likelihood estimate. Journal of the American Statis-
tical Association, 77(380):831–834.

Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection.
In Comp Ling UK, pages 1–7.

Joseph Berkson. 1944. Application of the logistic func-
tion to bio-essay. Journal of the American Statistical
Association, 39:357–365.

Ronald A. Fisher. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of Eugenics,
7(2):179–188.

Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The meaning factory: For-
mal semantics for recognizing textual entailment
In Work-
and determining semantic similarity.
shop on Semantic Evaluation (SemEval 2014), pages
642–646, Dublin, Ireland.

Catherine Blake, Julia Kampov, Andreas Orphanides,
David West, and Cory Lown. 2007. Query expan-
sion, lexical simpliﬁcation, and sentence selection
strategies for multi-document summarization.
In
DUC.

Laetitia Brouwers, Delphine Bernhard, Anne-Laure
Ligozat, and Thomas Francois. 2014. Syntactic sen-
tence simpliﬁcation for French. In PITR workshop,
pages 47–56.

Dominique Brunato, Felice Dell’Orletta, Giulia Ven-
turi, and Simonetta Montemagni. 2014. Deﬁning
an annotation scheme with a view to automatic text
simpliﬁcation. In CLICIT, pages 87–92.

Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simpliﬁcation.
Knowledge Based Systems, 10(3):183–190.

Ping Chen, John Rochford, David N. Kennedy, Sous-
san Djamasbi, Peter Fay, and Will Scott. 2016. Au-
tomatic text simpliﬁcation for people with intellec-
tual disabilities. In AIST, pages 1–9.

Stanley F Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 9–16.

Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: Measuring text reuse.
In ACL, pages 152–159.

Jacob Cohen. 1960. A coefﬁcient of agreement for
Educational and Psychological

nominal scales.
Measurement, 20(1):37–46.

Pascale Fung and Percy Cheung. 2004. Mining very
non-parallel corpora: Parallel sentence and lexicon
extraction vie bootstrapping and em. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 57–63.

William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Comp Linguistics, 19(1):75–102.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
PPDB: The paraphrase

Callison-Burch. 2013.
database. In NAACL-HLT, pages 758–764.

Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In ACL, pages 864–872.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with con-
In EMNLP, pages
volutional neural networks.
1576–1586, Lisbon, Portugal.

Sanjika Hewavitharana and Stephan Vogel. 2011. Ex-
tracting parallel phrases from comparable data.
In
4th Workshop on Building and Using Comparable
Corpora, pages 61–68.

Siddhartha Jonnalagadda, Luis Tari, J¨org Hakenberg,
Chitta Baral, and Graciela Gonzalez. 2009. Towards
effective sentence simpliﬁcation for automatic pro-
In NAACL HLT 2009,
cessing of biomedical text.
pages 177–180.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
In Neural Information Processing Systems (NIPS),
pages 3294–3302.

Tadashi Kumano, Hideki Tanaka, and Takenobu Toku-
naga. 2007. Extracting phrasal alignments from
comparable corpora by using joint probability SMT
model. In Int Conf on Theoretical and Methodolog-
ical Issues in Machine Translation.

91Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. In Workshop on Semantic Evaluation (SemEval
2014), pages 239–334, Dublin, Ireland.

Gondy Leroy, David Kauchak, and Obay Mouradi.
2013. A user-study measuring the effects of lexical
simpliﬁcation and coherence enhancement on per-
ceived and actual text difﬁculty. Int J Med Inform,
82(8):717–730.

Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. So-
viet physics. Doklady, 707(10).

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identiﬁcation. In NAACL-HLT, pages
182–190.

Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, pages
1–6.

George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to wordnet: An on-line lexical database.
Technical report, WordNet.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
In AAAI Conference on Artiﬁcial Intelligence,
ity.
pages 2786–2792.

Dragos Stefan Munteanu and Daniel Marcu. 2002.
Processing comparable corpora with bilingual sufﬁx
trees. In EMNLP, pages 289–295.

Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.

Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In COLING-ACL, pages 81–88.

Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In EACL, pages 161–168.

Gustavo H. Paetzold and Lucia Specia. 2016. Bench-
In LREC,

marking lexical simpliﬁcation systems.
pages 3074–3080.

Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity signiﬁcance
In Empirical Methods in Natural
classiﬁcation.
Language Processing, pages 18–26, Sydney, Aus-
tralia.

Chris Quirk, Raghavendra U. Udupa, and Arul
Menezes. 2007. Generative models of noisy trans-
lations with applications to parallel fragment extrac-
tion. In Machine Translation Summit XI, pages 377–
384.

Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386–408.

Frank Rosenblatt. 1961. Principles of Neurodynamics:
Perceptrons and the Theory of Brain Mechanisms.
Spartan Books, Washington DC.

Barbara Rychalska, Katarzyna Pakulska, Krystyna
Chodorowska, WojciechWalczak, and Piotr An-
druszkiewicz. 2016. Samsung Poland NLP team
at SemEval-2016 task 1: Necessity for diversity;
combining recursive autoencoders, wordnet and en-
semble methods to measure semantic similarity. In
SemEval-2016, pages 614–620.

David L. Sackett, William M. C. Rosenberg, Jeffrey A.
MuirGray, R. Brian Haynes, and W. Scott Richard-
son. 1996. Evidence based medicine: what it is and
what it isn’t. BMJ, 312(7023):71–2.

Gerard Salton and Chris Buckley. 1988. Term weight-
ing approaches in automatic text retrieval. Informa-
tion Processing & Management, 24(5):513–523.

Helmut Schmid. 1994. Probabilistic part-of-speech
In Int Conf on New

tagging using decision trees.
Methods in Language Processing, pages 44–49.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual similar-
ity with structural representations. In Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 714–718.

Ji Y. Son, Linda B. Smith, and Robert L. Gold-
stone. 1008. Simplicity and generalization: Short-
cutting abstraction in children’s object categoriza-
tions. Cognition, 108:626–638.

Sara Stymne, J¨org Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In NODALIDA,
pages 1–12.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
In Annual Meeting of the Association for
works.
Computational Linguistics, pages 1556–1566, Bei-
jing, China.

Christoph Tillmann and Jian-Ming Xu. 2009. A simple
sentence-level extraction algorithm for comparable
data. In Companion Vol. of NAACL HLT.

Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2016. Non-linear similarity learn-
ing for compositionality. In AAAI Conference on Ar-
tiﬁcial Intelligence, pages 2828–2834.

Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 72–79.

92Vladimir Vapnik and A. Lerner. 1963. Pattern recogni-
tion using generalized portrait method. Automation
and Remote Control, 24:709–715.

David Vickrey and Daphne Koller. 2008. Sentence
In An-
simpliﬁcation for semantic role labeling.
nual Meeting of the Association for Computational
Linguistics-HLT, pages 344–352.

Sanja ˇStajner and Maja Popovi´c. 2016. Can text sim-
pliﬁcation help machine translation? Baltic J. Mod-
ern Computing, 4(2):230–242.

Tu Thanh Vu, Giang Binh Tran, and Son Bao
Pham. 2014. Learning to simplify children stories
In Intelligent Information and
with limited data.
Database Systems, pages 31–41.

Stephen Wan, Mark Dras, Robert Dale, and Cecile
Paris. 2006. Using dependency-based features to
In Aus-
take the ”para-farce” out of paraphrase.
tralasian Language Technology Workshop, pages
131–138.

Chih-Hsuan Wei, Robert Leaman, and Zhiyong Lu.
2014. Simconcept: A hybrid approach for simpli-
fying composite named entities in biomedicine. In
BCB ’14, pages 138–146.

Christopher C. Yang and Kar Wing Li. 2003. Auto-
matic construction of English/Chinese parallel cor-
pora. J. Am. Soc. Inf. Sci. Technol., 54(8):730–742.

Yitao Zhang and Jon Patrick. 2005. Paraphrase iden-
In Australasian

tiﬁcation by text canonicalization.
Language Technology Workshop, pages 160–166.

Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web bilingual news col-
In IEEE Int Conf on Data Mining, pages
lection.
745–748.

Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One stone two birds: Ensemble of heteroge-
nous measures for semantic relatedness and textual
In Workshop on Semantic Evaluation
entailment.
(SemEval 2014), page 271–277.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simpliﬁcation. In COLING 2010, pages
1353–1361.

Dan S, tef˘anescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In 16th EAMT Conference, pages
137–144.

93
BinLin: A Simple Method of Dependency Tree Linearization

Yevgeniy Puzikov and Iryna Gurevych

Ubiquitous Knowledge Processing Lab (UKP-TUDA)

Department of Computer Science, Technische Universit¨at Darmstadt
{puzikov,gurevych}@ukp.informatik.tu-darmstadt.de

Research Training Group AIPHES

Abstract

Surface Realization Shared Task 2018 is
a workshop on generating sentences from
lemmatized sets of dependency triples.
This paper describes the results of our par-
ticipation in the challenge. We develop a
data-driven pipeline system which ﬁrst or-
ders the lemmas and then conjugates the
words to ﬁnish the surface realization pro-
cess. Our contribution is a novel sequential
method of ordering lemmas, which, despite
its simplicity, achieves promising results.
We demonstrate the effectiveness of the
proposed approach, describe its limitations
and outline ways to improve it.
Introduction

1
Natural Language generation (NLG) is the task of
generating natural language utterances from tex-
tual inputs or structured data representations. For
many years one of the research foci in the NLG
community has been Surface Realization (SR) –
the process of transforming a sentence plan into a
linearly-ordered, grammatical string of morpholog-
ically inﬂected words (Langkilde-Geary, 2002).

The SR Shared Task is aimed at developing a
common input representation that could be used
by a variety of NLG systems to generate realiza-
tions from (Belz et al., 2011). In the case of the
Surface Realization Shared Task 2018 (Mille et al.,
2018) there are two different representations the
contestants can use, depending on the track they
participate in:
Shallow Track: unordered dependency trees con-
sisting of lemmatized nodes with part-of-
speech (POS) tags and morphological infor-
mation as found in the Universal Dependen-
cies (UD) annotations (version 2.0).1

1http://universaldependencies.org/

Deep Track: same as above, but having functional
words and morphological features removed.

We participated in the shallow track, and there-
fore our task was to generate a sentence by order-
ing the lemmas and inﬂecting them to the correct
surface forms. The outputs of the participating sys-
tems are assessed using both automatic and manual
evaluation. The former is performed by computing
BLEU (Papineni et al., 2002), NIST (Doddington,
2002), CIDEr (Vedantam et al., 2015) scores and
normalized string edit distance (EDIST) between
the reference sentence and a system output. Man-
ual evaluation is based on preference judgments:
third-year undergraduate students from Cambridge,
Oxford and Edinburgh rate pairs of candidate out-
puts (including the target sentence), scoring them
for Clarity, Fluency and Meaning Similarity.

The data used for the task is the UD treebanks
distributed in the 10-column CoNLL-U format.2
The data is available for Arabic, Czech, Dutch, En-
glish, Finnish, French, Italian, Portuguese, Russian
and Spanish. According to the requirements of the
Shallow Track, the information on word order was
removed by randomized scrambling of the token
sequence; the words were also replaced by their
lemmas.

Our contribution is a simple method of depen-
dency tree linearization which orders a bag of lem-
mas based on the available syntactic information.
The major limitation of the method is its input or-
der sensitivity; solving this problem is reserved for
future work.

Our paper has the following structure. Section 2
describes related work done in the past. Section 3
presents the results of the exploratory data analy-
sis conducted prior to system development. The
details of our system architecture are speciﬁed in

2http://universaldependencies.org/

format.html

ProceedingsoftheFirstWorkshoponMultilingualSurfaceRealisation,pages13–28Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics13Section 4 which is followed by the description of
the experimental setup and evaluation (Section 5).
Section 6 mentions the limitations of the proposed
surface realization method and outlines future work
directions.

2 Related Work

As mentioned in Section 1, the task at hand is to
generate a sentence by ordering the lemmas and
inﬂecting them to the correct surface forms. Past
research work proposed both joint and pipeline so-
lutions for the problem. Taking into consideration
the pipeline nature of our system, we separate the
related work stage-wise.

2.1 Syntactic Ordering
Given a bag of input words, a syntactic ordering
algorithm constructs an output sentence. Prior
work explored a range of approaches to syntac-
tic ordering: grammar-based methods (Elhadad
and Robin, 1992; Carroll et al., 1999; White et al.,
2007), generate-and-rerank approaches (Banga-
lore and Rambow, 2000; Langkilde-Geary, 2002),
tree linearization using probabilistic language mod-
els (Guo et al., 2008), inter alia. Depending on
how much syntactic information is available as
input, the research on syntactic ordering can be
categorized into (1) free word ordering, (2) full tree
linearization and (3) partial tree linearization (Liu
et al., 2015). The setup of the Surface Realization
Task corresponds to the full tree linearization case,
since the dependency tree information is provided.
Conceptually, the problem of tree linearization
is simple. However, given no constraints, the
search space is exponential in the number of to-
kens, which makes exhaustive search intractable.
This stimulated the line of research focusing on
the development of approximate search methods.
Current state-of-the-art (evaluated on the English
data only) belongs to the system of Puduppully
et al. (2016) who extended the work of Liu et al.
(2015) on developing a transition-based generator.
The authors treated language generation process
as a generalized form of dependency parsing with
unordered token sequences, and used a learning
and search framework of Zhang and Clark (2011)
to keep the decoding process tractable. A similar
approach to dependency tree linearization was ex-
plored in (Bohnet et al., 2010), who approximated
exact decoding with a beam search. Our method
of syntactic ordering is also based on search ap-

proximation, but follows a different approach: we
use a greedy search strategy, but restrict the scoring
procedure to a smaller set of plausible candidate
pairs, which speeds up the search procedure and
reduces the number of mistakes the system might
make.

2.2 Word Inﬂection

Word inﬂection in the context of the Surface Real-
ization Task can be deﬁned as the subtask of gen-
erating a surface form (was) from a given source
lemma (be) and additional morphological/syntactic
attributes (Number=Sing, Person=3, Tense=Past).
Early work proposed to approach the task with
ﬁnite state transducers (Koskenniemi, 1983; Ka-
plan and Kay, 1994). While being accurate, these
systems require a lot of time and linguistic exper-
tise to construct and maintain. With the advance
of machine learning, the community mostly shifted
towards data-driven methods of automatic morpho-
logical paradigm induction and string transduction
as the method of morphological inﬂection gener-
ation (Yarowsky and Wicentowski, 2000; Wicen-
towski, 2004; Dreyer and Eisner, 2011; Durrett
and DeNero, 2013; Ahlberg et al., 2015). In com-
parison with their rule-based counterparts, these
approaches scale better across languages and do-
mains, but require manually-deﬁned comprehen-
sive feature representation of the inputs.

Current research focuses on data-driven mod-
els which learn a high-dimensional feature repre-
sentation of the input data during the optimiza-
tion procedure in an end-to-end fashion. Recent
work (Faruqui et al., 2016) proposed to model the
problem as a sequence-to-sequence learning task,
using the encoder-decoder neural network archi-
tecture developed in the machine translation com-
munity (Cho et al., 2014; Sutskever et al., 2014).
This approach showed an improvement over con-
ventional machine learning models, but failed to ad-
dress the issue of poor sample complexity of com-
plex neural networks – in practice, the approach
did not perform well on low-resource or morpho-
logically rich languages.

An attempt to address this issue was made by
Aharoni and Goldberg (2017), who proposed to
directly model an almost monotonic alignment be-
tween the input and output character sequences
by using a controllable hard attention mechanism
which allows the network to jointly align and trans-
duce, while maintaining a focused representation at

14Language

Property
unique features
OOV lemmas
OOV forms
OOV chars

ar
37
1056
1745
0

cs
112
3299
8070
2

en
36
1180
1313
3

es
56
1368
2131
1

ﬁ
89
1598
3666
5

fr
35
1895
2387
12

it
41
439
683
2

nl
66
973
1131
0

pt
48
535
785
0

ru
40
2723
8190
0

Table 1: Cross-lingual data analysis.

each step. The authors proposed to utilize indepen-
dently learned character-level alignments instead
of the weighted sum of representations (as done
in the soft attention models). Experimental results
demonstrated better sample efﬁciency of the mod-
els trained according to the proposed method, and
considerable improvements over the previous ap-
proaches.

3 Data Analysis

For the input to the shallow track, the organizers
separated the reference sentences from the respec-
tive structures. Although the one-to-one correspon-
dence between sentences and dependency trees was
preserved, the alignment between the lemmas in
the trees and the word forms in the sentences was
lost. To circumvent this issue and ease the burden
of aligning lemmas with the corresponding surface
forms, we decided to use the original UD data ﬁles
for all our experiments – they contain the same
dependency trees as the shared task data, but the or-
der of the tokens is not scrambled and each surface
form is aligned with the respective lemma.

Prior to system development, we analyzed the
data along the dimensions which we deemed rele-
vant for the task. Due to space constraints here we
show ﬁgures and numbers mainly for English; the
analysis results for other languages can be found in
Appendix A.1.

First, we examined the lemma-to-form ratio
(Figure 1). The majority of lemmas have only
one surface form, which suggests a strong major-
ity baseline for the morphological inﬂection sub-
task. However, languages with rich morphology
(Czech, Finnish, Russian) pose a challenge in this
regard and call for a more elaborate approach which
takes into account complex grammar inﬂection
paradigms. The number of unique features (val-
ues in the FEAT column of the input data) served
as a rough estimate of the latter (Table 1). We have
not performed any language-speciﬁc engineering
to address these linguistic properties, but took them

Figure 1: Lemma-to-form ratio (English).

into consideration for future work.

Another important data property is the length dis-
tribution of lemmas, surface forms and sentences.
We computed the training data statistics and used
the obtained estimates to establish cut-off thresh-
olds for ﬁltering out outlier lemmas and forms from
the training data.

The number of out-of-vocabulary (OOV) lan-
guage units can be viewed as a crude measure of
the expected difference between training and de-
velopment data distributions. Table 1 shows the
number of OOV lemmas, surface forms and charac-
ters for each of the languages. Some of the datasets
included foreign names and terms which are used in
their original language forms. For example, out of
356464 French data tokens, 419 include characters
that are not digits, punctuation signs or letters of
the French alphabet. Since such words are usually
not conjugated, but copied verbatim, we consider
them as outliers and exclude them from the training
procedure. Finally, tokens deﬁned in the UD anno-
tation guidelines as multi-word expressions (MWE)
and empty nodes were excluded from the training
data, because they require language-speciﬁc treat-
ment (e.g., the French data includes 9750 tokens
which were identiﬁed as MWE; out of 870033 to-
kens in the Russian dataset, 1092 correspond to
empty nodes).

When approaching the task of syntactic ordering,

15min=1,max=18mean=1.24,std=0.64len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4like

I

juicy

like

I

apples

Egle

fresh

juicy

fresh apples

Egle

(a) Dependency tree

(b) Binary tree

Figure 3: A dependency tree and a binary tree,
constructed according to Algorithm 1. Reference
sentence: “I like fresh juicy Egle apples”.

4.1 Syntactic Component
The ﬁrst step of the proposed pipeline orders the
nodes of the dependency tree into a sequence which
ideally mirrors the order of words in the reference
sentence. The main difﬁculty of this step is ﬁnding
a sorting or ranking method which avoids making
many node comparisons or scoring decisions. We
propose an ordering procedure which uses a given
dependency tree and constructs a binary tree storing
the original dependency nodes (lemmas) in a sorted
order (Algorithm 1) .

As input, the algorithm takes a dependency tree
and a classiﬁer trained to make binary decisions of
positioning child nodes to the right/left of the head
node. First, we decompose the tree into local sub-
trees, represented by (head, children) node groups.
This is achieved by running a breadth-ﬁrst search
(BFS) algorithm on the input dependency tree (line
4 of the pseudocode). For each (head, children)
group, we further apply the following steps:

• initialize a binary tree with the head node (line

5)

• iterate over the child nodes and use the clas-
siﬁer to predict whether the child should be
inserted to the left or to the right of the head
node (lines 6-7)

When the binary tree construction is ﬁnished, we
can obtain a sorted lemma sequence by performing
in-order traversal on the resulting binary tree.

The core of the procedure is the insertion of a
new node into the binary tree (Algorithm 2). Given
a node pair (ni, nj), a classiﬁer is used to predict
whether nj should be positioned to the left or to

Figure 2: Branching factor of the dependency trees
(English).

one needs to take into account the complexity of
the tree structures. We found the branching factor
to be very informative in this regard: for each node
in each tree we counted the number of children the
node has. Most nodes in the dependency trees of
all examined languages have one to three children
(Figure 2 shows the distribution of branching factor
values for English). This solicits decomposition
of the syntactic ordering procedure over subtrees,
similar to what was done in (He et al., 2009).

4 Our Approach

This section describes the approach we developed
for the shared task.

Given a dependency tree, we ﬁrst decompose
it into subtrees each having one head and an ar-
bitrary number of children. Each subtree is being
linearized as follows: for each of the children nodes
we predict whether it should be positioned to the
left or to the right of the head node, and store this
positional information in a binary tree structure.
We move up the original tree, linearizing subtrees
until we reach the root node. At this point we
have processed all nodes from the original depen-
dency tree – it can be now completely linearized
by traversing the binary tree with the root as a head
node.

Since each dependency node is labeled with the
corresponding lemma, it is trivial to obtain a lemma
sequence from the linearized dependency tree. We
further use the morphological inﬂection genera-
tor component to predict a surface form for each
lemma in the sequence and in this way generate a
sentence.

16min=0,max=19mean=0.94,std=1.67# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10root ← dg.root
decisions ← {}
for head, children ∈ BF S(dg) do

Algorithm 1: Given a dependency tree dg and
a binary classiﬁer clf , construct a binary tree and
traverse it to order dependency nodes. BFS denotes
the breadth-ﬁrst search procedure.
1: function ORDERNODES(clf , dg)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end function

end for
order = OrderDec(root, decisions)
return order

end for
decisions[head] = T raverse(bt)

bt ← BinT ree(head)
for child ∈ children do

InsertN ode(bt, child, clf )

the right of ni. The decision is made based on the
feature representation of the two nodes.

if bt.lef t is N one then

bt.lef t ← BinT ree(child)

bf ← GetF eat(bt)
cf ← GetF eat(child)
label ← M akeDecision(clf , cf, bf )
if label is LEF T then

Algorithm 2: A recursive procedure of inserting a
new node child into a binary tree bt, using a binary
classiﬁer clf .
1: procedure INSERTNODE(bt, child, clf )
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end procedure

bt.right ← BinT ree(child)

InsertN ode(bt.right, child, clf )

InsertN ode(bt.lef t, child, clf )

if bt.right is N one then

end if

end if

end if

else

else

else

For simplicity, we decided to use a multi-layer

perceptron as a classiﬁer (Figure 4).

Given a pair of nodes (ni, nj), we ﬁrst extract
their features. We consider the node itself, it’s head
and one (any) child in the dependency tree as the
neighborhood elements and extract the correspond-
ing lemmas, POS-tags (both XPOS and UPOS),
and dependency edge labels. Thus, the feature set

Figure 4: Schematic view of the neural network
architecture used as a classiﬁer in the syntactic
ordering component of our system.

for one node in the node pair consists of N =
3 (neighborhood elements) × 4 (features) = 12
components.

is

Each component

represented as a d-
dimensional embedding vector. The embedding
matrix which contains all such vectors is denoted as
E ∈ Rd×|V |, where V is the vocabulary of unique
lemmas, XPOS, UPOS and dependency edge la-
bels, observed in the training data.

The embedding vectors for the two nodes un-
der consideration are (1) concatenated to form the
input to the classiﬁer, (2) projected onto a lower-
dimensional space via a linear transformation, (3)
squeezed further via another linear transforma-
tion followed by applying the Leaky ReLu func-
tion (Maas et al., 2013). The last layer of the net-
work consists of one node, followed by the sigmoid
function. The decision of whether to insert node
nj to the right or to the left of node ni is made
according to the following rule:

(cid:40)

decision =

right,
left,

if o ≥ 0.5,
otherwise.

The neural network components were imple-
mented using PyTorch (Paszke et al., 2017). No
pretrained embedding vectors or other external re-
sources were used for the experiments.

4.2 Morphological Component
To create a sentence from an ordered sequence of
lemmas, we need to predict the correct morpholog-
ical form for each of them. This is the purpose of
the second component of our system. While we
focused mostly on the syntactic realization compo-
nent, as part of the system development we experi-
mented with the following three different morpho-
logical inﬂection models:

17...nej...nei......(1)x=[nej;nei](2)h1=W1x(3)h2=lrelu(W2h1)(4)o=sigm(h2)Accuracy (nocase)
ar
LEMMA
13.47
69.15
MAJOR
86.63 ± 0.507
MORPHMLP
88.48 ± 2.409
MORPHRNNSOFT
MORPHRNNHARD 93.07 ± 0.515
Accuracy (case)
86.63 ± 0.507
MORPHMLP
88.48 ± 2.409
MORPHRNNSOFT
MORPHRNNHARD 93.07 ± 0.515

Language

cs
56.43
63.50
94.40 ± 0.052
96.61 ± 0.598
99.53 ± 0.031

en
85.47
86.80
96.41 ± 0.053
93.57 ± 1.370
98.11 ± 0.054

es
71.45
76.13
96.72 ± 0.151
97.20 ± 0.801
99.59 ± 0.027

ﬁ
44.43
51.04
78.26 ± 0.217
81.05 ± 7.405
95.46 ± 0.923

fr
70.44
74.02
92.73 ± 0.094
92.30 ± 0.797
95.56 ± 0.066

it
67.88
72.48
94.09 ± 0.062
92.54 ± 3.721
97.44 ± 0.240

nl
79.35
82.74
91.05 ± 0.110
85.82 ± 1.993
95.68 ± 0.115

pt
74.19
75.85
94.12 ± 0.198
94.27 ± 3.424
99.30 ± 0.035

ru
50.06
55.64
90.43 ± 0.122
93.65 ± 2.980
98.22 ± 0.056

87.31 ± 0.083
89.98 ± 0.638
93.07 ± 0.047

88.79 ± 0.169
86.32 ± 1.446
90.76 ± 0.186

93.52 ± 0.195
94.15 ± 0.823
96.60 ± 0.037

71.90 ± 0.286
75.65 ± 6.869
89.32 ± 0.861

88.17 ± 0.128
87.90 ± 0.803
91.24 ± 0.099

89.54 ± 0.085
88.05 ± 3.524
93.08 ± 0.296

85.79 ± 0.236
80.69 ± 1.903
90.58 ± 0.219

89.90 ± 0.171
90.06 ± 3.379
95.19 ± 0.119

83.32 ± 0.152
87.70 ± 2.763
92.32 ± 0.079

Table 2: Evaluation of the morphological inﬂection system component on the original UD development
set using the percentage of exact string matches as a metric. For the neural architectures, we report both
case-sensitive and case-insensitive mean scores and standard deviation (averaged across ten random seed
values).

• a simple multi-layer perceptron similar to the
one employed for the syntactic component
(MORHPMLP)

• an encoder-decoder architecture with an at-
tention mechanism of Bahdanau et al. (2014)
(MORHRNNSOFT)

• an encoder-decoder model with a hard mono-
tonic attention (Aharoni and Goldberg, 2017)
(MORPHRNNHARD)

OOV lemmas and characters during decoding

were copied without any changes.

5 Experimental Setup and Evaluation
Training data was ﬁltered to exclude outliers ac-
cording to the results of the data analysis (Section
3). The system components were trained separately
ten times with different random seeds. In this sec-
tion, we report mean scores and standard deviation
for each model evaluated on the development data
and averaged across the random seed values. The
evaluation of the proposed approach was done both
independently for each of the single components
and as a whole in the pipeline mode. All the results
are computed on the tokenized data instances.

Morphological component. We start with the
evaluation of the morphological inﬂection genera-
tor, and report the exact string match accuracy for
each of the tested approaches (Table 2). Two sim-
ple baselines were developed for the experiment:
given a lemma, LEMMA copies the lemma itself
as a prediction of the surface form, MAJOR out-
puts the most frequent surface form if the lemma
is not an OOV item, or the lemma itself, otherwise.
Lemma-form frequencies were computed on the
training data. For the baselines, we report case-
insensitive scores only; the results can be easily
extrapolated to the case-sensitive scenario.

As expected, the baselines are outperformed by
all data-driven methods examined. Strong perfor-
mance of the majority baseline for English and
Dutch data can be attributed to the simpler mor-
phology of the languages.

The best

results
of Aharoni

are
and Goldberg

achieved by the
(2017)
model
(MORPHRNNHARD), which outperforms all
other methods across all languages. Despite
the fact that the approach has a bias towards
languages with concatenative morphology (due
to the assumption of the monotonic alignment
between the input and output character sequences),
it also performs well on Arabic. This model was
chosen for our further pipeline experiments.

Bad sample complexity of the soft attention
model (MORPHRNNSOFT) explains its inferior per-
formance compared to the hard attention model.
MORPHRNNSOFT model seems to be highly sen-
sitive to the different values of hyperparameters;
its performance has the highest standard devia-
tion among all models, which is most likely due
to the same sample complexity issue.
Interest-
ingly enough, on English, French, Italian and
Dutch data the multi-layer perceptron architecture
(MORPHMLP) achieves better results. The latter
has a considerably simpler, but less ﬂexible struc-
ture, which prohibits the usage of such networks
for languages with rich morphology – the number
of parameters needed to account for various forms
and morphological features grows rapidly until the
model can no longer ﬁt into the memory. This also
highlights the importance of cross-lingual evalua-
tion of morphological analyzers and generators.

In order to better understand the most common
errors made by each of the approaches (excluding
the baselines), we examined the predictions of the
models on the English development set. We ﬁl-
tered out incorrect predictions of capitalization of

18Error types
wrong lemma
wrong form
alt. form
non-exist. form
proper noun err.
wrong digit seq.

MORPHMLP MORPHRNNSOFT MORPHRNNHARD
–
26
57
4
–
–

42
29
29
–
–
–

–
8
17
29
27
13

Table 3: Major error types made by each of the tested morphological component models.

the ﬁrst letter of the word, because these cases are
ignored by the ofﬁcial evaluation protocol. After
the ﬁltering, we randomly sampled one hundred er-
roneous predictions and manually examined them;
the results are shown in Table 3.

Unlike character-based models, MORPHMLP
treats each surface form as an atomic unit and is
therefore prone to errors caused by the data sparsity
issues, failing to predict correct forms for unseen
lemmas or unseen grammar patterns (wrong lemma
error type). If the model correctly identiﬁes the
base form and still makes a mistake, in half of the
cases it is an incorrect prediction of verb tenses,
singular/plural noun forms or indeﬁnite English ar-
ticles (wrong form). The latter cases are caused by
the fact that our model does not use any informa-
tion about the next token when predicting the form
of the current lemma. This limitation is inherent to
the pipeline architecture we employed and can be
accounted for in a joint morphology/syntax model-
ing scenario. Finally, there are also cases where a
model predicts an alternative surface form which
does not match the ground truth, but is grammati-
cally correct (alt. form): “not” vs. “n’t”, “are” vs.
“’re”, “have” vs. “’ve”). Strictly speaking, the latter
cases are not errors, but for simplicity we will treat
them as such in this section.

MORPHRNNSOFT model predicts fewer wrong
morphological variants, but suffers from another
problem – hallucinating non-existing surface forms:
“singed” instead of “sung”, “dened” instead of “de-
nied”, “siaseme” vs. “siamese”. This is not sur-
prising, given the sequential nature of the model;
usually this happens in cases with ﬂat probability
distributions over a number of possible characters
following the already predicted character sequence.
A large portion of such errors includes incorrect
spellings of proper nouns (proper noun err): “Jer-
sualm” vs. “Jerusalem”, “Mconal” instead of “Mc-
Donal”. Finally, one prominent group of errors is
that of incorrect digit sequences. MORPHMLP does
not make these mistakes, because it uses a heuristic:

OOV lemmas are copied verbatim as predictions
of the surface forms.

The majority of erroneous cases for MOR-
PHRNNHARD model constitute the group of alter-
native forms. Compared to other models, there are
considerably fewer cases of predicting non-existent
forms (“allergys”, “goining”). The wrong form
error type is mainly represented by incorrect pre-
dictions of verb forms: “sing” instead of “sung”,
“got” instead of “gotten”, “are” instead of “’m”, etc.
The results of the error analysis suggest that
there is still a large room for improvement of the
morphological inﬂection generation component. A
principled approach to handling unseen tokens and
a way to constrain the predictions to well-formed
outputs would be interesting directions to investi-
gate further.

Syntactic component. The syntactic compo-
nent has been evaluated by computing system-level
BLEU, NIST and edit distance scores (Table 4).
Following the ofﬁcial evaluation protocol, output
texts were normalized prior to computing metrics
by lower-casing all tokens.

To the best of our knowledge, surface realization
systems have not been evaluated on all the data
used in the shared task. A simple baseline (RAND)
which outputs a random permutation of the sen-
tence tokens performs poorly across all languages.
Compared to it, the 74.88% of the development
data sentences ordered correctly by our method
seem to indicate a good performance.

To get an idea of where our approach breaks, we
sampled a few erroneous predictions and examined
them manually. Generally speaking, the syntactic
ordering procedure works well on the deeper tree
levels, but as we move up, it gets harder to account
for the many descendants a node has. An example
of this error mode is given in Figure 5.

We tried to improve the prediction capabilities of
the system by incorporating feature representations
of the leftmost and the rightmost descendant nodes
and conditioning the model on the previous pre-

19BLEU
RAND
SYNMLP
EDIST
RAND
SYNMLP
NIST
RAND
SYNMLP

ar
0.013
0.896 ± 0.003

cs
0.023
0.778 ± 0.005

en
0.026
0.889 ± 0.007

es
0.016
0.812 ± 0.005

Language

ﬁ
0.031
0.762 ± 0.008

fr
0.018
0.889 ± 0.005

it
0.022
0.849 ± 0.006

nl
0.024
0.800 ± 0.007

pt
0.020
0.901 ± 0.004

ru
0.024
0.820 ± 0.005

0.078
0.910 ± 0.004

0.115
0.833 ± 0.004

0.149
0.912 ± 0.006

0.089
0.840 ± 0.006

0.139
0.827 ± 0.007

0.104
0.897 ± 0.005

0.110
0.849 ± 0.007

0.126
0.844 ± 0.007

0.127
0.924 ± 0.005

0.120
0.839 ± 0.005

10.40
14.14 ± 0.011

11.86
15.83 ± 0.022

9.88
13.97 ± 0.025

9.78
14.51 ± 0.030

11.18
13.09 ± 0.019

9.86
14.48 ± 0.018

9.40
12.92 ± 0.016

9.45
12.59 ± 0.017

9.05
12.87 ± 0.014

11.35
15.49 ± 0.025

Table 4: Evaluation of the syntactic ordering component on the original UD development set. We report
mean scores and standard deviation for the SYNMLP model; the scores were averaged over ten models
trained with different random seeds. RAND is the random baseline. The scores are case-insensitive.

brings

right

move

future

closer

.

Every makes

this particular

Google

Every move Google makes brings this particular future closer.

Every move Google makes closer brings this particular future.

Figure 5: A common error our syntactic ordering
component makes. The node in the rectangle is
current head, the node in the oval indicates its child
for which the position prediction was incorrect.
The upper sentence is the gold ordering, the one
below is predicted by our system.

dictions, but this did not yield any improvements.
Further investigation with regard to this issue is
reserved for future work.

Full pipeline. Table 5 shows the metric evalua-
tion results of the pipeline on the development and
test data provided by the organizers (Dev-SR and
Test-SR), as well as the development data from
the original UD dataset, which was used in our
preliminary experiments (Dev-UD).

Given the large gap between the system per-
formance on Dev-SR and Dev-UD, we manually
inspected the predictions and observed that the
Dev-SR outputs were less grammatical than those
made for the Dev-UD data. We investigated the is-
sue and discovered that the morphological compo-
nent worked as expected, but the syntactic ordering
module was ﬂawed. The proposed method’s per-
formance varies depending on the children nodes’
order returned by the BFS procedure (line 4 of Al-
gorithm 1). Figure 6 shows an example where our

’s

folks

.

That

,

Figure 6: An example sentence which poses a chal-
lenge to our system: “That ’s right , folks .”

system fails.

It is easier to determine the order of node’s chil-
dren starting with content words and then inserting
punctuation signs; if it is the other way round, or-
dering tokens becomes harder. As mentioned in
Section 3, we have used the original UD training
and development data which contains token infor-
mation in the natural order of token occurrence in
the sentences. However, in the shared task data the
word order information was removed by random-
ized scrambling of the tokens, which made it harder
for the syntactic linearizer to make predictions on
Dev-SR and Test-SR. Unfortunately, we did not
anticipate that this will have such a great inﬂuence
on the prediction capabilities of the proposed ap-
proach. We plan to investigate ways of improving
it in future.

6 Discussion and Future Work

This section summarizes our ﬁndings and outlines
perspectives for future work. The syntactic order-
ing component which we propose is capable of
performing accurate tree linearization, but its per-
formance varies depending on the order in which
nodes are being inserted into the binary tree. Per-
muting the tokens randomly and training the syn-
tactic component on scrambled token sequences
seems to be the easiest way of solving the issue.
However, this heuristic method does not guarantee
that the model will not encounter an unseen input
sequence order, in which case it could fail.

20ar
BLEU
Dev-UD 0.757
Dev-SR 0.148
Test-SR 0.162
EDIST
Dev-UD 0.854
Dev-SR 0.192
Test-SR 0.444
NIST
Dev-UD 12.91
6.64
Dev-SR
Test-SR
6.94

cs
0.782
0.250
0.251

0.840
0.239
0.357

15.79
10.61
10.74

en
0.862
0.291
0.296

0.909
0.359
0.659

13.64
9.40
9.58

es
0.816
0.332
0.322

0.851
0.274
0.370

14.47
10.23
10.21

Language

ﬁ
0.701
0.230
0.233

fr
0.800
0.244
0.205

0.808
0.294
0.412

12.44
9.35
9.36

0.868
0.270
0.286

13.63
8.68
7.21

it
0.805
0.225
0.236

0.837
0.258
0.407

12.48
7.73
7.60

nl
0.733
0.221
0.227

0.824
0.252
0.482

11.85
8.61
8.64

pt
0.888
0.261
0.246

0.922
0.343
0.514

12.78
7.76
7.54

ru
0.796
0.34
0.343

0.835
0.318
0.346

15.5
12.94
13.06

Table 5: Final metric evaluation results of the system pipeline. Dev-UD denotes the development set of
the original UD dataset. Dev-SR and Test-SR is the data provided by the organizers (with scrambled
lemmas).

been previously explored, but we found no easy
way of doing so for the proposed approach. Nev-
ertheless, it seems like a promising direction to
pursue, and we plan to investigate it further.

7 Conclusion
In this paper, we have presented the results of
our participation in the Surface Realization Shared
Task 2018. We developed a promising method of
syntactic ordering; evaluation results on the de-
velopment data indicate that once the problem of
order-sensitivity is solved, it can be successfully
applied as a component in the syntactic realization
pipeline.

Acknowledgments
This work was supported by the German Research
Foundation (DFG) under grant No.GU 798/17-1
and the DFG-funded research training group “Adap-
tive Preparation of Information form Heteroge-
neous Sources” (AIPHES, GRK 1994/1). The ﬁrst
author of the paper is supported by the FAZIT Foun-
dation scholarship. We thank Claire Gardent for the
insightful comments and our colleagues Michael
Bugert, Tobias Falke, Jan-Christoph Klie, Ji-Ung
Lee and Nils Reimers who provided suggestions
that greatly assisted our research. Calculations for
this research were conducted on the Lichtenberg
high performance computer of TU Darmstadt.

A more principled approach would be to deﬁne
an adaptive model which encodes some notion of
processing preference: given a set of tokens, the
system should ﬁrst make predictions it is most con-
ﬁdent about, similar to easy-ﬁrst dependency pars-
ing algorithm (Goldberg and Elhadad, 2010) or im-
itation learning methods (Lampouras and Vlachos,
2016).

Another limitation of the proposed method is
its inability to handle non-projective dependencies.
This is a simpliﬁcation decision we made when
designing the algorithm: at each point we assume
that the perfect token order can be retrieved by re-
cursively ordering head-children subtrees, which
excludes long-range crossing dependencies from
consideration. By doing so we aggressively prune
the search space and simplify the inference proce-
dure, but also rule out a smaller class of more com-
plex constructions. This might not be a problem for
the English UD data, which has a small number of
non-projective dependencies. However, according
to the empirical study of Nivre (2006), almost 25%
of the sentences in the Prague Dependency Tree-
bank of Czech (B¨ohmov´a, Alena and Hajiˇc, Jan
and Hajiˇcov´a, Eva and Hladk´a, Barbora, 2003),
and more than 15% in the Danish Dependency
Treebank (Kromann, 2003) contain non-projective
dependencies. This implies that for multi-lingual
surface realization such an assumption could be too
strong.

Finally, another simpliﬁcation which could be
addressed is the decomposition of the prediction
process into two separate stages of syntactic order-
ing and word inﬂection. The beneﬁts of joint mor-
phological inﬂection and syntactic ordering have

21References
Roee Aharoni and Yoav Goldberg. 2017. Morpholog-
ical Inﬂection Generation with Hard Monotonic At-
In Proceedings of the 55th Annual Meet-
tention.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2004–2015, Van-
couver, Canada. Association for Computational Lin-
guistics.

Malin Ahlberg, Markus Forsberg, and Mans Hulden.
2015. Paradigm Classiﬁcation in Supervised Learn-
ing of Morphology. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1024–1029, Denver, Col-
orado. Association for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural Machine Translation by
Jointly Learning to Align and Translate. CoRR,
abs/1409.0473.

Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a Probabilistic Hierarchical Model for Gener-
In Proceedings of the 18th Conference on
ation.
Computational Linguistics: Volume 1, pages 42–48,
Saarbr¨ucken, Germany. Association for Computa-
tional Linguistics.

Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The First
Surface Realisation Shared Task: Overview and
Evaluation Results. In Proceedings of the Genera-
tion Challenges Session at the 13th European Work-
shop on Natural Language Generation, pages 217–
226, Nancy, France. Association for Computational
Linguistics.

B¨ohmov´a, Alena and Hajiˇc, Jan and Hajiˇcov´a, Eva and
Hladk´a, Barbora. 2003. The Prague Dependency
Treebank: a Three-level Annotation Scenario, chap-
ter 7. Kluwer Academic Publishers.

Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad Coverage Multilingual Deep
Sentence Generation with a Stochastic Multi-level
Realizer. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages 98–
106, Beijing, China. Association for Computational
Linguistics.

John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An Efﬁcient Chart Generator
for (Semi-)Lexicalist Grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation, pages 86–95, Toulouse, France.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations Using RNN Encoder-
decoder for Statistical Machine Translation.
In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),

pages 1724–1734, Doha, Qatar. Association for
Computational Linguistics.

George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138–145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.

Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
In Proceedings
Dirichlet Process Mixture Model.
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 616–627, Edin-
burgh, Scotland, UK. Association for Computational
Linguistics.

Greg Durrett and John DeNero. 2013.

Supervised
Learning of Complete Morphological Paradigms. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1185–1195, Atlanta, Georgia. Association for
Computational Linguistics.

Michael Elhadad and Jacques Robin. 1992. Control-
ling Content Realization with Functional Uniﬁca-
tion Grammars. In Proceedings of the 6th Interna-
tional Workshop on Natural Language Generation:
Aspects of Automated Natural Language Generation,
pages 89–104, London, UK, UK. Springer-Verlag.

Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and
Chris Dyer. 2016. Morphological Inﬂection Gener-
ation Using Character Sequence to Sequence Learn-
ing. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 634–643, San Diego, California. Associ-
ation for Computational Linguistics.

Yoav Goldberg and Michael Elhadad. 2010. An Efﬁ-
cient Algorithm for Easy-ﬁrst Non-directional De-
In Human Language Technolo-
pendency Parsing.
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 742–750, Los Angeles, Cal-
ifornia. Association for Computational Linguistics.

Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based N-gram Models for Gen-
eral Purpose Sentence Realisation. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING 2008), pages 297–304,
Manchester, UK. COLING 2008 Organizing Com-
mittee.

Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency Based Chinese Sentence Realiza-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2 - Volume 2, ACL
’09, pages 809–816, Suntec, Singapore. Association
for Computational Linguistics.

22Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic Differentiation in PyTorch.
In NIPS 2017 Workshop Autodiff, Long Beach, Cali-
fornia, USA.

Ratish Puduppully, Yue Zhang, and Manish Shrivas-
tava. 2016. Transition-based Syntactic Lineariza-
In Proceedings of
tion with Lookahead Features.
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 488–
493, San Diego, California. Association for Compu-
tational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works.
In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. CIDEr: Consensus-based Im-
In 2015 IEEE Con-
age Description Evaluation.
ference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 4566–4575, Boston, MA, USA.
IEEE Computer Society.

Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards Broad Coverage Surface Real-
ization with CCG. In Proceedings of the 2007 Work-
shop on Using Corpora for NLG: Language Genera-
tion and Machine Translation (UCNLG+MT), pages
22–30, Copenhagen, Danmark.

Richard Wicentowski. 2004. Multilingual Noise-
robust Supervised Morphological Analysis Using
In Proceedings of the 7th
the Wordframe Model.
Meeting of the ACL Special Interest Group in Com-
putational Phonology: Current Themes in Compu-
tational Phonology and Morphology, SIGMorPhon
’04, pages 70–77, Barcelona, Spain. Association for
Computational Linguistics.

David Yarowsky and Richard Wicentowski. 2000. Min-
imally Supervised Morphological Analysis by Mul-
timodal Alignment. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’00, pages 207–216, Hong Kong. As-
sociation for Computational Linguistics.

Yue Zhang and Stephen Clark. 2011. Syntactic Pro-
cessing Using the Generalized Perceptron and Beam
Search. Computational Linguistics, 37(1):105–151.

Ronald M. Kaplan and Martin Kay. 1994. Regular
Models of Phonological Rule Systems. Computa-
tional Linguistics, 20(3):331–378.

Kimmo Koskenniemi. 1983. Two-level Morphology:
a General Computational Model for Word-form
Recognition and Production. Publications, 11:1–
160.

Matthias Trautner Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG Treebank Tool.
In
Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories, pages 217–220.

Gerasimos Lampouras and Andreas Vlachos. 2016.
Imitation Learning for Language Generation from
Unaligned Data. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 1101–1112,
Osaka, Japan. The COLING 2016 Organizing Com-
mittee.

Irene Langkilde-Geary. 2002. An Empirical Veriﬁ-
cation of Coverage and Correctness for a General-
In Proceedings of
purpose Sentence Generator.
the 2nd International Natural Language Genera-
tion Conference, pages 17–24, Harriman, New York,
USA.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based Syntactic Linearization. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 113–122, Denver, Colorado. Association for
Computational Linguistics.

Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng.
2013. Rectiﬁer Nonlinearities Improve Neural Net-
work Acoustic Models. In ICML Workshop on Deep
Learning for Audio, Speech and Language Process-
ing, Atlanta, USA.

Simon Mille, Anja Belz, Bernd Bohnet, Yvette Gra-
ham, Emily Pitler, and Leo Wanner. 2018. The
First Multilingual Surface Realisation Shared Task
(SR’18): Overview and Evaluation Results. In Pro-
ceedings of the 1st Workshop on Multilingual Sur-
face Realisation (MSR), 56th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1–10, Melbourne, Australia.

Joakim Nivre. 2006. Constraints on Non-Projective
In Proceedings of the 11th
Dependency Parsing.
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 73–80,
Trento, Italy. Association for Computational Lin-
guistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

23A Supplementary Material
A.1 Data Analysis Results

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 7: Data statistics computed for the Arabic data.

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 8: Data statistics computed for the Czech data.

24min=1,max=16mean=3.95,std=2.33len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=19mean=5.12,std=2.61len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=398mean=36.85,std=27.43len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=29mean=0.97,std=1.37# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=41mean=1.71,std=1.70len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4min=1,max=34mean=4.83,std=3.22len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=38mean=4.62,std=3.03len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=194mean=17.13,std=11.07len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=83mean=0.94,std=1.52# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=53mean=2.17,std=2.28len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 9: Data statistics computed for the English data.

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 10: Data statistics computed for the Spanish data.

25min=1,max=140mean=4.06,std=3.04len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=140mean=3.90,std=2.96len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=159mean=16.31,std=12.40len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=19mean=0.94,std=1.67# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=18mean=1.24,std=0.64len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4min=1,max=28mean=4.42,std=3.11len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=28mean=4.30,std=2.98len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=149mean=31.08,std=16.76len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=31mean=0.97,std=1.60# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=32mean=1.56,std=1.74len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 11: Data statistics computed for the Finnish data.

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 12: Data statistics computed for the French data.

26min=1,max=44mean=6.49,std=4.29len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=44mean=5.73,std=3.85len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=238mean=13.31,std=9.48len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=33mean=0.92,std=1.56# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=133mean=2.05,std=3.16len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4min=1,max=62mean=4.33,std=3.10len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=62mean=4.27,std=3.03len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=2,max=392mean=24.49,std=13.75len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=34mean=0.96,std=1.65# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=37mean=1.34,std=1.19len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 13: Data statistics computed for the Italian data.

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 14: Data statistics computed for the Dutch data.

27min=1,max=60mean=4.33,std=3.19len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=60mean=4.49,std=3.18len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=310mean=21.09,std=15.41len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=35mean=0.95,std=1.58# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=40mean=1.54,std=1.68len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4min=1,max=57mean=4.70,std=3.30len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=57mean=4.29,std=2.86len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=74mean=15.12,std=10.15len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=30mean=0.93,std=1.62# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=53mean=1.41,std=1.49len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 15: Data statistics computed for the Portuguese data.

(a) Surface form length distribution.

(b) Lemma length distribution.

(c) Sentence length distribution.

(d) Branching factor distribution.

(e) Lemma-to-form ratio.

Figure 16: Data statistics computed for the Russian data.

28min=1,max=25mean=4.15,std=3.20len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=25mean=4.02,std=3.09len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=1,max=201mean=24.82,std=16.93len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=21mean=0.96,std=1.59# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=34mean=1.48,std=1.58len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4min=1,max=47mean=5.13,std=3.74len(form): 0-5len(form): 5-10len(form): 10-15len(form): 15-20len(form): 20-25len(form) > 30min=1,max=46mean=5.06,std=3.71len(lemma): 0-5len(lemma): 5-10len(lemma): 10-15len(lemma): 15-20len(lemma): 20-25len(lemma) > 30min=2,max=205mean=17.82,std=10.79len(snt): 0-10len(snt): 10-20len(snt): 20-30len(snt): 30-40len(snt): 40-50len(snt): 50-60len(snt): 60-70len(snt) > 70min=0,max=19mean=0.94,std=1.35# children: 0-1# children: 2-3# children: 4-5# children: 6-7# children: 8-9# children: > 10min=1,max=60mean=2.71,std=3.50len(lemma) == 1len(lemma) == 2len(lemma) == 3len(lemma) == 4len(lemma) > 4
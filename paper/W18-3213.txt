Simple Features for Strong Performance on

Named Entity Recognition in Code-Switched Twitter Data

Devanshu Jain Maria Kustikova Mayank Darbari

Rishabh Gupta

Stephen Mayhew

University of Pennsylvania

{devjain, mkust, mdarbari, rgupt, mayhew}@seas.upenn.edu

Abstract

In this work, we address the problem of
Named Entity Recognition (NER) in code-
switched tweets as a part of the Workshop
on Computational Approaches to Linguis-
tic Code-switching (CALCS) at ACL’18
(Aguilar et al., 2018). Code-switching is
the phenomenon where a speaker switches
between two languages or variants of the
same language within or across utter-
ances, known as intra-sentential or inter-
sentential code-switching,
respectively.
Processing such data is challenging using
state of the art methods since such technol-
ogy is generally geared towards process-
ing monolingual text. In this paper we ex-
plored ways to use language identiﬁcation
and translation to recognize named enti-
ties in such data, however, utilizing simple
features (sans multi-lingual features) with
Conditional Random Field (CRF) classi-
ﬁer achieved the best results. Our exper-
iments were mainly aimed at the (ENG-
SPA) English-Spanish dataset but we sub-
mitted a language-independent version of
our system to the (MSA-EGY) Arabic-
Egyptian dataset as well and achieved
good results.

Introduction

1
Recently, social media texts such as tweets and
Facebook posts have attracted attention from
the Natural Language Processing (NLP) research
community. This content has many applications
as it provides clues to analyze sentiments of the
masses towards areas ranging from basic elec-
tronic products to mental health issues to even
national political candidates. These applications
have motivated the NLP community to rethink

strategies for common tools, such as tokenizers,
named entity taggers, POS taggers, dependency
parsers, in the context of informal and noisy text.
As access to the internet becomes more and
more universal, a linguistically diverse population
has come online. Hong et al. (2011) showed that
in a collection of 62 million tweets, only a little
over 50% of them were in English. This multilin-
gualism has given rise to such interesting patterns
as transliteration and code-switching. The multi-
lingual behavior combined with the informal na-
ture of the content makes the task of building NLP
tools even harder.

In this paper, we solve the problem of Named
Entity Recognition (NER) for code-switched twit-
ter data as a part of the ACL’18 Computa-
tional Approaches to Linguistic Code-switching
(CALCS) Shared Task (Aguilar et al., 2018).
Code-switching is a phenomenon that occurs
when multilingual speakers alternate between two
or more languages or dialects. This phenomenon
can be observed across different sentences, within
the same sentence or even in the same word. This
shared task is similar to other social media tasks,
except that the data is explicitly chosen to con-
tain code-switching. The entities for the task are:
Event, Group, Location, Organization, Other, Per-
son, Product, Time, and Title. Below is an ex-
ample of some code-switched data, switching be-
tween English and Spanish:

[Facebook]P rod,

My
[Ig]P rod &
[Twitter]P rod is hellaa dead yall Jk soy
yo que has no life!

In this example, there is a combination of English
and Spanish words and slang words within a tweet,
with 3 entities: Facebook, Instagram (commonly
referred to as ‘Ig’) and Twitter.

ProceedingsofTheThirdWorkshoponComputationalApproachestoCode-Switching,pages103–109Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics103Value / Data
Total number of tweets
Total number of tokens
Average number of tokens per tweet
Standard deviation of the number of tokens per tweet

Train Development
832
50,757
9,583
616,069
11.52
12.14
7.6
7.12

Test
15,634
183,011
15.9
7.11

Table 1: (ENG-SPA) English-Spanish number of tweets and tokens for train, development, and test data

Value / Data
Total number of tweets
Total number of tokens
Average number of tokens per tweet
Standard deviation of the number of tokens per tweet

Train Development
1,122
10,103
204,323
22,742
20.27
20.22
6.63
6.76

Test
1,110
21,414
21.91
6.18

Table 2: (MSA-EGY) Modern Standard Arabic-Egyptian number of tweets and tokens for train, devel-
opment, and test data

2 Related Work

NER is a fundamental part of the Information Ex-
traction pipeline. Most of the available off-the-
shelf systems are trained on formal content, and
consequently do not generalize well when eval-
uated on twitter data (Ritter et al., 2011). This
can be explained by the fact that such systems rely
on hand-crafted standard local features and some
background knowledge, which is not reliable in
data as noisy as tweets. With only a limited num-
ber of characters, people use a variety of creative
ways to express their thoughts, including emoti-
cons and novel abbreviations.

There have been few recent workshops and
shared-tasks on analysis of such noisy social
media data, such as Workshop on Noisy User-
Generated Text (WNUT) at EMNLP (2014, 2016,
2017), Workshop on Approaches to Subjectivity,
Sentiment and Social Media (WASSA) at NAACL
(2016), and Forum for Information Retrieval Eval-
uation (FIRE: 2015, 2016, 2017).

3 Experimental Setup

Here we describe the data, evaluation, and the
model we used.

3.1 Data
In our experiments, we focus primarily on the
English-Spanish (ENG-SPA) dataset. However,
we submitted our basic system results for Arabic-
Egyptian (MSA-EGY) dataset as well.

The organizers provided annotated train and de-
velopment sets for each language. They also pro-
vided an unannotated set of test data, which we
annotated with our system, and submitted for eval-
uation. We never had access to the gold annotated
test set, before or after the evaluation.

Tables 1 and 2 provide information about the
data in terms of number of tweets and tokens for
the (EN-SPA) English-Spanish and (MSA-EGY)
Modern Standard Arabic-Egyptian language pairs.
Tables 3 and 4 provide statistics of the named
entities for both (EN-SPA) English-Spanish and
(MSA-EGY) Modern Standard Arabic-Egyptian
language pairs, where each cell can be interpreted
as Number (Percentage) and entity ‘O’ represents
all non-NE tokens. Please note that the data has
been tagged using the IOB scheme and data in Ta-
bles 3 and 4 is the result of grouping named enti-
ties according to the IOB scheme.

3.2 Evaluation
We used the standard harmonic mean F1 score to
evaluate the system performance. Additionally,
we used surface form F1 score as described in Der-
czynski et al. (2017). Both of these metrics were a
part of the evaluation in the CALCS shared task.

3.3 Method
We used the sklearn implementation of Condi-
tional Random Field (CRF)1 (McCallum and Li,
2003) as the base model in our NER system.

1https://sklearn-crfsuite.readthedocs.

io/

104Entity
O
Event
Group
Location
Organization
Other
Person
Product
Time
Title

Train Count Development Count
9,361 (97.68%)
4 (0.04%)
4 (0.04%)
10 (0.1%)
9 (0.09%)
6 (0.06%)
75 (0.78%)
16 (0.17%)
6 (0.06%)
22 (0.23%)

597,526 (97%)
232 (0.04%)
718 (0.12%)
2,810 (0.46%)
811 (0.13%)
324 (0.05%)
4,701 (0.76%)
1,369 (0.22%)
577 (0.09%)
824 (0.13%)

Table 3: (ENG-SPA) English-Spanish named entities counts for train and development data

Entity
O
Event
Group
Location
Organization
Other
Person
Product
Time
Title

Train Count Development Count
20,031 (88.08%)
69 (0.3%)
191 (0.84%)
358 (1.57%)
149 (0.66%)
17 (0.07%)
698 (3.07%)
55 (0.24%)
61 (0.27%)
115 (0.51%)

181,230 (88.7%)
535 (0.26%)
1,799 (0.88%)
3,275 (1.6%)
1504 (0.74%)
116 (0.06%)
5705 (2.79%)
538 (0.26%)
466 (0.23%)
896 (0.44%)

Table 4: (MSA-EGY) Modern Standard Arabic-Egyptian entities counts for train and development data

System
Org. Baseline
Experiment 1
Top System

ENG-SPA MSA-EGY
62.70
67.44
71.61

53.28
62.13
63.76

Table 5: (ENG-SPA) and (MSA-EGY) Our best
F1 scores on the test datasets compared with the
organizer’s baseline and the top performing sys-
tem in the Shared Task.

4 Experiments

This section gives an overview of our experiments.
First, we identify various local and global fea-
tures using a variety of monolingual tweets and
Gazetteers and train a CRF-based classiﬁer on the
data. Second, we try to improve system recall us-
ing a 2-step NER process. Third, we convert the
convert the code-mixed data to monolingual data
using language identiﬁcation (using a character-
based language model) and translation.

Of the three experiments that we tried, the ﬁrst
method gave the best results. We compare against
the best performing system in the shared task as
well as the organizer’s baseline in Table 5. The
baseline was provided by the organizers and used
Bi-directional LSTMs followed by softmax layer
(trained for 5 epochs) to infer the output labels.

The shared task used Surface Form F1 scores as
well, but we omit them from our results as they
were the same as harmonic mean F1 in all cases.
All scores are reported in Table 6. Detailed scores
are available in the appendix.

4.1 Experiment 1
Our ﬁrst experiment used a standard set of fea-
tures, augmented with some task-speciﬁc ideas,
and deﬁned as follows. Given a sequence of words
in a sentence: ..., wi−2 , wi−1 , wi , wi+1 , wi+2 ,
... and the current word in consideration is wi , we
used the following features:

• If wi is in the beginning of sentence

105Development Data

ENG-SPA

Exp. 1
Exp. 2
Exp. 3

MSA-EGY Exp. 1 (no Gaz)

Precision Recall
32.89
47.37
36.18
73.91

69.44
71.29
66.27
83.29

Test Data
F1 Precision Recall
54.22
64.66
54.00
61.65

72.75
46.22
71.88
74.43

44.64
56.92
46.81
78.32

F1
62.13
53.91
61.67
67.44

Table 6: Results on all submissions. Bold indicates best performance for that language.

• If wi is in the end of sentence
• Lower-case version of wi
• If wi is title-cased
• Preﬁxes and Sufﬁxes of length 4 of wi
• Brown Clusters2 (Cluster Size - 40) of wi
• Word2Vec Clusters: We trained a Word2Vec
( ˇReh˚uˇrek and Sojka, 2010) model on the
combined tweets dataset (dimension: 100 ;
window: 7). Then, we clustered these em-
beddings into 40 clusters and used cluster IDs
as features.

• Gazetteer: We used the Gazetteer

(ex-
tracted from Wikidata by Mishra and Diesner
(2016)) labels as features.
• For each word wk in a context window of ±2:

– The word wk itself
– If wk is upper case
– Shape and Short shape (where same
consecutive characters in the shape are
compressed to a single character) of wk
– If wk contains any special symbol like:

,#,$,-,,,etc. or an emoji.

– If wk is alphabetic or alphanumeric
– Emoji Description: We identiﬁed the
40 most common emojis present in our
dataset and manually labelled them with
representative words, such as smile,
kiss, sad, etc. These emoji description
(sense) of every context word were used
as another feature.

We also ran the experiment on the MSA-EGY

dataset (without the Gazetteer features).

4.2 Experiment 2
Following the ﬁrst experiment, our main observa-
tion was that the recall was quite low. One reason
for this could be the presence of a large amount
of tokens tagged as ‘O’ (∼97%). In contrast, the

2https://github.com/percyliang/

brown-cluster

standard CONLL 2002 Spanish training NER cor-
pus (Tjong Kim Sang, 2002) had ∼87% of the to-
kens tagged as ‘O’.

To solve this issue, we experimented with a 2-
step NER process (similar to (Eiselt and Figueroa,
2013)):

1. Train a CRF model to identify whether a to-

ken is ‘O’ or not

2. Train a CRF model to identify the type of

named-entity (if identiﬁed as non-‘O’)

As expected, we saw major improvements in re-
call, but these were offset by a substantial drop in
precision. Overall, this led to a lower F1 score than
before. In light of these results, we did not use the
2-step approach for any other experiments.

4.3 Experiment 3
In this experiment, we tried to eliminate the code-
switching by converting the data to a monolingual
form. Our method is to identify the language of
each token in the dataset and translate into a com-
mon language.

We collected training data for language iden-
tiﬁcation using the Twitter API. We downloaded
tweets for English and Spanish and assumed that
each word in those tweets belonged to that partic-
ular language. The statistics for the downloaded
data is shown below:
1. 3000 Spanish tweets (7700 tokens ∼56%)
2. 1900 English tweets (6100 tokens ∼44%)
Then, we trained a character-level RNN-based
language model on this data to do language iden-
tiﬁcation. In order to validate, we split our data
and used 80% for training and rest for validat-
ing, achieving an accuracy of 79% on this vali-
dation data. We used this model to identify the
language of all the tokens in dataset, then used
Google Translate API to translate English tokens
to Spanish.

106Andreas Eiselt and Alejandro Figueroa. 2013. A
two-step named entity recognizer for open-domain
In Sixth International Joint Con-
search queries.
ference on Natural Language Processing, IJCNLP
2013, Nagoya, Japan, October 14-18, 2013, pages
829–833. Asian Federation of Natural Language
Processing / ACL.

Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
In Proceedings of the Fifth International
study.
AAAI Conference on Weblogs and Social Media,
pages 519–521.

Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom ﬁelds, feature induction and web-enhanced lex-
icons. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 188–191. Association for Computational Lin-
guistics.

Shubhanshu Mishra and Jana Diesner. 2016. Semi-
supervised named entity recognition in noisy-text.
In Proceedings of the 2nd Workshop on Noisy User-
generated Text, pages 203–212. The COLING 2016
Organizing Committee.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An
In Proceedings of the 2011
experimental study.
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2011, 27-31 July 2011,
John McIntyre Conference Centre, Edinburgh, UK,
A meeting of SIGDAT, a Special Interest Group of
the ACL, pages 1524–1534. ACL.

Erik F. Tjong Kim Sang. 2002.

Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning - Volume
20, COLING-02, pages 1–4, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Finally, we used the language identiﬁcation and
the translation as features in our CRF model, in
addition to all the features used in experiment 1.

As compared to the results from experiment 1,
this improved the recall on both development and
test sets, but again, the loss in precision caused a
slight overall drop in performance.

5 Conclusion

Our submissions earned 4th place out of 8 submis-
sions in the ENG-SPA task, and 3rd place out of 6
submissions in the MSA-EGY task.

Surprisingly, our simplest NER model, trained
without using any language identiﬁcation or trans-
lations, worked best. The other more sophisticated
experiments showed promise in improving the re-
call, but damaged the precision too much to im-
prove the F1 score.

One of the challenges we faced was dissimi-
larity between development and test dataset. Al-
though some of the techniques that we tried on the
development dataset improved the system perfor-
mance, the same effect was not seen in the test
dataset. For example, see the change in perfor-
mance between Table 7 and Table 8. The F1 score
on the development set jumped 12 points, but the
score on the test set dropped 9 points. This could
be explained by the very small size of the devel-
opment dataset, where a few errors or successes
could change the score dramatically. Without ac-
cess to the test data, we could not do any qualita-
tive error analysis.

Finally, since the 2-Step NER achieved such a
high recall, we believe that creating an ensemble
of 1-Step and 2-Step systems could achieve a bet-
ter overall F1 score.

References
Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona
Diab, Julia Hirschberg, and Thamar Solorio. 2018.
Overview of the CALCS 2018 Shared Task: Named
Entity Recognition on Code-switched Data.
In
Proceedings of
the Third Workshop on Compu-
tational Approaches to Linguistic Code-Switching,
Melbourne, Australia. Association for Computa-
tional Linguistics.

Leon Derczynski, Eric Nichols, Marieke van Erp, and
Nut Limsopatham. 2017. Results of the wnut2017
shared task on novel and emerging entity recogni-
tion. In Proceedings of the 3rd Workshop on Noisy
User-generated Text, pages 140–147. Association
for Computational Linguistics.

107Appendices
A ENG-SPA detailed results

We show detailed results for ENG-SPA experiments in the following tables.

Development Data

Event
Group
Location
Organization
Person
Product
Time
Title
Other
Overall

Precision Recall
0.00
25.00
40.00
11.11
44.00
43.75
50.00
4.55
0.00
32.89

0.00
100.00
66.67
100.00
73.33
58.33
50.00
100.00
0.00
69.44

Test Data
F1 Precision Recall
11.11
31.96
68.16
22.77
69.5
45.19
12.58
22.17
0.00
54.22

41.67
68.89
72.16
51.11
83.33
66.41
18.10
47.57
0.00
72.75

0.00
40.00
50.00
20.00
55.00
50.00
50.00
8.70
0.00
44.64

F1
17.54
43.66
70.1
31.51
75.79
53.79
14.84
30.25
0.00
62.13

Table 7: (ENG-SPA) Results for Experiment 1: simple features and gazetteers

Development Data

Event
Group
Location
Organization
Person
Product
Time
Title
Other
Overall

Precision Recall
25.00
25.00
50.00
11.11
58.67
62.50
100.00
9.09
33.33
47.37

50.00
100.00
50.00
50.00
74.58
62.50
100.00
66.67
100.00
71.29

Test Data
F1 Precision Recall
17.78
38.14
71.38
30.20
80.70
51.17
64.90
31.67
5.17
64.66

18.60
25.34
57.16
36.31
60.19
50.64
13.19
28.23
5.56
46.22

33.00
40.00
50.00
18.18
65.67
62.50
100.00
16.00
50.00
56.92

Table 8: (ENG-SPA) Results for Experiment 2: 2-step NER

Development Data

Event
Group
Location
Organization
Person
Product
Time
Title
Other
Overall

Precision Recall
0.00
25.00
50
11.11
50.67
43.75
50.00
0.00
0.00
36.18

0.00
100.00
55.56
50
74.51
53.85
50.00
0.00
0.00
66.27

Test Data
F1 Precision Recall
11.11
30.93
67.80
19.80
69.43
45.45
13.91
22.17
0.00
54.00

45.45
68.18
70.97
48.78
83.19
65.54
18.10
45.37
0.00
71.88

0.00
40.00
52.63
18.18
60.32
48.28
50.00
0.00
0.00
46.81

F1
18.18
30.45
63.48
32.97
68.95
50.90
21.92
29.85
5.36
53.91

F1
17.86
42.55
69.35
28.17
75.69
53.68
15.73
29.79
0.00
61.67

Table 9: (ENG-SPA) Results for Experiment 3: Language Identiﬁcation + Translation

108B MSA-EGY detailed results
We show detailed results for the one MSA-EGY experiment in the following table.

Development Data

Event
Group
Location
Organization
Person
Product
Time
Title
Other
Overall

Precision Recall
43.48
78.01
75.70
65.77
79.66
69.09
77.05
55.65
76.47
73.91

66.67
86.63
87.14
74.24
85.28
79.17
74.60
77.11
92.86
83.29

Test Data
F1 Precision Recall
35.71
73.50
57.95
61.60
64.70
54.55
68.00
50.00
50.00
61.65

67.57
69.92
76.64
68.75
79.34
66.67
68.00
26.32
100.00
74.43

52.63
82.09
81.02
69.75
82.37
73.79
75.81
64.65
83.87
78.32

F1
46.73
71.67
66.00
64.98
71.27
60.00
68.00
34.48
66.67
67.44

Table 10: (MSA-EGY) Results for Experiment 1 (without Gazetteer features)

109
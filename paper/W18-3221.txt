Code-Switched Named Entity Recognition with Embedding Attention

Changhan Wang†, Kyunghyun Cho†‡ and Douwe Kiela†
{changhan,kyunghyuncho,dkiela}@fb.com

† Facebook AI Research; ‡ New York University

Abstract

We describe our work for the CALCS
2018 shared task on named entity recog-
nition on code-switched data. Our system
ranked ﬁrst place for MS Arabic-Egyptian
named entity recognition and third place
for English-Spanish.

1

Introduction

The tendency for multilingual speakers to engage
in code-switching—i.e, alternating between multi-
ple languages or language varieties—poses impor-
tant problems for NLP systems: traditional mono-
lingual techniques quickly break down with input
from mixed languages. Even for problems such
as POS-tagging and language identiﬁcation, which
the community often considers “solved”, perfor-
mance deteriorates proportional to the degree of
code-switching in the data. The shared task for
the third workshop on Computational Approaches
on Linguistic Code-Switching concerned named
entity recognition (NER) for two code-switched
language pairs (Aguilar et al., 2018): Modern
Standard Arabic and Egyptian (MSA-EGY); and
English-Spanish (ENG-SPA). Here, we describe
our work on the shared task.

Traditional NER systems used to rely heavily
on hand-crafted features and gazetteers, but have
since been replaced by neural architectures that
combine bidirectional LSTMs and CRFs (Lample
et al., 2016). Equipped with supervised character-
level representations and pre-trained unsupervised
word embeddings, such neural architectures have
not only come to dominate named entity recog-
nition, but have also successfully been applied
to code-switched language identiﬁcation (Samih
et al., 2016), which makes them highly suitable
for the current task as well.

In this paper, we exploit recent advances in
neural NLP systems, tailored to code-switching.
We use high-quality FastText embeddings trained
on Common Crawl (Grave et al., 2018; Mikolov
et al., 2018) and employ shortcut-stacked sentence
encoders (Nie and Bansal, 2017) to obtain deep
token-level representations to feed into the CRF. In
addition, we make use of an embedding-level at-
tention mechanism that learns task-speciﬁc atten-
tion weights for multilingual and character-level
representations, inspired by context-attentive em-
beddings (Kiela et al., 2018). In what follows, we
describe our system in detail.

2 Approach

The input data consists of noisy user-generated
social media text collected from Twitter. Code-
switching can occur between different tweets in
the training data, with many tweets being mono-
lingual, but can also occur within tweets (e.g.
“[USER]: en los ﬁnales be like [URL]”) or even
“pero esta
morphologically within words (e.g.
twitteando y pitchandome los textos”). The goal
is to predict the correct IOB entity type for the fol-
lowing categories:

• [BI]-PER: Person
• [BI]-LOC: Location
• [BI]-ORG: Organization
• [BI]-GROUP: Group
• [BI]-TITLE: Title
• [BI]-PROD: Product
• [BI]-EVENT: Event
• [BI]-TIME: Time
• [BI]-OTHER: Other
• O: Any other token that is not an NE

ProceedingsofTheThirdWorkshoponComputationalApproachestoCode-Switching,pages154–158Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics154The train/valid/test split for MSA-EGY was
10102/1122/1110. The train/valid/test split for
ENG-SPA was 50757/832/15634.

The ﬁrst work to combine CRFs with modern
neural representation learning for NER is, to our
knowledge, by Collobert et al. (2011). Our archi-
tecture is similar to more recent neural architec-
tures for NER, e.g. Huang et al. (2015); Lample
et al. (2016); Ma and Hovy (2016).
Instead of
using a straightforward bidirectional LSTM (BiL-
STM), we use several layers and add shortcut
connections.
Instead of simply feeding in word
(and/or character) embeddings, we add a self-
attention mechanism.

2.1 Embedding Attention
We represent the input tweets on the word level
and character level. For all available words in
the data, we obtained FastText embeddings trained
on Common Crawl and Wikipedia1 for each lan-
guage. For every word, we try to ﬁnd an exact
match in the FastText embeddings, or if that is not
available we check if it is present in lower case.
When a word embedding is available in one lan-
guage but not in the other, it is initialized as a
zero-vector in the second language. Totally un-
seen words are initialized uniformly at random in
the range [−0.1, 0.1]. Thus, for every language
pair, we obtain word embeddings wL.

On the character level, we encode every word
using a BiLSTM, to which we apply max-pooling
to obtain the token-level representation. That is,
for a sequence of T characters, {ct}t=1,...,T a stan-
dard BiLSTM computes two sets of T hidden
states, one for each direction. The hidden states
are subsequently concatenated for each timestep
to obtain the ﬁnal hidden states, after which a
max-pooling operation is applied over their com-
ponents:

−→
hc
t =
←−
hc
t =

−−−−→
LSTMt(c1, . . . , ct)
←−−−−
LSTMt(ct, . . . , cT )
wchar = max({[

←−
t ]}t=1,...,T )
hc

−→
hc
t ,

We take inspiration from context-attentive em-
beddings (Kiela et al., 2018), in that we learn
weights over the embeddings, but do not include
the contextual dependency for reasons of efﬁ-
ciency given the shared task’s tight deadline. That
1Available at https://fasttext.cc/docs/en/

crawl-vectors.html.

is, we combine the language-speciﬁc word embed-
dings wL1 and wL2 with the character-level word
representation via a simple self-attention mecha-
nism:

αi = softmax(U tanh(V [wL1, wL2, wchar])),

wword+char = [α1wL1, α2wL2, α3wchar]

2.2 Capitalization
Additionally, we concatenate an embedding to
indicate the capitalization of the word, which
be either no-capitals, starting-with-capitals or all-
capitals:

w = [wword+char, wcap]

This is already captured by the character-level en-
coder, but made more explicit using this method.

2.3 Shortcut-Stacked Sentence Encoders
The ﬁnal word representations w are fed into a
stacked BiLSTM with residual connections (i.e.,
“shortcuts”). This type of architecture has been
found to work well for text classiﬁcation, in con-
junction with a ﬁnal max-pooling operation (Nie
and Bansal, 2017). Denoting the input and hidden
state of the i-th stacked BiLSTM layer at timestep
t as xi

t respectively, we have:

t and hi

(cid:40)

xi

t =

wt
[wt, h1

t , . . . , hi−1

t

i = 1
i > 1

]

2.4 CRFs for NER
The hidden states of the last stacked BiLSTM
layer are fed into a CRF (Lafferty et al., 2001).
CRFs are used to estimate probabilities for entire
sequences of tags s corresponding to sequences of
tokens x:

p(s|x; w) =

=

=

(cid:80)
exp(w · Φ(x, s))
exp((cid:80)
s(cid:48) exp(w · Φ(x, s(cid:48)))
(cid:80)
s(cid:48) exp((cid:80)
(cid:81)
s(cid:48)(cid:81)
(cid:80)

j w · φj(x, j, sj−1, sj))
j w · φj(x, j, s(cid:48)
j−1, s(cid:48)
j))
j exp(ψj(w, x, j, sj−1, sj))
j−1, s(cid:48)
j exp(ψj(w, x, j, s(cid:48)

j))

.

To make the CRF tractable, the potentials must
look only at local features. We experiment with
two different score functions ψj. One that uses bi-
grams:

155ψj(w, x, j, p, q) = W

(cid:124)
[p,q,:]xj + B[p,q],

where W ∈ R|S|×|S|×H is w but unﬂattened,
|S| is the number of possible tags, H is the di-
mensionality of the encoder’s features x and B ∈
R|S|×|S| is a bias matrix; and a smaller score func-
tion with unigrams:

ψj(w, x, j, p, q) = W

(cid:124)
[q,:]xj + B[p,q].

where instead W ∈ R|S|×H. The terms in the
score function can be thought of as the emission
and transition potentials, respectively.

Implementational Details

3
3.1 Preprocessing
The noisy nature of the data makes it necessary to
apply appropriate preprocessing steps. We apply
the following steps to the Twitter data:

• Replaced URLs with [url]
• Replaced users (starting with @) with [user]
• Replaced hashtags (starting with # but not

followed by a number) with [hash tag]

• Replaced punctuation tokens with [punct]
• Replaced integer and real numbers by [num]
• Replaced [num]:[num] with [time]
• Replaced [num]-[num] with [date]
• Replaced emojis2 by [emoji]

In addition, we found that the Arabic tokenizer
may have been imperfect: some words still had
punctuation attached to them. In order to mitigate
this, we removed any leading and trailing punctu-
ation from tokens for MSA-EGY.

3.2 Training
The LSTMs are initialized orthogonally (Saxe
et al., 2013), and the attention mechanism is ini-
tialized with Xavier (Glorot and Bengio, 2010).
Word embeddings are kept ﬁxed during training,
but character embeddings and capitalization em-
beddings are updated. We set dropout to 0.5
and optimize using Adam (Kingma and Ba, 2014)
with a learning rate of 4e−4 and batch size of

2We used the emojis in https://pypi.org/project/emoji/.

Model
Baseline
Ours

Dev F1 Test F1
68.17
67.74

60.28
62.39

Table 1: Results for ENG-SPA.

Model
Baseline
Ours

Dev F1 Test F1
79.55
81.41

70.08
71.62

Table 2: Results for MSA-EGY.

64. We shrink the learning rate with a factor
or 0.2 every time there has been no improve-
ment for one epoch, until a minimum learning
rate of 1e−5. We early stop on the valida-
tion set, optimizing for F1. We sweep over the
two CRF types and BiLSTM hidden dimensions
via grid search, trying [128, 128], [128, 128, 128],
[64, 128],
[64, 64, 128, 128] and
[64, 128, 128, 128].

[64, 128, 128],

4 Results & Discussion

For both tasks, we compare the proposed model to
a simpler baseline where we simply concatenate
the FastText embeddings as input to the network.
Table 1 shows the results for ENG-SPA. We ob-
serve that our system outperforms the baseline on
the test set. The dev set for this task was very
small (832, versus a test set of 15.6k), which ex-
plains the discrepancy between dev set and test set
performance—this discrepancy also made it difﬁ-
cult to tune hyperparameters properly for this task.
We also tried a very simple ensembling strategy,
where we took our top three models and randomly
sampled a response, which only marginally im-
proved test score performance to 62.67. We did
not pursue proper ensembling due to time con-
straints. The best performing model had hidden
dimensions [128, 128, 128] and used the bigram
CRF.

The results for the MSA-EGY task are reported
in Table 2. While English and Spanish are two
distinct languages, Modern Standard Arabic and
Egyptian are more closely related, leading to inter-
esting challenges. We observe a similar improve-
ment in this task. As noted in the previous section,
we did ﬁnd that this task required slightly differ-
ent preprocessing. We did not try any ensembling
strategies on this task. The best performing model

156Precision Recall Entity F1
29.51
42.86
69.98
37.93
3.08
72.09
54.46
43.24
33.83
62.39

56.25
69.77
70.75
62.50
14.29
76.52
63.76
51.58
49.14
70.62

20.00
30.93
69.23
27.23
1.71
68.15
47.53
37.09
25.79
55.88

EVENT
GROUP
LOC
ORG
OTHER
PER
PROD
TIME
TITLE
Overall

Precision Recall Entity F1
68.80
73.17
71.78
66.67
100.00
73.21
77.61
67.92
41.38
71.62

78.18
69.77
76.19
66.14
100.00
77.29
76.47
64.29
31.58
73.95

61.43
76.92
67.84
67.20
100.00
69.53
78.79
72.00
60.00
69.42

EVENT
GROUP
LOC
ORG
OTHER
PER
PROD
TIME
TITLE
Overall

Table 3: ENG-SPA test performance breakdown.

Table 4: MSA-EGY test performance breakdown.

had hidden dimensions [64, 64, 128, 128] and used
the unigram CRF.

While developing our system, we made some
interesting observations. For instance, we noticed
that performance on the Event and Time cate-
gories was greatly improved through preprocess-
ing the numbers and splitting out patterns into date
and time categories. Adding explicit capitalization
features improved performance on the Person, Lo-
cation and Organization categories. Tables 3 and
4 show a breakdown of the performance per task
by category on the respective test sets. It is inter-
esting to observe that the Title category is consis-
tently hard for both tasks. The Other category was
perfectly handled for MSA-EGY, while this was
very bad for ENG-SPA — this could however also
be an artifact, since that category was quite small.
We felt that we could have beneﬁted from hav-
ing a strong gazetteer, but also believe that this
would kind of defeat the purpose of our gen-
eral neural network architecture, which should not
have to rely on those kinds of features.

5 Conclusion

Dealing with code-switching is a prominent prob-
lem in handling noisy user-generated social media
data. The tendency for speakers to code-switch
poses difﬁculties for standard NLP pipelines.
Here, we described our work on the shared
task: we introduced a system that performs self-
attention over pre-trained or character-encoded
word embeddings together with a shortcut-stacked
sentence encoder. The system performed impres-
sively on the task.
In the future, we would like
to analyze the system to see whether it has indeed
learned to “code-switch” via embedding attention.

References
Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona
Diab, Julia Hirschberg, and Thamar Solorio. 2018.
Overview of the CALCS 2018 Shared Task: Named
Entity Recognition on Code-switched Data.
In
Proceedings of
the Third Workshop on Compu-
tational Approaches to Linguistic Code-Switching,
Melbourne, Australia. Association for Computa-
tional Linguistics.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neu-
In Proceedings of the thirteenth in-
ral networks.
ternational conference on artiﬁcial intelligence and
statistics, pages 249–256.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings of
LREC.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Douwe Kiela, Changhan Wang, and Kyunghyun
Cho. 2018. Context-attentive embeddings for im-
arxiv preprint
proved sentence representations.
arXiv:1804.07983.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data.

157Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL.

Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
pages 1064—1074.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
Christian Puhrsch, and Armand Joulin. 2018. Learn-
ing word vectors for 157 languages. In Proceedings
of LREC.

Yixin Nie and Mohit Bansal. 2017.

Shortcut-
stacked sentence encoders for multi-domain infer-
ence. arXiv preprint arXiv:1708.02312.

Younes Samih, Suraj Maharjan, Mohammed Attia,
Laura Kallmeyer, and Thamar Solorio. 2016. Multi-
lingual code-switching identiﬁcation via lstm recur-
rent neural networks. In Proceedings of the Second
Workshop on Computational Approaches to Code
Switching, pages 50–59.

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint arXiv:1312.6120.

158
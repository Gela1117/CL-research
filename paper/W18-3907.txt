Iterative Language Model Adaptation for Indo-Aryan Language

Identiﬁcation

Tommi Jauhiainen
University of Helsinki

@helsinki.fi

Heidi Jauhiainen

University of Helsinki

@helsinki.fi

Krister Lind´en

University of Helsinki

@helsinki.fi

Abstract

This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan
Language Identiﬁcation shared task of the VarDial 2018 Evaluation Campaign. The shared task
was an open one, but we did not use any corpora other than what was distributed by the or-
ganizers. A total of eight teams provided results for this shared task. Our submission using a
HeLI-method based language identiﬁer with iterative language model adaptation obtained the
best results in the shared task with a macro F1-score of 0.958.

1

Introduction

In the past, the VarDial workshops have hosted several different shared tasks related to language identi-
ﬁcation and especially the identiﬁcation of close languages, language varieties, and dialects (Zampieri et
al., 2014; Zampieri et al., 2015; Malmasi et al., 2016; Zampieri et al., 2017). The ﬁfth VarDial workshop
included for the ﬁrst time a shared task for Indo-Aryan language identiﬁcation (ILI) (Zampieri et al.,
2018). The goal of the shared task was to identify the language used in unlabeled texts written in Hindi
and four related languages using the Devanagari script: Bhojpuri, Awadhi, Magahi, and Braj.

We have participated in the shared tasks of three previous VarDial workshops using systems based
on different variations of the HeLI method (Jauhiainen et al., 2015b; Jauhiainen et al., 2016; Jauhiainen
et al., 2017a). The HeLI method has turned out to be robust and competitive with other state-of-the-art
language identiﬁcation methods, gaining shared ﬁrst place in the VarDial 2016 Discriminating between
Similar Languages (DSL) shared task. The HeLI method is not especially tailored to be a dialect iden-
tiﬁcation method, but it is a general purpose language identiﬁcation method capable of distinguishing
between hundreds of languages, some of which might be very close to each other (Jauhiainen et al.,
2017b). In the Kone foundation funded Finno-Ugric Languages and the Internet project, a language
identiﬁer implementing the HeLI method has been used together with the Heritrix web-crawler to collect
text in Uralic languages from the internet (Jauhiainen et al., 2015a). The language identiﬁer using the
HeLI method is available for download in GitHub1. In the current workshop, we wanted to try out some
new variations and possible improvements to the original method. For the ILI task, we used the basic
HeLI method, HeLI with adaptive language models, as well as an iterative version of the language model
adaptation method.

2 Related work

The ﬁrst automatic language identiﬁer for digital text was described by Mustonen (1965). During more
than 50 years, hundreds of conference and journal articles describing language identiﬁcation experiments
and methods have been published. For a recent survey on language identiﬁcation and the methods used
in the literature, see Jauhiainen et al. (2018). The HeLI method was ﬁrst presented by Jauhiainen (2010)

This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/.

1https://github.com/tosaja/HeLI

ProceedingsoftheFifthWorkshoponNLPforSimilarLanguages,VarietiesandDialects,pages66–75SantaFe,NewMexico,USA,August20,2018.66and later more formally by Jauhiainen et al. (2016), but we also provide a full description of the exact
variation of the method used to submit the best results on the ILI shared task.

2.1 Language identiﬁcation for Devanagari script
The language identiﬁcation between languages using the Devanagari script has been considered earlier.
Kruengkrai et al. (2006) presented language identiﬁcation results between ten Indian languages, includ-
ing four languages written in Devanagari: Sanskrit, Marathi, Magahi, and Hindi. For the ten Indian
languages they obtained over 90% accuracy with mystery texts 70 bytes in length. As language identiﬁ-
cation method, they used support vector machines (SVM) with string kernels. Murthy and Kumar (2006)
compared the use of language models based on bytes and aksharas. Aksharas are the syllables or ortho-
graphic units of the Brahmi scripts (Vaid and Gupta, 2002). After evaluating the language identiﬁcation
between different pairs of languages, they concluded that the akshara-based models perform better than
byte-based. They used multiple linear regression as the classiﬁcation method.

Sreejith et al. (2013) tested language identiﬁcation with Markovian character and word n-grams from
one to three with Hindi and Sanskrit. A character bigram-based language identiﬁer fared the best and
managed to gain the accuracy of 99.75% for sentence-sized mystery texts. Indhuja et al. (2014) contin-
ued the work of Sreejith et al. (2013) investigating the language identiﬁcation between Hindi, Sanskrit,
Marathi, Nepali, and Bhojpuri. They also evaluated the use of Markovian character and word n-grams
from one to three. For this set of languages word unigrams performed the best, obtaining 88% accuracy
with the sentence-sized mystery texts.

Bergsma et al. (2012) collected tweets in three languages written with the Devanagari script: Hindi,
Marathi, and Nepali. They managed to identify the language of the tweets with 96.2% accuracy using a
logistic regression (LR) classiﬁer (Hosmer et al., 2013) with up to 4-grams of characters. Using an addi-
tional training corpus, they reached 97.9% accuracy with the A-variant of prediction by partial matching
(PPM). Later, Pla and Hurtado (2017) experimented with the corpus of Bergsma et al. (2012). Their ap-
proach using words weighted with TF-IDF (product of term frequency and inverse document frequency)
and SVMs reached 97.7% accuracy on the tweets when using only the provided tweet training corpora.
Hasimu and Silamu (2018) included the same three languages in their test setting. They used a two-stage
language identiﬁcation system, where the languages were ﬁrst identiﬁed as a group using Unicode code
ranges. In the second stage, the languages written with the Devanagari script were individually identiﬁed
using SVMs with character bigrams. Their tests resulted in an F1-score of 0.993 within the group of lan-
guages using Devanagari with 700 best distinguishing bigrams. Indhuja et al. (2014) provided test results
for several different combinations of the ﬁve languages and for the set of languages used by Hasimu and
Silamu (2018) they reached 96% accuracy with word unigrams.

Rani et al. (2018) described a language identiﬁcation system, which they used for discriminating
between Hindi and Magahi. Their language identiﬁer using lexicons and three character sufﬁxes obtained
an accuracy of 86.34%. Kumar et al. (2018) provided an overview of experiments on an earlier version
of the dataset used in this shared task. They managed to obtain the accuracy of 96.48% and a macro
F1-score of 0.96 on the dataset they used. For sentence level identiﬁcation these results are quite good,
and as such they indicate that the languages, at least in their written form as evidenced by the corpus, are
not as closely related as for example the Balkan languages Croatian, Serbian, and Bosnian.

2.2 Unsupervised language model adaptation
In unsupervised language model adaptation, the language models are modiﬁed while identifying the
language of previously unseen and unlabeled text. The goal is to adapt the models to better suit the
language or languages used in the texts to be identiﬁed in order to reach higher identiﬁcation accuracy.
The use of on-line language model adaptation for language identiﬁcation of digital text has been very
limited. Blodgett et al. (2017) experimented with a method where they ﬁrst identiﬁed the language
of tweets using standard langid-py (Lui and Baldwin, 2012), and then collected the tweets with high
posterior probability for English. From the collected tweets they generated a second language model for
English to be used by the language identiﬁer. Language identiﬁers can have several language models

67for one language, all of them providing the same classiﬁcation if chosen. Their experiments produced a
small increase in recall.

Chen and Liu (2005) use language model adaptation with language identiﬁcation of speech similarly
as we are using it in the language identiﬁcation of text. The language identiﬁcation system used by Chen
and Liu (2005) ﬁrst runs the speech through Hidden Markov Model-based phone recognizers (one for
each language), which tokenize the speech into sequences of phones. The probabilities of these phone
sequences for corresponding languages are calculated using language models and the most probable
language is selected. An adaptation routine is then used so that each of the phonetic transcriptions of the
individual speech utterances is used to calculate probabilities for words t, given a word n-gram history
of h as in Equation 1.

Pa(t|h) = λPo(t|h) + (1 − λ)Pn(t|h),

(1)

where Po is the original probability calculated from the training material, Pn the probability calculated
from the data being identiﬁed, and Pa the new adapted probability. λ is the weight given to original
probabilities. Using this adaptation method resulted in decreasing the language identiﬁcation error rate
in a three-way identiﬁcation between Chinese, English, and Russian by 2.88% and 3.84% on an out-of-
domain (different channels) data, and by 0.44% on in-domain (same channel) data.

Zhong et al. (2007) describe a conﬁdence measure which they use with language identiﬁcation of

speech and deﬁne as follows:

C(gi, M ) =

1
n

[log(P (M|gi)) − log(P (M|gj))],

(2)

where M is the sequence to be identiﬁed, n the number of frames in the utterance, gi the best identiﬁed
language, and gj the second best identiﬁed language. In the evaluations of Zhong et al. (2007), this
conﬁdence measure performed clearly better than two other ones they experimented with. They also
evaluated an ensemble of all three conﬁdence measures which managed to slightly improve the results.
They then use the same language adaptation method as Chen and Liu (2005), using the conﬁdence
measures to set the λ for each utterance.

Bacchiani and Roark (2003) used unsupervised language model adaptation in a speech recognition
task. They experimented with iterative adaptation on their language models. One additional adaptation
iteration raised the accuracy gain of the language model adaptation from 3.4% to 3.9%, but subsequent
iterations made the accuracy worse.

3 Task setup and data

For the preparation of the shared task, the participants were provided with training and development
datasets. An early version of the dataset used, as well as its creation, was described by Kumar et al.
(2018). The dataset used for the shared task included text in ﬁve languages, Bhojpuri, Hindi, Awadhi,
Magahi, and Braj as shown in Table 1. The size of the training material was considerably smaller for the
Awadhi language at slightly over 9,000 lines compared with the others which were around 15,000 long.
The difference in size of the training material might produce problems for some methods that have been
used for language identiﬁcation. The HeLI method has turned out to be very robust in this respect, so we
did not need to take this into any special consideration.

Language name Code used Training data (lines) Development data (lines)
Bhojpuri
Hindi
Awadhi
Magahi
Braj

14,897
15,642
9,307
15,306
15,111

BHO
HIN
AWA
MAG
BRA

2,003
2,253
1,480
2,285
2,308

Table 1: List of languages with the sizes of their training and development sets.

68The task was an open one, allowing the use of any additional data or means. However, we did not
try to use any external means and our results would have been exactly the same in a closed version
of the task. Participants were allowed to submit three runs for the ILI task and the best out of those
submissions would be ranked. We submitted one with the original HeLI method, one using language
model adaptation, and one using an iterative version of language model adaptation.

4 The HeLI method, run 1

To make this article more self-contained, we present the full description of the method as used in the
submitted runs. This description differs from the original by Jauhiainen et al. (2016) mostly in that we
leave out the cut-off value c for the size of the language models. In this year’s shared tasks we found,
and have already noticed it earlier, that if the corpora used as the training corpus is of good quality it
is generally advisable to use all the available material. Furthermore, the penalty value compensates for
some of the possible impurities in the language models. The ﬁnal submissions were done with a system
not using words at all, so we leave them out of the description as well.
Description of the HeLI method The goal is to correctly guess the language g ∈ G for each of the
lines in the test set. In the HeLI method, each language g is represented by several different language
models only one of which is used for every word t in the line M. The language models in this version
are based on character n-grams from one to nmax. When none of the n-grams of the size nmax generated
from the word under scrutiny are found in any of the language models, we back off to using the n-grams
of the size nmax − 1. If needed, we continue backing off until character unigrams.

The training data is tokenized into words using non-alphabetic and non-ideographic characters as
delimiters. The data is lowercased, even though the actual Devanagari script does not use capital letters,
but there is some material in the data in other scripts as well. The relative frequencies of character
n-grams from 1 to nmax are calculated inside the words, so that the preceding and the following space-
characters are included. The n-grams are overlapping, so that for example a word with three characters
includes three character trigrams. Then we transform the relative frequencies into scores using 10-based
logarithms. Among the language models generated from the ILI training corpus, the largest model, Hindi
5-grams, included 80,539 different n-grams.

The corpus containing only the n-grams of the length n in the language models is called Cn. The
domain dom(O(Cn)) is the set of all character n-grams of length n found in the models of any language
g (u) are calculated similarly for all n-grams u ∈ dom(O(Cn)) for each language
g ∈ G. The values vCn
g, as shown in Equation 3.

(cid:40) − log10

(cid:16) c(Cn

g ,u)

lCn
g

(cid:17)

vCn

g (u) =

p

, if c(Cn
, if c(Cn

g , u) > 0
g , u) = 0,

(3)

g , u) is the number of n-grams u found in the corpus of the language g and lCn

where c(Cn
g is the total
number of the n-grams of length n in the corpus of language g. These values are used when scoring the
words while identifying the language of a text. When using n-grams, the word t is split into overlapping
n-grams of characters un
i is then scored
separately for each language g.

i , where i = 1, ..., lt − n, of the length n. Each of the n-grams un

If the n-gram un

i is not
found in any of the models, it is simply discarded. We deﬁne the function dg(t, n) for counting n-grams
in t found in a model in Equation 4.

g )), the values in the models are used. If the n-gram un

i is found in dom(O(Cn

lt−n(cid:88)

(cid:26) 1

0

i=1

dg(t, n) =

i ∈ dom(O(Cn))

, if un
, otherwise.

(4)

When all the n-grams of the size n in the word t have been processed, the word gets the value of the

average of the scored n-grams un

i for each language, as in Equation 5.

69(cid:40) 1

(cid:80)lt−n

dg(t,n)

vg(t, n − 1)

i=1 vCn

g (un
i )

vg(t, n) =

, if dg(t, n) > 0
, otherwise,

(5)

where dg(t, n) is the number of n-grams un
g )). If all of the n-grams of
the size n were discarded, dg(t, n) = 0, the language identiﬁer backs off to using n-grams of the size
n − 1. If no values are found even for unigrams, a word gets the penalty value p for every language, as
in Equation 6.

i found in the domain dom(O(Cn

vg(t, 0) = p

(6)

The mystery text is tokenized into words using the non-alphabetic and non-ideographic characters as
delimiters. The words are lowercased when lowercased models are being used. After this, a score vg(t)
is calculated for each word t in the mystery text for each language g, as shown in Equation 7.

(7)
If the length of the word lt is at least nmax − 2, the language identiﬁer backs off to using character

vg(t) = vg(t, min(nmax, lt + 2))

n-grams of the length nmax. In case the word t is shorter than nmax − 2 characters, n = lt + 2.

The whole line M gets the score Rg(M ) equal to the average of the scores of the words vg(t) for each

language g, as in Equation 8.

(cid:80)lT (M )

i=1 vg(ti)
lT (M )

Rg(M ) =

,

(8)

where T (M ) is the sequence of words and lT (M ) is the number of words in the line M. Since we
are using negative logarithms of probabilities, the language having the lowest score is returned as the
language with the maximum probability for the mystery text.

Results of the run1 on the development and the test sets The development set was used for ﬁnding
the best values for the parameter p and to decide which language models to use. We experimented
with several different combinations of language models and the resulting recall-values of these trials can
be seen in Table 2. “Original nmax” refers to the maximum size used with the original n-grams and
“Lowercased nmax” to the size used with the lowercased n-grams. The differences in recall between the
combinations are not very high.

Original words Original nmax

Lowercased words Lowercased nmax

no
no
no
no
no
no
no
no
yes
yes
no
no

-
-
-
8
7
6
-
-
8
8
8
-

no
no
no
no
no
no
no
no
yes
no
yes
yes

6
5
7
8
8
8
8
4
8
8
8
8

Penalty p

5.9
6.4
6.0
5.7
5.7
5.7
5.7
6.7
6.0
6.0
6.0
6.0

Recall
95.26%
95.11%
95.08%
95.01%
95.01%
95.01%
95.01%
95.00%
94.81%
94.81%
94.81%
94.81%

Table 2: Baseline HeLI recall in development data with different combinations of parameters.

We decided to use lowercased character n-grams from one to six with the penalty value of 5.9 for the
ﬁrst run. We included the development set in the training material to generate the ﬁnal language models.
The recall for the test set was 89.28% and the macro F1-score, which is used for ranking in the ILI shared
task, was 0.8873.

705 Unsupervised language model adaptation, run 2

The idea behind language model adaptation is to incorporate new language material into the language
models while previously unseen and untagged text is processed. Most language identiﬁers that can
indicate how well they perform could be used with language model adaptation. The system also beneﬁts
if adding new information to the language models is reasonably easy. Our method is recursive and it
builds on the fact that we can process the same batch of previously unseen texts several times before
providing the ﬁnal labels. In our method, the information from the sentences in the unseen text is added
to the language models one sentence at a time. The sentence to be processed next is always the one that
the language identiﬁer deems to be the one that is most probably correctly identiﬁed using the current
language models. In order to determine which of the sentences is most identiﬁable, we could use the
probabilities given to the sentence by the language models. However, this probability can be almost
equally high for several languages if they are very close to each other. What we want to ﬁnd is a sentence
that gains high probability in one of the languages, but low probability in others. We achieve this by
maximizing the difference between the probabilities of the ﬁrst and the second identiﬁed languages.

Description of the unsupervised language model adaptation method All the lines M are ﬁrst iden-
tiﬁed using the HeLI method. Then the best identiﬁed line, as ranked by the conﬁdence score CM, is
set as identiﬁed. In order to rank the identiﬁed lines to use for language model adaptation, we must be
able to tell how conﬁdent the language identiﬁer is in its decision. As conﬁdence measure CM, we used
the difference between the scores of the best Rg(M ) and the second best Rh(M ) identiﬁed language for
each line. This is basically the same as the conﬁdence measure proposed by Zhong et al. (2007). We
did not test the other two methods presented by Zhong et al. (2007), or their Bayesian classiﬁer-based
ensemble. In our case, the conﬁdence measure is calculated using Equation 9:

CM (Cg, M ) = Rh(M ) − Rg(M ),

(9)

where M is the line containing the mystery text. The character n-grams up to the length of six are
created from the line with the best conﬁdence and they are added to the language models of the winning
language. After this, the rest of the lines are re-identiﬁed with the adapted models and the line with the
best conﬁdence is again added to the models of the language it was identiﬁed to be written with. This
process is repeated until all the lines have been added to the language models. Each time the lines are
re-identiﬁed there is one less line to process. Nevertheless, the number of identiﬁcations is exponential
relative to the number of lines to be identiﬁed when compared with only identifying them once.

Results of run2 on the development and the test sets
In preparation for the second run, we used
the same language models and penalty value as for the ﬁrst run. The language identiﬁer with language
model adaptation achieved 96.22% recall on the development set. It was an increase of 0.96% on top
of the recall of the basic HeLI method. For the submission run, we used both the development and
the training sets to generate the initial language models. The submitted second run reached a recall of
95.66% on the test set, a formidable increase of 6.38% when compared with the ﬁrst run. In other words,
using the language model adaptation reduced the error rate by 59.5%. The fact that the percentage gain
using language model adaptation was clearly more considerable on the test set than on the development
set indicates that the test set is more out-of-domain from the combined development and trainings sets
than the development set was from the training set. The macro F1-score obtained on our second run was
0.9553.

6

Iterative language model adaptation, run 3

While we were experimenting with language model adaptation, we noticed that if the initial language
models are good enough, the adaptation process can be repeated. The additional accuracy gained was
usually very small, but the repeated adaptation only very rarely affected the results in any negative
manner. This is in contrast to the ﬁndings of Bacchiani and Roark (2003), who found that performing
subsequent adaptations made the results worse.

71Iterative language model adaptation basically means that the process for language model adaptation
is restarted after one learning epoch. We noted the time it took to produce the results on the second run
and decided to use four epochs for our third run on the basis of time left before the submissions were
due. We used iterative adaptation with four epochs on the test set, gaining a small additional increase of
0.22% to recall, reaching 95.88% with the F1 score of 0.9576. The ﬁnal results of all our runs and the
best runs of the other teams are listed in Table 3.

Method (or team)
HeLI with iterative language model adaptation (run3)
HeLI with language model adaptation (run2)
taraka rama
XAC
ILIdentiﬁcation
HeLI (run1)
saﬁna
dkosmajac
we are indian
LaMa
Random Baseline

F1 (macro)
0.9576
0.9553
0.9022
0.8933
0.8895
0.8873
0.8627
0.8472
0.8360
0.8195
0.2024

Table 3: Macro F1 scores obtained by different runs submitted by the SUKI-team (bolded) and the best
runs of the other teams.

Figure 1 shows the confusion matrix for the Indo-Aryan languages in our third and ﬁnal run. From the
ﬁgure it seems that the largest misclassiﬁed group was 146 sentences in Bhojpuri, which were identiﬁed
as Hindi. We randomly selected some Bhojpuri sentences to try with Google translator and it detected
them all as Hindi and was also able to produce seemingly intelligible English translations for them.
Unfortunately, our limited understanding of the languages in question prevents us from doing any deeper
error analysis.

Figure 1: Confusion matrix for ﬁnal submitted run.

7 Other experiments
We experimented with leaving out shorter lowercased n-grams with nmax = 6. Leaving out character
unigrams and bigrams did not affect the recall, but leaving out trigrams dropped the recall to 94.53%
indicating that the HeLI back-off function is also needed for these languages. With the German dialect
identiﬁcation task we ended up using only 4-grams of characters.

We also experimented with an unsupervised language set adaptation method. In unsupervised lan-
guage set adaptation, the mystery text is ﬁrst identiﬁed using all the available languages. The language
with the worst score is left out and the text re-identiﬁed with the remaining languages. The process is
continued until only one language is left. In a non-discriminative language identiﬁcation method, the ef-
fect of leaving out languages with the worst scores does not affect the order of the top scoring languages.

72AWABHOBRAHINMAGPredicted labelAWABHOBRAHINMAGTrue label137918444912418131814625521335412183281627152136Confusion Matrix0.00.20.40.60.8However, if the back-off function of the HeLI method is used, it gives equal penalty values to those
languages in which a word is not found. If the word was found in an otherwise poorly scoring language,
which was subsequently left out, the following run might use the back-off function with the word in
question and ﬁnd a difference between the better candidates using character n-grams. We expected the
effect to be small, and it turned out to be slightly negative reducing the recall from 95.26% to 95.22%.

We, furthermore, evaluated the same non-linear mappings, the gamma and the loglike functions, we
used in the DSL shared task at VarDial 2017 (Jauhiainen et al., 2017a). The experiments with the gamma
function ended up with the same recall of 95.26% as the original method. Several different trials with
loglike functions fell short of the recall of the original method at 95.25%.

8 Conclusions
The language model adaptation scheme works very well on the ILI test set. With the German dialect
identiﬁcation task, we noticed that the language adaptation method works especially well when the test
set is out-of-domain compared with the training set. The very good results in the ILI task might indicate
that there is a clear domain difference between the training/development sets and the test set. The iterative
use of the adaptation method with 4 epochs also turned to be beneﬁcial, reducing the remaining errors
by 5.1%.

Acknowledgments
This research was partly conducted with funding from the Kone Foundation Language Programme (Kone
Foundation, 2012).

References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised language model adaptation.

International Conference on Acoustics, Speech and Signal Processing (ICASSP 2003), pages 224–227.

In Proceedings of the

Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language Identiﬁ-
cation for Creating Language-speciﬁc Twitter Collections. In Proceedings of the Second Workshop on Language
in Social Media (LSM2012), pages 65–74, Montr´eal, Canada.

Su Lin Blodgett, Johnny Tian-Zheng Wei, and Brendan O’Connor. 2017. A Dataset and Classiﬁer for Recogniz-
ing Social Media English. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 56–61,
Copenhagen, Denmark.

Yingna Chen and Jia Liu. 2005. Language Model Adaptation and Conﬁdence Measure for Robust Language
Identiﬁcation. In Proceedings of International Symposium on Communications and Information Technologies
2005 (ISCIT 2005), volume 1, pages 270–273, Beijing, China.

Maimaitiyiming Hasimu and Wushour Silamu. 2018. On Hierarchical Text Language-Identiﬁcation Algorithms.

Algorithms, 11(39).

David W. Hosmer, Stanley Lemeshow, and Rodney X. Sturdivant. 2013. Applied logistic regression. Wiley Series

in Probability and Statistics. Wiley, Hoboken, N.J., USA, 3rd ed edition.

K. Indhuja, M. Indu, C. Sreejith, and P. C. Reghu Raj. 2014. Text Based Language Identiﬁcation System for
Indian Languages Following Devanagiri Script. International Journal of Engineering Reseach and Technology,
3(4):327–331.

Heidi Jauhiainen, Tommi Jauhiainen, and Krister Lind´en. 2015a. The Finno-Ugric Languages and The Internet

Project. Septentrio Conference Series, 0(2):87–98.

Tommi Jauhiainen, Heidi Jauhiainen, and Krister Lind´en. 2015b. Discriminating Similar Languages with Token-
Based Backoff. In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages,
Varieties and Dialects (LT4VarDial), pages 44–51, Hissar, Bulgaria.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2016. HeLI, a Word-Based Backoff Method for Lan-
In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and

guage Identiﬁcation.
Dialects (VarDial3), pages 153–162, Osaka, Japan.

73Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2017a. Evaluating HeLI with Non-Linear Mappings.
In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
102–108, Valencia, Spain.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2017b. Evaluation of Language Identiﬁcation Methods
Using 285 Languages. In Proceedings of the 21st Nordic Conference on Computational Linguistics (NoDaLiDa
2017), pages 183–191, Gothenburg, Sweden. Link¨oping University Electronic Press.

Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lind´en. 2018. Automatic Lan-

guage Identiﬁcation in Texts: A Survey. arXiv preprint arXiv:1804.08186.

Tommi Jauhiainen. 2010. Tekstin kielen automaattinen tunnistaminen. Master’s thesis, University of Helsinki,

Helsinki.

Kone Foundation. 2012. The language programme 2012-2016. http://www.koneensaatio.ﬁ/en.

Canasai Kruengkrai, Virach Sornlertlamvanich, and Hitoshi Isahara. 2006. Language, Script, and Encoding
Identiﬁcation with String Kernel Classiﬁers. In Proceedings of the 1st International Conference on Knowledge,
Information and Creativity Support Systems (KICSS 2006), Ayutthaya, Thailand.

Ritesh Kumar, Bornini Lahiri, Deepak Alok, Atul Kr. Ojha, Mayank Jain, Abdul Basit, and Yogesh Dawar. 2018.
Automatic Identiﬁcation of Closely-related Indian Languages: Resources and Experiments. In Proceedings of
the Eleventh International Conference on Language Resources and Evaluation (LREC), Miyazaki, Japan.

Marco Lui and Timothy Baldwin. 2012. langid.py: An Off-the-shelf Language Identiﬁcation Tool. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages
25–30, Jeju, Republic of Korea.

Shervin Malmasi, Marcos Zampieri, Nikola Ljubeˇsi´c, Preslav Nakov, Ahmed Ali, and J¨org Tiedemann. 2016.
Discriminating between Similar Languages and Arabic Dialect Identiﬁcation: A Report on the Third DSL
Shared Task. In Proceedings of the 3rd Workshop on Language Technology for Closely Related Languages,
Varieties and Dialects (VarDial), Osaka, Japan.

Kavi Narayana Murthy and G. Bharadwaja Kumar. 2006. Language Identiﬁcation from Small Text Samples.

Journal of Quantitative Linguistics, 13(1):57–80.

Seppo Mustonen. 1965. Multiple Discriminant Analysis in Linguistic Problems. Statistical Methods in Linguis-

tics, 4:37–44.

Ferran Pla and Llu´ıs-F. Hurtado. 2017. Language Identiﬁcation of Multilingual Posts from Twitter: A Case Study.

Knowledge and Information Systems, 51(3):965–989.

Priya Rani, Atul Kr. Ojha, and Girish Nath Jha. 2018. Automatic language identiﬁcation system for hindi and
In Proceedings of the Eleventh International Conference on Language Resources and Evaluation

magahi.
(LREC 2018), Miyazaki, Japan.

C. Sreejith, M. Indu, and P. C. Reghu Raj. 2013. N-gram based Algorithm for Distinguishing Between Hindi and
Sanskrit Texts. In Proceedings of the Fourth IEEE International Conference on Computing, Communication
and Networking Technologies, Tiruchengode, India.

Jyotsna Vaid and Ashum Gupta. 2002. Exploring Word Recognition in a Semi-Alphabetic Script: The Case of

Devanagari. Brain and Language, 81:679–690.

Marcos Zampieri, Liling Tan, Nikola Ljubeˇsi´c, and J¨org Tiedemann. 2014. A Report on the DSL Shared Task
2014. In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial), pages 58–67, Dublin, Ireland.

Marcos Zampieri, Liling Tan, Nikola Ljubeˇsi´c, J¨org Tiedemann, and Preslav Nakov. 2015. Overview of the
DSL Shared Task 2015. In Proceedings of the Joint Workshop on Language Technology for Closely Related
Languages, Varieties and Dialects (LT4VarDial), pages 1–9, Hissar, Bulgaria.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubeˇsi´c, Preslav Nakov, Ahmed Ali, J¨org Tiedemann, Yves Scherrer,
and No¨emi Aepli. 2017. Findings of the VarDial Evaluation Campaign 2017. In Proceedings of the Fourth
Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), Valencia, Spain.

74Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, Suwon Shon, James Glass, Yves Scherrer, Tanja
Samardˇzi´c, Nikola Ljubeˇsi´c, J¨org Tiedemann, Chris van der Lee, Stefan Grondelaers, Nelleke Oostdijk, Antal
van den Bosch, Ritesh Kumar, Bornini Lahiri, and Mayank Jain. 2018. Language Identiﬁcation and Mor-
phosyntactic Tagging: The Second VarDial Evaluation Campaign. In Proceedings of the Fifth Workshop on
NLP for Similar Languages, Varieties and Dialects (VarDial), Santa Fe, USA.

Shan Zhong, Yingna Chen, Chunyi Zhu, and Jia Liu. 2007. Conﬁdence measure based incremental adaptation for
online language identiﬁcation. In Proceedings of International Conference on Human-Computer Interaction
(HCI 2007), pages 535–543, Beijing, China.

75
Transition-based Parsing with Lighter Feed-Forward Networks

David Vilares

Universidade da Coru˜na

FASTPARSE Lab, LyS Group
Departamento de Computaci´on
Campus de Elvi˜na s/n, 15071

A Coru˜na, Spain

Carlos G´omez-Rodr´ıguez
Universidade da Coru˜na

FASTPARSE Lab, LyS Group
Departamento de Computaci´on
Campus de Elvi˜na s/n, 15071

A Coru˜na, Spain

david.vilares@udc.es

carlos.gomez@udc.es

Abstract

We explore whether it is possible to build
lighter parsers, that are statistically equivalent
to their corresponding standard version, for a
wide set of languages showing different struc-
tures and morphologies. As testbed, we use
the Universal Dependencies and transition-
based dependency parsers trained on feed-
forward networks. For these, most existing re-
search assumes de facto standard embedded
features and relies on pre-computation tricks
to obtain speed-ups. We explore how these
features and their size can be reduced and
whether this translates into speed-ups with a
negligible impact on accuracy. The experi-
ments show that grand-daughter features can
be removed for the majority of treebanks with-
out a signiﬁcant (negative or positive) LAS dif-
ference. They also show how the size of the
embeddings can be notably reduced.

1

Introduction

Transition-based models have achieved signiﬁcant
improvements in the last decade (Nivre et al.,
2007; Chen and Manning, 2014; Rasooli and
Tetreault, 2015; Shi et al., 2017). Some of them
already achieve a level of agreement similar to
that of experts on English newswire texts (Berzak
et al., 2016), although this does not generalize
to other conﬁgurations (e.g.
lower-resource lan-
guages). These higher levels of accuracy of-
ten come at higher computational costs (Andor
et al., 2016) and lower bandwidths, which can be
a disadvantage for scenarios where speed is more
relevant than accuracy (G´omez-Rodr´ıguez et al.,
2017). Furthermore, running neural models on
small devices for tasks such as part-of-speech tag-
ging or word segmentation has become a matter
of study (Botha et al., 2017), showing that small
feed-forward networks are suitable for these chal-
lenges. However, for parsers that are trained using

neural networks, little exploration has been done
beyond the application of pre-computation tricks,
initially intended for fast neural machine transla-
tion (Devlin et al., 2014), at a cost of affordable
but larger memory.
Contribution We explore efﬁcient and light de-
pendency parsers for languages with a variety of
structures and morphologies. We rely on neural
feed-forward dependency parsers, since their ar-
chitecture offers a competitive accuracy vs band-
width ratio and they are also the inspiration for
more complex parsers, which also rely on embed-
ded features but previously processed by bidirec-
tional LSTMs (Kiperwasser and Goldberg, 2016).
In particular, we study if the de facto standard
embedded features and their sizes can be reduced
without having a signiﬁcant impact on their accu-
racy. Building these models is of help in down-
stream applications of natural language process-
ing, such as those running on small devices and
also of interest for syntactic parsing itself, as it
makes it possible to explore how the same con-
ﬁguration affects different languages. This study
is made on the Universal Dependencies v2.1, a
testbed that allows us to compare a variety of lan-
guages annotated following common guidelines.
This also makes it possible to extract a robust and
fair comparative analysis.

2 Related Work
2.1 Computational efﬁciency
The usefulness of dependency parsing is partially
thanks to the efﬁciency of existing transition-
based algorithms, although to the date it is an
open question which algorithms suit certain lan-
guages better. To predict projective structures, a
number of algorithms that run in O(n) with re-
spect to the length of the input string are available.
Broadly speaking, these parsers usually keep two

ProceedingsoftheSecondWorkshoponUniversalDependencies(UDW2018),pages162–172Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics162structures: a stack (containing the words that are
waiting for some arcs to be created) and a buffer
(containing words awaiting to be processed). The
ARC-STANDARD parser (Nivre, 2004) follows a
strictly bottom-up strategy, where a word can
only be assigned a head (and removed from the
stack) once every daughter node has already been
processed. The ARC-EAGER parser avoids this
restriction by including a speciﬁc transition for
the reduce action. The ARC-HYBRID algorithm
(Kuhlmann et al., 2011) mixes characteristics of
both algorithms. More recent algorithms, such
as ARC-SWIFT, have focused on the ability to
manage non-local transitions (Qi and Manning,
2017) to reduce the limitations of transition-based
parsers with respect to graph-based ones (Mc-
Donald et al., 2005; Dozat and Manning, 2017),
that consider a more global context. To manage
non-projective structures, there are also different
options available. The Covington (2001) algo-
rithm runs in O(n2) in the worst case, by com-
paring the word in the top of the buffer with a
subset of the words that have been already pro-
cessed, deciding whether or not to create a link
with each of them. More efﬁcient algorithms such
as SWAP (Nivre, 2009) manage non-projectivity
by learning when to swap pairs of words that are
involved in a crossing arc, transforming it into
a projective problem, with expected execution in
linear time. The 2-PLANAR algorithm (G´omez-
Rodr´ıguez and Nivre, 2010) decomposes trees into
at most two planar graphs, which can be used to
implement a parser that runs in linear time. The
NON-LOCAL COVINGTON algorithm (Fern´andez-
Gonz´alez and G´omez-Rodr´ıguez, 2018) combines
the advantages of the wide coverage of the Cov-
ington (2001) algorithm with the non-local capa-
bilities of the Qi and Manning (2017) transition
system, running in quadratic time in the worst
case.

2.2 Fast dependency parsing strategies

Despite the advances in transition-based algo-
rithms, dependency parsing still is the bottleneck
for many applications. This is due to collateral is-
sues such as the time it takes to extract features
and the multiple calls to the classiﬁer that need
to be made.
In traditional dependency parsing
systems, such as MaltParser (Nivre et al., 2007),
the oracles are trained relying on machine learn-
ing algorithms, such as support vector machines,

and hand-crafted (Huang et al., 2009; Zhang and
Nivre, 2011) or automatically optimized sets of
features (Ballesteros and Nivre, 2012). The goal
usually is to maximize accuracy, which often
comes at a cost of bandwidth. In this sense, efforts
were made in order to obtain speed-ups. Using lin-
ear classiﬁers might lead to faster parsers, at a cost
of accuracy and larger memory usage (Nivre and
Hall, 2010). Bohnet (2010) illustrates that map-
ping the features into weights for a support vector
machine is the major issue for the execution time
and introduces a hash kernel approach to mitigate
it. Volokh (2013) made efforts on optimizing the
feature extraction time for the Covington (2001)
algorithm, deﬁning the concept of static features,
which can be reused through different conﬁgura-
tion steps. The concept itself does not imply a re-
duction in terms of efﬁciency, but it is often em-
ployed in conjunction with the reduction of non-
static features, which causes a drop in accuracy.

In more modern parsers, the oracles are trained
using feed-forward networks (Titov and Hender-
son, 2007; Chen and Manning, 2014; Straka et al.,
2015) and sequential models (Kiperwasser and
Goldberg, 2016). In this sense, to obtain signiﬁ-
cant speed improvements it is common to use the
pre-computation trick from Devlin et al. (2014),
initially intended for machine translation. Broadly
speaking, they precompute the output of the hid-
den layer for each individual feature and each po-
sition in the input vector where they might oc-
cur, saving computation time during the test phase,
with an affordable memory cost. Vacariu (2017)
proposes an optimized parser and also includes a
brief evaluation about reducing features that have
a high cost of extraction, but the analysis is lim-
ited to English and three treebanks. However, lit-
tle analysis has been made on determining if these
features are relevant across a wide variety of lan-
guages that show different particularities. Our
work is also in line with this line of research. In
particular, we focus on feed-forward transition-
based parsers, which already offer a very competi-
tive accuracy vs bandwidth ratio. The models used
in this work do not use any pre-computation trick,
but it is worth pointing out that the insights of this
paper could be used in conjunction with it, to ob-
tain further bandwidth improvements.

1633 Motivation

Transition-based dependency parsers whose ora-
cles are trained using feed-forward neural net-
works have adopted as the de facto standard set
of features the one proposed by Chen and Man-
ning (2014) to parse the English and Chinese Penn
Treebanks (Marcus et al., 1993; Xue et al., 2005).
We hypothesize this de facto standard set of
features and the size of the embeddings used to
represent them can be reduced for a wide variety
of languages, obtaining signiﬁcant speed-ups at a
cost of a marginal impact on their performance. To
test this hypothesis, we are performing an evalua-
tion over the Universal Dependencies v2.1 (Nivre
et al., 2017) a wide multilingual testbed to approx-
imate relevant features over a wide variety of lan-
guages from different families.

4 Methods and Materials

This section describes the parsing algorithms
(§4.1), the architecture of the feed-forward net-
work (§4.2) and the treebanks (§4.3).
4.1 Transition-based algorithms
Let w = [w1, w2, ..., w|w|] be an input sentence, a
dependency tree for w is an edge-labeled directed
tree T = (V, A) where V = {0, 1, 2, . . . ,|w|} is
the set of nodes and A = V × D × V is the set of
labeled arcs. Each arc a ∈ A, of the form (i, d, j),
corresponds to a syntactic dependency between
the words wi and wj; where i is the index of the
head word, j is the index of the child word and d is
the dependency type representing the kind of syn-
tactic relation between them. Each transition con-
ﬁguration is represented as a 3-tuple c = (σ, β, A)
where:

• σ is a stack that contains the words that are
awaiting for remaining arcs to be created. In
σ|i, i represents the ﬁrst word of the stack.

• β is a buffer structure containing the words
that still have not been processed (awaiting
to be moved to σ. In i|β, i denotes the ﬁrst
word of the buffer.

• A is the set of arcs that have been created.

We rely on two transition-based algorithms: the
stack-based ARC-STANDARD (Nivre, 2008) algo-
rithm for projective parsing and its correspond-
ing version with the SWAP operation (Nivre, 2009)

to manage non-projective structures. The elec-
tion of the algorithms is based on their compu-
tational complexity as both run in O(n) empiri-
cally. The set of transitions is shown in Table 1.
Let ci = ([0], β,{}) be an initial conﬁguration,
the parser will apply transitions until a ﬁnal con-
ﬁguration cf = ([0], [], A) is reached.

Transition Step t

Step t+1

(σ|i|j, β, A) (σ|j, β, A ∪ (j, l, i))
STANDARD LEFT-ARCl
(projective) RIGHT-ARCl (σ|i|j, β, A) (σ|i, β, A ∪ (i, l, j))
(σ, i|β, A)
(σ|i, β, A)
(σ|i|j, β, A) (σ|j, i|β, A)

SHIFT
SWAP

SWAP
(above +)

Table 1: Transitions for the projective version of the
stack-based ARC-STANDARD algorithm and its non-
projective version including the SWAP operation

4.2 Feed-forward neural network
We reproduce the Chen and Manning (2014) ar-
chitecture and more in particular the Straka et al.
(2015) version. These two parsers report the
fastest architectures for transition-based depen-
dency parsing (using the pre-computation trick
from Devlin et al. (2014)), and obtain results close
to the state of the art. Let MLPθ(v) be an abstrac-
tion of our multilayered perceptron parametrized
by θ, the output for an input v (in this paper, a
concatenation of embeddings, as described in §5)
is computed as:
MLPθ(v) = softmax (W2·relu(W1·v+b1)+b2)
(1)
where Wi and bi are the weights and bias ten-
sors to be learned at the ith layer and softmax and
relu correspond to the activation functions in their
standard form.

4.3 Universal Dependencies v2.1
Universal dependencies (UD) v2.1 (Nivre et al.,
2017) is a set of 101 dependency treebanks for up
to 60 different languages. They are labeled in the
CoNLLU format, heavily inspired in the CoNLL
format (Buchholz and Marsi, 2006). For each
word in a sentence there is available the follow-
ing information:
ID, WORD, LEMMA, UPOSTAG
(universal postag, available for all
languages),
XPOSTAG (language-speciﬁc postag, available for
some languages), FEATS (additional morphosyn-
tactic information, available for some languages),
HEAD, DEPREL and other optional columns with
additional information.

164In this paper, we are only considering ex-
periments on the unsufﬁxed treebanks (where
UD English is
an unsufﬁxed treebank and
UD English-PUD is a sufﬁxed treebank). The mo-
tivation owes to practical issues and legibility of
tables and discussions.

5 Experiments

We followed the training conﬁguration proposed
by Straka et al. (2015).
All models where
trained using mini-batches (size=10) and stochas-
tic gradient descent (SGD) with exponential decay
(lr = 0.02, decay computed as lr × e−0.2×epoch).
Dropout was set to 50%. With our implemen-
tation dropout was observed to work better than
regularization with less effort in terms of tuning.
We used internal embeddings, initialized accord-
ing to a Glorot uniform (Glorot and Bengio, 2010),
which are learned together with the oracle during
the training phase. In the experiments we use no
external embeddings, following the same criteria
as Straka et al. (2015). The aim was to evaluate all
parsers under a homogeneous conﬁguration, and
high-quality external embeddings may be difﬁcult
to obtain for some languages.

The experiments explore two paths:

(1) is it
possible to reduce the number of features without
a signiﬁcant loss in terms of accuracy? and (2) is
it possible to reduce the size of the embeddings
representing those features, also without causing
signiﬁcant loss in terms of accuracy? To evaluate
this, we used as baseline the following conﬁgura-
tion.

5.1 Baseline conﬁguration
This conﬁguration reproduces that of Straka et al.
(2015) which is basically a version of the Chen
and Manning (2014) parser whose features were
speciﬁcally adapted to the UD treebanks:

De facto standard features The initial set of
features, which we call the de facto standard
features, is composed of: FORM, UPOSTAG and
FEATS for the ﬁrst 3 words in β and the ﬁrst 3
words of σ. The FORM, UPOSTAG, FEATS and DE-
PREL1 for the 2 leftmost and rightmost children of
the ﬁrst 2 words in σ. And the FORM, UPOSTAG,
FEATS and DEPREL of the leftmost of the leftmost
and rightmost of the rightmost children of the ﬁrst
2 words in σ. This makes a total of 18 elements

1Once it has been assigned

and 66 different features. In the case of UD tree-
banks, it is worth noting that for some languages
the FEATS features are not available. We thought
of two strategies in this situation: (1) not to con-
sider any FEATS vector as input or (2) assume that
a dummy input vector is given to represent the
FEATS of an element of the tree. The former would
be more realistic in a real environment, but we be-
lieve the latter offers a fairer comparison of speeds
and memory costs, as the input vector is homoge-
neous across all languages. Thus, this is the op-
tion we have implemented. The dummy vector is
expected to be given no relevance by the neural
network during the training phase. We also rely
on gold UPOSTAGs and FEATS to measure the im-
pact of the reduced features and their reduced size
in an isolated environment.2

Size of the embeddings The embedding size for
the FORM features is set to 50 and for the UP-
OSTAG, FEATS and DEPREL features it is set to 20.
Given an input conﬁguration, the ﬁnal dimension
of the input vector is 1860: 540 dimensions from
directly accessible nodes in σ and β, 880 dimen-
sions corresponding to daughter nodes and 440 di-
mensions corresponding to grand-daughter nodes.

Metrics We use LAS (Labeled Attachment
Score) to measure the performance. To determine
whether the gain or loss with respect to the de facto
standard features is signiﬁcant or not, we used
Bikel’s randomized parsing evaluation comparator
(p < 0.05), a stratiﬁed shufﬂing signiﬁcance test.
The null hypothesis is that the two outputs are pro-
duced by equivalent models and so the scores are
equally likely. To refute it, it ﬁrst measures the
difference obtained for a metric by the two mod-
els. Then, it shufﬂes scores of individual sentences
between the two models and recomputes the eval-
uation metrics, measuring if the new difference is
smaller than the original one, which is an indicator
that the outputs are signiﬁcantly different. Thou-
sands of tokens parsed per second is the metric
used to compare the speed between different fea-
ture sets. To diminish the impact of running time
outliers, this is averaged across ﬁve runs.

Hardware All models were run on the test set
on a single thread on a Intel(R) Core(TM) i7-7700
CPU @ 3.60GHz.

2The use of predicted PoS-tags and/or tokenization would
make harder to measure which is the actual impact of using
different features and size of embeddings.

165No precomputation trick All the parsers pro-
posed in this work do not use the precomputation
trick from Devlin et al. (2014). There is no ma-
jor reason for this, beyond measuring the impact
of the strategies in a simple scenario. We would
like to remark that the speed-ups obtained here by
reducing the number of features could also be ap-
plied to the parsers implementing this precompu-
tation trick, in the sense that the feature extrac-
tion time is lower. No time will be further gained
in terms of computation of the hidden activation
values. However, in this context, at least in the
case of the Chen and Manning (2014) parser, the
pre-computation trick is only applied to the 10 000
most common words. The experiments here pro-
posed are also useful to save memory resources,
even if the trick is used.

5.2 Reducing the number of features

Table 2 shows the impact of ignoring features
that have a larger cost of extraction, i.e., daugh-
ter and grand-daughter nodes, for both the ARC-
STANDARD and SWAP algorithms.
It compares
three sets of features in terms of performance
and speed: (1) de facto standard features, (2) no
grand-daughter (NO-GD) features (excluding ev-
ery leftmost of leftmost and rightmost of rightmost
feature) and (3) no daughter (NO-GD/D) features
(excluding every daughter and grand-daughter fea-
ture from nodes of σ).

Impact of using the NO-GD feature set The
results show that these features can be removed
without causing a signiﬁcant difference in most of
the cases. In the case of the ARC-STANDARD al-
gorithm, for 47 out of 52 treebanks there is no sig-
niﬁcant accuracy loss with respect to the de facto
standard features. In fact, for 22 treebanks there
was a gain with respect to the original set of fea-
tures, from which 5 of them were statistically sig-
niﬁcant. With respect to SWAP, we observe simi-
lar tendencies. For 38 out of 52 treebanks there is
no loss (or the loss is again not statistically sig-
niﬁcant). There is however a larger number of
differences that are statistically signiﬁcant, both
gains (11) and losses (13). On average, the ARC-
STANDARD models trained with these features lost
0.1 LAS points with respect to the original models,
while the average speed-up was ∼23%. The mod-
els trained with SWAP gained instead 0.15 points
and the bandwidth increased by ∼28%.

Impact of the NO-GD/D features As expected,
the results show that removing daughter features
in conjunction with grand-daughter causes a big
drop in performance for the vast majority of cases
(most of them statistically signiﬁcant). Due to this
issue and despite the (also expected) larger speed-
ups, we are not considering this set of features for
the next section.

5.3 Reducing the embedding size of the

selected features

We now explore whether by reducing the size of
the embeddings for the FORM, POSTAG, FEATS
and DEPREL features the models can produce bet-
ter bandwidths without suffering a lack of accu-
racy. We run separate experiments for the ARC-
STANDARD and SWAP algorithms, using as the
starting point the NO-GD feature set, which had a
negligible impact on accuracy, as tested in Table 2.
Table 3 summarizes the experiments when reduc-
ing the size of each embedding from 10% to 50%,
at a step size of 10 percentage points, for the ARC-
STANDARD. The results include information in-
dicating whether the difference in performance is
statistically signiﬁcant from that obtained by the
de facto standard set. In general terms, reducing
the size of the embeddings causes a small but con-
stant drop in the performance. However, for the
vast majority of languages this drop is not statisti-
cally signiﬁcant. Reducing the size of the embed-
dings by a factor of 0.2 was the conﬁguration with
the minimum number of signiﬁcant losses (6), and
reducing them by a factor of 0.5 the one with the
largest (14). On average, the lightest models lost
0.45 LAS points to obtain an speed-up of ∼40%.
Similar tendencies were observed in the case of
the non-projective algorithm, whose results reduc-
ing the size of the embeddings by a factor of 0.1
and 0.5 can be found in Table 4.

5.4 Discussion
Different deep learning frameworks to build neu-
ral networks might present differences and imple-
mentation details that might cause the speed ob-
tained empirically to differ from theoretical expec-
tations.
From a theoretical point of view, both tested ap-
proaches (§5.2, 5.3) should have a similar impact,
as their use directly affects the size of the input
to the neural network. The smaller the input size,
the lighter and faster parsers are obtained. As a
side note, with respect to the case of reducing the

166Treebank
Afrikaans
Anc Greek
Arabic
Basque
Belarusian
Bulgarian
Catalan
Chinese
Coptic
Croatian
Czech
Danish
Dutch
English
Estonian
Finnish
French
Galician
German
Gothic
Greek
Hebrew
Hindi
Hungarian
Indonesian
Irish
Italian
Japanese
Kazakh
Korean
Latin
Latvian
Lithuanian
Marathi
Old Church
Slavonic
Persian
Polish
Portuguese
Romanian
Russian
Serbian
Slovak
Slovenian
Spanish
Swedish
(Sw) Sign
Language
Tamil
Telugu
Turkish
Ukrainian
Urdu
Vietnamese
AVERAGE

STANDARD
kt/s
LAS
3.3
82.72
56.85
3.5
3.1
77.46
3.6
74.26
2.4
70.12
3.4
88.42
87.57
3.4
3.2
79.23
1.9
78.68
3.2
81.23
3.5
85.74
80.93
3.1
3.3
78.67
3.6
84.16
3.1
81.57
81.25
3.3
3.0
84.65
3.5
80.51
3.3
79.86
3.2
74.57
84.71
3.1
3.2
82.16
3.5
90.8
3.1
73.34
2.9
79.47
60.07
2.8
2.9
89.21
3.3
92.16
3.4
22.78
60.84
3.5
3.3
43.31
3.4
75.14
1.5
42.74
1.7
66.02
78.33
3.3

82.1
90.92
86.27
82.12
79.47
84.9
85.54
88.73
85.16
83.73
23.4

69.18
75.17
59.51
81.29
83.12
64.73
75.67

3.1
3.5
3.1
3.3
3.0
3.3
3.2
3.2
2.8
3.4
1.1

2.0
1.5
3.2
3.2
3.3
3.4
3.0

ARC-STANDARD

NO-GD/D
LAS
71.67−−
50.27−−
70.69−−
68.13−−
61.43−−
77.88−−
76.79−−
64.66−−
71.32−−
72.11−−
78.1−−
70.54−−
66.82−−
72.68−−
72.63−−
69.08−−
73.15−−
68.82−−
72.3−−
66.19−−
77.53−−
67.9−−
81.8−−
66.39−−
62.58−−
52.66−−
78.16−−
74.2−−
16.1−−
46.27−−
41.33−−
64.88−−
40.75−
61.89−−
71.07−−
66.16−−
83.03−−
74.09−−
68.89−−
70.0−−
76.57−−
76.44−−
78.06−−
71.78−−
71.21−−
25.53+
66.47−−
74.76−
53.47−−
71.95−−
71.84−−
54.71−−
66.42

kt/s
8.4
8.9
7.7
9.0
5.8
8.4
8.9
8.3
4.9
7.8
8.3
7.3
8.4
8.8
7.6
8.0
7.2
8.5
8.1
8.0
7.8
7.9
8.8
7.0
7.3
7.4
7.2
8.6
8.9
8.9
7.8
8.3
3.5
4.4
8.4

7.6
8.8
8.3
8.2
7.5
8.4
7.8
7.3
6.7
8.6
2.5

4.8
3.1
7.4
7.9
8.5
9.0
7.5

NO-GD

LAS
82.42−
56.63−
77.87+
74.05−
67.73−
88.24−
87.5−
79.2−
76.0−−
81.4+
86.09++
81.28+
79.41+
84.42+
81.74+
82.08++
84.83+
80.29−
79.67−
74.18−
85.07+
82.63+
90.69−
73.14−
79.3−
59.94−
89.33+
92.19+
27.41++
60.13−
44.16++
75.36+
42.64−
62.14−−
78.97++
81.1−−
90.9−
86.58+
81.73−
79.14−
85.17+
85.45−
88.74+
83.75−−
83.63−
22.7−

69.58+
74.48−
59.29−
81.71+
83.03−
64.39−
75.57

kt/s
4.0
4.3
3.7
4.4
2.9
4.1
4.2
4.0
2.3
3.8
4.2
3.7
4.1
4.4
3.8
4.1
3.5
4.2
4.1
3.8
3.7
3.8
4.3
3.5
3.5
3.5
3.3
4.1
4.3
4.4
3.9
4.1
1.8
2.1
4.1

3.8
4.3
4.0
4.1
3.7
4.0
4.0
3.8
3.5
4.2
1.3

2.4
1.6
3.9
3.8
4.1
4.3
3.7

STANDARD
kt/s
LAS
3.0
82.55
58.97
2.9
3.0
76.77
3.2
73.98
2.4
69.75
3.2
87.95
87.01
3.1
3.2
78.26
1.3
77.25
3.0
80.63
3.4
85.55
79.79
2.9
3.1
77.02
3.6
83.19
2.9
80.65
81.47
3.3
2.7
83.54
3.3
79.85
3.1
78.52
2.7
72.92
84.13
3.0
3.1
81.87
3.2
90.46
2.9
72.33
2.8
78.86
61.82
2.8
2.9
88.34
3.3
91.95
3.4
29.32
60.46
3.5
2.6
47.11
3.3
74.73
1.4
46.79
1.7
65.05
79.48
3.0

80.79
90.49
83.87
80.92
78.55
85.8
84.96
89.35
84.32
84.37
10.64

71.04
74.2
60.32
81.8
81.29
64.35
75.29

3.0
3.5
2.7
3.3
2.9
3.2
3.2
3.0
2.7
3.4
0.9

1.7
1.4
3.0
3.0
2.9
3.5
2.8

SWAP
NO-GD/D
LAS
70.59−−
51.36−−
70.4−−
67.31−−
62.81−−
77.41−−
76.48−−
62.74−−
70.08−−
70.54−−
77.9−−
65.4−−
64.83−−
72.76−−
72.68−−
69.36−−
72.31−−
69.25−−
70.69−−
65.92−−
76.54−−
68.77−−
80.5−−
66.88−−
63.62−−
54.28−−
78.13−−
74.09−−
20.47−−
47.63−−
45.33−−
65.11−−
38.21−−
59.95−−
69.65−−
65.35−−
83.46−−
71.84−−
68.82−−
68.63−−
76.04−−
77.72−−
77.67−−
71.78−−
69.96−−
23.05++
67.77−−
75.45+
53.14−−
69.82−−
69.42−−
55.51−−
66.07

kt/s
7.7
7.8
7.4
8.4
5.6
8.2
8.4
8.0
3.1
7.6
7.9
7.1
7.8
8.7
6.6
7.8
6.8
8.3
8.2
7.6
7.5
7.6
8.3
6.8
7.1
7.0
7.0
9.0
8.7
8.8
7.1
8.1
3.4
4.2
8.3

7.3
8.7
7.0
7.9
7.4
7.7
8.2
7.1
7.2
8.5
2.4

4.7
3.0
7.7
7.6
7.9
8.8
7.2

NO-GD

LAS
82.96+
58.48−
77.5++
72.44−−
70.33+
87.98+
87.06+
78.8+
77.44+
81.39++
85.42−
79.55−
78.15++
84.09++
81.06+
80.4−−
83.27−
80.01+
77.65−−
73.19+
84.21+
82.06+
90.01−−
73.36++
79.07+
60.3−−
88.81++
91.91−
33.0++
57.98−−
46.54−
75.55++
43.4−−
66.26+
79.76+

81.08+
90.29−
85.23++
81.05+
77.62−−
85.64−
84.25−−
88.31−−
82.96−−
83.78−−
21.99++

71.09+
75.03++
59.35−
81.19−−
80.93−
63.71−
75.44

kt/s
3.8
3.8
3.7
3.8
2.9
4.2
3.9
4.0
1.5
3.6
4.2
3.6
3.8
4.4
3.7
3.9
3.5
4.2
3.4
3.4
3.8
4.0
4.0
3.1
3.5
3.4
3.6
4.2
4.1
4.3
3.4
4.1
1.8
2.2
3.8

3.8
4.4
3.6
4.1
3.7
4.0
3.9
3.7
3.5
4.3
1.2

2.4
1.6
3.8
3.9
3.4
4.3
3.6

Table 2: Performance for the (1) de facto standard, (2) NO-GD/D and (3) NO-GD set of features, when used to
train oracles with the ARC-STANDARD and SWAP algorithms. Red cells indicate a signiﬁcant loss (- -) with respect
to the baseline, the yellow ones a non-signiﬁcant gain(+)/loss (-) and the green ones a signiﬁcant gain (++).

167Treebank
Afrikaans
Anc Greek
Arabic
Basque
Belarusian
Bulgarian
Catalan
Chinese
Coptic
Croatian
Czech
Danish
Dutch
English
Estonian
Finnish
French
Galician
German
Gothic
Greek
Hebrew
Hindi
Hungarian
Indonesian
Irish
Italian
Japanese
Kazakh
Korean
Latin
Latvian
Lithuanian
Marathi
Old Church
Slavonic
Persian
Polish
Portuguese
Romanian
Russian
Serbian
Slovak
Slovenian
Spanish
Swedish
(Sw) Sign
Language
Tamil
Telugu
Turkish
Ukrainian
Urdu
Vietnamese
AVERAGE

STANDARD

LAS
82.72
56.85
77.46
74.26
70.12
88.42
87.57
79.23
78.68
81.23
85.74
80.93
78.67
84.16
81.57
81.25
84.65
80.51
79.86
74.57
84.71
82.16
90.8
73.34
79.47
60.07
89.21
92.16
22.78
60.84
43.31
75.14
42.74
66.02
78.33

82.1
90.92
86.27
82.12
79.47
84.9
85.54
88.73
85.16
83.73
23.4

69.18
75.17
59.51
81.29
83.12
64.73
75.67

kt/s
3.3
3.5
3.1
3.6
2.4
3.4
3.4
3.2
1.9
3.2
3.5
3.1
3.3
3.6
3.1
3.3
3.0
3.5
3.3
3.2
3.1
3.2
3.5
3.1
2.9
2.8
2.9
3.3
3.4
3.5
3.3
3.4
1.5
1.7
3.3

3.1
3.5
3.1
3.3
3.0
3.3
3.2
3.2
2.8
3.4
1.1

2.0
1.5
3.2
3.2
3.3
3.4
3.0

kt/s
4.1
4.4
3.8
4.5
2.9
4.2
4.3
4.1
2.4
3.9
4.3
3.8
4.2
4.5
3.9
4.2
3.6
4.3
4.2
3.9
3.7
3.8
4.4
3.5
3.5
3.5
3.3
4.3
4.5
4.5
4.0
4.2
1.8
2.2
4.2

size -10%
LAS
82.66−
57.03+
77.24−−
74.78++
68.67−
87.62−−
86.77−−
79.0−
76.58−
80.76−
85.98++
81.02+
78.63−
84.09−
82.2+
81.37+
84.88+
79.67−−
79.0−−
74.77+
84.87+
81.94−
90.92+
72.78−
78.81−−
59.17−−
89.34+
92.14−
26.79++
58.97−−
43.59++
75.83++
44.06+
64.32−
78.86+
81.95−
90.87−
86.47+
81.71−
79.49+
85.15+
85.07−−
88.6−
85.1−
84.22++
24.47+

3.9
4.4
4.0
4.2
3.7
4.1
4.1
3.9
3.6
4.3
1.4

69.83+
75.73+
60.49++
82.05++
83.79++
63.73−−
75.65

2.4
1.7
4.0
3.9
4.1
4.4
3.8

ARC-STANDARD

size -20%
LAS
82.64−
56.52−
76.55−−
74.05−
68.74−
87.95−
87.63+
79.31+
78.68+
81.44+
85.88++
80.68−
78.63−
83.91−
81.55−
81.8+
85.18+
80.24−
79.54−
74.63+
84.45−
82.13−
90.66−
73.02−
79.07−
59.72−
89.16−
91.95−
24.82++
58.8−−
43.45++
75.23+
44.43+
65.53−
79.01++

kt/s
4.4
4.6
4.0
4.7
3.0
4.5
4.7
4.3
2.5
4.1
4.5
4.0
4.4
4.7
4.0
4.4
3.8
4.5
4.5
4.2
4.0
4.1
4.7
3.7
3.7
3.7
3.5
4.4
4.8
4.6
4.2
4.5
1.9
2.3
4.5

NO GD

size -30%
LAS
82.72
56.56−
77.97++
74.12−
68.02−−
87.58−−
87.22−−
79.15−
79.73+
81.2−
86.01++
80.61−−
78.87+
84.49+
81.9+
81.52+
84.74+
79.88−−
79.65−
73.75−−
84.61−
81.83−
90.46−−
72.73−
79.23−
57.94−−
88.33−−
91.97−
24.22++
59.89−−
42.19−−
75.26+
40.75−
64.81−
78.81++

kt/s
4.3
4.6
4.0
4.7
3.0
4.4
4.6
4.3
2.5
4.0
4.4
4.0
4.4
4.7
4.0
4.3
3.8
4.4
4.4
4.1
3.9
4.0
4.6
3.7
3.7
3.7
3.5
4.4
4.7
4.7
4.2
4.4
1.8
2.3
4.4

size -40%
LAS
82.5−
56.24−−
77.18−
73.8−
69.18−
87.9−−
87.28−−
79.13−
77.25−
81.58+
86.02++
80.83−
78.13−
84.35+
81.05−
81.71+
84.51−
80.36−
79.54−
73.96−−
84.18−−
81.42−
90.73−
72.6−
79.31−
58.6−−
89.16−
92.27+
20.17−−
59.85−−
43.84+
74.85−
41.98−
62.86−
78.55+

kt/s
4.6
4.8
4.2
4.9
3.1
4.6
4.8
4.5
2.6
4.2
4.6
4.2
4.6
4.9
4.2
4.5
3.9
4.7
4.6
4.3
4.1
4.2
4.8
3.9
3.9
3.9
3.6
4.6
4.9
4.8
4.4
4.7
1.9
2.3
4.6

size -50%
LAS
83.11+
56.87+
77.41−
74.21−
66.86−−
87.53−−
87.35−−
78.8−
75.62−−
80.5−−
85.39−−
80.81−
79.36++
83.78−
81.48−
81.03−
85.19+
80.59+
79.38−
73.93−
85.21+
81.67−
90.42−−
72.85−
79.1−−
57.55−−
89.57+
91.86−
23.64++
59.37−−
40.22−−
75.1−
40.94−
63.11−
78.86++

kt/s
4.5
4.8
4.7
4.9
3.3
4.6
4.7
4.4
2.5
4.3
4.9
4.4
4.6
4.5
4.5
4.6
4.3
4.8
4.4
4.5
4.5
4.4
4.8
4.2
4.0
3.8
3.8
4.7
4.7
4.9
4.5
4.6
2.0
2.4
4.5

82.23+
90.34−−
86.5+
80.47−−
79.28−
85.81++
85.52−
88.63−
84.58−−
83.91+
27.3+

69.28+
75.73+
59.74+
81.61+
83.41+
63.5−−
75.67

4.1
4.7
4.3
4.3
3.9
4.4
4.4
4.1
3.8
4.6
1.4

2.5
1.7
4.1
4.1
4.4
4.7
3.9

82.3+
90.65−
86.72+
81.28−−
79.1−
85.16+
85.02−
88.59−
84.63−
84.0+
24.82+
68.58−
74.48−
59.53+
81.79+
83.65++
64.32−
75.45

4.1
4.7
4.2
4.4
3.9
4.3
4.3
4.0
3.7
4.5
1.4

2.5
1.7
4.0
4.0
4.3
4.6
3.9

82.12+
90.44−
86.02−
81.13−−
79.4−
85.02+
84.49−−
88.46−
84.47−−
82.73−−
24.11+

70.04+
73.65−
59.6+
82.08++
83.68++
64.7−
75.29

4.2
4.9
4.5
4.5
4.0
4.5
4.5
4.2
3.9
4.8
1.5

2.6
1.7
4.3
4.2
4.6
4.9
4.1

82.63+
90.02−−
86.53+
81.55−
79.21−
85.71++
85.06−−
88.43−
84.93−
83.51−
22.7−

70.14++
74.06−
59.64+
81.46+
83.48+
63.96−−
75.22

4.3
4.7
4.2
4.6
4.0
4.4
4.5
4.4
4.1
4.6
1.4

2.7
1.9
4.5
4.2
4.5
4.8
4.2

Table 3: ARC-STANDARD baseline conﬁguration versus different runs with the NO-GD feature set and embedding
size reduction from 10% to 50%. See Table 2 for color scheme deﬁnition.

168NO-GD

size -10%

size -50%

NO-GD

size -10%

size -50%

Treebank

Afrikaans
Arabic
Belarusian
Catalan
Coptic
Czech
Dutch
Estonian
French
German
Greek
Hindi
Indonesian
Italian
Kazakh
Latin
Lithuanian
Old Church
Slavonic
Polish
Romanian
Serbian
Slovenian
Swedish

Tamil
Turkish
Urdu

— STANDARD

LAS
82.55
76.77
69.75
87.01
77.25
85.55
77.02
80.65
83.54
78.52
84.13
90.46
78.86
88.34
29.32
47.11
46.79
79.48

90.49
80.92
85.8
89.35
84.37

71.04
60.32
81.29

kt/s
3.0
3.0
2.4
3.1
1.3
3.4
3.1
2.9
2.7
3.1
3.0
3.2
2.8
2.9
3.4
2.6
1.4
3.0

3.5
3.3
3.2
3.0
3.4

1.7
3.0
2.9

LAS
83.19+
76.8+
69.54−
87.16+
78.01+
83.49−−
77.93+
80.01−
82.53−−
77.53−−
83.91−
89.86−−
79.0+
88.39+
29.64++
45.05−−
44.72−
80.07+

kt/s
3.9
3.8
2.9
3.9
1.7
4.3
3.9
3.7
3.5
3.5
3.9
4.0
3.6
3.4
4.3
3.3
1.8
3.9

90.49
80.08−−
85.02−−
88.85−−
82.8−−
70.54−
59.71−
81.42+

4.4
4.1
4.1
3.6
4.3

3.2
3.9
3.7

LAS
80.55−−
76.71−
68.52−
86.8−
76.29−
84.97−−
77.53+
80.72+
83.78+
77.6−−
84.11−
89.58−−
79.19+
88.51+
29.77+
44.3−−
44.91−
77.62−−
90.17−
80.1−−
85.24−
88.88−−
83.72−−
70.79−
58.45−−
82.3++

kt/s
4.2
4.1
3.2
4.6
1.5
4.7
4.3
4.1
3.9
4.2
4.4
4.4
3.9
4.0
4.8
3.6
1.9
4.4

4.9
4.5
4.5
4.2
4.8

2.6
4.3
4.0

Treebank
Anc Greek
Basque
Bulgarian
Chinese
Croatian
Danish
English
Finnish
Galician
Gothic
Hebrew
Hungarian
Irish
Japanese
Korean
Latvian
Marathi
Persian

Portuguese
Russian
Slovak
Spanish
(Sw) Sign
Language
Telugu
Ukrainian
Vietnamese

STANDARD

LAS
58.97
73.98
87.95
78.26
80.63
79.79
83.19
81.47
79.85
72.92
81.87
72.33
61.82
91.95
60.46
74.73
65.05
80.79

83.87
78.55
84.96
84.32
10.64

74.2
81.8
64.35

kt/s
2.9
3.2
3.2
3.2
3.0
2.9
3.6
3.3
3.3
2.7
3.1
2.9
2.8
3.3
3.5
3.3
1.7
3.0

2.7
2.9
3.2
2.7
0.9

1.4
3.0
3.5

LAS
59.04+
73.74−
87.06−−
79.76++
81.28+
78.79−−
83.61+
80.98−−
80.44++
72.96+
82.07+
73.77++
60.67−−
92.02+
59.66−−
75.05+
64.81−
81.54+
83.09−
77.78−−
85.25+
83.84−
21.28++

75.17++
80.66−−
64.32−

kt/s
3.6
4.2
4.2
4.1
4.0
3.7
4.5
4.1
4.4
3.5
4.0
3.7
3.5
4.3
4.4
4.2
2.2
3.7

LAS
60.3++
72.48−−
87.73−
78.19−
81.37++
78.55−−
83.92++
81.25−
80.01+
71.43−−
82.41+
72.8+
60.88−−
91.91−
58.75−−
75.05+
65.53+
80.84+

kt/s
3.8
4.3
4.9
4.6
4.1
4.0
4.9
4.4
4.8
3.8
4.5
3.3
3.9
4.7
5.1
4.8
2.4
4.3

2.9
3.7
3.9
3.6
1.2

1.7
4.0
4.4

84.41+
77.75−−
84.27−−
83.28−−
17.73++
73.79−
80.86−−
64.67+

3.6
4.1
4.4
3.7
1.2

1.7
4.4
5.0

Table 4:
SWAP baseline conﬁguration versus different runs with the NO-GD feature set and embedding size
reduction by a factor of 0.1 and 0.5. The average LAS/speed for the baseline is 75.29/2.8, for the NO-GD feature
set with embedding reduction by a factor of 0.1 is 75.27/3.6, and with embedding reduction by a factor of 0.5
75.02/4.0. See Table 2 for color scheme deﬁnition.

number of features (§5.2), an additional speed im-
provement is expected, as less features need to be
collected. But broadly speaking, the speed ob-
tained by skipping half of the features should be
in line with that obtained by reducing the size of
the embeddings of the original features by a factor
of 0.5.

For a practical point of view, in this work we
relied on keras (Chollet et al., 2015). With re-
spect to the part reported in §5.2, the experiments
went as expected. Taking as examples the re-
sults for the ARC-STANDARD algorithm, using no
grand-daughter features implies to diminish the
dimension of the input vector from 1860 dimen-
sions to 1420, a reduction of ∼23%. The aver-
age thousands of tokens parsed per second of the
de facto standard features was 3.0 and the average
obtained without grand-daughter features was 3.7,
a gain of ∼20%. If we also skip daughter features
and reduce the size of the input vector by ∼71%,
the speed increased by a factor of 2.5. Similar ten-
dencies were observed with respect to the SWAP
algorithm. When reducing the size of the embed-
dings (§5.3), the obtained speed-ups were however
lower than those expected in theory. In this sense,
an alternative implementation or a use of a differ-

ent framework could lead to reduce these times to
values closer to the theoretical expectation.

Trying other neural architectures is also of high
interest, but this is left as an open question for fu-
ture research. In particular, in the popular BIST-
based parsers (Kiperwasser and Goldberg, 2016;
de Lhoneux et al., 2017; Vilares and G´omez-
Rodr´ıguez, 2017), the input is ﬁrst processed by a
bidirectional LSTM (Hochreiter and Schmidhuber,
1997) that computes an embedding for each token,
taking into account its left and right context. These
embeddings are then used to extract the features
for transition-based algorithms, including the head
of different elements and their leftmost/rightmost
children. Those features are then fed to a feed-
forward network, similar to the one evaluated in
this work. Thus, the results of this work might be
of future interest for this type of parsers too, as the
output of the LSTM can be seen as improved and
better contextualized word embeddings.

6 Conclusion

We explored whether it is possible to reduce the
number and size of embedded features assumed
as de facto standard by feed-forward network
transition-based dependency parsers. The aim was

169to train efﬁcient and light parsers for a vast amount
of languages showing a rich variety of structures
and morphologies.

To test

the hypothesis we used a multilin-
the Universal Dependencies v2.1.
gual testbed:
The study considered two transition-based algo-
rithms to train the oracles: a stack-based ARC-
STANDARD and its non-projective version, by
adding the SWAP operation. We ﬁrst evalu-
ated three sets of features, clustered according
(1) the de facto stan-
to their extraction costs:
dard features that usually are fed as input to feed-
forward parsers and consider daughter and grand-
daughter features, (2) a no grand-daughter feature
set and (3) a no grand-daughter/daughter feature
set. For the majority of the treebanks we found
that the feature set (2) did not cause a signiﬁcant
loss, both for the stack-based ARC-STANDARD
and the SWAP algorithms. We then took that set of
features and reduced the size of the embeddings
used to represent each feature, up to a factor of
0.5. The experiments also show that for both the
ARC-STANDARD and the SWAP algorithms these
reductions did not cause, in general terms, a signif-
icant loss. As a result, we obtained a set of lighter
and faster transition-based parsers that achieve a
better accuracy vs bandwidth ratio than the origi-
nal ones. It was observed that these improvements
were not restricted to a particular language family
or speciﬁc morphology.

As future work, it would be interesting to try al-
ternative experiments to see whether reducing the
size of embeddings works the same for words as
for other features. Also, the results are compat-
ible with existent optimizations and can be used
together to obtain further speed-ups. Related to
this, quantized word vectors (Lam, 2018) can save
memory and be used to outperform traditional em-
beddings.

Acknowledgments

We would like to thank the anonymous review-
ers for their useful suggestions and detailed com-
ments. This work has received funding from
the European Research Council (ERC), under
the European Union’s Horizon 2020 research
and innovation programme (FASTPARSE, grant
agreement No 714150), from the TELEPARES-
UDC project (FFI2014-51978-C2-2-R) and the
ANSWER-ASAP project (TIN2017-85160-C2-1-
R) from MINECO, and from Xunta de Galicia

(ED431B 2017/01). We gratefully acknowledge
NVIDIA Corporation for the donation of a GTX
Titan X GPU.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
In Pro-
malized transition-based neural networks.
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 2442–2452, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Miguel Ballesteros and Joakim Nivre. 2012. Maltop-
In

timizer: A system for maltparser optimization.
LREC, pages 2757–2763.

Yevgeni Berzak, Yan Huang, Andrei Barbu, Anna Ko-
rhonen, and Boris Katz. 2016. Anchoring and agree-
In Proceedings of
ment in syntactic annotations.
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2215–2224, Austin,
Texas. Association for Computational Linguistics.

Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ’10, pages 89–97,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov,
Alex Salcianu, David Weiss, Ryan McDonald, and
Slav Petrov. 2017. Natural language processing with
small feed-forward networks. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 2879–2885. Associa-
tion for Computational Linguistics.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164.
Association for Computational Linguistics.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
In Proceedings of the 2014 conference on
works.
empirical methods in natural language processing
(EMNLP), pages 740–750.

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/keras-team/keras.

Michael A Covington. 2001. A fundamental algorithm
for dependency parsing. In Proceedings of the 39th
annual ACM southeast conference, pages 95–102.
Citeseer.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.

170Fast and robust neural network joint models for sta-
In Proceedings of the
tistical machine translation.
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1370–1380.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biafﬁne attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations.

Daniel Fern´andez-Gonz´alez

the 2018 Conference of

and Carlos G´omez-
Rodr´ıguez. 2018.
Non-projective dependency
In Proceedings
parsing with non-local transitions.
of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
2 (Short Papers), pages 693–700. Association for
Computational Linguistics.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neu-
ral networks. In Proceedings of the Thirteenth In-
ternational Conference on Artiﬁcial Intelligence and
Statistics, pages 249–256.

Carlos G´omez-Rodr´ıguez, Iago Alonso-Alonso, and
David Vilares. 2017. How important is syntactic
parsing accuracy? an empirical evaluation on rule-
based sentiment analysis. Artiﬁcial Intelligence Re-
view.

Carlos G´omez-Rodr´ıguez and Joakim Nivre. 2010.
A transition-based parser for 2-planar dependency
structures. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1492–1501. Association for Computa-
tional Linguistics.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1222–1231. Asso-
ciation for Computational Linguistics.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
In Pro-
for transition-based dependency parsers.
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 673–682. As-
sociation for Computational Linguistics.

M. Lam. 2018. Word2Bits - Quantized Word Vectors.

ArXiv e-prints.

Miryam de Lhoneux, Yan Shao, Ali Basirat, Eliyahu
Kiperwasser, Sara Stymne, Yoav Goldberg, and
Joakim Nivre. 2017. From raw text to universal
Proceedings of the
dependencies-look, no tags!
CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, pages
207–217.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional linguistics, 19(2):313–330.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 523–530. Association for Computa-
tional Linguistics.

Joakim Nivre. 2004.

Incrementality in deterministic
In Proceedings of the Work-
dependency parsing.
shop on Incremental Parsing: Bringing Engineer-
ing and Cognition Together, IncrementParsing ’04,
pages 50–57, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Joakim Nivre. 2009. Non-projective dependency pars-
In Proceedings of the
ing in expected linear time.
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 351–359. Association for
Computational Linguistics.

Joakim Nivre and Johan Hall. 2010.

A quick
guide to MaltParser optimization. http://maltparser.
org/guides/opt/quick-opt.pdf.

Joakim Nivre,

Johan Hall,

Jens Nilsson, Atanas
Chanev, G¨uls¸en Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering,
13(2):95–135.

Joakim Nivre et al. 2017. Universal dependencies 2.1.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics ( ´UFAL), Faculty of
Mathematics and Physics, Charles University.

Peng Qi and Christopher D. Manning. 2017. Arc-swift:
A novel transition system for dependency parsing.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 110–117, Vancouver, Canada.
Association for Computational Linguistics.

Mohammad Sadegh Rasooli and Joel Tetreault. 2015.
Yara parser: A fast and accurate dependency parser.
arXiv preprint arXiv:1503.06733.

171Tianze Shi, Liang Huang, and Lillian Lee. 2017.
Fast(er) exact decoding and global
training for
transition-based dependency parsing via a minimal
In Proceedings of the 2017 Confer-
feature set.
ence on Empirical Methods in Natural Language
Processing, pages 12–23. Association for Compu-
tational Linguistics.

Milan Straka, Jan Hajic, Jana Strakov´a, and Jan Ha-
jic jr. 2015. Parsing universal dependency treebanks
using neural networks and search-based oracle. In
International Workshop on Treebanks and Linguis-
tic Theories (TLT14), pages 208–220.

Ivan Titov and James Henderson. 2007. Fast and robust
multilingual dependency parsing with a generative
latent variable model. In Proc. of the CoNLL shared
task. Joint Conf. on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), Prague,
Czech Republic.

Andrei Vlad Vacariu. 2017. A high-throughput de-
pendency parser. Ph.D. thesis, Applied Sciences:
School of Computing Science, Simon Fraser Uni-
versity.

David Vilares and Carlos G´omez-Rodr´ıguez. 2017. A
non-projective greedy dependency parser with bidi-
rectional LSTMs. Proceedings of the CoNLL 2017
Shared Task: Multilingual Parsing from Raw Text to
Universal Dependencies, pages 152–162.

Alexander Volokh. 2013. Performance-Oriented De-
pendency Parsing. Doctoral dissertation, Saarland
University, Saarbr¨ucken, Germany.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193. Association for Computational Linguis-
tics.

172
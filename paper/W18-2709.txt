On the Impact of Various Types of Noise on Neural Machine Translation

Huda Khayrallah

Philipp Koehn

Center for Language & Speech Processing

Center for Language & Speech Processing

Computer Science Department

Johns Hopkins University

Computer Science Department

Johns Hopkins University

huda@jhu.edu

phi@jhu.edu

Abstract

We examine how various types of noise
in the parallel training data impact the
quality of neural machine translation sys-
tems. We create ﬁve types of artiﬁcial
noise and analyze how they degrade per-
formance in neural and statistical machine
translation. We ﬁnd that neural models are
generally more harmed by noise than sta-
tistical models. For one especially egre-
gious type of noise they learn to just copy
the input sentence.

1

Introduction

While neural machine translation (NMT) has
shown large gains in quality over statistical ma-
chine translation (SMT) (Bojar et al., 2017), there
are signiﬁcant exceptions to this, such as low
resource and domain mismatch data conditions
(Koehn and Knowles, 2017).

In this work, we consider another challenge to
neural machine translation: noisy parallel data. As
a motivating example, consider the numbers in Ta-
ble 1. Here, we add an equally sized noisy web
crawled corpus to high quality training data pro-
vided by the shared task of the Conference on Ma-
chine Translation (WMT). This addition leads to
a 1.2 BLEU point increase for the statistical ma-
chine translation system, but degrades the neural
machine translation system by 9.9 BLEU.

The maxim more data is better that holds true
for statistical machine translation does seem to
come with some caveats for neural machine trans-
lation. The added data cannot be too noisy. But
what kind of noise harms neural machine transla-
tion models?

In this paper, we explore several types of noise
and assess their impact by adding synthetic noise

WMT17
+ noisy corpus

NMT
27.2
17.3 (–9.9)

SMT
24.0
25.2 (+1.2)

Table 1: Adding noisy web crawled data (raw
data from paracrawl.eu) to a WMT 2017 German–
English statistical system obtains small gains
(+1.2 BLEU), a neural system falls apart (–9.9
BLEU).

to an existing parallel corpus. We ﬁnd that for al-
most all types of noise, neural machine translation
systems are harmed more than statistical machine
translation systems. We discovered that one type
of noise, copied source language segments, has a
catastrophic impact on neural machine translation
quality, leading it to learn a copying behavior that
it then exceedingly applies.

2 Related Work

There is a robust body of work on ﬁltering out
noise in parallel data. For example: Taghipour
et al. (2011) use an outlier detection algorithm
to ﬁlter a parallel corpus; Xu and Koehn (2017)
generate synthetic noisy data (inadequate and non-
ﬂuent translations) and use this data to train a clas-
siﬁer to identify good sentence pairs from a noisy
corpus; and Cui et al. (2013) use a graph-based
random walk algorithm and extract phrase pair
scores to weight the phrase translation probabili-
ties to bias towards more trustworthy ones.

Most of this work was done in the context of sta-
tistical machine translation, but more recent work
(Carpuat et al., 2017) targets neural models. That
work focuses on identifying semantic differences
in translation pairs using cross-lingual textual en-
tailment and additional length-based features, and
demonstrates that removing such sentences im-
proves neural machine translation performance.

Proceedingsofthe2ndWorkshoponNeuralMachineTranslationandGeneration,pages74–83Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics74As Rarrick et al. (2011) point out, one prob-
lem of parallel corpora extracted from the web
is translations that have been created by machine
translation. Venugopal et al. (2011) propose a
method to watermark the output of machine trans-
lation systems to aid this distinction. Antonova
and Misyurev (2011) report that rule-based ma-
chine translation output can be detected due to cer-
tain word choices, and statistical machine transla-
tion output due to lack of reordering.

In 2016, a shared task on sentence pair ﬁltering
was organized1 (Barbu et al., 2016), albeit in the
context of cleaning translation memories which
tend to be cleaner than web crawled data. This
year, a shared task is planned for the type of noise
that we examine in this paper.2

Belinkov and Bisk (2017) investigate noise in
neural machine translation, but they focus on cre-
ating systems that can translate the kinds of or-
thographic errors (typos, misspellings, etc.) that
humans can comprehend. In contrast, we address
noisy training data and focus on types of noise oc-
curring in web-crawled corpora.

There is a rich literature on data selection which
aims at sub-sampling parallel data relevant for a
task-speciﬁc machine translation system (Axelrod
et al., 2011). van der Wees et al. (2017) ﬁnd that
the existing data selection methods developed for
statistical machine translation are less effective for
neural machine translation. This is different from
our goals of handling noise since those methods
tend to discard perfectly ﬁne sentence pairs (say,
about cooking recipes) that are just not relevant
for the targeted domain (say, software manuals).
Our work is focused on noise that is harmful for
all domains.

Since we begin with a clean parallel corpus
and potentially noisy data to it, this work can be
seen as a type of data augmentation. Sennrich
et al. (2016a) incorporate monolingual corpora
into NMT by ﬁrst translating it using an NMT sys-
tem trained in the opposite direction. While such
a corpus has the potential to be noisy, the method
is very effective. Currey et al. (2017) create ad-
ditional parallel corpora by copying monolingual
corpora in the target language into the source, and
ﬁnd it improves over back-translation for some
language pairs. Fadaee et al. (2017) improve NMT
performance in low-resource settings by altering

1NLP4TM 2016: rgcl.wlv.ac.uk/nlp4tm2016/shared-task
2statmt.org/wmt18/parallel-corpus-ﬁltering.html

Type of Noise
Okay
Misaligned sentences
Third language
Both English
Both German
Untranslated sentences
Short segments (≤2 tokens)
Short segments (3–5 tokens)
Non-linguistic characters

Count
23%
41%
3%
10%
10%
4%
1%
5%
2%

Table 2: Noise in the raw Paracrawl corpus.

existing sentences to create training data that in-
cludes rare words in different contexts.

3 Real-World Noise

types of noise are prevalent

What
in crawled
web data? We manually examined 200 sentence
pairs of the above-mentioned Paracrawl corpus
and classiﬁed them into several error categories.
Obviously, the results of such a study depend very
much on how crawling and extraction is executed,
but the results (see Table 2) give some indication
of what noise to expect.

We classiﬁed any pairs of German and English
sentences that are not translations of each other as
misaligned sentences. These may be caused by
any problem in alignment processes (at the doc-
ument level or the sentence level), or by forcing
the alignment of content that is not indeed parallel.
Such misaligned sentences are the biggest source
of error (41%).

There are three types of wrong language con-
tent (totaling 23%): one or both sentences may be
in a language different from German and English
(3%), both sentences may be German (10%), or
both languages may be English (10%).

4% of sentence pairs are untranslated,

i.e.,
source and target are identical. 2% sentence pairs
consist of random byte sequences, only HTML
markup, or Javascript. A number of sentence pairs
have very short German or English sentences, con-
taining at most 2 tokens (1%) or 5 tokens (5%).

Since it is a very subjective value judgment
what constitutes disﬂuent language, we do not
classify these as errors. However, consider the fol-
lowing sentence pairs that we did count as okay,
although they contain mostly untranslated names
and numbers.

75DE: Anonym 2 24.03.2010 um 20:55 314 Kom-
mentare
EN: Anonymous 2 2010-03-24 at 20:55 314
Comments
DE: &lt; &lt; erste &lt; zur¨uck Seite 3 mehr
letzte &gt; &gt;
EN: &lt; &lt; ﬁrst &lt; prev. page 3 next last
&gt; &gt;
At ﬁrst sight, some types of noise seem to be
easier to automatically identify than others. How-
ever, consider, for instance, content in a wrong
language. While there are established methods for
language identiﬁcation (typically based on charac-
ter n-grams), these do not work well on a sentence-
level basis, especially for short sentences. Or,
take the apparently obvious problem of untrans-
lated sentences. If they are completely identical,
that is easy to spot — although even those may
have value, such as the list of country names which
are often spelled identical in different languages.
However, there are many degrees of near-identical
content of unclear utility.

4 Types of Noise

The goal of this paper is not to develop methods to
detect noise but to ascertain the impact of different
types of noise on translation quality when present
in parallel data. We hope that our ﬁndings inform
future work on parallel corpus cleaning.

We now formally deﬁne ﬁve types of naturally
occurring noise and describe how we simulate
them. By creating artiﬁcial noisy data, we avoid
the hard problem of detecting speciﬁc types of
noise but are still able to study their impact.

MISALIGNED SENTENCES As shown above,
a common source of noise in parallel corpora is
faulty document or sentence alignment. This re-
sults in sentences that are not matched to their
translation. Such noise is rare in corpora such
as Europarl where strong clues about debate top-
ics and speaker turns reduce the scale of the task
of alignment to paragraphs, but more common in
the alignment of less structured web sites. We ar-
tiﬁcially create misaligned sentence data by ran-
domly shufﬂing the order of sentences on one side
of the original clean parallel training corpus.

MISORDERED WORDS Language may be dis-
ﬂuent in many ways. This may be the product
of machine translation, poor human translation,
or heavily specialized language use, such as bul-

let points in product descriptions (recall also the
examples above). We consider one extreme case
of disﬂuent language: sentences from the original
corpus where the words are reordered randomly.
We do this on the source or target side.
WRONG LANGUAGE A parallel corpus may be
polluted by text in a third language, say French
in a German–English corpus. This may occur on
the source or target side of the parallel corpus. To
simulate this, we add French–English (bad source)
or German–French (bad target) data to a German–
English corpus.
UNTRANSLATED SENTENCES Especially in
parallel corpora crawled from the web, there are
often sentences that are untranslated from the
source in the target. Examples are navigational el-
ements or copyright notices in the footer. Purport-
edly multi-lingual web sites may be only partially
translated, while some original text is copied.
Again, this may show up on the source or the tar-
get side. We take sentences from either the source
or target side of the original parallel corpus and
simply copy them to the other side.
SHORT SEGMENTS Sometimes additional data
comes in the form of bilingual dictionaries. Can
we simply add them as additional sentence pairs,
even if they consist of single words or short
phrases? We simulate this kind of data by sub-
subsampling a parallel corpus to include only sen-
tences of maximum length 2 or 5.

5 Experimental Setup
5.1 Neural Machine Translation
Our neural machine translation systems are trained
using Marian (Junczys-Dowmunt et al., 2018).3
We build shallow RNN-based encoder-decoder
models with attention (Bahdanau et al., 2015).
We train Byte-Pair Encoding segmentation mod-
els (BPE) (Sennrich et al., 2016b) with a vocab
size of 50, 000 on both sides of the parallel cor-
pus for each experiment. We apply drop-out with
20% probability on the RNNs, and with 10% prob-
ability on the source and target words. We stop
training after convergence of cross-entropy on the
development set, and we average the 4 highest per-
forming models (as determined by development
set BLEU performance) to use as an ensemble
for decoding (checkpoint assembling). Training of

3marian-nmt.github.io

76each system takes 2–4 days on a single GPU (GTX
1080ti).

While we focus on RNN-based models with at-
tention as our NMT architecture, we note that dif-
ferent architectures have been proposed, including
based on convolutional neural networks (Kalch-
brenner and Blunsom, 2013; Gehring et al., 2017)
and the self-attention based Transformer model
(Vaswani et al., 2017).

5.2 Statistical Machine Translation

Our statistical machine translation systems are
trained using Moses (Koehn et al., 2007).4 We
build phrase-based systems using standard fea-
tures commonly used in recent system submis-
sions to WMT (Haddow et al., 2015; Ding et al.,
2016, 2017). We trained our systems with the
following settings: a maximum sentence length
of 80, grow-diag-ﬁnal-and symmetrization of
GIZA++ alignments, an interpolated Kneser-Ney
smoothed 5-gram language model with KenLM
(Heaﬁeld, 2011), hierarchical lexicalized reorder-
ing (Galley and Manning, 2008), a lexically-
driven 5-gram operation sequence model (OSM)
(Durrani et al., 2013), sparse domain indicator,
phrase length, and count bin features (Blunsom
and Osborne, 2008; Chiang et al., 2009), a max-
imum phrase-length of 5, compact phrase table
(Junczys-Dowmunt, 2012) minimum Bayes risk
decoding (Kumar and Byrne, 2004), cube prun-
ing (Huang and Chiang, 2007), with a stack-size
of 1000 during tuning. We optimize feature func-
tion weights with k-best MIRA (Cherry and Fos-
ter, 2012).

While we focus on phrase based systems as our
SMT paradigm, we note that there are other statis-
tical machine translation approaches such as hier-
archical phrase-based models (Chiang, 2007) and
syntax-based models (Galley et al., 2004, 2006)
that may have better performance in certain lan-
guage pairs and in low resource conditions.

5.3 Clean Corpus

In our experiments, we translate from German to
English. We use datasets from the shared trans-
lation task organized alongside the Conference
on Machine Translation (WMT)5 as clean train-
ing data.
For our baseline we use: Europarl

4statmt.org/moses
5statmt.org/wmt17/

(Koehn, 2005),6 News Commentary,7 and the
Rapid EU Press Release parallel corpus. The cor-
pus size is about 83 million tokens per language.
We use newstest2015 for tuning SMT systems,
newstest2016 as a development set for NMT
systems, and report results on newstest2017.

Note that we do not add monolingual data to
our systems since this would make our study more
complex. So, we always train our language model
on the target side of the parallel corpus for that ex-
periment. While using monolingual data for lan-
guage modelling is standard practice in statistical
machine translation, how to use such data for neu-
ral models is less obvious.

5.4 Noisy Corpora
For MISALIGNED SENTENCE and MISORDERED
WORD noise, we use the clean corpus (above) and
perturb the data. To create UNTRANSLATED SEN-
TENCE noise, we also use the clean corpus and
create pairs of identical sentences.

For WRONG LANGUAGE noise, we do not have
French–English and German–French data of the
same size. Hence, we use the EU Bookstore cor-
pus (Skadin¸ˇs et al., 2014).8

The SHORT SEGMENTS are extracted from
OPUS corpora (Tiedemann, 2009, 2012; Lison
and Tiedemann, 2016):9 EMEA (descriptions of
medicines),10 Tanzil (religious text),11 Open Sub-
titles 2016,12 Acquis (legislative text),13 GNOME
(software localization ﬁles),14 KDE (localization
ﬁles), PHP (technical manual),15 Ubuntu (local-
ization ﬁles),16 and Open Ofﬁce.17 We use only
pairs where both the English and German seg-
ments are at most 2 or 5 words long. Since this re-
sults in small data sets (2 million and 15 tokens per
language, respectively), they are duplicated multi-
ple times.

We also show the results for naturally occurring
noisy web data from the raw 2016 ParaCrawl cor-
pus (deduplicated raw set).18

6statmt.org/europarl
7casmacat.eu/corpus/news-commentary.html
8opus.nlpl.eu/EUbookshop.php
9opus.nlpl.eu
10emea.europa.eu
11tanzil.net/trans
12opensubtitles.org
13ec.europa.eu/jrc/en/language-technologies/jrc-acquis
14l10n.gnome.org
15se.php.net/download-docs
16translations.launchpad.net
17openofﬁce.org
18paracrawl.eu

77We sample the noisy corpus in an amount equal
to 5%, 10%, 20%, 50%, and 100% of the clean
corpus. This reﬂects the realistic situation where
there is a clean corpus, and one would like to add
additional data that has the potential to be noisy.
For each experiment, we use the target side of the
parallel corpus to train the SMT language model,
including the noisy text.

6

Impact on Translation Quality

Table 3 shows the effect of adding each type
of noise to the clean corpus.19 For some types
of noise NMT is harmed more than SMT: MIS-
MATCHED SENTENCES (up to -1.9 for NMT, -0.6
for SMT), MISORDERED WORDS (source) (-1.7
vs.
-0.3), WRONG LANGUAGE (target) (-2.2 vs.
-0.6).

SHORT

SEGMENTS,

UNTRANSLATED
SOURCE SENTENCES and WRONG SOURCE
LANGUAGE have little impact on either (at most
a degradation of -0.7). MISORDERED TARGET
WORDS decreases BLEU scores for both SMT
and NMT by just over 1 point (100% noise).

The most dramatic difference is UNTRANS-
LATED TARGET SENTENCE noise. When added at
5% of the original data, it degrades NMT perfor-
mance by 9.6 BLEU, from 27.2 to 17.6. Adding
this noise at 100% of the original data degrades
performance by 24.0 BLEU, dropping the score
from 27.2 to 3.2. In contrast, the SMT system only
drops 2.9 BLEU, from 24.0 to 21.1.

6.1 Copied output
Since the noise type where the target side is a copy
of the source has such a big impact, we examine
the system output in more detail.

We report the percent of sentences in the eval-
uation set that are identical to the source for the
UNTRANSLATED TARGET SENTENCE and RAW
CRAWL data in Figures 1 and 2 (solid bars). The
SMT systems output 0 or 1 sentences that are ex-
act copies. However, with just 20% of the UN-
TRANSLATED TARGET SENTENCE noise, 60% of
the NMT output sentences are identical to the
source.

This suggests that the NMT systems learn to
copy, which may be useful for named entities.
However, with even a small amount of this data
it is doing far more harm than good.

19We report case-sensitive detokenized BLEU (Papineni

et al., 2002) calculated using mteval-v13a.pl.

Figure 1: Copied sentences in the UNTRANS-
LATED (TARGET) experiments. NMT is the left
green bars, SMT is the right blue bars. Sentences
that are exact matches to the source are the solid
bars, sentences that are more similar to the source
than the target are the shaded bars.

Figure 2: Copied sentences in the RAW CRAWL ex-
periments. NMT is the left green bars, SMT is the
right blue bars. Sentences that are exact matches
to the source are the solid bars, sentences that are
more similar to the source than the target are the
shaded bars.

780%10%20%30%40%50%60%70%80%90%100%0%5%10%20%50%100%Percentage of Sentencesamount of noiseUɴᴛʀᴀɴsʟᴀᴛᴇᴅ Tᴀʀɢᴇᴛ Copying0%10%20%30%40%50%60%70%80%90%100%0%5%10%20%50%100%Percentage of Sentencesamount of noiseRᴀᴡ Cʀᴀᴡʟ CopyingMISALIGNED SENTENCES

MISORDERED WORDS

(SOURCE)

MISORDERED WORDS

(TARGET)

WRONG LANGUAGE
(FRENCH SOURCE)

WRONG LANGUAGE
(FRENCH TARGET)

UNTRANSLATED
(ENGLISH SOURCE)

UNTRANSLATED
(GERMAN TARGET)

5%

26.5 24.0
-0.7 -0.0

10%

26.5 24.0
-0.7 -0.0

26.9 24.0
-0.3 -0.0

26.6 23.6
-0.6 -0.4

20%

26.3 23.9
-0.9 -0.1

26.4 23.9
-0.8 -0.1

50%

26.1 23.9
-1.1 -0.1

26.6 23.6
-0.6 -0.4

27.0 24.0
-0.2 -0.0

26.8 24.0
-0.4 -0.0

26.4 23.4
-0.8 -0.6

26.7 23.2
-0.5 -0.8

26.9 24.0
-0.3 -0.0

26.8 23.9
-0.4 -0.1

26.8 23.9
-0.4 -0.1

26.8 23.9
-0.4 -0.1

26.7 24.0
-0.5 -0.0

26.6 23.9
-0.6 -0.1

26.7 23.8
-0.5 -0.2

27.2 23.9
-0.0 -0.1

27.0 23.9
-0.2 -0.1

26.7 23.6
-0.5 -0.4

17.6 23.8
-0.2

11.2 23.9
-0.1

5.6 23.8
-0.2

26.2 23.5
-1.0 -0.5

26.8 23.7
-0.4 -0.3

3.2 23.4
-0.6

100%

25.3 23.4
-1.9 -0.6
25.5 23.7
-1.7 -0.3
26.1 22.9

-1.1 -1.1

26.8 23.8
-0.4 -0.2

25.0 23.4
-0.6

-2.2

26.9 23.5
-0.3 -0.5

3.2 21.1

-2.9

SHORT SEGMENTS

(max 2)

SHORT SEGMENTS

(max 5)

RAW CRAWL DATA

-9.8

-16.0

-21.6

-24.0

-24.0

27.1 24.1
-0.1 +0.1

26.5 23.9
-0.7 -0.1

26.7 23.8
-0.5 -0.2

27.8 24.2
+0.6 +0.2

27.6 24.5
+0.4 +0.5

28.0 24.5
+0.8 +0.5

26.6 24.2
-0.6 +0.2

27.4 24.2
+0.2 +0.2

26.6 24.2
-0.6 +0.2

24.7 24.4
+0.4

-2.5

20.9 24.8
+0.8

17.3 25.2
+1.2

-6.3

-9.9

Table 3: Results from adding different amounts of noise (ratio of original clean corpus) for various types
of noise in German-English Translation. Generally neural machine translation (left green bars) is harmed
more than statistical machine translation (right blue bars). The worst type of noise are segments in the
source language copied untranslated into the target.

79(WMT 14 EN↔DE and EN↔FR), copies are
over-represented in the output of beam search. Us-
ing a subset of training data from WMT 17, they
replace a subset of the true translations with a copy
of the input. They analyze varying amounts of
copied noise, and a variety of beam sizes. Larger
beams are more effected by this kind of noise;
however, for all beam sizes performance degrades
completely with 50% copied sentences.20

Incorrect Language output

6.2
Another interesting case is when a German–
French corpus is added to a German–English cor-
pus (WRONG TARGET LANGUAGE). Both neural
and statistical machine translation are surprisingly
robust, even when these corpora are provided in
equal amounts.

We performed a manual analysis of the neu-
ral machine translation experiments. For the each
of the noise levels, we report the percentage of
NMT output sentences in French (out of of 3004:
5%: 0.20%, 10%: 0.60%, 20%: 1.7%, 50%:
3.3%, 100%: 6.7%. Most NMT output sentences
were either entirely French or English, with the
exception of a few mis-translated cognates (e.g.:
‘fac¸ade’, ‘accessibilit´e’).

In the SMT experiment with 100% noisy data
added,
there are a couple of French words in
mostly English sentences. These are much less
frequent
than unknown German words passed
through. Only 1 sentence is mostly French.

It is surprising that such a small percentage of
the output sentences were French, since up to half
of the target data in training was in French. We at-
tribute this to the domain of the added data differ-
ing from the test data. Source sentences in the test
set are more similar to the domain-relevant clean
parallel training corpus than the domain-divergent
noise corpus.

7 Conclusion

We deﬁned ﬁve types of noise in parallel data, mo-
tivated by a study of raw web crawl data. We
found that neural machine translation is less ro-
bust to many types of noise than statistical ma-
chine translation. In the most extreme case, when
the reference is an untranslated copy of the source
data, neural machine translation may learn to ex-
cessively copy the input. These ﬁndings should
inform future work on corpus cleaning.

20See Figure 3 in Ott et al. (2018).

Figure 3: Learning curves for the NMT UN-
TRANSLATED TARGET SENTENCE experiments.

Figures 1 and 2 show the percent of sentences
that have a worse TER score against the reference
than against the source (shaded bars). This means
that it would take fewer edits to transform the sen-
tence into the source than it would to transform it
into the target. When just 10% UNTRANSLATED
TARGET SENTENCE data is added, 57% of the sen-
tences are more similar to the source than to the
reference, indicating partial copying.

This suggests that the NMT system is overﬁt-
ting the copied portion of the training corpus. This
is supported by Figure 3, which shows the learning
curve on the development set for the UNTRANS-
LATED TARGET SENTENCE noise setup. The per-
formance for the systems trained on noisy corpora
begin to improve, before over-ﬁtting to the copy
portion of the training set. Note that we plot the
BLEU performance on the development set with
beam search, while the system is optimizing cross-
entropy given a perfect preﬁx.

Other work has also considered copying in
NMT. Currey et al. (2017) add copied data and
back-translated data to a clean parallel corpus.
They report improvements on EN ↔ RO when
adding as much back-translated and copied data
as they have parallel (1:1:1 ratio). For EN↔TR
and EN↔DE, they add twice as much back trans-
lated and copied data as parallel data (1:2:2 ra-
tio), and report improvements on EN↔TR but
not on EN↔DE. However, their EN↔DE sys-
tems trained with the copied corpus did not per-
form worse than baseline systems. Ott et al.
(2018) found that while copied training sentences
represent less than 2.0% of their training data

800102001020304050BLEUIterations (thousands)Uɴᴛʀᴀɴsʟᴀᴛᴇᴅ Tᴀʀɢᴇᴛ Learning Curve0%5%10%20%50%100%Acknowledgements

This work has been partially supported by the
DARPA LORELEI and the IARPA MATERIAL
programs.
It also beneﬁted from the Paracrawl
web crawling project which is funded by a Google
faculty grant and the European Union (CEF).

References
Alexandra Antonova and Alexey Misyurev. 2011.
Building a web-based parallel corpus and ﬁl-
In Pro-
tering out machine-translated text.
ceedings of
the 4th Workshop on Building and
Using Comparable Corpora: Comparable Cor-
pora and the Web. Association for Computational
Linguistics, Portland, Oregon, pages 136–144.
http://www.aclweb.org/anthology/W11-1218.

Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
In Proceedings of the 2011 Con-
data selection.
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Edinburgh, Scotland, UK., pages 355–362.
http://www.aclweb.org/anthology/D11-1033.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
In ICLR.

Bengio. 2015.
jointly learning to align and translate.
http://arxiv.org/pdf/1409.0473v6.pdf.

Eduard Barbu, Carla Parra Escart´ın, Luisa Ben-
tivogli, Matteo Negri, Marco Turchi, Constantin
Orasan, and Marcello Federico. 2016.
The
ﬁrst
cleaning
shared task. Machine Translation 30(3):145–166.
https://doi.org/10.1007/s10590-016-9183-x.

translation memory

automatic

Yonatan Belinkov and Yonatan Bisk. 2017.

Syn-
and natural noise both break neural
CoRR abs/1711.02173.

thetic
machine translation.
http://arxiv.org/abs/1711.02173.

Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Honolulu, Hawaii, pages 215–
223. http://www.aclweb.org/anthology/D08-1023.

Ondrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Logacheva, Christof Monz, Matteo Negri, Matt
Post, Raphael Rubino, Lucia Specia, and Marco
Findings of the 2017 conference
Turchi. 2017.
In Proceedings
on machine translation (WMT17).
of the Second Conference on Machine Translation.
Association for Computational Linguistics, Copen-
hagen, Denmark, pages 169–214.

Detecting cross-lingual

Marine Carpuat, Yogarshi Vyas, and Xing Niu.
semantic di-
2017.
In
vergence for neural machine translation.
Proceedings of
the First Workshop on Neu-
ral Machine Translation. Association for Com-
putational Linguistics, Vancouver, pages 69–79.
http://www.aclweb.org/anthology/W17-3209.

In Proceedings of

Colin Cherry and George Foster. 2012.

Batch
tuning strategies for statistical machine transla-
the 2012 Conference
tion.
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, Montr´eal, Canada, pages 427–436.
http://www.aclweb.org/anthology/N12-1047.

David Chiang. 2007.

Hierarchical phrase-based
Computational Linguistics 33(2).

translation.
http://www.aclweb.org/anthology-new/J/J07/J07-
2003.pdf.

David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
In Proceedings of Human Language Tech-
lation.
nologies: The 2009 Annual Conference of the North
American Chapter of
the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Boulder, Colorado, pages 218–
226. http://www.aclweb.org/anthology/N/N09/N09-
1025.

Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li,
Bilingual data clean-
and Ming Zhou. 2013.
In
ing for smt using graph-based random walk.
Proceedings of
the
the 51st Annual Meeting of
Association for Computational Linguistics (Vol-
ume 2: Short Papers). Association for Computa-
tional Linguistics, Soﬁa, Bulgaria, pages 340–345.
http://www.aclweb.org/anthology/P13-2061.

Anna Currey, Antonio Valerio Miceli Barone, and
Copied monolingual
Kenneth Heaﬁeld. 2017.
data improves low-resource neural machine trans-
the Second Confer-
lation.
ence on Machine Translation, Volume 1: Re-
search Paper. Association for Computational Lin-
guistics, Copenhagen, Denmark, pages 148–156.
http://www.aclweb.org/anthology/W17-4715.

In Proceedings of

Shuoyang Ding, Kevin Duh, Huda Khayrallah,
The jhu
Philipp Koehn, and Matt Post. 2016.
In
machine translation systems for wmt 2016.
Proceedings of
the First Conference on Ma-
chine Translation. Association for Computational
Linguistics, Berlin, Germany, pages 272–280.
http://www.aclweb.org/anthology/W/W16/W16-
2310.

Shuoyang Ding, Huda Khayrallah, Philipp Koehn,
Matt Post, Gaurav Kumar, and Kevin Duh. 2017.
The jhu machine translation systems for wmt
the Second Confer-
2017.
ence on Machine Translation, Volume 2: Shared

In Proceedings of

81Task Papers. Association for Computational Lin-
guistics, Copenhagen, Denmark, pages 276–282.
http://www.aclweb.org/anthology/W17-4724.

Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013.
Can
Markov Models Over Minimal Translation Units
In Proceedings of the
Help Phrase-Based SMT?
51st Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Soﬁa, Bulgaria.

Marzieh Fadaee, Arianna Bisazza, and Christof Monz.
2017. Data augmentation for low-resource neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Association
for Computational Linguistics, Vancouver, Canada,
pages 567–573.
http://aclweb.org/anthology/P17-
2090.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006.
Scalable inference and train-
ing of context-rich syntactic translation models.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of
the Association for Computa-
tional Linguistics. Association for Computational
Linguistics, Sydney, Australia, pages 961–968.
http://www.aclweb.org/anthology/P/P06/P06-1121.

Kenlm:

Kenneth Heaﬁeld. 2011.

Faster and
In Proceed-
smaller language model queries.
ings of
the Sixth Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Edinburgh, Scotland, pages 187–197.
http://www.aclweb.org/anthology/W11-2123.

Liang Huang and David Chiang. 2007.

Forest
rescoring: Faster decoding with integrated lan-
In Proceedings of the 45th An-
guage models.
nual Meeting of the Association of Computational
Linguistics. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 144–151.
http://www.aclweb.org/anthology/P/P07/P07-1019.

M. Junczys-Dowmunt. 2012. A phrase table without
phrases: Rank encoding for better phrase table com-
pression. In Mauro Cettolo, Marcello Federico, Lu-
cia Specia, and Andy Way, editors, Proceedings of
th 16th International Conference of the European
Association for Machine Translation (EAMT). pages
245–252. http://www.mt-archive.info/EAMT-2012-
Junczys-Dowmunt.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, Andr´e F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. arXiv preprint
arXiv:1804.00344 https://arxiv.org/abs/1804.00344.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of
the Joint Conference on Hu-
man Language Technologies and the Annual Meet-
ing of the North American Chapter of the Associ-
ation of Computational Linguistics (HLT-NAACL).
http://www.aclweb.org/anthology/N04-1035.pdf.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
In Proceedings of
continuous translation models.
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Seattle, Washington, USA, pages
1700–1709. http://www.aclweb.org/anthology/D13-
1176.

Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
the 2008 Con-
ing model.
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Honolulu, Hawaii, pages 848–856.
http://www.aclweb.org/anthology/D08-1089.

In Proceedings of

Jonas Gehring, Michael Auli, David Grangier, and
Yann Dauphin. 2017. A convolutional encoder
In Pro-
model for neural machine translation.
ceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Vancouver, Canada, pages 123–135.
http://aclweb.org/anthology/P17-1012.

Barry Haddow, Matthias Huck, Alexandra Birch,
Nikolay Bogoychev, and Philipp Koehn. 2015.
The edinburgh/jhu phrase-based machine trans-
In Proceed-
lation systems for wmt 2015.
the Tenth Workshop on Statistical Ma-
ings of
chine Translation. Association for Computational
Linguistics, Lisbon, Portugal, pages 126–133.
http://aclweb.org/anthology/W15-3013.

Philipp Koehn. 2005. Europarl: A parallel corpus for
In Proceedings of
statistical machine translation.
the Tenth Machine Translation Summit (MT Summit
X). Phuket, Thailand. http://mt-archive.info/MTS-
2005-Koehn.pdf.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondˇrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P/P07/P07-2045.

Six
Philipp Koehn and Rebecca Knowles. 2017.
In
challenges for neural machine translation.
the First Workshop on Neu-
Proceedings of
ral Machine Translation. Association for Com-
putational Linguistics, Vancouver, pages 28–39.
http://www.aclweb.org/anthology/W17-3204.

82J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces.
In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing, John Benjamins,
Amsterdam/Philadelphia, Borovets, Bulgaria, vol-
ume V.

J¨org Tiedemann. 2012. Parallel Data, Tools and Inter-
faces in OPUS. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation.

Marlies van der Wees, Arianna Bisazza, and Christof
Dynamic data selection for
Monz. 2017.
In Proceedings
neural machine translation.
of
the 2017 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1411–1421.
http://aclweb.org/anthology/D17-1148.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser,
Atten-
CoRR abs/1706.03762.
tion is all you need.
http://arxiv.org/abs/1706.03762.

and Illia Polosukhin. 2017.

Ashish Venugopal, Jakob Uszkoreit, David Talbot,
Franz Och, and Juri Ganitkevitch. 2011. Wa-
termarking the outputs of structured prediction
with an application in statistical machine trans-
In Proceedings of the 2011 Conference
lation.
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguis-
tics, Edinburgh, Scotland, UK., pages 1363–1372.
http://www.aclweb.org/anthology/D11-1126.

Hainan Xu and Philipp Koehn. 2017.

Zippo-
rah: a fast and scalable data cleaning system for
In Proceed-
noisy web-crawled parallel corpora.
ings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 2935–2940.
http://aclweb.org/anthology/D17-1318.

Shankar Kumar and William J. Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In HLT-NAACL. pages 169–176.

Pierre Lison and J¨org Tiedemann. 2016. Opensub-
titles2016: Extracting large parallel corpora from
movie and tv subtitles. In Proceedings of the 10th
International Conference on Language Resources
and Evaluation (LREC 2016).

Myle Ott, Michael Auli, David Grangier,

Marc’Aurelio Ranzato. 2018.
certainty in neural machine translation.
abs/1803.00047. http://arxiv.org/abs/1803.00047.

and
Analyzing un-
CoRR

Bleu:

Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002.
a method
for automatic evaluation of machine transla-
In Proceedings of 40th Annual Meeting
tion.
of
the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Spencer Rarrick, Chris Quirk, and Will Lewis. 2011.
MT detection in web-scraped parallel corpora.
In Proceedings of the 13th Machine Translation
Summit (MT Summit XIII). International Associ-
ation for Machine Translation, pages 422–430.
http://www.mt-archive.info/MTS-2011-Rarrick.pdf.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation
2016a.
In Proceed-
models with monolingual data.
ings of
the As-
sociation for Computational Linguistics (Volume
1:
Long Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 86–96.
http://www.aclweb.org/anthology/P16-1009.

the 54th Annual Meeting of

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Neural Machine Translation of Rare
2016b.
In Proceedings
Words with Subword Units.
of
the Associa-
tion for Computational Linguistics. Association
for Computational Linguistics, Berlin, Germany.
http://www.aclweb.org/anthology/P16-1162.

the 54th Annual Meeting of

Raivis Skadin¸ˇs, J¨org Tiedemann, Roberts Rozis, and
Daiga Deksne. 2014. Billions of parallel words for
free: Building and using the eu bookshop corpus.
In Proceedings of the 9th International Conference
on Language Resources and Evaluation (LREC-
2014). European Language Resources Association
(ELRA), Reykjavik, Iceland.

Kaveh Taghipour, Shahram Khadivi, and Jia Xu. 2011.
Parallel corpus reﬁnement as an outlier detection
In Proceedings of the 13th Machine
algorithm.
Translation Summit (MT Summit XIII). Interna-
tional Association for Machine Translation, pages
414–421.
http://www.mt-archive.info/MTS-2011-
Taghipour.pdf.

83
Sentiment Analysis using Imperfect Views from Spoken Language and

Acoustic Modalities

Imran Sheikh, Sri Harsha Dumpala, Rupayan Chakraborty, Sunil Kumar Kopparapu

{imran.as, d.harsha, rupayan.chakraborty, sunilkumar.kopparapu}@tcs.com

TCS Research and Innovations-Mumbai, INDIA

Abstract

Multimodal sentiment classiﬁcation in
practical applications may have to rely on
erroneous and imperfect views, namely (a)
language transcription from a speech rec-
ognizer and (b) under-performing acoustic
views. This work focuses on improving
the representations of these views by per-
forming a deep canonical correlation anal-
ysis with the representations of the bet-
ter performing manual transcription view.
Enhanced representations of the imper-
fect views can be obtained even in ab-
sence of the perfect views and give an
improved performance during test condi-
tions. Evaluations on the CMU-MOSI and
CMU-MOSEI datasets demonstrate the ef-
fectiveness of the proposed approach.

Introduction

1
Use of multimodal cues is especially useful for
analyzing sentiment in audio-visual data like opin-
ion videos on social media websites, call-center
audio recordings etc. The different modalities, viz.
language (spoken words), acoustic (speech) and
visual (facial and gestures), can carry a different
view of the same information like for example,
sentiment. While the representations/features ex-
tracted from these individual different views add
richness to the sentiment classiﬁcation,
the in-
tra and inter view-interactions play an important
role in better sentiment classiﬁcation (Zadeh et al.,
2017; Chen et al., 2018; Rajagopalan et al., 2016;
Nojavanasghari et al., 2016; Xu et al., 2013).

Although fusion of multi-view information is
being extensively explored, the challenges asso-
ciated with the presence of noise and irregulari-
ties in a view has received very less attention. For
instance, multimodal sentiment classiﬁcation sys-

tems have typically used manual, and hence, error
free language transcriptions and exploited the in-
teraction of other views with this noise free lan-
guage view (Zadeh et al., 2018, 2017). However,
a practical system will have to rely on a language
transcription from an Automatic Speech Recogni-
tion (ASR) engine, which is inherently prone to er-
rors due to ambient/channel noises in acoustic en-
vironments (Gong, 1995; Li et al., 2014), language
domain mismatch, emotion in speech (Athanaselis
et al., 2005), etc. Similarly, existing and pop-
ularly used representations of the acoustic view
have generally under-performed compared to the
language view (Poria et al., 2017; Zadeh et al.,
2018; P´erez-Rosas et al., 2013), indicating that
the acoustic view or its representations, by them-
selves, may not be discriminative enough for ro-
bust sentiment classiﬁcation.

Assuming the ASR (language transcription) and
acoustic views as imperfect views, the focus of this
work is on improving the representations of these
noisy views,
riding on the representations of the
better performing view. We show that the repre-
sentations obtained from automatic transcriptions
of spoken language and those from the acoustic
views can be enhanced using corresponding rep-
resentations from manual transcriptions of spoken
language. Enhanced representations of the imper-
fect views can be obtained even in absence of the
perfect views during test conditions. Deep canon-
ical correlation analysis (DCCA) (Andrew et al.,
2013) is used to improve the representations of
the imperfect views. The rest of the paper is or-
ganized as follows. Section 2 describes a method
to improve imperfect or erroneous views. Section
3 presents the different components in our mul-
timodal sentiment classiﬁcation system. Exper-
iments are discussed in Section 4 followed by a
discussion on results and conclusion in Section 5.

ProceedingsoftheFirstGrandChallengeandWorkshoponHumanMultimodalLanguage(Challenge-HML),pages35–39,Melbourne,AustraliaJuly20,2018.c(cid:13)2018AssociationforComputationalLinguistics352

Improving representations of spoken
language and acoustic views

Multimodal sentiment classiﬁcation works have
mainly relied on the manual transcription of the
spoken utterances. In a practical and real life sce-
nario, the text transcriptions are not readily avail-
able and are required to be obtained from an ASR
engine. While ASR systems have seen large im-
provements with the use of deep learning methods,
their performance is impacted by mismatched
train-test conditions. As a result, practical mul-
timodal sentiment classiﬁcation systems will have
to rely on imperfect spoken language views.

On the other hand, acoustic views used by
multimodal sentiment classiﬁcation systems have
shown poor performance compared to that of the
language view. This might indicate that either the
acoustic view or its utterance level audio repre-
sentations are not discriminative enough for senti-
ment classiﬁcation. Recent classiﬁcation models
capture interactions across view/modality and pro-
duce better sentiment classiﬁcation results (Zadeh
et al., 2018, 2017). In contrast to this, our work
focuses on improving representations of the im-
perfect views using representations of the better
performing view. Utterance level representations
obtained from ASR view and the acoustic view
are improved using the representations extracted
from manual transcriptions of spoken language.
These representation improvements are achieved
using DCCA (Andrew et al., 2013).

2.1 Deep canonical correlation analysis
Given the representations of two different views
of the same signal, DCCA learns a pair of non-
linear transformations such that the transformed
representations for the two views are maximally
correlated. The individual transformed represen-
tations from a DCCA model have been shown to
capture information from both the views and as a
result outperform the original individual represen-
tations (Andrew et al., 2013; Wang et al., 2015;
Shao et al., 2015). Figure 1 shows a high level
block representation of DCCA. (fv1, fv2) are
representations of
two views of the same input
data, nonlinear transformations are carried out
using DNNs and canonical correlations are com-
puted on the DNN transformed representations
( ˆfv1, ˆfv2). During training, representations for the
two views are extracted from the train set and
used to train the DNN’s such that canonical corre-

Figure 1: Improving views using DCCA.

lation between the transformed representations is
maximized. Thus, the goal is to learn parameters
W ∗
1 , W ∗
2 for DN NV iew1, DN NV iew2, such that:
1 , W ∗
(W ∗

corr(g1(fv1; W1),

2 ) = argmax
W1,W2

g2(fv2; W2))
ˆfv1 = g1(fv1; W1) , ˆfv2 = g2(fv2; W2)

where, g1, g2 denote the nonlinear transforma-
tions of DN NV iew1 and DN NV iew2 respectively.
Once the DNNs are trained they are used to obtain
the transformed or enhanced representations.

3 Sentiment classiﬁcation using language

and acoustic views

This section describes our complete system for
sentiment classiﬁcation which uses language and
acoustic views. We ﬁrst discuss the views and
their representations and then describe the method
adopted to fuse and classify these representations.

3.1 Spoken language views & representations
3.1.1 Manual and ASR views
A typical view of the spoken language modality is
the word level manual transcription of the spoken
utterances. However, in a realistic scenario man-
ual transcriptions are not available and the system
has to rely on automatic transcriptions of the spo-
ken language. Therefore, we consider the auto-
matic transcriptions from a general purpose ASR
engine as a practical spoken language view.

To obtain the ASR view, we use the public do-
main Kaldi ASR toolkit (Povey et. al., 2011) along
with the ASpIRE Chain acoustic models (Peddinti
et al., 2015; Povey, 2017). The accompanying pre-
trained language model is used as it is. When eval-
uated on the 2199 speech utterances in the CMU-
MOSI dataset (Zadeh et al., 2016), this ASR setup
gives a mean word error rate of 49.2% (with a
standard deviation of 32.0).
Its performance in
terms of correctly recognized words is 66.8%.

363.1.2 CNN based representation
Representations for the spoken language views are
obtained using a text convolutional neural network
(CNN) (Kim, 2014). Each utterance is represented
as the concatenation of 300-dimensional GloVe
embeddings (Pennington et al., 2014). Then 1-
dimensional convolution kernels are applied to the
concatenated word embeddings. The CNN has
two convolutional layers, with the ﬁrst layer hav-
ing two kernels of size 3 and 4 with 50 feature
maps each and the second layer having a kernel of
size 2 with 100 feature maps. Each convolutional
layer was followed by a 2 × 2 max pooling layer.
A fully connected layer transforms the CNN ex-
tracted features into a 300-dimensional vector.

3.2 Representation of acoustic view
As a representation of the acoustic view, we ex-
tract a large set of high level descriptors (HLDs)
from low level audio descriptors (LLDs) like voice
probability, MFCCs, pitch, RMS energies and
their delta regression coefﬁcients. Since the HLDs
are (up to fourth order) statistics of LLDs extracted
over smaller (20 ms) frames, the dimension of the
acoustic features remain same (i.e. 384) for all ut-
terances. We used the IS09 conﬁguration from
the openSMILE toolkit (Eyben et al., 2009).

3.3 Fusion and sentiment classiﬁcation
Bi-modal representations for utterance level senti-
ment classiﬁcation are obtained by ﬁrst extracting
the representations of (manually transcribed and
ASR) spoken language views and those for the
acoustic view, as discussed in Section 3.1. Then
representations of automatically transcribed spo-
ken language view and those for the acoustic view
are improved using DCCA, as discussed in Sec-
tion 2.1. Finally the improved representations are
concatenated to obtain a bi-modal representation.
We use a bi-directional LSTM-RNN to label
utterance level sentiments based on the bi-modal
representations. Sequence labeling with LSTM-
RNNs can account for contextual
information
from adjacent inputs as well as the overall input
sequence and has been shown to perform better
on several tasks (Graves and Schmidhuber, 2005;
Graves et al., 2008; Poria et al., 2017; Sheikh et al.,
2017). Let us denote the bi-modal representations
as (x1, ...xt−1, xt, xt+1..., xN ), where xt repre-
sents the current utterance and N is the number of
utterances in a video. We followed the hierarchical

(cid:80)

training discussed in (Poria et al., 2017). Each bi-
modal representation (xt) is input to the forward
and backward LSTM-RNNs to obtain the hidden
layer activations hF
t . These concatenated
activations (ct) are fed to softmax classiﬁer,

t and hB

pt(i) =

exp(cti.WC + bC)
exp(ctj.WC + bC)

(1)

j

where pt(i) denotes the posterior probability of
output class i for utterance at t; WC and bC are
weight and bias parameters of the softmax layer.
4 Experiments and results
4.1 Datasets
results and analysis on two
We present our
datasets, namely, (a) CMU-MOSI (Zadeh et al.,
2016) and (b) CMU-MOSEI (Zadeh, 2018a).
CMU-MOSI consists of 93 movie related opin-
ion videos from YouTube, segmented into 2199
clips/utterances.
CMU-MOSEI consists of
about 2500 multi-domain monologue videos from
YouTube, segmented into 23, 500 clips/utterances.
Both CMU-MOSI and CMU-MOSEI datasets
are annotated with utterance level sentiment la-
bels in the range [−3, 3]. We focus on binary sen-
timent classiﬁcation in which labels [−3, 0] are
considered as negative and [1, 3] are considered
as positive sentiments. For CMU-MOSI, we used
the train, validation and test split provided by the
CMU Multimodal Data SDK (Zadeh, 2018b). The
SDK also provides a train, validation and test split
for CMU-MOSEI. However, as the test set labels
were not available at the time of submission of
this paper, we treated 200 videos from the original
validation set as our test set. The remaining 100
videos from the original validation set and an ad-
ditional 150 videos from the original train set are
considered as our validation set.

4.2 Experiment setup
We evaluate the performance of the spoken lan-
guage and acoustic views,
individually and in
combination. The manual and ASR transcrip-
tions of the language view are denoted as MT and
AT, respectively. The acoustic view is denoted as
AU. Enhanced (representations of) ASR view and
acoustic view are denoted as AT↑ and AU↑, respec-
tively. They were enhanced using (representations
of) the manual transcription view, using the DCCA
model described in Section 2.1. Our DCCA mod-
els use DNNs with 3 hidden layers and sigmoids.

374.3 Sentiment classiﬁcation results
Table 1 presents the % accuracy (Acc.) and F-
score (F1) for binary sentiment classiﬁcation on
the CMU-MOSI and CMU-MOSEI datasets. The
results are divided into four sections, viz. (I) the
‘ideal’ baseline results achieved by the LSTM-
RNN classiﬁer on the manual transcription and
acoustic views, (II) the ‘practical’ baseline results
achieved with the imperfect ASR view, (III) the
results obtained, for the practical scenario, by the
proposed approach with DCCA enhanced views
and (IV) the improvement on using DCCA en-
hanced acoustic view with manual transcriptions.
Table 1: Sentiment classiﬁcation performance us-
ing a bi-directional LSTM-RNN classiﬁer.

AU
MT

MT+AU

AT

AT+AU

AU↑
AT↑

I

II

III

MOSI

MOSEI
F1
58.0
68.6
68.7
67.5
67.9
59.3
68.7
69.0
69.3

F1 Acc.
59.4
50.0
73.1
68.7
68.7
71.0
68.0
68.6
68.1
69.2
51.1
58.9
68.8
69.7
69.1
70.7
74.1
69.4

Acc.
50.6
73.5
71.4
69.1
69.4
51.6
70.2
70.9
74.6

AT↑+AU↑
IV MT+AU↑
5 Discussion
5.1 Performance of ASR view (AT)
Comparison of MT and AT views in sections I and
II of Table 1 shows that the AT view degrades the
classiﬁcation performance Accuracy and F-score
reduce by 4.4% and 4.5% absolute for CMU-
MOSI and by 0.6% and 0.8% absolute for CMU-
MOSEI1. Similarly, degradations are also present
in the bimodal setup (MT+AU vs AT+AU).
5.2 Performance of acoustic view (AU)
The acoustic view (AU) in itself gives a poor per-
formance for CMU-MOSI and a relatively better
performance for CMU-MOSEI. However, when
fused along with the language views (MT or AT),
it results in small or no improvement and some-
times a degradation. This indicates that the raw
acoustic views or its existing representations may
not always contribute for sentiment classiﬁcation,
due to the existence of encoded and decoded sen-
timents as discussed in (Chakraborty et al., 2018).
1We found that manual transcriptions of several utterances
in the CMU-MOSEI dataset are unreliable and hence its per-
formance of MT would be higher than that obtained.

Improvements with DCCA

5.3
As discussed above, the ASR and acoustic views
(AT and AU) reduced the classiﬁcation scores.
Section III of Table 1 shows that our approach
to enhance the imperfect views using DCCA can
lead to signiﬁcant improvements. ASR view (AT
vs AT↑) F-scores improve by 1.1% (CMU-MOSI)
and 1.2% (CMU-MOSEI) absolute. Acoustic
view (AU vs AU↑) F-scores improve by 1.1%
(CMU-MOSI) and 1.3% (CMU-MOSEI) abso-
lute. F-scores for the bimodal system with ASR
view (AT+AU vs AT↑+AU↑) improve by 1.5%
(CMU-MOSI) and 1.1% (CMU-MOSEI) abso-
lute. Bimodal system with manual transcription
and DCCA enhanced acoustic view (MT+AU vs
MT+AU↑) also shows F-score improvements, of
3.1% (CMU-MOSI) and 0.6% (CMU-MOSEI).
5.4 ASR view improvements with non

contextual classiﬁer

As discussed in (Poria et al., 2017),
the bi-
directional LSTM-RNN exploits contextual infor-
mation from the adjacent utterances and the entire
video.
In order to obtain the improvements due
to DCCA alone we evaluated the performances of
MT, AT and AT↑ with a non contextual classiﬁer.
We trained logistic regression models which clas-
sify the utterance level CNN representations inde-
pendently into positive and negative sentiments.
Table 2 reports the resulting % accuracies. ASR
view (AT vs AT↑) accuracies improve by 1.4% and
1.9% absolute due to DCCA.
Table 2: Improvement in ASR view accuracy us-
ing a non contextual classiﬁer.

MOSI MOSEI
71.1
63.7
65.1

67.5
63.8
65.7

MT
AT
AT↑

6 Conclusion
Erroneous ASR views and weak acoustic views of
videos can degrade sentiment classiﬁcation perfor-
mance in practical scenarios. We observed degra-
dations (up to 4.5% absolute) in F-score on stan-
dard CMU-MOSI dataset, using a popular ASR
setup and an utterance level contextual LSTM-
RNN classiﬁer The effect could be more severe on
multimodal systems relying on word level fusion.
Our approach to improve the imperfect views us-
ing canonical correlation analysis shows signiﬁ-
cant improvements (up to 3.1% absolute).

38References
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen
Livescu. 2013. Deep canonical correlation analysis.
In Proceedings of the International Conference on
Machine Learning, volume 28, pages 1247–1255.

T. Athanaselis, S. Bakamidis, I. Dologlou, R. Cowie,
E. Douglas-Cowie, and C. Cox. 2005. Asr for emo-
tional speech: Clarifying the issues and enhancing
performance. Neural Netw., 18(4):437–444.

Rupayan Chakraborty, Meghna Pandharipande, and
Sunil Kumar Kopparapu. 2018. Analyzing Emotion
in Spontaneous Speech, 1st edition. Springer Pub-
lishing Company, Incorporated.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-
trusaitis, Amir Zadeh, and Louis-Philippe Morency.
2018. Multimodal sentiment analysis with word-
level fusion and reinforcement learning. CoRR,
abs/1802.00924.

Florian Eyben, Felix Weninger, Martin Woellmer,
and Bjoern Schuller. 2009.
openSMILE.
http://www.audeering.com/research/
opensmile. Accessed: 2017.

Yifan Gong. 1995. Speech recognition in noisy envi-
ronments: A survey. Speech Commun., 16(3):261–
291.

A. Graves and J. Schmidhuber. 2005.

Framewise
phoneme classiﬁcation with bidirectional lstm net-
works. In Proceedings of International Joint Con-
ference on Neural Networks, pages 2047–2052.

Alex Graves, Marcus Liwicki, Horst Bunke, J¨urgen
Schmidhuber, and Santiago Fern´andez. 2008. Un-
constrained on-line handwriting recognition with re-
current neural networks. In Advances in Neural In-
formation Processing Systems 20, pages 577–584.

Yoon Kim. 2014. Convolutional neural networks for
In EMNLP, pages 1746–

sentence classiﬁcation.
1751.

J. Li, L. Deng, Y. Gong, and R. Haeb-Umbach.
An overview of noise-robust automatic
2014.
IEEE Transactions on Audio,
speech recognition.
Speech, and Language Processing, 22(4):745–777.

Behnaz Nojavanasghari, Deepak Gopinath, Jayanth
Koushik, Tadas Baltruˇsaitis, and Louis-Philippe
Morency. 2016. Deep multimodal fusion for per-
suasiveness prediction. In ICMI, pages 284–288.

V. Peddinti, G. Chen, V. Manohar, T. Ko, D. Povey,
and S. Khudanpur. 2015. Jhu aspire system: Robust
lvcsr with tdnns, ivector adaptation and rnn-lms. In
2015 IEEE Workshop on Automatic Speech Recog-
nition and Understanding (ASRU), pages 539–546.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Ver´onica P´erez-Rosas, Rada Mihalcea, and Louis
Philippe Morency. 2013. Utterance-level multi-
In Proceedings of the
modal sentiment analysis.
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 973–982.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017. Context-dependent sentiment anal-
ysis in user-generated videos. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics, pages 873–883.
Daniel Povey. 2017. Kaldi models.

http://kaldi-

asr.org/models.html.

Daniel Povey et. al. 2011. The kaldi speech recogni-
tion toolkit. In IEEE Workshop on Automatic Speech
Recognition and Understanding.

Shyam Sundar Rajagopalan, Louis-Philippe Morency,
Tadas Baltrusaitis, and Roland Goecke. 2016. Ex-
tending long short-term memory for multi-view
In Computer Vision – ECCV
structured learning.
2016, pages 338–353.

J. Shao, Z. Zhao, F. Su, and T. Yue. 2015. 3view
deep canonical correlation analysis for cross-modal
retrieval. In 2015 Visual Communications and Im-
age Processing (VCIP), pages 1–4.

I. Sheikh, D. Fohr, and I. Illina. 2017. Topic segmen-
tation in asr transcripts using bidirectional rnns for
change detection. In IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 512–
518.

W. Wang, R. Arora, K. Livescu, and J. A. Bilmes. 2015.
Unsupervised learning of acoustic features via deep
In ICASSP, pages
canonical correlation analysis.
4590–4594.

Chang Xu, Dacheng Tao, and Chao Xu. 2013. A sur-
vey on multi-view learning. CoRR, abs/1304.5634.
CMU-MOSEI dataset.

Amir Zadeh. 2018a.

http://multicomp.cs.cmu.edu/
resources/cmu-mosei-dataset/.
cessed: 2018.

Ac-

Amir Zadeh. 2018b.

CMU Multimodal Data
https://github.com/A2Zadeh/

SDK.
CMU-MultimodalDataSDK. Accessed: 2018.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. CoRR, abs/1707.07250.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018. Memory fusion network for multi-
view sequential learning. CoRR, abs/1802.00927.

Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-
Philippe Morency. 2016. MOSI: multimodal cor-
pus of sentiment intensity and subjectivity analysis
in online opinion videos. CoRR, abs/1606.06259.

39
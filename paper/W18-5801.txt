Efﬁcient Computation of Implicational Universals in Constraint-Based

Phonology Through the Hyperplane Separation Theorem

Giorgio Magri
CNRS, SFL, UPL

magrigrg@gmail.com

Abstract

This paper focuses on the most basic im-
plicational universals in phonological
the-
ory, called T-orders after Anttila and Andrus
(2006).
It develops necessary and sufﬁcient
constraint characterizations of T-orders within
Harmonic Grammar and Optimality Theory.
These conditions rest on the rich convex ge-
ometry underlying these frameworks. They
are phonologically intuitive and have signiﬁ-
cant algorithmic implications.

1

Introduction

A typology T is a collection of grammars
G1, G2, . . . For instance, T could be the set of
syntactic grammars corresponding to all possi-
ble combinations of values of a set of parame-
ters (Chomsky, 1981). Or the set of phonological
grammars corresponding to all possible orderings
of an underlying set of phonological rules (Chom-
sky and Halle 1968). Or the set of grammars cor-
responding to all rankings of an underlying con-
straint set (Prince and Smolensky, 2004).

The structure induced by a typology T can be
investigated though its implicational universals of
the form (1). This implication holds provided ev-
ery grammar in the typology T that satisﬁes the
antecedent property P also satisﬁes the conse-

quent property (cid:98)P (Greenberg 1963).

P T−→ (cid:98)P

(1)

To illustrate, suppose that T is the typology of syn-
tactic grammars. Consider the antecedent property
P of having VSO as the basic word order. And

the consequent property (cid:98)P of having prepositions

(as opposed to postpositions). In this case, (1) is
Greenberg’s implicational universal #3.

In this paper, we are interested in typologies
of phonological grammars. We assume a rep-

resentational framework which distinguishes be-
tween two representational levels: underlying rep-

resentations (URs), denoted as x,(cid:98)x, . . . ; and sur-
face representations (SRs), denoted as y,(cid:98)y, . . . or
z,(cid:98)z, . . . . A phonological grammar G is a func-

tion which takes a UR x and returns a SR y. For
instance, the phonology of German maps the UR
x = /bE:d/ to the SR y = [bE:t] (‘bath’). A phono-
logical typology T is a collection of phonological
grammars G1, G2, . . . that we assume are all de-
ﬁned over the same set of URs (Richness of the
Base assumption; Prince and Smolensky 2004).

Since phonological grammars are functions
from URs to SRs, the most basic or atomic an-
tecedent property P of an implicational universal
(1) is the property of mapping a certain UR x to
a certain SR y. Analogously, the most basic con-

sequent property (cid:98)P is the property of mapping a
certain UR(cid:98)x to a certain SR(cid:98)y. We thus focus

on implicational universals of the form (2). This
implication holds provided every grammar in the
typology T that succeeds on the antecedent map-
ping (i.e., it maps the antecedent UR x to the an-
tecedent SR y), also succeeds on the consequent

mapping (i.e., it also maps the consequent UR(cid:98)x to
the consequent SR(cid:98)y). This deﬁnition makes sense
applied to the two URs x and(cid:98)x.
(x, y) T−→ ((cid:98)x,(cid:98)y)

because every grammar in the typology T is de-
ﬁned on every UR, so that every grammar can be

(2)
The relation T→ thus deﬁned over mappings turns
out to be a partial order (under mild additional as-
sumptions). It is called the T-order induced by the
typology T (Anttila and Andrus, 2006).

A familiar example concerns coda cluster sim-
pliﬁcation in English. Suppose that a coda t/d
deletes before vowels in a certain dialect, so that
the UR /cost us/ is realized as the SR [cos’ us].

Proceedingsofthe15thSIGMORPHONWorkshoponComputationalResearchinPhonetics,Phonology,andMorphology,pages1–10Brussels,Belgium,October31,2018.c(cid:13)2018TheSpecialInterestGrouponComputationalMorphologyandPhonologyhttps://doi.org/10.18653/v1/P171Then the coda also deletes before consonants in
that same dialect, so that the UR /cost me/ is re-
alized as the SR [cos’ me] (Guy, 1991; Kiparsky,
1993; Coetzee, 2004). In other words, the impli-
cation (/tV/, [V]) T→ (/tC/, [C]) holds relative to the
typology T of English dialects.

Two important phonological frameworks ex-
plored in the literature are Harmonic Grammar
(HG; Legendre et al., 1990; Smolensky and Leg-
endre, 2006; Potts et al., 2010) and Optimality
Theory (OT; Prince and Smolensky, 2004). The
crucial idea shared by HG and OT is that the rel-
evant properties of phonological mappings are ex-
tracted by a set of n phonological constraints that
effectively represent discrete phonological map-
pings as points of Rn. The goal of this paper is to

express an implication (x, y) → ((cid:98)x,(cid:98)y) in HG and
mappings (x, y) and ((cid:98)x,(cid:98)y) and their competitors.

OT in terms of the constraint violations of the two

Section 2 presents the constraint condition for
HG T-orders.
It rests on the rich geometry un-
derlying HG, as it follows from a classical re-
sult of convex geometry (the Hyperplane Separa-
tion Theorem), as detailed in section 3. Section 4
presents the constraint condition for OT T-orders.
It rests on an equivalence between OT and HG T-
orders established in section 5.

These constraint conditions admit a straightfor-
ward interpretation and thus help us better under-
stand the phonological import of T-orders. Fur-
thermore, they allow us to compute T-orders efﬁ-
ciently, circumventing the laborious computation
of the entire HG or OT typology (as it is currently
done in the literature; see for instance the OT T-
order Generator by Anttila and Andrus, 2006).

2 Constraint Conditions for HG T-orders

HG assumes a relation Gen which pairs each
UR x with a set Gen(x) of candidate SRs.
It
also assumes a set of n phonological constraints
C1, . . . , Cn. Each constraint Ck takes a phono-
logical mapping (x, y) of a UR x and a candidate
SR y in Gen(x) and returns the corresponding
number of violations Ck(x, y) ∈ N, a nonnega-
tive integer which quantiﬁes the “badness” of that
mapping (x, y) from the phonological perspective
encoded by that constraint Ck. A weight vector
w = (w1, . . . , wn) ∈ Rn
+ assigns a nonnegative
weight wk ≥ 0 to each constraint Ck.

The w-harmony of a mapping (x, y) is the
weighted sum of the constraint violations multi-

plied by −1, namely −(cid:80)n

k=1 wkCk(x, y). Be-
cause of the minus sign, mappings with a large
harmony have few constraint violations. The HG
grammar corresponding to a weight vector w
maps a UR x to the candidate SR y in Gen(x)
such that
the mapping (x, y) has a larger w-
harmony than the mapping (x, z) corresponding to
any other candidate z in Gen(x) (Legendre et al.,
1990; Smolensky and Legendre, 2006; Potts et al.,
2010). The HG typology (relative to a candidate
relation and a constraint set) consists of the HG
grammars corresponding to all weight vectors.

HG−→ ((cid:98)x,(cid:98)y) the implication
sequent mapping ((cid:98)x,(cid:98)y) relative to the HG typol-

between an antecedent mapping (x, y) and a con-

We denote by (x, y)

ogy. We assume that the antecedent UR x comes
with only a ﬁnite number m of antecedent loser
candidates z1, . . . , zm besides the antecedent win-
ner candidate y. Analogously, we assume that the

consequent UR(cid:98)x comes with only a ﬁnite num-
ber (cid:98)m of consequent loser candidates(cid:98)z1, . . . ,(cid:98)z(cid:98)m
besides the consequent winner candidate(cid:98)y. This

assumption is nonrestrictive.
In fact, a UR ad-
mits only a ﬁnite number of HG optimal candi-
dates (Magri, 2018). Candidate sets can thus be
assumed to be ﬁnite without loss of generality.

For each antecedent loser zi, we deﬁne the an-
tecedent difference vector C(x, y, zi) as in (3). It
has a component for each constraint Ck deﬁned
as the violation difference Ck(x, y, zi) between the
number Ck(x, zi) of violations assigned by Ck
to the loser mapping (x, zi) minus the number
Ck(x, y) of violations assigned to the antecedent
winner mapping (x, y).



C1(x, zi) − C1(x, y)
Ck(x, zi) − Ck(x, y)
Cn(x, zi) − Cn(x, y)

...
...



C(x, y, zi) =

(3)

ﬁned analogously, as pitting the consequent win-

The consequent difference vector C((cid:98)x,(cid:98)y,(cid:98)zj) is de-
ner mapping ((cid:98)x,(cid:98)y) against one of its losers ((cid:98)x,(cid:98)zj).
((cid:98)x,(cid:98)y) requires every HG grammar which succeeds

The deﬁnition of the HG implication (x, y) HG→

on the antecedent mapping to also succeed on the
consequent mapping. This condition is trivially
satisﬁed if no HG grammar succeeds on the an-
tecedent mapping, namely the mapping (x, y) is
HG unfeasible. Thus, let’s suppose that is not the
case. The following proposition then provides a

2•

•

•

•

•

•

•

•

a.

◦
Figure 1: Geometric representation of condition (4).

b.

complete (both necessary and sufﬁcient) charac-

terms of condition (4) stated entirely in terms of
antecedent and consequent difference vectors.
Proposition 1 If the antecedent mapping (x, y) is

terization of the HG implication (x, y) HG→ ((cid:98)x,(cid:98)y) in
HG feasible, the HG implication (x, y) HG→ ((cid:98)x,(cid:98)y)
didate(cid:98)zj with j = 1, . . . ,(cid:98)m, there exist m non-

holds if and only if for every consequent loser can-
negative coefﬁcients λ1, . . . , λm ≥ 0 (one for each
antecedent loser candidate z1, . . . , zm) such that

C((cid:98)x,(cid:98)y,(cid:98)zj) ≥ m(cid:88)

λi C(x, y, zi)

(4)

i=1

and furthermore at least one of these coefﬁcients
λ1, . . . , λm is different from zero.
2
Proposition 1 admits the following phonologi-
cal interpretation. Condition (4) says that each

consequent loser(cid:98)zj violates the constraints at least
losers. The consequent winner(cid:98)y thus has an “eas-

as much as (some conic combination of) the an-
tecedent losers z1, . . . , zm.
In other words, the
consequent losers are “worse” than the antecedent

ier” time beating its losers than the antecedent
winner y, as required by the deﬁnition of T-order.
Proposition 1 has important algorithmic impli-
cations.
In fact, checking the deﬁnition of T-
order (in general, of any implicational universal)
directly is costly, because it requires computing
the entire typology, which can be large. But propo-
sition 1 says that, in the case of HG, T-orders can
be determined locally, by only looking at the an-
tecedent and consequent mappings together with
their losers.
Indeed, this proposition effectively
reduces the problem of computing HG T-orders to
the problem of ﬁnding coefﬁcients λi which sat-
isfy the inequality (4). The latter is a polyhedral
feasibility problem that can be solved efﬁciently
with standard linear programming technology. A
Python package to compute HG T-orders using
condition (4) will be released shortly.

C

P

E

D

A
A

X

E

S

N

T
N
0
0
1

0
0
1
1

O
0
0
0

0
0
0
0

C

O

O

M
2
0
0

3
0
0
1

V

P

E

D
0
0
0

0
0
0
0

D
0
2
1

0
3
2
1

/CC/

/CCC/

[null]
[CV.CV]
[CVC]

[null]
[CV.CV.CV]
[CV.CVC]
[CVC]

Table 1: Violation proﬁles for the mappings of the URs /CC/
and /CCC/ to their non-harmonically bounded candidates.

ure 1a. The region {(cid:80)m

Proposition 1 admits the following geometric
interpretation. Suppose there are only n = 2 con-
straints and m = 4 antecedent difference vectors
C(x, y, zi), represented as the black dots in ﬁg-
i=1 λiC(x, y, zi)| λi ≥ 0}
is the convex cone generated by these antecedent
difference vectors, depicted in dark gray in ﬁgure
1a. The region in light gray singles out the points
which are at least as large (component by compo-
nent) as some point in this cone. Condition (4)
thus says that each consequent difference vector

C((cid:98)x,(cid:98)y,(cid:98)zj) must belong to this light gray region.

Indeed, suppose that some consequent differ-
ence vector does not belong to this light gray re-
gion, as represented by the white dot in ﬁgure 1b.
The dashed line leaves the antecedent difference
vectors (black dots) and the consequent difference
vector (white dot) on two different sides. This
means that the HG grammar corresponding to a
nonnegative weight vector orthogonal to this line
succeeds on the antecedent mapping (x, y) but it

fails on the consequent mapping ((cid:98)x,(cid:98)y), defying the
implication (x, y) HG→ ((cid:98)x,(cid:98)y).

The existence of (a weight vector corresponding
to) a dashed line such as the one depicted in ﬁgure
1b is geometrically obvious in the case with only
n = 2 constraints. For an arbitrary number n of
constraints, a fundamental result of convex geom-
etry, the Hyperplane Separation Theorem (HST;
Rockafellar, 1970, §11; Boyd and Vandenberghe,
2004, §2.5), indeed guarantees the existence of a
weight vector which separates the cone generated
by the antecedent difference vectors from the out-
lier consequent difference vector. This is the core
of the proof of proposition 1 provided in section 3.
Let’s ﬁnally look at a couple of examples (based
on Bane and Riggle 2009). We assume n = 5 con-

3

ONSET

NOCODA

MAX

DEPV

DEPC

≥ 1.5


+0


,



≥ 0



+1


,

0
0
3
−3
0

0
0
2
−2
0

0
0
2
−2
0
Table 2: Verifying that condition (4) holds for the HG implication (CC, CV.CV) → (CCC, CV.CV.CV).

0
1
0
−1
0

0
0
2
−2
0

0
1
0
−1
0

0
1
1
−2
0

0
1
0
−1
0



≥ 0.5


+1




0
1
0
−1
0

straints: ONSET, which penalizes surface syllables
starting with a vowel (V); NOCODA, which pe-
nalizes surface syllables ending with a consonant
(C); MAX, which penalizes deletion of underlying
segments; and DEPV and DEPC, which penalize
epenthetic vowels and consonants, respectively.
We focus on the two URs /CC/ and /CCC/. We only
consider their non-harmonically bounded candi-
dates, listed in table 1 with their constraint vio-
lations (the candidate [CVC.CV] is omitted because
indistinguishable by the constraints from [CV.CVC]).
We focus on the implication (CC, CV.CV) →
(CCC, CV.CV.CV). The antecedent UR x = /CC/
comes with the winner candidate y = [CV.CV]
and the m = 2 loser candidates z1 = [null] and
z2 = [CVC]. There are therefore two antecedent
difference vectors C(x, y, zi), repeated on the right
hand side of each of the three inequalities in table

2. The consequent UR(cid:98)x = /CCC/ comes with the
winner candidate(cid:98)y = [CV.CV.CV] and the (cid:98)m = 3
loser candidates(cid:98)z1 = [null],(cid:98)z2 = [CV.CVC], and
(cid:98)z3 = [CVC]. There are therefore three consequent
difference vectors C((cid:98)x,(cid:98)y,(cid:98)zj), which appear on the

left hand side of the three inequalities in table 2.
Condition (4) holds: each consequent difference
vector is at least as large as a conic combination
of the antecedent difference vectors, as shown in
table 2. Proposition 1 thus establishes the HG im-
plication (CC, CV.CV) HG→ (CCC, CV.CV.CV).

Proposition 1 can also be used to show that an
implication fails in HG. To illustrate, we focus
on the implication (CC, CVC) → (CCC, CV.CVC).
We consider the consequent difference vector
C(/CCC/, [CV.CVC], [null]), which appears on the
left hand side of
There are two an-
tecedent difference vectors C(/CC/, [CVC], [null])
and C(/CC/, [CVC], [CV.CV]), which appear on the
right hand side of (5).

(5).



ONSET

NOCODA

MAX

DEPV

DEPC

0
−1
3
−2
0

 (cid:54)≥ λ1



 + λ2





0
−1
0
1
0

0
−1
2
−1
0

(5)

Condition (4) fails: the consequent difference vec-
tor is not larger than any conic combination of
the two antecedent difference vectors, no mat-
ter the choice of the coefﬁcients λ1, λ2 ≥ 0.
the inequality (5) for DEPV requires
In fact,
λ1 ≥ 2, whereby the inequality fails for MAX.
Proposition 1 thus establishes that the implication
(CC, CVC) (cid:54) HG→ (CCC, CV.CVC) fails in HG.
3 Proof of Proposition 1
The HST has a number of algebraic consequences
known as theorems of the alternatives.1 One of
these theorems is the Motzkin Transposition The-
orem (MTT; Bertsekas, 2009, proposition 5.6.2),
which is particularly suited to our needs. It states
that conditions (C1) and (C2) below are mutually
exclusive (one and only one of them holds) for any
two matrices A ∈ Rp×n and B ∈ Rq×n.
(C1) There exists a vector w ∈ Rn such that

Aw < 0 and Bw ≤ 0.

(C2) There exist two nonnegative vectors ξ ∈
+ with µ (cid:54)= 0 such that

+ and µ ∈ Rp
Rq
ATµ + BTξ = 0.

1 , . . . ,−eT

1 , . . . ,−aT

It is useful to specialize the MTT as follows.
Consider some vectors a1, . . . , am, b ∈ Rn.
Let A be the matrix whose p = m rows are
−aT
m. Let B be the matrix whose q =
n+1 rows are −eT
n , bT (where ei ∈ Rn
has all components equal to 0 but for the ith com-
ponent which is equal to 1). The two conditions
(C1) and (C2) thus become (C1(cid:48)) and (C2(cid:48)).
(C1(cid:48)) There exists a nonnegative vector w ∈ Rn
+
mw > 0 but

1 w > 0, . . . , aT

such that aT
bTw ≤ 0.

(C2(cid:48)) There exist some nonnegative coefﬁcients
µ1, . . . , µm, ξ ≥ 0 with at least one of
the coefﬁcients µ1, . . . , µm different from 0

such that ξb ≥(cid:80)m

i=1 µiai.

1 Throughout this section, all vectors are column vectors;
T stands for matrix transposition. Vector inequalities must
hold component-wise.

4With these preliminaries in place, we now con-

sider the HG implication (x, y) HG→ ((cid:98)x,(cid:98)y). Suppose

that the HG grammar corresponding to some non-
negative weight vector w ∈ Rn
+ succeeds on the
antecedent mapping (x, y). This means that the w-
harmony of this mapping (x, y) is larger than that
of every antecedent loser mapping (x, zi). This
condition can be stated in terms of the antecedent
difference vectors as in (6), taking advantage of
the linearity of the HG harmony.

i = 1, . . . , m (6)

C(x, y, zi)Tw > 0,

stated in terms of the consequent difference vec-
tors as in (7).

HG grammar corresponding to that weight vec-
tor w to also succeed on the consequent mapping

The implication(x, y) HG→ ((cid:98)x,(cid:98)y) then requires the
((cid:98)x,(cid:98)y). This means that the w-harmony of this
mapping ((cid:98)x,(cid:98)y) is larger than that of every conse-
quent loser mapping ((cid:98)x,(cid:98)zj). This condition can be
C((cid:98)x,(cid:98)y,(cid:98)zj)Tw > 0,
j = 1, . . . ,(cid:98)m (7)
((cid:98)x,(cid:98)y) holds if and only if every nonnegative
if for every j = 1, . . . ,(cid:98)m, it is false that there ex-
but C((cid:98)x,(cid:98)y,(cid:98)zj)Tw ≤ 0.
every j = 1, . . . ,(cid:98)m, condition (C1(cid:48)) is false,
C((cid:98)x,(cid:98)y,(cid:98)zj). By the MTT, condition (C2(cid:48)) must
therefore be true for every j = 1, . . . ,(cid:98)m. This

weight vector w which satisﬁes (6) also satisﬁes
(7). Equivalently, the HG T-order holds if and only
ists a nonnegative weight vector w ∈ Rn
+ such
that C(x, y, zi)Tw > 0 for every i = 1, . . . , m
In other words, for

In other words, the HG implication (x, y) HG→

with the positions ai = C(x, y, zi) and b =

means that there exist some non-negative coefﬁ-
cients µ1, . . . , µm, ξ ≥ 0 such that at least one of
the coefﬁcients µ1, . . . , µm is strictly positive and
furthermore the inequality (8) holds.

ξ C((cid:98)x,(cid:98)y,(cid:98)zj) ≥ m(cid:88)

i=1

µi C(x, y, zi)

(8)

0 ≥ m(cid:88)

The coefﬁcient ξ in (8) must be strictly positive. In
fact, suppose by contradiction that ξ = 0, whereby
inequality (8) becomes (9).

µi C(x, y, zi)

(9)

i=1

Consider a weight vector w whose correspond-
ing HG grammar maps the antecedent UR x to

the antecedent winner y, which exists by hypoth-
esis. This weight vector w thus satisﬁes condi-
tion (6). Since w is non-negative, the scalar prod-
uct of both sides of (9) with w preserves the in-
equality, yielding (10). But the latter inequality
requires µ1 = ··· = µm = 0, contradicting the
assumption that at least one of the nonnegative co-
efﬁcients µ1, . . . , µm ≥ 0 is strictly positive.

0 ≥ m(cid:88)

i=1

(cid:124)

(cid:123)(cid:122)

>0

(cid:125)

µi C(x, y, zi)Tw

(10)

Since the coefﬁcient ξ is strictly positive, both
sides of (8) can be divided by ξ, yielding the in-
equality (4) with the position λi = µi/ξ.

4 Constraint Conditions for OT T-orders

This section extends the convex geometric analy-
sis of T-orders developed in the preceding sections
from HG to OT. We start by recalling that in OT a
constraint Ck is said to prefer a mapping (x, y) to
another mapping (x, z) provided Ck assigns less
violations to the former than to the latter, namely
Ck(x, y) < Ck(x, z). A constraint ranking is an
arbitrary linear order (cid:29) over the constraint set.
A constraint ranking (cid:29) prefers a mapping (x, y)
to another mapping (x, z) provided the highest (cid:29)-
ranked constraint which distinguishes between the
two mappings (x, y) and (x, z) prefers (x, y). The
fact that the highest (cid:29)-ranked relevant constraint
deﬁnes the preference of the entire ranking, irre-
spectively of the preferences of lower (cid:29)-ranked
constraints, is captured by saying that the former
constraint strictly dominates the latter constraints.
The OT grammar corresponding to a ranking (cid:29)
maps a UR x to that SR y such that (cid:29) prefers the
mapping (x, y) to the mapping (x, z) correspond-
ing to any other candidate z in Gen(x) (Prince and
Smolensky, 2004). The OT typology (for a given
candidate relation and constraint set) consists of
the OT grammars corresponding to all rankings.

We denote by (x, y) OT→ ((cid:98)x,(cid:98)y) the implication
sequent mapping ((cid:98)x,(cid:98)y) relative to the OT typol-

between an antecedent mapping (x, y) and a con-

ogy. By deﬁnition, this implication holds pro-
vided every constraint ranking that succeeds on the
antecedent mapping also succeeds on the conse-
quent mapping. Thus, a natural strategy to check

the OT implication (x, y) OT→ ((cid:98)x,(cid:98)y) would be to

use Recursive Constraint Demotion (RCD; Tesar
and Smolensky, 1998) to check that for every j =

51, . . . ,(cid:98)m, no ranking is consistent simultaneously
with the two mappings (x, y) and ((cid:98)x,(cid:98)zj). In this

section, we develop instead an alternative strategy
which uses the HG-to-OT-portability result of Ma-
gri (2013) to extend to OT the convex geometric
characterization of HG T-orders developed in sec-
tions 2-3.

To start, we recall that an OT grammar can be
construed as an HG grammar (as long as the con-
straint violations are bounded, which is the case
when the set of URs and the candidate sets are
ﬁnite).
In fact, OT’s strict domination can be
mimicked through HG weights which decrease ex-
ponentially.
Indeed, if a weight is much larger
than every smaller weight, the preferences of the
constraint with the larger weight cannot be over-
come by the preferences of the constraints with
smaller weights (Prince and Smolensky, 2004;
Keller, 2006). Since the OT typology is a sub-
set of the HG typology, whenever an implica-

tion (x, y) HG→ ((cid:98)x,(cid:98)y) holds in HG, the implication
(x, y) OT→ ((cid:98)x,(cid:98)y) holds in OT.

Lemma 1 slightly strengthens this conclusion.
In fact, OT only cares about constraints’ prefer-
ences. Equivalently, about the sign of the violation
differences. Thus, the HG implication (x, y) HG→

((cid:98)x,(cid:98)y) entails not only the corresponding OT im-
plication (x, y) OT→ ((cid:98)x,(cid:98)y) but also any other OT
implication (x∗, y∗) OT→ ((cid:98)x∗,(cid:98)y∗) whose antecedent
and consequent mappings (x∗, y∗) and ((cid:98)x∗,(cid:98)y∗)
(x, y) and ((cid:98)x,(cid:98)y). The proof of this lemma simply

yield violation differences with the same sign as
the original antecedent and consequent mappings

1, . . . , z∗

uses the observation that exponentially decaying
HG weights mimic OT strict domination and it is
therefore omitted.
Lemma 1 Given an antecedent mapping (x, y)
with its m antecedent loser candidates z1, . . . , zm,
consider another mapping (x∗, y∗) with the same
number m of loser candidates z∗
m such that
the m corresponding violation differences have
the same sign, in the sense that condition (11)
holds for k = 1, . . . , n and i = 1, . . . , m.
Ck(x, y, zi) (cid:84) 0 ⇐⇒ Ck(x∗, y∗, z∗
i ) (cid:84) 0 (11)
Analogously, given the consequent mapping
loser candidates

((cid:98)x,(cid:98)y) with its (cid:98)m consequent
(cid:98)z1, . . . ,(cid:98)z(cid:98)m, consider another mapping ((cid:98)x∗,(cid:98)y∗)
with the same number (cid:98)m of
(cid:98)z∗
1, . . . ,(cid:98)z∗(cid:98)m such that the (cid:98)m corresponding viola-

tion differences have the same sign, in the sense

loser candidates

that condition (12) holds for k = 1, . . . , n and

j = 1, . . . ,(cid:98)m.
Ck((cid:98)x,(cid:98)y,(cid:98)zj) (cid:84) 0 ⇐⇒ Ck((cid:98)x∗,(cid:98)y∗,(cid:98)z∗
The HG implication (x, y) HG−→ ((cid:98)x,(cid:98)y) then entails
the OT implication (x∗, y∗) OT−→ ((cid:98)x∗,(cid:98)y∗).

j ) (cid:84) 0 (12)

The preceding lemma establishes an entailment
from HG to OT implications. We now want to in-
vestigate the reverse entailment from OT to HG
implications. Thus, we suppose that an implica-

tion (x, y) OT→ ((cid:98)x,(cid:98)y) holds in OT. Of course, that
does not entail that the implication (x, y) HG→ ((cid:98)x,(cid:98)y)

We will try to establish something weaker in-

between the same two mappings also holds in
HG. That is because the HG typology is usually a
proper superset of the OT typology. And a larger
typology yields sparser T-orders. Thus, it makes
no sense to try to establish that the OT implica-

tion (x, y) OT→ ((cid:98)x,(cid:98)y) entails the HG implication
(x, y) HG→ ((cid:98)x,(cid:98)y) between the same two mappings.
stead: the OT implication (x, y) OT→ ((cid:98)x,(cid:98)y) entails
an HG implication (xdif, ydif) HG→ ((cid:98)xeasy,(cid:98)yeasy) be-
from (x, y) and a consequent mapping ((cid:98)xeasy,(cid:98)yeasy)
different from ((cid:98)x,(cid:98)y). And we will choose this new
quent mapping ((cid:98)xeasy,(cid:98)yeasy) in such a way that the
new HG implication (xdif, ydif) HG→ ((cid:98)xeasy,(cid:98)yeasy) is
(x, y) HG→ ((cid:98)x,(cid:98)y) and thus validates the entailment

tween an antecedent mapping (xdif, ydif) different

antecedent mapping (xdif, ydif) and this new conse-

“more likely to hold” than the original implication

from OT to HG implications.

What does it mean that an implication is “more
likely to hold”? Intuitively, an implication from
an antecedent to a consequent mapping is “likely
to hold” when the antecedent mapping is “difﬁ-
cult” to obtain, namely it is consistent with very
few grammars. In the limit, the implication holds
trivially when the antecedent mapping is consis-
tent with no grammars at all. Thus, we want to
deﬁne the new antecedent mapping (xdif, ydif) in
such a way that it is “more difﬁcult” to obtain in
HG than the original antecedent mapping (x, y),
whereby the superscript “diff”. Analogously, an
implication from an antecedent to a consequent
mapping is intuitively “likely to hold” when the
consequent mapping is “easy” to obtain, namely
it is consistent with very many grammars. In the
limit, the implication holds trivially when the con-
sequent mapping is consistent with every gram-
mar. Thus, we want to deﬁne the new consequent

6mapping ((cid:98)xeasy,(cid:98)yeasy) in such a way that it is “eas-
mapping ((cid:98)x,(cid:98)y), whereby the superscript “easy”.

ier” to obtain in HG than the original consequent

Let us now turn to the details. As discussed
above around (6), it sufﬁces to deﬁne the differ-
ence vectors corresponding to the new difﬁcult
mapping antecedent (xdif, ydif). Given the original
antecedent mapping (x, y) with its m loser can-
didates z1, . . . , zm, we assume that the new an-
tecedent mapping (xdif, ydif) comes with the same
number m of loser candidates zdif
m whose
violation differences are deﬁned as in (13). Here,
Ωi is the total number of constraints Ck such that
Ck prefers the original antecedent winner map-
ping (x, y) to the original antecedent loser map-
ping (x, zi), in the sense that Ck(x, y, zi) > 0.

1 , . . . , zdif

Ck(xdif, ydif, zdif

i ) =

 1

=

if Ck(x, y, zi) > 0
if Ck(x, y, zi) = 0
0
−Ωi − 1 if Ck(x, y, zi) < 0

(13)

The intuition behind this deﬁnition (13) is as
follows. OT only cares about the sign of the vi-
olation differences. Thus, the new violation dif-
ference Ck(xdif, ydif, zdif
i ) is deﬁned in such a way
that it has the same sign as the original violation
difference Ck(x, y, zi): one is positive or nega-
tive if and only if the other is as well. HG also
cares about the size of the violation differences,
not only about their sign. In order for the map-
ping (xdif, ydif) to be “difﬁcult” in HG, we want
its positive violation differences to be as small as
possible. For this reason, the positive violation dif-
ferences in (13) have been set equal to 1, which is
the smallest positive integer. Analogously, in or-
der for the mapping (xdif, ydif) to be “difﬁcult” in
HG, we want its negative violation differences to
be large (in absolute value) relative to the strength
of the positive violation differences they have to
“ﬁght off”. Since the positive entries are all equal
to 1 in (13), the “strength” of the positive entries
only depends on their number Ωi. For this rea-
son, the absolute value of the negative violation
differences in (13) has been set equal to Ωi + 1.
In conclusion, this deﬁnition (13) ensures that the
mapping (xdif, ydif) is “difﬁcult” in HG, because
the positive violation differences are small and the
negative ones are large (in absolute value).

We now turn to the consequents. Given the orig-

inal consequent mapping ((cid:98)x,(cid:98)y) with its (cid:98)m loser
candidates (cid:98)z1, . . . ,(cid:98)z(cid:98)m, we assume that the new

1

whose violation differences are deﬁned as in (14).

such that Ck prefers the original consequent loser

consequent mapping ((cid:98)xeasy,(cid:98)yeasy) comes with the
same number(cid:98)m of loser candidates(cid:98)zeasy
, . . . ,(cid:98)zeasy(cid:98)m
Here (cid:98)Λj is the total number of constraints Ck
mapping ((cid:98)x,(cid:98)zj) to the original consequent winner
mapping ((cid:98)x,(cid:98)y), in the sense that Ck((cid:98)x,(cid:98)y,(cid:98)zj) < 0.
Ck((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy


(cid:98)Λj + 1 if Ck((cid:98)x,(cid:98)y,(cid:98)zj) > 0
if Ck((cid:98)x,(cid:98)y,(cid:98)zj) = 0
if Ck((cid:98)x,(cid:98)y,(cid:98)zj) < 0

0
−1

(14)

) =

=

j

j

The intuition behind this deﬁnition (14) is as
follows. Whenever the original violation differ-

ence Ck((cid:98)x,(cid:98)y,(cid:98)zj) is positive or negative, the new
violation difference Ck((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy

) is posi-
tive or negative as well, so that the original and
the new violation differences have the same sign.
The size of the new violation differences has been
In order for the mapping
chosen as follows.

((cid:98)xeasy,(cid:98)yeasy) to be “easy” in HG, we want its nega-

tive violation differences to be as small as possible
(in absolute value). For this reason, the negative
violation differences in (14) have been set equal to
−1, which is the negative integer smallest in ab-
solute value. Analogously, in order for the map-

ping ((cid:98)xeasy,(cid:98)yeasy) to be “easy” in HG, we want its

For this reason, the positive violation differences

sion, this deﬁnition (14) ensures that the mapping

positive violation differences to be large relative to
the strength of the negative violation differences
they have to “ﬁght off”. Since the negative entries
are all equal to −1 in (14), the “strength” of the

violation differences are large and the negative vi-
olation differences are small (in absolute value).

negative entries only depends on their number(cid:98)Λj.
in (14) have been set equal to(cid:98)Λj + 1. In conclu-
((cid:98)xeasy,(cid:98)yeasy) is “easy” in HG, because the positive
anticipated, the OT implication (x, y) OT→ ((cid:98)x,(cid:98)y)
((cid:98)x,(cid:98)y) with the same antecedent and consequent
ensures that the OT implication (x, y) OT→ ((cid:98)x,(cid:98)y)
((cid:98)xeasy,(cid:98)yeasy).
(x, y) HG→ ((cid:98)x,(cid:98)y), because its antecedent is “difﬁ-

HG→
the lat-
ter is less demanding than the HG implication

mappings. Nonetheless, the following lemma 2

We are now ready to put the pieces together. As

might not entail the HG implication (x, y)

the HG implication (xdif, ydif)

The intuition is that

cult” (namely, consistent with few HG grammars)
and its consequent is “easy” (namely, consistent

does entail

HG→

7with many HG grammars). The proof of this
lemma is provided in section 5, mimicking a rea-
soning in Magri (2013).

Lemma 2 The OT implication (x, y) OT→ ((cid:98)x,(cid:98)y) en-
tails the HG implication (xdif, ydif) HG→ ((cid:98)xeasy,(cid:98)yeasy)
consequent mapping ((cid:98)xeasy,(cid:98)yeasy) whose violation

between the antecedent mapping (xdif, ydif) and the

differences are deﬁned in (13) and (14).

2

i

i = zdif

As remarked explicitly above,

(13) ensures
that the original antecedent violation differences
Ck(x, y, zi) and the new antecedent violation dif-
ferences Ck(xdif, ydif, zdif
i ) have the same sign. In
other words, condition (11) holds with the posi-
tions x∗ = xdif, y∗ = ydif, and z∗
. Anal-
ogously, (14) ensures that the original consequent

)
have the same sign. In other words, condition (12)

violation differences Ck((cid:98)x,(cid:98)y,(cid:98)zj) and the new con-
sequent violation differences Ck((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy
holds with the positions(cid:98)x∗ =(cid:98)xeasy,(cid:98)y∗ =(cid:98)yeasy, and
(cid:98)z∗
i =(cid:98)zeasy
OT implication (x, y) OT→ ((cid:98)x,(cid:98)y) holds if and only
HG→ ((cid:98)xeasy,(cid:98)yeasy)

. The two lemmas 1 and 2 can therefore
the

the HG implication (xdif, ydif)
holds. We can thus extend to OT the characteri-
zation of HG T-orders provided by the HG propo-
sition 1 above, obtaining the following:

be combined into the following conclusion:

i

i

Proposition 2 If the antecedent mapping (x, y) is

OT feasible, the OT implication (x, y) OT→ ((cid:98)x,(cid:98)y)
holds iff for every j = 1, . . . ,(cid:98)m, there exist m non-
C((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy

negative coefﬁcients λ1, . . . , λm ≥ 0 such that

) ≥ m(cid:88)

λi C(xdif, ydif, zdif
i )

j

i=1

(15)
and furthermore at least one of these coefﬁcients
λ1, . . . , λm is different from zero.
2

But

the HG implication (CC, CVC)

tors C((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy

To illustrate, we have seen at the end of sec-
HG→
tion 2 that
(CCC, CV.CVC) fails in HG because condition (4)
fails, as shown in (5).
this entailment
OT→ (CCC, CV.CVC) does hold in OT. In
(CC, CVC)
fact, the three “easy” consequent difference vec-
) in this case are listed on
the left hand side of the three inequalities in ta-
ble 3. The two “difﬁcult” antecedent difference
vectors C(xdif, ydif, zdif
i ) are repeated on the right
hand side of the three inequalities. The table thus
shows that condition (15) holds.

j

the

OT→

The

k=1 wkCk(xdif, ydif, zdif

assumption
tor w succeeds
tecedent mapping

5 Proof of Lemma 2
We assume that the OT implication (x, y)

weight vector w = (w1, . . . , wn) which succeeds
on the “difﬁcult” antecedent mapping (xdif, ydif)
and we prove that it is also succeeds on the “easy”

((cid:98)x,(cid:98)y) holds. We consider an arbitrary nonnegative
consequent mapping ((cid:98)xeasy,(cid:98)yeasy), thus securing
the HG implication (xdif, ydif) HG−→ ((cid:98)xeasy,(cid:98)yeasy).
(cid:80)n

vec-
an-
that
every
i = 1, . . . , m. The latter inequality can be
unpacked as in (16).
In step (16a), we have
used the deﬁnition (13). Here W (x, y, zi) and
L(x, y, zi) are the sets of winner-preferring and
loser-preferring constraints relative to the winner
(x, y) and the loser (x, zi). In step (16b), we have
h∈W (x,y,zi) wh with its
largest term maxh∈W (x,y,zi) wh times the number
Ωi of its addenda. In step (16c), we have lower
k∈L(x,y,zi) wk with one of its

the weight
that
on
“difﬁcult”
(xdif, ydif) means
i ) > 0 for

upper bounded the sum(cid:80)
bounded the sum(cid:80)
n(cid:88)
(a)⇐⇒ (cid:88)

terms, as the addenda are all non-negative.

wkCk(xdif, ydif, zdif

wh > (Ωi + 1)

i ) > 0

wk

k=1

(cid:88)
(cid:88)

k∈L(x,y,zi)

h∈W (x,y,zi)
=⇒ Ωi max

(b)

h∈W (x,y,zi)

(c)

=⇒ Ωi max

h∈W (x,y,zi)

wh > (Ωi + 1)

wk

k∈L(x,y,zi)

wh > (Ωi + 1)wk

for every k ∈ L(x, y, zi)

=⇒ max

h∈W (x,y,zi)

wh > wk

max

wh > max

wk

(17)

(16)
We now show that the conclusion reached in the
last line of (16) entails that the strict inequality

(17) holds for every j = 1, . . . ,(cid:98)m.
k∈L((cid:98)x,(cid:98)y,(cid:98)zj )

h∈W ((cid:98)x,(cid:98)y,(cid:98)zj )

In fact, suppose by contradiction that (17) fails

for some j = 1, . . . ,(cid:98)m. Consider a ranking (cid:29)

which respects the relative size of the weights, in
the sense that conditions [A] and [B] hold for any
two constraints Cs, Ct with weights ws, wt.
[A] If ws > wt, then Cs is (cid:29)-ranked above Ck.

[B] If ws = wt and Cs ∈ L((cid:98)x,(cid:98)y,(cid:98)zj) and Ct ∈
W ((cid:98)x,(cid:98)y,(cid:98)zj), then Cs is (cid:29)-ranked above Ck.

8

0
−1
3
−1
0

ONSET

NOCODA

MAX

DEPV

DEPC

≥ 0.5




≥ 0



+0.5




0
−2
1
−2
0

0
−2
1
−2
0
Table 3: Verifying that condition (15) holds for the OT implication (CC, CVC) → (CCC, CV.CVC).

0
−2
0
1
0

0
−1
0
2
0

0
−2
1
−2
0

0
−2
0
1
0

0
0
2
−1
0



≥ 0.5


+0




+0




0
−2
0
1
0

The ranking (cid:29) succeeds on the antecedent
mapping (x, y).
In fact, the condition obtained
in the last line of (16) says that there exists a
constraint which prefers the winner (x, y) to the
loser (x, zi) whose weight is strictly larger than the
weight of every constraint which instead prefers
the loser (x, zi) to the winner (x, y). By [A],
this means that a constraint which prefers the
winner (x, y) is (cid:29)-ranked above every constraint
that instead prefers the loser (x, zi). The rank-
ing (cid:29) therefore prefers the winner (x, y) to the
loser (x, zi). Since this conclusion holds for ev-
ery i = 1, . . . , m, the ranking (cid:29) succeeds on the
antecedent mapping (x, y).

On the other hand,

the ranking (cid:29) fails on

words, there exists a constraint which prefers the

tradictory assumption that (17) fails means that

the consequent mapping ((cid:98)x,(cid:98)y). In fact, the con-
maxh∈W ((cid:98)x,(cid:98)y,(cid:98)zj ) wh ≤ maxk∈L((cid:98)x,(cid:98)y,(cid:98)zj ) wk. In other
loser ((cid:98)x,(cid:98)zj) to the winner ((cid:98)x,(cid:98)y) whose weight is
constraints which instead prefer the winner ((cid:98)x,(cid:98)y)
to the loser ((cid:98)x,(cid:98)zj). By [A] and [B], the ranking (cid:29)
cannot prefer ((cid:98)x,(cid:98)y) to ((cid:98)x,(cid:98)zj).

strictly larger than or equal to the weights of the

The conclusion that (cid:29) succeeds on the an-

contradicts the assumption that the implication
(x, y)
the inequality (17). This inequality can in turn
be unpacked as in (18).
In step (18a), we have

tecedent (x, y) but fails on the consequent ((cid:98)x,(cid:98)y)
OT→ ((cid:98)x,(cid:98)y) holds in OT, thus establishing
lower bounded(cid:98)Λj maxk∈L((cid:98)x,(cid:98)y,(cid:98)zj ) wk with the sum
(cid:80)
k∈L((cid:98)x,(cid:98)y,(cid:98)zj ) wk, because(cid:98)Λj is the number of ad-
the sum(cid:80)
bounded the maximum maxh∈W ((cid:98)x,(cid:98)y,(cid:98)zj ) wh with
h∈W ((cid:98)x,(cid:98)y,(cid:98)zj ) wh, because the weights be-

denda in the sum. In step (18b), we have upper

ing summed over are all non-negative.
In step
(18c), we have used the deﬁnition (14) of the con-

wk

j

(a)

).

max

wk =⇒

wh > max

wh >(cid:98)Λj max
k∈L((cid:98)x,(cid:98)y,(cid:98)zj )
(cid:88)
k∈L((cid:98)x,(cid:98)y,(cid:98)zj )
(cid:88)
k∈L((cid:98)x,(cid:98)y,(cid:98)zj )

straint differences Ck((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy
k∈L((cid:98)x,(cid:98)y,(cid:98)zj )
h∈W ((cid:98)x,(cid:98)y,(cid:98)zj )
=⇒ ((cid:98)Λj + 1) max
h∈W ((cid:98)x,(cid:98)y,(cid:98)zj )
=⇒ ((cid:98)Λj + 1) max
h∈W ((cid:98)x,(cid:98)y,(cid:98)zj )
(cid:88)
=⇒ ((cid:98)Λj + 1)
h∈W ((cid:98)x,(cid:98)y,(cid:98)zj )
=⇒ n(cid:88)
ery j = 1, . . . ,(cid:98)m, ensuring that the weights w
succeed on the consequent mapping ((cid:98)xeasy,(cid:98)yeasy).

(18)
The inequality in the last line of (18) holds for ev-

wkCk((cid:98)xeasy,(cid:98)yeasy,(cid:98)zeasy

wh >

wh >

) > 0

wk

wk

k=1

(c)

(b)

j

6 Conclusions

A central task of linguistic theory is to character-
ize the typological structure predicted by a gram-
matical formalism in order to match it to linguistic
data. A classical strategy to characterize typologi-
cal structure is to chart the implicational universals
predicted by the formalism. In this paper, we have
focused on the two constraint-based phonological
formalisms of HG and OT. And we have consid-
ered the simplest type of implicational universals,
namely T-orders. The main result of this paper
has been a complete constraint characterization of
T-orders in HG and OT. These constraint condi-
tions rely on an elegant underlying convex geom-
etry. These conditions are phonologically intuitive
and have important algorithmic implications.

Acknowledgments

title:

The research reported in this paper has been
funded by the Agence National de la Recherche
(project
‘The mathematics of segmental
phonotactics’). This paper is part of a larger
project on T-orders, developed in collaboration
with Arto Anttila. His comments on this paper are
gratefully acknowledged.

9References
Arto Anttila and Curtis Andrus. 2006.

Manuscript and software (Stanford).

T-orders.

Maximilian Bane and Jason Riggle. 2009. Evaluating
Strict Domination: The typological consequences of
In Proceedings of the 45th
weighted constraints.
annual meeting of the Chicago Linguistics Society,
pages 13–27.

Dimitri P. Bertsekas. 2009. Convex Optimization The-

ory. Athena Scientiﬁc, Belmont, MA, USA.

Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in generative gram-
mar. Blackwell, Oxford. Original version, Techni-
cal Report CU-CS-696-93, Department of Computer
Science, University of Colorado at Boulder, and
Technical Report TR-2, Rutgers Center for Cogni-
tive Science, Rutgers University, April 1993. Avail-
able from the Rutgers Optimality Archive as ROA
537.

R. Tyrrell Rockafellar. 1970. Convex Analysis. Prince-
ton Landmarks in Mathematics; Princeton Univer-
sity Press.

Stephen Boyd and Lieven Vandenberghe. 2004. Con-

vex Optimization. Cambridge University Press.

Paul Smolensky and G´eraldine Legendre. 2006. The

Harmonic Mind. MIT Press, Cambridge, MA.

Noam Chomsky. 1981. Lectures on Government and

Binding. Mouton de Gruyter.

Bruce Tesar and Paul Smolensky. 1998. Learnability in
Optimality Theory. Linguistic Inquiry, 29:229–268.

Noam Chomsky and Morris Halle. 1968. The Sound

Pattern of English. Harper and Row, New York.

Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory.
Ph.D. thesis, University of Massachusetts,
Amherst.

Joseph H. Greenberg. 1963. Some universals of gram-
mar with particular reference to the order of mean-
ingful elements.
In Joseph H. Greenberg, editor,
Universals of Language, pages 73–113. MIT Press,
Cambridge, MA.

G. Guy. 1991. Explanation in variable phonology.

Language Variation and Change, 3:1–22.

Frank Keller. 2006.

Linear Optimality Theory as
In Gisbert
a model of gradience in grammar.
Fanselow, Caroline F´ery, Ralph Vogel, and Matthias
Schlesewsky, editors, Gradience in Grammar: Gen-
erative Perspectives, pages 270–287. Oxford Uni-
versity Press, Oxford.

Paul Kiparsky. 1993. An OT perspective on phonolog-

ical variation. Handout (Stanford).

G´eraldine Legendre, Yoshiro Miyata,

and Paul
Smolensky. 1990. Harmonic Grammar: A for-
mal multi-level connectionist theory of linguistic
In An-
well-formedness: Theoretical foundations.
nual conference of the Cognitive Science Society 12,
pages 388–395, Mahwah, NJ. Lawrence Erlbaum.

Giorgio Magri. 2013. HG has no computational ad-
vantages over OT: towards a new toolkit for compu-
tational OT. Linguistic Inquiry, 44.4:569–609.

Giorgio Magri. 2018.

Finiteness of optima in

constraint-based phonology. Manuscript, CNRS.

Christopher Potts, Joe Pater, Karen Jesney, Rajesh
Bhatt, and Michael Becker. 2010. Harmonic Gram-
mar with Linear Programming: From linear systems
to linguistic typology. Phonology, 27(1):1–41.

10
The 13th Conference of 

The Association for Machine Translation 

in the Americas 

www.conference.amtaweb.org

De-mystifying Neural MT

TUTORIALMarch 17, 2018Presenters: Dragos Munteanu (SDL), Ling Tsou (SDL)De-mystifying Neural MT

Dragos Munteanu
Ling Tsou

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 1What you will get out of this tutorial
‚Ä¢ Learn what‚Äôs behind the ‚Äúmagic‚Äù
‚Ä¢ Make sense of the ‚Äúbuzzwords‚Äù
‚Ä¢ Gain insights about why Neural Networks are 
so successful
‚Ä¢ Better understand the limitations/difficulties 
in this new paradigm

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 2Who are we

‚Ä¢ Dragos Munteanu

‚Äì Director of Research and Development
‚Äì 10+ years of experience
‚Äì Started out at Language Weaver

‚Ä¢ Ling Tsou

‚Äì Research Engineer
‚Äì 5+ years of experience

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 3Agenda

‚Ä¢ Neural Networks

‚Äì Basic structure of a 

Neural Network

‚Äì Deep Neural Networks
‚Äì Training 

‚Ä¢ Neural Machine 

Translation
‚Äì NMT vs SMT
‚Äì Word embeddings
‚Äì Architectures
‚Äì Limitations
‚Äì Future Outlook

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 4Rule-based vs. Statistical vs. Neural

Rule-Based

Statistical

Neural

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 5AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 6Statistical Learning

input

data

Machine

Training + Decoding

model

output

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 78

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 8A Neural Network

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 9A Neural Network

Activation Function

Input

Bias

Parameters

Output

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 10A Neural Network

.15

.78

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 11A Neural Network

1.0 x 3.7 + 0.0 x 3.7 + 1 x -1.5 = 2.2

11+ùëíùëí‚àí2.2 = 0.90

.15

.78

Sigmoid  f(x) =  

11+ùëíùëí‚àíùë•ùë•

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 1213

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 130.6

0.6

-1

X1

X2

1

X1

X2

Target

0
0
1
1

14

0
1
0
1

0
0
0
1

Network 
output

-1
-0.4
-0.4
0.2

1.1

1.1

-1

X1

X2

1

X1

X2

Target

Network 
output

0
0
1
1

0
1
0
1

0
1
1
1

-1
0.1
0.1
0.2

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 14weather

friend

bus station

1

Should you go to the 
cheese festival?
Decision factors:
‚Ä¢ Is weather good?
‚Ä¢ Is friend coming?
‚Ä¢ Is festival near bus 

station?

15

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 15Should you go to the 
cheese festival?
Decision factors:
‚Ä¢ Is weather good?
‚Ä¢ Is friend coming?
‚Ä¢ Is festival near bus 

station?

16

weather

friend

bus station

2

2

6

-5

1

Going, unless weather is bad

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 16Should you go to the 
cheese festival?
Decision factors:
‚Ä¢ Is weather good?
‚Ä¢ Is friend coming?
‚Ä¢ Is festival near bus 

station?

17

weather

friend

bus station

1

0

0

1

2

2

6

-5

1

6+0+0-5

Going, unless weather is bad

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 17Should you go to the 
cheese festival?
Decision factors:
‚Ä¢ Is weather good?
‚Ä¢ Is friend coming?
‚Ä¢ Is festival near bus 

station?

18

weather

friend

bus station

3

3

6

-5

1

Going if weather is good OR 
friend+bus

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 18Should you go to the 
cheese festival?
Decision factors:
‚Ä¢ Is weather good?
‚Ä¢ Is friend coming?
‚Ä¢ Is festival near bus 

station?

19

weather

friend

bus station

0

1

1

1

3

3

6

-5

1

3+3-5

Going if weather is good OR 
friend+bus

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 19Playing games: Tic Tac Toe

‚Ä¢ 255,168 unique games
‚Äì 131,184 are won by the 

first player

‚Äì 77,904 are won by the 

second player

‚Äì 46,080 are drawn

Jesper Juul. ‚Äú255,168 ways of playing Tic Tac Toe‚Äù

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 20Tic Tac Toe

Input

Hidden

1

2

9

bias

1

1

Output
1

2

9

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 21Tic Tac Toe

1

4

7

2

5

8

3

6

9

Input representation
‚Ä¢ Marked by self: 1
‚Ä¢ Marked by opponent: -1
‚Ä¢ Empty: 0

‚Ä¢ If computer is O, then:
[-1, 0, -1, 0, 1, 0, 0, 0, 0]

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 22Tic Tac Toe

1

4

7

2

5

8

3

6

9

1

2

3

4

5

9

bias

Input
-1

Hidden

Output
1

2

9

1

0

-1

0

1

0
1

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 23Tic Tac Toe

1

4

7

2

5

8

3

6

9

‚Ä¢ Input:

[-1, 0, -1, 0, 1, 0, 0, 0, 0]

‚Ä¢ Output:
0.8

[0.12, 0.8, 0.05,
0.3, 0.05, 0.37,
0.41, 0.2, 0.49]

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 24Deep Learning & Deep Neural Networks

Deep Neural Networks

Multiple Layers

Millions of Parameters

Various Architectures

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 25Deep Neural Network ‚Äì Image Classification

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 26A Deep Neural Network (convolutional)

cat

dog

60 million parameters

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 27Why are Deep Networks better?
‚Ä¢ Different layers can learn different levels of 
abstraction
‚Ä¢ Mathematically, it can represent more 
complex functions

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 28Deep Neural Network ‚Äì how they learn

Li, Yixuan, et al. "Convergent Learning: Do different neural networks learn the same representations?."

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 29TRAINING

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 30Training: what does a model consist of?

‚Ä¢ Each circle with 

represents an activation 
function

‚Ä¢ Each arrow represents a 

multiplication
‚Äì input x weight

Parameters to train

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 31Training
‚Ä¢ What does training actually do?

‚Äì Determine parameter values by minimizing error

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 32Training: parameters

‚Ä¢ 9 input nodes
‚Ä¢ 1 hidden layer: 2 nodes
‚Ä¢ 9 output nodes
‚Ä¢ Number of parameters

= (9 + 1) * 2 + (2 + 1) * 9 
= 47

Input

Hidden

1

2

9

bias

1

1

Output
1

2

9

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 33Training
‚Ä¢ Steps:

1. Compute current model output (forward pass) 

for each training example

2. Compute cost
3. Update parameters (backpropagation)

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 34Training: 1. Forward pass

1

4

7

2

5

8

3

6

9

An example of 
training data

Input

[-1, 0, -1,
0, 1, 0,
0, 0, 0]

Expected output
[0, 1, 0,
0, 0, 0,
0, 0, 0]

Model output

[.2, .13, .56,
.8, .3, .49,
.52, .23, .01]

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 35Training: 2. Compute cost

‚Ä¢ Cost = error between expected 
‚Ä¢ Example of a cost function:

and model output

Input

[-1, 0, -1,
0, 1, 0,
0, 0, 0]

Cost = 19(0‚àí.2 2+ 1‚àí.13 2+‚ãØ)

= 0.2671

Expected output
[0, 1, 0,
0, 0, 0,
0, 0, 0]

Model output

[.2, .13, .56,
.8, .3, .49,
.52, .23, .01]

AMTA 2018 Tutorial: De-mystifying Neural MT Boston, March 17, 2018  | Page 36Training: 3. Backpropagation
‚Ä¢ Update weights

Input

Hidden

1

2

9

bias

-1

0

0

1

0.1

0.3

1

Output
1
.2

.13

2

.01

9

Total 
Error

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 37Training: 3. Backpropagation
‚Ä¢ Update weights: proportional to activation

Input

Hidden

1

2

9

bias

-1

0

0

1

0.1

0.3

1

Output
1
.2

.13

2

Partial 
Error

.01

9

Total 
Error

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 38Training: 3. Backpropagation
‚Ä¢ Update weights: proportional to activation

Input

Hidden

1

2

9

bias

-1

0

0

1

0.1
0.1

0.3
0.3

1

Output
1
.2

.13

2

Partial 
Error

Wo

1,1

Wo

2,1

Wo

3,1

.01

9

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 39Training: 3. Backpropagation
‚Ä¢ Update weights: proportional to activation

Hidden
Partial 
Error
0.1

1,1

Wh
Wh

2,1

Wh

9,1

0.3

1

Wh

10,1

Input

1

2

9

bias

-1

0

0

1

Output
1
.2

.13

2

.01

9

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 40Visualize Neural Network training
‚Ä¢ http://www.emergentmind.com/neural-network

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 41Difficulties
‚Ä¢ If you just take some data and run backprop,
you won‚Äôt get a good network
‚Äì Especially a deep network
‚Ä¢ Some of the problems are:

‚Äì Overfitting
‚Äì Exploding/vanishing gradient

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 42Training tricks: drop-out

Input

Hidden

Output

1

1

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 43Training tricks: synthetic data
‚Ä¢ Increase the amount of data
‚Ä¢ Add noise

Oravec, Milos, et al. "Efficiency of recognition methods for single sample per person based face 
recognition." Reviews, Refinements and New Ideas in Face Recognition. InTech, 2011.

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 44MACHINE 
TRANSLATION

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 45Machine Translation

1949

RBMT
1970s

SMT Phrase-Based

2001

Adaptive MT
Neural MT
2017

1966

ALPAC
Report

1993

SMT

2008

SMT Syntax-Based

AMTA 2018 Tutorial: De-mystifying Neural MTMarch 17, 2018  | Page 46Statistical Machine Translation

Translation

Model

Syntax
Model

Distortion

Part of
Speech

Lexicalized
Reordering

Language
Model

Alignment

Reordering

Morphology

Smoothing

Preordering

Capitalization

Transliteration

Word 
Deletion

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 47Neural Machine Translation

Input
Text

ENCODER

-0.2
-0.1
0.1
0.4
-0.3
1.1
4.3
-0.2
0.5
0.9
1.3
3.4
-5.3
-6.2
4.8
9.3
3.4
‚Ä¶
2.6
4.9
0.1
2.6
8.3
-7.3
5.1
1.5
0.6
9.3
-6.2
2.9
1.4
-1.3

Output
Text

DECODER

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 48The Machine Translation pyramid

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 49NMT: Word Representations

‚ÄúMy name is Dragos‚Äù

‚ÄúJe m'appelle Dragos"

Some NN model

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 50NMT: Word Representations ‚Äì one hot

Vocab
a
burger
dragos
for
had
i
is
lunch
my
name

1
1
0
0
0
0
0
0
0
0
0

2
0
1
0
0
0
0
0
0
0
0

3
0
0
1
0
0
0
0
0
0
0

4
0
0
0
1
0
0
0
0
0
0

5
0
0
0
0
1
0
0
0
0
0

6
0
0
0
0
0
1
0
0
0
0

7
0
0
0
0
0
0
1
0
0
0

8
0
0
0
0
0
0
0
1
0
0

9
0
0
0
0
0
0
0
0
1
0

10
0
0
0
0
0
0
0
0
0
1

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 51NMT: Word Representations ‚Äì one hot
‚Ä¢ ‚Äúmy name is dragos‚Äù

Index: [9, 10, 7, 3]
One-hot:
‚Äì ‚Äúmy‚Äù (9):
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
‚Äì ‚Äúname‚Äù (10): [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
‚Äì ‚Äúis‚Äù (7):
[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
‚Äì ‚Äúdragos‚Äù (3):  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 52NMT: Word Representations ‚Äì one hot
‚Ä¢ Problem with this method:

‚Äì Large number of vocab => curse of dimensionality
‚Äì Hard to capture the relationships between words

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 53Word representations

Sparse

All words are equally different

Dense

Similar words have similar vectors

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 54NMT: Word Representations and Word Embedding

XKing ‚Äì XMan + XWoman = XQueen

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 55BREAK

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 56Rule-based vs. Statistical vs. Neural

Rule-Based

Statistical

Neural

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 57Statistical Machine Translation

Translation

Model

Syntax
Model

Distortion

Part of
Speech

Lexicalized
Reordering

Language
Model

Alignment

Reordering

Morphology

Smoothing

Preordering

Capitalization

Transliteration

Word 
Deletion

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 58Neural Machine Translation

Input
Text

ENCODER

-0.2
-0.1
0.1
0.4
-0.3
1.1
4.3
-0.2
0.5
0.9
1.3
3.4
-5.3
-6.2
4.8
9.3
3.4
‚Ä¶
2.6
4.9
0.1
2.6
8.3
-7.3
5.1
1.5
0.6
9.3
-6.2
2.9
1.4
-1.3

Output
Text

DECODER

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 59Encoder Decoder

[.34, .02, .194, ‚Ä¶]

Source sentence
representation

‚ÄúJe m'appelle Dragos‚Äù

Target sentence

Encoder

Decoder

Source sentence

‚ÄúMy name is Dragos.‚Äù

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 60Sequence-to-sequence learning: Encoder

Representation of source sentence

Embedding

Embedding

Embedding

Source
word 1

Source
word 2

Source
word 3

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 61Sequence-to-sequence learning: Decoder

Target
word 1

Target
word 2

Target
word 3

Projection

Projection

Projection

Representation of source sentence

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 62Let‚Äôs use a simple NN for machine translation

my

name

is

dragos

Je

m'appelle

dragos

<eos>

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 63Sequence-to-sequence learning

‚Ä¢ Example sentences

‚Äì ‚ÄúMy name is Dragos.‚Äù
‚Äì ‚ÄúMachine translation, sometimes referred to by the abbreviation MT
(not to be confused with computer-aided translation, machine-aided
human translation (MAHT) or interactive translation) is a sub-field of
computational linguistics that investigates the use of software to
translate text or speech from one language to another.‚Äù [Wikipedia]

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 64Vanilla Recurrent Network

Representation of 

source word 1 

Representation of 
source word 1 & 2

Representation of 
source sentence

Embedding

Source
word 1

Embedding

Source
word 2

Embedding

Source
word 3

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 65Recurrent Neural Network

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 66Different RNN units

Vanilla

GRU

LSTM

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 67Encoder Decoder unrolled

Encoder

Decoder

Je

m'appelle

Dragos

<eos>

[.34, .02, .194, ‚Ä¶]

My

name

is

Dragos

<GO>

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 68Encoder Decoder with Attention

Encoder

Decoder

Je

Attention

[.34, .02, .194, ‚Ä¶]

My

name

is

Dragos

<GO>

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 69Encoder Decoder with Attention

Encoder

Decoder

m'appelle

Attention

[.34, .02, .194, ‚Ä¶]

My

name

is

Dragos

<GO>

Je

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 70Encoder Decoder with Attention

Encoder

Decoder

Dragos

Attention

[.34, .02, .194, ‚Ä¶]

My

name

is

Dragos

<GO>

Je

m'appelle

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 71Encoder Decoder with Attention

Encoder

Decoder

<eos>

Attention

[.34, .02, .194, ‚Ä¶]

My

name

is

Dragos

<GO>

Je

m'appelle

Dragos

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 72Convolutional Model for Machine Translation

Kalchbrenner, Nal, et al. "Neural machine translation in linear time." arXiv

preprint arXiv:1610.10099 (2016).

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 73Transformer Model

‚Ä¢ No recurrence
‚Ä¢ No convolution
‚Ä¢ More parallelizable
‚Ä¢ Use self-attention

Vaswani, Ashish, et al. "Attention is all you need." 
Advances in Neural Information Processing Systems. 
2017.

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 74Winograd schema sentences

He didn‚Äôt put the trophy in the suitcase because it was too small.
He didn‚Äôt put the trophy in the suitcase because it was too big.

The cow ate the hay because it was delicious.
The cow ate the hay because it was hungry.

The councilmen refused the demonstrators a permit because they advocated violence.
The councilmen refused the demonstrators a permit because they feared violence.

The animal didn‚Äôt cross the street because it was too tired.
The animal didn‚Äôt cross the street because it was too wide.

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 75Self-Attention for coreference resolution

l

a
m
n
a

t
‚Äô
n
d
d

i

i

s
s
o
r
c

e
h
t

e
s
u
a
c
e
b

t
e
e
r
t
s

 

s
a
 w

t
i

o
o
t

d
e
r
i
t

l

a
m
n
a

t
‚Äô
n
d
d

i

i

s
s
o
r
c

e
h
t

e
s
u
a
c
e
b

t
e
e
r
t
s

 

s
a
 w

t
i

o
o
t

d
e
r
i
t

 

e
h
T

 

e
h
T

 
l

a
m
n
a

t
‚Äô
n
d
d

i

i

 
s
s
o
r
c

e
h
T

e
h
t

 

e
s
u
a
c
e
b

 
t
e
e
r
t
s

 
s
a
 w

t
i

 

o
o
t

e
d
w

i

 
l

a
m
n
a

t
‚Äô
n
d
d

i

i

 
s
s
o
r
c

e
h
T

e
h
t

 

e
s
u
a
c
e
b

 
t
e
e
r
t
s

 
s
a
 w

t
i

 

o
o
t

e
d
w

i

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 76Limitations of NMT

‚Ä¢ Unseen words

‚Äì Sentence: ‚ÄúI had a hamburger for lunch‚Äù
‚Äì The model: ‚ÄúI had a UNK for lunch‚Äù
‚Äì Sentence: "I don't like rollercoasters"
‚Äì The model: "I don't like UNK"

‚Ä¢ Solutions
‚Äì Subword
‚Äì Dictionary ÔÉß Not so simple

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 77Limitations of NMT

‚Ä¢ Subword

‚Äì "ham"+"burger" => "hamburger"
‚Äì "roll" + "er" + "coast" + ers" => "rollercoasters"
‚Äì "d" + "r" + "a" + "g" + "o" + "s" => "Dragos"

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 78Limitations of NMT

‚Ä¢ Resource requirements
‚Äì Large amount of data
‚Äì GPU

‚Ä¢ User constraints: names, numbers, terminology
‚Ä¢ Coverage

‚Äì Dropping translation

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 79Limitations of NMT

‚Ä¢ Neurobabble

‚Äì "if you do not have any questions , please do not

have any questions"

‚Äì "in the middle of the middle of the river , the river

flows into the south of the river"

‚Äì "‚Ä¶ confronting the history of the history of the

history of the history"

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 80Advantages of NMT
‚Ä¢ Example:

‚Äì "Âõ†Ê≠§ÔºåË¶ÅÊîπÂñÑÊú∫Âô®ÁøªËØëÁöÑÁªìÊûúÔºå‰∫∫‰∏∫ÁöÑ‰ªãÂÖ•‰ªçÊòæÁõ∏ÂΩìÈáçË¶Å„ÄÇ"
‚Äì Literal: "Therefore, to improve machine translation results, human

intervention is still very important‚Äú

‚Äì SMT: "Therefore, it is necessary to improve machine translation

results, human intervention is still the video was important."

‚Äì NMT: "Therefore, human intervention is still significant in order to

improve the results of machine translation."

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 81Future Outlook
‚Ä¢ Adaptation
‚Ä¢ Low-resource languages
‚Ä¢ Multi-lingual models
‚Ä¢ Multi-modal models (speech, image, etc.)

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 82QUESTIONS
&

ANSWERS

AMTA 2018 Tutorial: De-mystifying Neural MTBoston, March 17, 2018  | Page 83
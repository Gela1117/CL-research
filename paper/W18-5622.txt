Evaluation of a Sequence Tagging Tool for Biomedical Texts

Julien Tourille1,6, Matthieu Doutreligne1,2, Olivier Ferret3,

Nicolas Paris1,2,4, Aur´elie N´ev´eol1, Xavier Tannier4,5

1LIMSI, CNRS, Universit´e Paris-Saclay, 2WIND-DSI, AP-HP, 3CEA, LIST,

{julien.tourille,matthieu.doutreligne,aurelie.neveol}@limsi.fr,

4Sorbonne Universit´e, 5Inserm, LIMICS, 6Univ. Paris-Sud

olivier.ferret@cea.fr, nicolas.paris@aphp.fr,

xavier.tannier@sorbonne-universite.fr

Abstract

Many applications in biomedical natural lan-
guage processing rely on sequence tagging as
an initial step to perform more complex analy-
sis. To support text analysis in the biomedical
domain, we introduce Yet Another SEquence
Tagger (YASET), an open-source multi pur-
pose sequence tagger that implements state-of-
the-art deep learning algorithms for sequence
tagging. Herein, we evaluate YASET on part-
of-speech tagging and named entity recogni-
tion in a variety of text genres including ar-
ticles from the biomedical literature in En-
glish and clinical narratives in French. To
further characterize performance, we report
distributions over 30 runs and different sizes
of training datasets. YASET provides state-
of-the-art performance on the CoNLL 2003
NER dataset (F1=0.87), MEDPOST corpus
(F1=0.97), MERLoT corpus (F1=0.99) and
NCBI disease corpus (F1=0.81). We believe
that YASET is a versatile and efﬁcient tool that
can be used for sequence tagging in biomedi-
cal and clinical texts.

Introduction

1
Many applications in biomedical Natural Lan-
guage Processing (NLP), including relation ex-
traction or text classiﬁcation, rely on sequence tag-
ging as an initial step to perform more complex
analysis. To support text analysis in the biomed-
ical domain, we present Yet Another SEquence
Tagger (YASET), an open-source multi-purpose
sequence tagger written in Python. YASET aims at
providing NLP researchers with fast and accurate
implementations of cutting-edge deep learning se-
quence tagging models. YASET is built using Ten-
sorFlow (Abadi et al., 2015), an open source soft-
ware library for numerical computation. The code
is licensed under the version 3 of the GNU Gen-

eral Public License1 and is freely available on-
line2. The main contributions of this work are:

• a fast and accurate implementation of
a state-of-the-art sequence tagging model
based on Long Short-Term Memory Net-
works (LSTMs) (Hochreiter and Schmidhu-
ber, 1997). The architecture is similar to the
one described in Lample et al. (2016) and is
able to process mini-batches for faster train-
ing. Furthermore, YASET supports the use
of handcrafted features in combination with
word and character embeddings;

• an easy-to-use interface based on a central
conﬁguration ﬁle that is used to setup exper-
iments, with default parameters that are suit-
able for most sequence tagging tasks;

• an evaluation on various biomedical corpora
and on the CoNLL 2003 corpus, studying the
stability of our model and the effect of train-
ing data size. We compare YASET perfor-
mance with state-of-the-art results published
in the literature.

2 Related Work
Several open-source implementations
se-
quence tagging based on neural network architec-
tures have become available over the last years.
Lample et al. (2016) provide a Python implemen-
tation3 for the two models presented in their pa-
per. They are implemented with Theano (Al-Rfou
et al., 2016), a Python library for deep learning.

for

1https://www.gnu.org/licenses/gpl-3.0.

en.html

2https://github.com/jtourille/yaset
3https://github.com/glample/tagger

Proceedingsofthe9thInternationalWorkshoponHealthTextMiningandInformationAnalysis(LOUHI2018),pages193–203Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics193NeuroNER4 (Dernoncourt et al., 2017) targets
non-expert users and is based on Lample’s Bi-
LSTM model. The authors intended to make the
tool easy to use by providing automatic format
conversion from the brat format (Stenetorp et al.,
2012) to the input format and from the output for-
mat to the brat format. The tool produces sev-
eral plots during training for performance analy-
sis.
It is implemented in Python and makes use
of the TensorFlow library. Both implementations
of the Bi-LSTM model suffer from a very long
training time which makes them cumbersome to
use. YASET offers a faster implementation of the
model by allowing mini-batch training and by us-
ing the pipeline API of TensorFlow.
Rei and Yannakoudakis (2016)

released a
Python implementation5 of different models pre-
sented in their works (Rei and Yannakoudakis,
2016; Rei et al., 2016; Rei, 2017). One major dif-
ference with YASET resides in the possibility to
use a language modeling objective during training.
Recently, Yang and Zhang (2018) introduced
NCRF++6, a tool presented as the neural ver-
sion of CRF++7 and implemented with Py-
Torch (Paszke et al., 2017). The tool is very close
to YASET, with the possibility to deﬁne hand-
crafted word features and to perform nbest decod-
ing.

Another type of neural network model, based
on Convolutional Neural Networks (CNNs) for
character level representation, is presented in Ma
and Hovy (2016). The authors implemented the
architecture with PyTorch. The code is freely
available online8. The same type of architecture
is implemented in the tool released by Reimers
and Gurevych (2017).
It is implemented with
Keras (Chollet et al., 2015) and is freely available
online9.

Outside the peer-reviewed scientiﬁc environ-
ment, many other implementations are freely
available online. However, we will not review
them in this paper.

4https://github.com/

Franck-Dernoncourt/NeuroNER

5https://github.com/marekrei/

sequence-labeler

6https://github.com/jiesutd/NCRFpp
7https://taku910.github.io/crfpp
8https://github.com/XuezheMax/

NeuroNLP2

9https://github.com/UKPLab/

emnlp2017-bilstm-cnn-crf

3 Neural Network Model
There is currently one neural network model im-
plemented in YASET. This model is mostly based
on Lample et al. (2016). However, similar ar-
chitectures are presented in other work (Collobert
et al., 2011; Ma and Hovy, 2016; Rei and Yan-
nakoudakis, 2016; Rei et al., 2016). Other network
architectures will be implemented in the future.

3.1 Main Component
Our approach relies on LSTMs. The architecture
of our model is presented in Figure 1. For a given
sequence of tokens, represented as vectors, we
compute representations of left and right contexts
of the sequence at every token. These represen-
tations are computed using two LSTMs (forward
and backward LSTM in Figure 1).

Figure 1:
YASET neural network architecture
overview. The example is extracted from the THYME
corpus (Styler IV et al., 2014) used during the shared
tasks Clinical TempEval (Bethard et al., 2015, 2016,
2017). One of the objectives was to extract medical
events. In the example, the event Surgery is marked as
a medical event with the type N/A.

These representations are concatenated and
passed through a tanh activation layer whose size
is equal to the size of the concatenated vector.

Finally, the output of the last layer is linearly
projected to a n-dimensional vector representing
the number of categories. Following previous
work (Dernoncourt et al., 2017; Lample et al.,
2016; Ma and Hovy, 2016), we add a ﬁnal Condi-
tional Random Field (CRF) layer to take into ac-
count the previous label during training and pre-
diction.

194Input Embeddings

3.2
Vectors representing tokens are built by concate-
nating a character-based embedding, a word em-
bedding and one embedding per feature.

An overview of the embedding computation is
presented in Figure 2. Following Lample et al.
(2016), the character-based representation is com-
puted with a Bi-LSTM whose parameters are de-
ﬁned by users. First, a random embedding is
generated for every character in the training cor-
pus. Token characters are then processed with a
forward and backward LSTM architecture. The
ﬁnal character-based representation results from
the concatenation of the forward and backward
representations. The character-based representa-
tion is then concatenated to a pre-trained word-
embedding and one embedding per feature. We
apply dropout on the ﬁnal input embedding.

Figure 2: YASET input embedding computation
overview. Embeddings result from the concatenation
of a pre-trained word embedding, a character-level rep-
resentation of the token and one embedding per cate-
gorical feature.

4 Tool Overview
In this section, we present a general overview of
YASET. First we describe input data formats (se-
quences and pre-trained word embeddings). Then
we present the input pipeline, the network training
phase, the implemented evaluation metrics and the
management of its parameters.

Input and Output Data

4.1
YASET takes CoNLL-like10 formatted ﬁles as in-
put. Sequences are separated by empty lines and
there must be one token per line. For each token,

10http://universaldependencies.org/

docs/format.html

the ﬁrst and last columns are reserved namely for
the token itself and the token label. Each token
must have a label.

Users can add several categorical features. Fea-
ture columns must be speciﬁed in the conﬁgura-
tion ﬁle by providing their indexes. Besides the
ﬁrst and last columns, and the feature columns,
users can add other columns. They will be ignored
by the system.

YASET can take training and development ﬁles
as input but can also create development instances
if there is no development ﬁle provided by users.
In this case, users can specify the train/dev split ra-
tio and may specify a random seed for experiment
reproducibility.

Train and development instances consistency is
checked upon startup. Each label and feature val-
ues from the development instances must appear
in the train instances. An example of YASET in-
put ﬁle format is presented in Figure 3.

In prediction mode,

test data is supplied in
the same format, without the token label column,
which will be added with the predicted labels.
...
Lien NNP I-NP I-PER
. . O O

China NNP I-NP I-LOC
says VBZ I-VP O
time NN I-NP O
right RB I-ADVP O
for IN I-PP O
Taiwan NNP I-NP I-LOC
talks NNS I-NP O
. . O O

BEIJING VBG I-VP I-LOC
1996-08-22 CD I-NP O
...

Figure 3: Example extracted from the CoNLL 2003
shared task corpus (Sang and Meulder, 2003). For each
token (col. #1), there is a label (col. #4, IOB format)
and two categorical features (cols. #2 and #3), the part-
of-speech tag and a chunk label (IOB format).

in

supports

embeddings

4.2 Word Embeddings
YASET
the
word2vec (Mikolov et al., 2013b) or Gen-
sim ( ˇReh˚uˇrek and Sojka, 2010) formats. Other
formats of embeddings, for instance FastText (Bo-
janowski et al., 2017) or Glove (Pennington et al.,
2014), must ﬁrst be converted to either accepted
format.

195Out Of Vocabulary (OOV) tokens can be ad-
dressed by two strategies. In the ﬁrst one, users
provide a vector for OOV tokens.
In this case,
the vector is expected to be part of the provided
word embedding matrix and users must specify the
vector ID. In the second strategy, we follow the
methodology described in Lample et al. (2016).
We insert a randomly initialized vector in the em-
bedding matrix that will be used for OOV tokens.
Then we replace singletons in the training corpus
by the OOV token with a probability of p, which
is deﬁned by users. Word embeddings can be ﬁne-
tuned during training in both cases by setting the
appropriate ﬂag to true in the conﬁguration ﬁle.

Input Pipeline

4.3
Minimizing the memory footprint was one of the
core objectives during the development of the tool.
YASET leverages the pipeline API of TensorFlow
to build a lightweight and fast input pipeline. In-
put instances are stored in the binary TensorFlow
format. This avoids the need to store the whole
dataset in memory. Mini-batches are extracted
randomly and fed to the network.

Training instances can be bucketized according
to their lengths. This possibility can speedup train-
ing with large corpora. Buckets boundaries are
automatically computed. To assert effective ran-
domization during training, buckets will contain
enough instances for several mini-batches.

4.4 Neural Network Training
YASET alternates between two phases during
training. In the iteration phase, mini-batches are
extracted from the input pipeline and fed to the
neural network model. Optimization is performed
according to the parameters set by users in the con-
ﬁguration ﬁle.

At the end of each iteration, YASET enters the
evaluation phase where the current model state is
used to make predictions on the development in-
stances. The performance score is logged for fur-
ther analysis and a snapshot of the model is kept
if the performance score is the best obtained so far
on the development instances. Since model snap-
shots can take a lot of memory, we only keep the
best one, i.e. the one that performs best on the de-
velopment instances.

Network training is performed via back-
propagation. Users can select the neural network
model architecture (only one available for now),

the maximum number of iterations n and the pa-
tience criterion p. Training will stop if the max-
imum number n is reached or if there are p iter-
ations without performance improvement on the
development instances.

Users can also set several parameters related to
the learning algorithm such as the initial learning
rate, and gradient clipping and exponential decay
factors. Finally, users can select the mini-batch
size, the number of CPU cores to use and the eval-
uation metric.

Another set of parameters are related to the neu-
ral network model. These parameters allow to se-
lect the dropout rate and the different hidden layer
and embedding sizes.

4.5 Metrics
Model performance is measured on the develop-
ment instances at the end of each iteration. Two
metrics are currently implemented in YASET: ac-
curacy and CoNLL. The former measures the frac-
tion of correctly predicted labels among all the
predicted labels. The latter is an entity-based met-
ric which outputs precision, recall and F1-measure
scores. Precision measures the fraction of cor-
rectly predicted entities among all predicted enti-
ties. Recall assesses the fraction of correctly pre-
dicted entities among all the entities that should
have been detected. F1-measure is the harmonic
mean of precision and recall. The implementa-
tion of the CoNLL metric is inspired by the of-
ﬁcial evaluation script used during the CoNLL-
2003 shared task (Sang and Meulder, 2003) and
by the Python portage of the script by Sampo
Pyysalo11. In the case of performance evaluation
with the CoNLL metric, token labels must follow
either the IOB, IOBE or IOBES tagging scheme12.

4.6 Parameters
YASET parameters are fully customizable and
centralized in one conﬁguration ﬁle which is used
to setup experiments. YASET also targets end-
users from a broader community by providing hy-
perparameter value suggestions and insights on
how to choose them for various sequence-tagging
situations.

11https://github.com/spyysalo/

conlleval.py

12Inside, Outside, Begin, End, Single

1965 Experiments
We demonstrate the performance of YASET by
applying the model to four different corpora se-
lected to cover a variety of languages, text gen-
res, sequence types and annotation densities. We
focus our effort on biomedical texts, using Med-
Post (Smith et al., 2004), a corpus of biomedi-
cal abstracts annotated with Part-of-Speech (PoS)
tags,
the NCBI Disease corpus (Islamaj Do-
gan and Lu, 2012), a dataset of biomedical ab-
stracts annotated with disease related entities and
MERLoT (Campillos et al., 2018), a corpus of
clinical documents written in French which were
annotated with two different Named Entity Recog-
nition (NER) tag sets (Protected Health Informa-
tion (PHI) and biomedical entities).

We also apply YASET on the CoNLL 2003 En-
glish NER corpus (Sang and Meulder, 2003), a
classic benchmark corpus for NER in the general
domain.

5.1 Presentation of the Corpora
We use the corpus partition provided with the
dataset distributions when available. For NCBI
and CoNLL 2003, models are trained on the train
sets and evaluated on the test sets while the devel-
opment sets are used to determine early stopping.
For MERLoT and MedPost, partition details are
outlined below. The code used to preprocess and
analyze the corpora is available online13.

The MedPost corpus is a collection of tok-
enized MEDLINE abstracts annotated with PoS
It contains 6,701 sentences (≈182,000 to-
tags.
kens). The corpus is divided in 13 ﬁles of differ-
ent sizes, from which we extracted one ﬁle to serve
as development set (tag mb.ioc). The tag set con-
tains originally 63 unique entities that we grouped
in a coarser set of 51 entities. Grouping affected
punctuation signs, which were assigned a unique
PUNCT tag.

The NCBI corpus contains 793 MEDLINE ab-
stracts with 11,350 annotations of diseases and
disease modiﬁers. There are 4 different entities.
Train and development sets contain 6,594 sen-
tences (≈151,000 tokens).

MERLoT is a French clinical corpus with sev-
eral levels of annotations.
It contains 500 med-
ical reports with a total of 25,087 sentences
(≈177,000 tokens). We used it for two tasks. The

13https://github.com/strayMat/

bio-medical_ner

ﬁrst one is de-identiﬁcation with a tag set com-
prising 11 types of PHI (e.g. names, dates). The
second one is medical NER with 19 entity classes
(e.g. anatomy, disorder).

The medical entity annotations include nested
entities. Because this study aims to experiment
on a variety of datasets using the same model, we
did not attempt to extract several levels of nested
entities. When nested entities occur, our experi-
ments only addressed the outer layer correspond-
ing to the largest entity. For example, the mention
“cancer du sein” (breast cancer) was originally
annotated with one DISORDER entity (cancer du
sein, breast cancer) and one ANATOMY entity
(sein, breast). Nested entity removal reduced the
annotations to only one tag per token and in this
case, the DISORDER annotation was kept while
the ANATOMY annotation was removed. In to-
tal, 5,218 entities (8.4% of the complete set) were
pruned. For our experiments, we also removed
the headers and footers of the documents, which
were not available to annotators working towards
the gold standard and could cause ambiguities
with some entity classes (e.g. person or hospi-
tal). This results in a smaller corpus, compared to
the corpus used for de-identiﬁcation experiments
(≈123,000 tokens).

The CoNLL 2003 corpus is a common dataset
for evaluating NER algorithms. It is based on the
Reuters corpus (Lewis et al., 2004) and is the only
dataset outside the biomedical domain used in our
experiments. The training and development set
together contain 18,451 sentences (≈256,000 to-
kens) annotated with 4 unique entities.

A detailed overview of the corpus characteris-

tics is available in Table 1.

5.2 Experimental Setup
In this section, we present the experimental setup
used for our experiments. First, we describe how
we selected the hyper-parameters. Then, we detail
the pre-trained word embeddings that were used in
this work. Finally, we present the neural network
training parameters.
5.2.1 Hyper-parameter Selection
We used Hyperopt14 (Bergstra et al., 2011, 2013)
to select a set of hyper-parameters that performed
well on the MERLoT de-identiﬁcation dataset.
Due to heavy computation times, we re-used this

14http://hyperopt.github.io/hyperopt/

197Corpus
Name: MedPost
Task: POS
Domain: MEDLINE abstracts
Name: NCBI disease
Task: NER
Domain: MEDLINE abstracts
Name: MERLoT medical
Task: NER
Domain: Medical reports from the
Hepato-gastro-enterology and Nutrition
ward

Name: MERLoT de-identiﬁcation
Task: NER
Domain: Same as MERLoT medical
Name: CoNLL 2003
Task: NER
Domain: News articles from the Reu-
ters corpus

# sent.
6,701

# tok.
182,319

# ann.
182,319
(100%)

Entities
51 POS tagging detailed in Smith et al. (2004)

7,279

151,005

11,350
(7.5%)

DiseaseClass, SpeciﬁcDisease, Composite-
Mention, Modiﬁer

5,137

123,942

56,680
(46%)

25,599

177,158

18,451

256,145

31,723
(18%)

42,646
(17%)

Concept Idea, MedicalProcedure, Hospital,
Persons, Temporal, BiologicalProcessOrFunc-
tion, Devices, Measurement, Disorder, As-
pect, Chemicals Drugs, Dosage, SignOrSymp-
tom, Anatomy, Localization, Livingbeings,
Strength, AdministrationRoute, Drugform
ﬁrst name, last name, initials, address, zip code,
town, date, hospital, identiﬁer, phone number,
email
PER, ORG, LOC, MISC

Table 1: Overview of the corpora. The number of annotations corresponds to the number of annotated tokens to
be comparable to the size of the corpus measured in number of tokens

set of hyper-parameters on the other datasets. Pa-
rameter search space includes the number of units
of the character Bi-LSTM, the number of units
of the main Bi-LSTM, the dimension of the char-
acter embeddings and the dropout rate. The re-
tained setup uses character embeddings of size 24,
32 units for the character-level Bi-LSTM, 64 units
for the main Bi-LSTM and a dropout rate of 0.5.
5.2.2 Embeddings
Pre-trained word embeddings were shown to boost
the performance in various NLP tasks (Collobert
et al., 2011; Mikolov et al., 2013a) and speciﬁcally
in NER (Lample et al., 2016; Dernoncourt et al.,
2017).

For each corpus, we used pre-trained word em-
beddings created with datasets that were consis-
tent in genre and domain with the corpora used
in this work. All word embedding models were
computed using word2vec. For CoNLL 2003, we
trained a model on Google News. For NCBI dis-
ease and MedPost, which both comprise abstracts
from biomedical articles, we trained a model on
the PubMed corpus. For MERLoT, we trained a
model on the corpus from which MERLoT is ex-
tracted.
It contains about 138,000 clinical notes
written in French (Campillos et al., 2018).

For each of these word vector models, we com-
puted low-dimensional representations by apply-
ing Principal Component Analysis (PCA) on the
original high-dimensional word vectors (300 di-

mensions). During hyper-parameter search, we
varied the dimension of the word embeddings and
observed important impacts on the performance
(up to 2.5 points of F1). We ﬁnally picked word
vectors of dimension 25 (reduced by PCA), which
showed to be the most efﬁcient. This choice im-
proved the scores as well as diminished drastically
the computation times.

We also ran a few experiments using Fast-
Text
(Bojanowski et al., 2017) embeddings
trained on Wikipedia.
Preliminary results on
CoNLL 2003 show a major performance improve-
ment. An analysis of the impact of the word em-
bedding model on performance will be conducted
in following work.
5.2.3 Training
In all our experiments, the network was trained us-
ing the Adam optimizer (Kingma and Ba, 2015)
with an initial learning rate of 0.001. Early-
stopping was set to 10 epochs.

6 Results
In this section, we present the performance ob-
tained on the corpora using the experimental setup
described in the previous section. First, we report
corpus-speciﬁc results. Then, we study the impact
of the corpus size on performance.

198Figure 4: Distributions of F1 scores for the four corpora over 30 trainings for each corpus. Vertical lines refer to
each sample. The scales are speciﬁc to each dataset for a proper visualization of each distribution.

6.1 Corpus Speciﬁc Results
Reimers and Gurevych (2017) report that neu-
ral network model
training is highly non-
deterministic and is subject to the random seed
choice. Because of this variability during the train-
ing phase, it is crucial to report results on numer-
ous trainings. Therefore we ran 30 experiments
for each task presented in this work. We report F1-
score statistics in Table 2. We also plot F1-score
distributions for each task in Figure 4.

Our experiments show that performance varies
across datasets, reﬂecting the heterogeneous difﬁ-
culties of the tasks, inherent to the nature of the
labels and the quality of the annotations. We no-
tice that the standard deviations are similar for all
NER tasks. It suggests that the variability of the
score is independent from the task and mainly due
to the model architecture. Previous work perfor-
mances are reported in Table 3 for every dataset.

Dataset

Task
NER NCBI disease
NER MERLoT medical
NER CoNLL 2003
POS MedPost
NER MERLoT de-identiﬁcation

Mean F1

81.33
82.87
87.31
97.83
99.40

σ
0.83
0.36
0.41
0.39
0.14

Max. Diff.
3.27 (4.1%)
1.40 (1.7%)
1.76 (2.0%)
1.44 (1.5%)
0.568 (0.57%)

Table 2: Performance (%) of YASET on the 4 datasets.

Although our model does not show state-of-the-art
performances for all of them, it obtains competi-
tive results, demonstrating the generalization abil-
ity of the selected shared set of hyper-parameters
and embeddings.

Dataset

NCBI

MERLoT med.a

CoNLL 2003

MedPosta

MERLoT PHI

Model
Dang et al. (2018)
Islamaj Dogan and Lu (2012)
This paper
Campillos et al. (2018)
This paper
Lample et al. (2016)
Ma and Hovy (2016)
Yang and Zhang (2018)
Peters et al. (2018)
This paper
Smith et al. (2004)
This paper
Grouin and N´ev´eol (2014)
This paper

F1
84.41
81.80
81.33
81.40
82.87
90.94
91.21
91.35
92.22
87.31
97.43
97.83
94.00
99.40

a Corpora where the experimental set-up differed between
our experiments and that of prior work. For MEDPOST,
we used 51 categories instead of 63; for MERLoT med.
we removed nested entities.

Table 3: State of the art results obtained in prior work
on the datasets.

199Figure 5: Effect of the training set size on the F1 score for the MERLoT dataset with de-identiﬁcation annotations.
These scores are computed over 6 training iterations per chunk. Vertical bars show the standard deviation σ.

6.2 Performance According to Training Data

Size

The development of a labeled dataset for training
annotation models is often a heavy investment in
time and resources. Having some insight on the
performance of the model for different training
data sizes is crucial. Thus, we investigate the im-
pact of this parameter on model performance. Fo-
cusing on the de-identiﬁcation task of MERLoT,
we split the training set into 20 subsets of equal
sizes. Then, we built sub-training sets of grow-
ing size, that we refer as chunks. This resulted in
20 chunks where small chunks are subsets of the
bigger one. Finally, we train YASET 6 times on
each chunk, measuring how the performance im-
proves with the size of the chunks. Figure 5 shows
the progression of the F1-score according to the
number of tokens. Table 4 presents the model per-
formance according to the dataset size.

Dataset

MERLoT PHI

5%
91.79
(0.55)

10% 25% 50% 100%
95.86
99.06
(0.19)
(0.47)

97.60
(0.30)

98.41
(0.33)

Table 4: Performance (F1) against train set size (as per-
centage of the total training set indicated on the ﬁrst
row). Standard errors appear between parentheses.

We observe that the performance improves log-
arithmically with every chunk added in the train-
ing data as shown in Table 4. This ﬁnding is sim-
ilar to the observation of Sun et al. (2017) for vi-
sion tasks. Further addition of data will slightly
improve the performance as the maximum perfor-
mance plateau is almost reached.

7 Conclusion and Future work
We propose an easy-to-use annotation tool im-
plementing a state-of-the-art Bi-LSTM-CRF neu-
ral architecture. We apply the tool to PoS tag-
ging and NER on clinical, biomedical and gen-
eral domain texts. By running multiple experi-
ment for each dataset, we conﬁrm previous obser-
vations that neural network training is highly non-
deterministic and dependent on the random seed
choice.

Although we did not search for optimal hyper-
parameters for each task, we obtain high perfor-
mance in all our experiments, suggesting that tai-
loring hyper-parameters for a speciﬁc task im-
proves only lightly the performance of the model
and the neural network architecture implemented
in YASET is robust with regards to the perfor-
mance obtained.

Concerning the impact of the training dataset
size on model performance, we conﬁrm the intu-
ition that adding more data allows to improve the
performance of neural network models. However,
result analysis suggests that the growth is logarith-
mic with the training data size.

One limitation of our model is its inability to di-
rectly handle nested entities such as those found in
the MERLoT medical dataset and commonly used
in the biomedical and clinical domains. When
ﬁltering these nested entities, speciﬁc classes are
heavily impacted, including anatomy, disorder, lo-
calization, and medical procedure entities. Sev-
eral strategies have been proposed to handle such
cases.
Campillos et al. (2018) present a 3-
layer CRF model that annotates different non-

200overlapping clinical entities at each layer. Ju et al.
(2018) present a dynamic end-to-end neural net-
work model capable of handling an undetermined
number of nesting levels. Katiyar and Cardie
(2018) model the task as an hypergraph whose
structure is learned with an LSTM network.

Future research will focus on the inﬂuence of
word embedding models which were shown to sig-
niﬁcantly impact on performance. Speciﬁcally,
models taking into account sub-token informa-
tion (Bojanowski et al., 2017) or emphasizing con-
text (Peters et al., 2018) should be further ex-
plored. Moreover, other neural network models
for NER such as the ones proposed by Rei et al.
(2016) and Ma and Hovy (2016) will be investi-
gated and implemented in YASET. Having a cen-
tralized implementation of different NER models
will allow us to compare their performances on
various corpora.

Acknowledgements
The authors thank the Biomedical Informatics De-
partment at the Rouen University Hospital for
providing access to the LERUDI corpus for this
work. This work was supported in part by the
French National Agency for Research under grant
CABeRneT ANR-13-JS02-0009-01 and by Labex
Digicosme, operated by the Foundation for Scien-
tiﬁc Cooperation (FSC) Paris-Saclay, under grant
C ˆOT.

References
Mart´ın Abadi, Ashish Agarwal, Paul Barham, et al.
2015. TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems.

Rami Al-Rfou, Guillaume Alain, Amjad Almahairi,
et al. 2016. Theano: A Python framework for fast
computation of mathematical expressions. CoRR.

James Bergstra, Daniel Yamins, and David Cox. 2013.
Making a Science of Model Search: Hyperparame-
ter Optimization in Hundreds of Dimensions for Vi-
In Proceedings of the 30th In-
sion Architectures.
ternational Conference on Machine Learning, pages
115–123.

James S. Bergstra, R´emi Bardenet, Yoshua Bengio, and
Bal´azs K´egl. 2011. Algorithms for Hyper-Parameter
In Advances in Neural Information
Optimization.
Processing Systems 24, pages 2546–2554.

Steven Bethard, Leon Derczynski, Guergana Savova,
James Pustejovsky, and Marc Verhagen. 2015.

SemEval-2015 Task 6: Clinical TempEval. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, pages 806–814. Association for
Computational Linguistics.

Steven Bethard, Guergana Savova, Wei-Te Chen, et al.
2016. SemEval-2016 Task 12: Clinical TempEval.
In Proceedings of the 10th International Workshop
on Semantic Evaluation, pages 1052–1062. Associ-
ation for Computational Linguistics.

Steven Bethard, Guergana Savova, Martha Palmer,
and James Pustejovsky. 2017. SemEval-2017 Task
In Proceedings of the
12: Clinical TempEval.
11th International Workshop on Semantic Evalua-
tion, pages 565–572. Association for Computational
Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. Transactions of the Associa-
tion of Computational Linguistics, 5:135–146.

Leonardo Campillos, Louise Del´eger, Cyril Grouin,
et al. 2018. A French Clinical Corpus with Com-
prehensive Semantic Annotations: Development of
the Medical Entity and Relation LIMSI annOtated
Text corpus (MERLOT). Language Resources and
Evaluation, 52(2):571–601.

Franc¸ois Chollet et al. 2015. Keras. https://

github.com/fchollet/keras.

Ronan Collobert, Jason Weston, L´eon Bottou, et al.
2011. Natural Language Processing (Almost) from
Journal of Machine Learning Research,
Scratch.
12:2493–2537.

Thanh Hai Dang, Hoang-Quynh Le, Trang M Nguyen,
and Sinh T Vu. 2018. D3NER: biomedical named
entity recognition using CRF-biLSTM improved
with ﬁne-tuned embeddings of various linguistic in-
formation. Bioinformatics.

Franck Dernoncourt, Ji Young Lee, ¨Ozlem Uzuner, and
Peter Szolovits. 2017. De-identiﬁcation of Patient
Journal
Notes with Recurrent Neural Networks.
of the American Medical Informatics Association,
24(3):596–606.

Cyril Grouin and Aur´elie N´ev´eol. 2014.

De-
identiﬁcation of clinical notes in French: towards a
protocol for reference corpus development. Journal
of Biomedical Informatics, 50:151–161.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Rezarta Islamaj Dogan and Zhiyong Lu. 2012. An
improved corpus of disease mentions in PubMed
citations. Proceedings of the 2012 Workshop on
Biomedical Natural Language Processing, pages
91–99.

201Meizhi Ju, Makoto Miwa, and Sophia Ananiadou.
2018. A Neural Layered Model for Nested Named
In Proceedings of the 2018
Entity Recognition.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1446–1459. Associ-
ation for Computational Linguistics.

Arzoo Katiyar and Claire Cardie. 2018. Nested Named
Entity Recognition Revisited. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 861–871. Asso-
ciation for Computational Linguistics.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
In Proceed-
Method for Stochastic Optimization.
ings of the 3rd International Conference on Learn-
ing Representations.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270. Association for Computational Lin-
guistics.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A New Benchmark Collection for
Text Categorization Research. Journal of Machine
Learning Research, 5:361–397.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end Se-
quence Labeling via Bi-directional LSTM-CNNs-
In Proceedings of the 54th Annual Meet-
CRF.
ing of the Association for Computational Linguis-
tics, pages 1064–1074. Association for Computa-
tional Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient Estimation of Word Repre-
sentations in Vector Space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Adam Paszke, Sam Gross, Soumith Chintala, et al.
In

2017. Automatic differentiation in PyTorch.
NIPS-W.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
In Proceedings of the 14th
Word Representation.
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 1532–
1543. Association for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, et al.
2018. Deep Contextualized Word Representations.
In Proceedings of the 2018 Conference of the North

American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 2227–2237. Association for Computa-
tional Linguistics.

Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50. Eu-
ropean Language Resources Association.

Marek Rei. 2017. Semi-supervised Multitask Learning
for Sequence Labeling. In Proceedings of the 55th
Conference of the Association for Computational
Linguistics, pages 2121–2130. Association for Com-
putational Linguistics.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to Characters in Neural Sequence La-
In Proceedings of the 26th Inter-
beling Models.
national Conference on Computational Linguistics,
pages 309–318.

Marek Rei and Helen Yannakoudakis. 2016. Com-
positional Sequence Labeling Models for Error De-
In Proceedings of the
tection in Learner Writing.
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1181–1191. Association
for Computational Linguistics.

Nils Reimers and Iryna Gurevych. 2017. Reporting
Score Distributions Makes a Difference: Perfor-
mance Study of LSTM-networks for Sequence Tag-
In Proceedings of the 2017 Conference on
ging.
Empirical Methods in Natural Language Process-
ing, pages 338–348.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In Proceedings of the 7th Conference on Nat-
ural Language Learning. Association for Computa-
tional Linguistics.

Laurence H. Smith, Thomas C. Rindﬂesch, and
W. John Wilbur. 2004. MedPost:
a Part-of-
speech Tagger for BioMedical Text. Bioinformatics,
20(14):2320–2321.

Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, et al.
2012. brat: a Web-based Tool for NLP-Assisted
In Proceedings of the 13th Con-
Text Annotation.
ference of the European Chapter of the Association
for Computational Linguistics, volume Demonstra-
tion Papers, pages 102–107. Association for Com-
putational Linguistics.

William F. Styler IV, Steven Bethard, Sean Finan, et al.
2014. Temporal Annotation in the Clinical Domain.
Transactions of the Association for Computational
Linguistics, 2:143–154.

Chen Sun, Abhinav Shrivastava, Saurabh Singh, and
Abhinav Gupta. 2017. Revisiting Unreasonable Ef-
fectiveness of Data in Deep Learning Era. CoRR,
abs/1707.02968.

202Jie Yang and Yue Zhang. 2018.

NCRF++: An
Open-source Neural Sequence Labeling Toolkit. In
Proceedings of ACL 2018, System Demonstrations,
pages 74–79. Association for Computational Lin-
guistics.

203
A Reinforcement Learning-driven Translation Model for Search-Oriented

Conversational Systems

Wafa Aissa

Sorbonne Universit´e

CNRS, LIP6

F-75005 Paris, France
wafa.aissa@lip6.fr

Laure Soulier

Sorbonne Universit´e

CNRS, LIP6

F-75005 Paris, France
laure.soulier@lip6.fr

Ludovic Denoyer
Sorbonne Universit´e

CNRS, LIP6

F-75005 Paris, France

ludovic.denoyer@lip6.fr

Abstract

Search-oriented conversational systems rely
on information needs expressed in natural lan-
guage (NL). We focus here on the understand-
ing of NL expressions for building keyword-
based queries. We propose a reinforcement-
learning-driven translation model framework
able to 1) learn the translation from NL ex-
pressions to queries in a supervised way, and,
2) to overcome the lack of large-scale dataset
by framing the translation model as a word se-
lection approach and injecting relevance feed-
back as a reward in the learning process.
Experiments are carried out on two TREC
datasets. We outline the effectiveness of our
approach.
Introduction

1
Artiﬁcial Intelligence, and more particularly deep
learning, have recently opened tremendous per-
spectives for reasoning over semantics in text-
based applications such as machine translation
(Lample et al., 2017), chat-bot (Bordes and We-
ston, 2016), knowledge base completion (Lin
et al., 2015) or extraction (Hoffmann et al., 2011).
Very recently, conversational information retrieval
(IR) has emerged as a new paradigm in IR (Burt-
sev et al., 2017; Joho et al., 2018), in which natu-
ral conversations between humans and computers
are used to satisfy an information need. As for
now, conversational systems are limited to sim-
ple conversational interactions (namely, chit-chat
conversations) (Li et al., 2016; Ritter et al., 2011),
closed worlds driven by domain-adapted or slot-
ﬁlling patterns (Bordes and Weston, 2016; Wang
and Lemon, 2013) (e.g., a travel planning task re-
quiring to book a ﬂight, then a hotel, etc...), or
knowledge-base extraction (e.g., information ex-
traction tasks) (Dhingra et al., 2017).

In contrast, search-oriented conversational sys-
tems (SOCS) aim at ﬁnding information in

an open world (both unstructured information
sources and knowledge-bases) in response to
users’ information needs expressed in natural lan-
guage (NL); the latter often being ambiguous.
Therefore, one key challenge of SOCS is to under-
stand users’ information needs expressed in NL to
identify relevant documents.

Formulating an information need through
queries has been outlined as a difﬁcult
task
(Vakulenko et al., 2017; Agichtein et al., 2006;
Joachims, 2002) which is generally tackled
by reﬁning/reformulating queries using pseudo-
relevance feedback or users’ clicks.
In SOCS,
there is an upstream challenge dealing with the
building of the query from a NL expression that
initiates the search session to avoid useless users’
interactions with the system. This problem could
be tackled for instance through deep neural trans-
lation models (e.g., encoder-decoder approaches)
as initiated by (Song et al., 2017; Yin et al., 2017).
However, these methods learn the query formu-
lation model independently of the search task at
hand. To overpass this limitation, (Nogueira and
Cho, 2017) have proposed a reinforcement learn-
ing model for query reformulation in which the re-
ward is based on terms of documents retrieved by
the IR system.

In this work, we propose to bridge these two
lines of work: 1) machine translation to learn the
mapping between information needs expressed
in NL and information needs formulated using
keywords (Song et al., 2017; Yin et al., 2017),
and 2) reinforcement learning to inject the task
objectives within the machine translation model
(Nogueira and Cho, 2017). More particularly,
we propose a two-step model which ﬁrst learns
the translation model through the supervision of
NL-query pairs and then reﬁnes the translation
model using a relevance feedback provided by

Proceedingsofthe2018EMNLPWorkshopSCAI:The2ndInt’lWorkshoponSearch-OrientedConversationalAI,pages33–39Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticsISBN978-1-948087-75-933Figure 1: Overview of our reinforcement learning-driven translation model for SOCS

It is worth mentioning that
the search engine.
there does not exist SOCS-oriented dataset that
both aligns users’ information needs in NL with
keyword-based queries and includes a document
collection to perform a retrieval task. To the best
of our knowledge, TREC datasets are the only
ones expressing such constraint, but the number
of NL-query pairs is however limited. To ﬁt with
the issue of dealing with large vocabulary and the
dataset constraint, we frame the translation model
as a word selection one which aims at identifying
which words in the NL expression can be used
to build the query. Our model is evaluated on
two TREC datasets. The obtained results outline
the effectiveness of combining reinforcement
learning with machine translation models.

The remaining of the paper is organized as fol-
lows. Section 2 details our translation model. Sec-
tion 3 presents the evaluation protocol and results
are highlighted in Section 4. The conclusion and
perspectives are discussed in Section 5.

2 Reinforcement learning-driven

translation model

reinforcement

2.1 Notation and problem formulation
Our
learning-driven translation
model allows to formulate a user’s information
need x expressed in NL into a keyword-based
query y. The user’s information need x is a se-
quence of n words (x = x1, ..., xi, ..., xn). To
ﬁt with our word selection objective, the query
y is modeled as a binary vector y ∈ {0, 1}n
of size n (namely, the size of the natural lan-
guage expression x). Each element yj ∈ y equals
to 1 if word xi ∈ x exists in query y and 0
otherwise. For example, if we consider the NL
as ”Identify documents that discuss sick build-
ing syndrome or building related illnesses.” and

the key-words query as ”sick building syndrome.”,
the expected query will be formulated as follows:
y = (0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0) .

The objective of our model fθ (with θ being the
parameters of our model) is to estimate the prob-
ability p(y|x) of generating the binary vector y
given the NL expression x. Since terms are not
independent within the formulation of NL expres-
sions and queries, it makes sense to consider that
the selection of a word is conditioned by the se-
quence of decisions taken on previous words y<i.
Thus, P (y|x) could be written as follows:

p(y|x) =

p(yi|y<i, x)

(1)

(cid:89)

yi∈y

This probability is ﬁrst learned using a maxi-
mum likelihood estimation (MLE) on the basis of
NL-query pairs (Section 2.2). Then, this proba-
bility is reﬁned using reinforcement learning tech-
niques (Section 2.3). We end up with the network
architecture used in the translation model.

2.2 Supervised translation model: from NL

to queries

The translation model works as a supervised word
selection model aiming at building queries y by
using the vocabulary available in NL expressions
x. To do so, we use a set D of N NL-query pairs
D = {(x1, y1), ..., (xk, yk), ..., (xN , yN )}.

i |ˆyk

i = yk

<i, xk) that the ith element ˆyk

The objective of the translation model is to pre-
in the NL expres-
dict whether each word xk
i
sion xk is included in the expected query yk. In
other words, it consists in predicting the probabil-
ity p(ˆyk
i of
in the
vector ˆyk is equal to the same element yk
i
original query yk (namely, that ˆyk
i ) given
i = yk
<i and the NL ex-
the state of previous elements ˆyk
i |ˆyk
pression xk. This probability p(ˆyk
<i, xk)
is modeled using a Bernoulli distribution in which
parameters are estimated through the probability
distribution.

i = yk

34Figure 2: Network architecture of our translation model

f(θ,xk) = (cid:80)

i ∈yk log(p(ˆyk
yk

Let’s deﬁne for a NL-query instance (xk, yk),
<i, xk)). The
translation model is trained by maximizing the fol-
lowing MLE over the set D of NL-query pairs
(xk, yk):

i = yk

i |ˆyk

LSM T =

log(f (θ, xk))

(2)

(cid:88)

(xk,yk)∈D

2.3 Reinforcement learning
To inject
the task objective in the translation
model, we consider that the process of query
building could be enhanced through reinforcement
learning techniques. Therefore, the word selection
could be seen as a sequence of choices of select-
ing word xt at each time step t. The choices are
rewarded at the end of the selection process by
a metric measuring the effectiveness of the query
building process within a retrieval task. Particu-
larly, the predicted query ˆy obtained from the bi-
nary vector ˆy is fed to a retrieval model to rank
documents. For each NL expression x (and ac-
cordingly the associated predicted query ˆy), we
dispose of a set Dx of relevant documents (also
called ground truth). We note GT the set of n pairs
(x;Dx). With this in mind, the effectiveness of
the obtained ranking could be estimated using an
effectiveness-driven metric (e.g., the MAP). Thus,
the reward R for a generated query ˆy given the
relevance feedback pair (x,Dx) is obtained as fol-
lows:

R(ˆy) = M AP (ˆy,Dx)

(3)
At the end of the selection process, the objective
function aims at maximizing the expectation of the
search effectiveness over the predicted queries:

LRL(θ) = arg max

θ

E(x;Dx)∈GT
ˆy∼fθ(x)

[R(ˆy)]

(4)

where ˆy is given by the translation model fθ(x).
This objective function is maximized using gradi-
ent descent techniques (Baxter et al., 1999).

2.4 Model architecture
The model is based on an encoder-decoder build-
ing a query ˆq from the input x. Particularly, each
element xi of x is modeled through word embed-
dings wxi; resulting in a sequence wx of word em-
beddings for input x. As shown in Figure 2, the
encoder is a bi-directional LSTM (Hochreiter and
Schmidhuber, 1997) aiming to transform the in-
put sequence wx to its continuous representation
hn. The decoder is composed of a LSTM in which
each word xi is injected to estimate the word se-
lection probability p(yi|y<i, x) using the hidden
vector hn learned in the encoder network and the
current word xi; leading to estimate probability
p(yi|y<i, xi, hn).
3 Protocol design

3.1 Datasets
Since there does not exist yet SOCS-driven
datasets including NL-query pairs, we use TREC
tracks (namely, Robust 2004 and Web 2000-
2001).
In these tracks, query topics include
a title, a topic description and a narrative text;
the two latter being formulated in natural lan-
guage. To build query-NL pairs, we use the ti-
tle to form the set of keyword queries and the
description for the set of information needs ex-
pressed in NL. An example of a query-NL pair is:
Title
Description What are some useful sites containing
information about the historic Lewis
and Clark expedition?

Lewis and Clark expedition

This NL-query building process results in 350

pairs in total as presented in Table 1.

We are aware that the use of TREC datasets is
biased in the sense that it does not exactly ﬁt with
the expression of NL information need in the con-
text of conversational systems, but we believe that
the description is enough verbose to evaluate the
impact of our query building model in this ex-

35collection
TREC track
disk4-5
TREC Robust (2004)
TREC Web (2000 2001) WT10G

pairs NL length
250
100

15.333
11.47

avg of duplic. word in NL

1.108
0.65

Table 1: Dataset statistics separated per document collections

ploratory work. Further experiments with gener-
ated datasets, as done in (Song et al., 2017), will
be carried out in the future.

We also analyze the issue of duplicate words
into TREC descriptions since it can directly im-
pact the query formulation process based on word
selection in the word sequence of TREC descrip-
tions. In practice, this might lead to select several
times the same word to build the query, and, there-
fore, directly impact the retrieval performance. As
shown in Table 1, the ratio of duplicate words in
TREC descriptions over the whole set of queries
is very low (1.1 duplicate words in average in each
query for TREC Robust and 0.65 for TREC Web).
This suggests that this issue is minor in the used
datasets. We, therefore, decided to skip this issue
for the moment.

3.2 Metrics and baselines
To evaluate our approach, we measure the retrieval
effectiveness of the predicted queries. To do so,
for each predicted query, we run the BM25 model
through an IR system (namely, PyLucene1) to
obtain a document ranking. The latter is evaluated
through the MAP metric.

To show the soundness of our approach
(namely, translating information needs expressed
in NL into queries), we compare our generated
queries to scenario NL feeding the natural lan-
guage information needs (TREC descriptions in
our protocol) to the IR retrieval system.

Since the objective of our model is to formulate
queries, we also evaluate the effectiveness of orig-
inal TREC titles (scenario Q). This setting rather
refers to the oracle that our model must reach.

We mentioned that before training the selection
model we transformed each x to its binary repre-
sentation y based on the presence of the words in
the ground truth query. The dataset being slightly
biased by this binary modeling, we observed that
not all the words existing in the query do exist in
x. To analyze this bias, we also compare our ap-
proach with these binary queries (scenario Q bin)

1http://lucene.apache.org/pylucene/

referring to the projection of queries Q on the vo-
cabulary available in the NL description.

We also compare our model

to a random
approach which randomly selects 3 words from x
to build queries (scenario Random).

Different variants of our model are also tested:
• SMT which only considers the ﬁrst com-
ponent of our model based on a supervised
machine translation approach (Section 2.2).
This variant could be assimilated to the ap-
proach proposed in (Song et al., 2017) in
the sense that the machine translation is per-
formed independently of the task objective.
• RL which only considers the reinforce-
ment
learning objective function (Section
2.3) without pre-training of the supervised
translation model.

• SMT+RL which is our full model in which
we start by pre-training the model using the
supervised translation model (Section 2.3),
and, then, we inject the reward signal in the
translation probabilities (Section 2.4).

Implementation details

3.3
To transform each word xi to its vector represen-
tation wxi, we use Fasttext 2 (Bojanowski et al.,
2017) pre-trained word embeddings. The encoder
and decoders have one hidden layer with 100 hid-
den units each.

To train our model, we perform 10-fold cross-
validation. For the SMT+RL model, we start
by a pre-training using the supervised translation
model for 100 iterations. The training is then pur-
sued by 1000 iterations while including the rein-
forcement learning approach. In the latter, the re-
ward, namely the MAP metric, is estimated over
document rankings obtained by the BM25 model
in PyLucene. We use a minibatch Adam (Kingma
and Ba, 2014) algorithm to pre-train the model and
SGD for the reinforcement learning part. Each up-
date is computed after a minibatch of 12 sentences.

2https://github.com/facebookresearch/

fastText/

36Baseline

NL
Q
Q bin
Random
SMT
RL
SMT+RL

TREC Robust(2004)
MAP
0.08925
0.09804
0.08847
0.01808
0.06845
0.08983
0.10286

%Chg
+15.25% ***
+4.92%
+16.26% *
+468.91% ***
+50.27% ***
+14.51% ***

TREC Web (2000-2001)
MAP
0.15913
0.16543
0.17402
0.04060
0.08891
0.16474
0.17963

%Chg
+12.88% *
+8.58%
+3.22%
+342.44% ***
+102.04% ***
+9.04%

Table 2: Comparative effectiveness analysis of our approach. %Chg: improvement of SMT+RL over
corresponding baselines. Paired t-test signiﬁcance *: 0.01 < t ≤ 0.05 ; **: 0.001 < t ≤ 0.01 ; ***:
t ≤ 0.001.
NL
what are new methods of producing
steel
what are the advantages and or disad-
vantages of tooth implant
ﬁnd documents that discuss the toronto
ﬁlm festival awards
ﬁnd documents that give growth rates of
pine trees

SMT+RL
new methods of pro-
ducing steel
advantages
tages tooth implant
the toronto ﬁlm festival
awards
growth rates of pine
trees

where can i ﬁnd growth
rates for the pine trees

Q bin
producing steel

Q
steel producing

growth rates pine trees

toronto ﬁlm awards

toronto ﬁlm awards

implant dentistry

disadvan-

implant

Table 3: Examples of query formulation for NL queries, the original query Q, the binary version Q bin
of the original query, and our model SMT+RL.

4 Results

We present here the effectiveness of our approach
aiming at generating queries from users’ informa-
tion needs expressed in NL. In Table 2, we present
the retrieval effectiveness (regarding the MAP)
of our model and the different baselines (NL,
Q, Q bin, Random, SMT, and RL) described
in section 3.2. From a general point of view,
results highlight that in both datasets, our pro-
posed model SMT+RL outperforms the different
baselines with improvements that are generally
signiﬁcant, ranging from +3.22% to +468.91%.

lows to draw the following statements:

More particularly, the effectiveness analysis al-
• The overall performance of the compared ap-
proaches generally outperforms the retrieval ef-
fectiveness of the NL baseline. For instance, on
TREC Robust, queries generated by our model al-
lows to signiﬁcantly improve the retrieval perfor-
mance of +15.25% regarding information needs
expressed in NL (MAP: 0.10286 vs. 0.08925).
This result validates the motivation of this work
to formulate queries from NL expressions. This
is relatively intuitive since NL expressions are
verbose by nature and might include non-speciﬁc
words willing to inject noise in the retrieval pro-
cess.
• Our approach SMT+RL provides similar re-

sults as the Q and Q bin. Since the objective func-
tion of our model is guided by the initial query
Q transformed in a binary vector (Q bin), these
baselines could be considered as oracles. We note
however that our model obtains higher results (im-
provements from +3.22% to +16.26%) with a
signiﬁcant difference for the Q bin baseline for
TREC Robust. To get a better understanding to
what extent our generated queries are different
from those used in baselines Q and Q bin, we il-
lustrate in Table 3 some examples. While queries
in Q identify the most important words leading to
an exploratory query (e.g. “steel productions”),
our model SMT+RL provides additional words
that precise which facet of the query is concerned
(e.g., “new methods of...”), and accordingly im-
proves the ranking of documents.

• Our model SMT+RL is signiﬁcantly higher
than the SMT baseline which converges to a rel-
atively low MAP value (0.06845 and 0.08891 for
TREC Robust and TREC Web, respectively). This
could be explained by the fact that our datasets
are very small (250 and 100 NL-query pairs re-
spectively for TREC Robust and TREC Web) and
that such machine translation approaches are well-
known to be data hungry. Reinforcement learn-
ing techniques could be a solution to overpass this
problem since they inject additional information
(namely, the reward) in the network learning.

37• The RL baseline achieves relatively good
retrieval performances. As we can see from TREC
Web, the RL model obtains a MAP of 0.16474
against 0.15913 for the NL baseline. The RL
baseline allows approaching the retrieval perfor-
mances of baselines Q and Q bin, although it
obtains lower results. This reinforces our intuition
that 1) applying machine translation approaches
should be driven by the task (retrieval task in our
context) and 2) reinforcement learning techniques
provide good strategies to build effective queries.
The latter statement has also been outlined in
previous work (Nogueira and Cho, 2017).

• The comparison of our model SMT+RL re-
garding SMT and RL baselines outlines that rein-
forcement learning techniques might be more ben-
eﬁcial when a pre-training is performed.
In our
context, the pre-training is performed using the
SMT model (Section 2.3) which helps the model
to be more general and effective before using the
reward signal to guide the selection process.

It is worth mentioning that we also trained in
preliminary experiments a state of the art transla-
tion models such as a generative encoder-decoder
RNN with attention mechanism, as done in (Yin
et al., 2017; Song et al., 2017). We did not report
the results since the model was not able to general-
ize in the testing phase over new samples from the
NL-query dataset used in the training phase. This
is probably due to the trade-off between the num-
ber of training pairs and the large size of the vo-
cabulary which is not enough represented in differ-
ent contexts. However, we believe that combining
reinforcement learning with attention-mechanism
for query-generation is promising. We let this per-
spective for future work.

5 Conclusion and future work

We propose a selection model to transform user’s
need in NL into a keyword query to increase the
retrieval effectiveness in a SOCS context. Our
model bridges two lines of work dealing with su-
pervised machine translation and reinforcement
learning. Our model has been evaluated using two
different TREC datasets and outlines promising
results in terms of effectiveness. Our approach
has some limitations we plan to overcome in the
future. First, our model is framed as a word se-
lection process that could be turned into a gener-
ative model. Second, experiments are carried out

on small datasets (250 and 100 NL-query pairs)
that could be augmented using the evaluation pro-
tocol proposed in (Song et al., 2017). In long term,
we plan to adapt our model by totally skipping
the query formulation step and designing retrieval
models dealing with NL expressions.

References
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ’06, pages 19–26.

Jonathan Baxter, Lex Weaver, and Peter Bartlett. 1999.
Direct gradient-based reinforcement learning:
Ii.
gradient ascent algorithms and experiments. Tech-
nical report, National University.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Antoine Bordes and Jason Weston. 2016.

ing end-to-end goal-oriented dialog.
abs/1605.07683.

Learn-
CoRR,

Mikhail Burtsev, Aleksandr Chuklin, Julia Kiseleva,
and Alexey Borisov. 2017. Search-oriented conver-
In ICTIR ’17, pages 333–334.
sational ai (scai).
ACM.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
In ACL’ 17,
logue agents for information access.
pages 484–495.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
In HLT ’11,
extraction of overlapping relations.
pages 541–550.

Thorsten Joachims. 2002. Optimizing Search Engines
In SIGKDD ’02, pages

Using Clickthrough Data.
133–142. ACM.

Hideo Joho, Lawrence Cavedon, Jaime Arguello, Mi-
lad Shokouhi, and Filip Radlinski. 2018. Cair’17:
First international workshop on conversational ap-
proaches to information retrieval at sigir 2017. SI-
GIR Forum, 51(3):114–121.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

38Guillaume

Lample,

Ludovic Denoyer,

and
Marc’Aurelio Ranzato. 2017.
Unsupervised
machine translation using monolingual corpora
only. CoRR, abs/1711.00043.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In HLT
’16, pages 110–119. ACL.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI,
pages 2181–2187. AAAI Press.

Rodrigo Nogueira and Kyunghyun Cho. 2017. Task-
oriented query reformulation with reinforcement
learning. In SCAI Workshop - ICTIR.

Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
EMNLP ’11.

Hyun-Je Song, A-Yeong Kim, and Seong-Bae Park.
2017. Translation of natural language query into
keyword query using a rnn encoder-decoder. In SI-
GIR ’17, pages 965–968.

Svitlana Vakulenko, Ilya Markov, and Maarten de Ri-
jke. 2017. Conversational exploratory search via in-
teractive storytelling. In NEUIR SIGIR’17.

Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dialog
state tracking challenge: On the believability of ob-
served information. In SIGDIAL’ 13, page 423–432.

Zi Yin, Keng-hao Chang, and Ruofei Zhang. 2017.
Deepprobe: Information directed sequence under-
standing and chatbot design via recurrent neural net-
works. In SIGKDD’ 17, pages 2131–2139.

39
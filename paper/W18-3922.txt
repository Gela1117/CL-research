Exploring Classiﬁer Combinations for Language Variety Identiﬁcation

Walter Daelemans
Tim Kreutz
CLiPS - Computational Linguistics Group

Department of Linguistics

University of Antwerp

{tim.kreutz,walter.daelemans}@uantwerpen.be

Abstract

This paper describes CLiPS’s submissions for the Discriminating between Dutch and Flemish in
Subtitles (DFS) shared task at VarDial 2018. We explore different ways to combine classiﬁers
trained on different feature groups. Our best system uses two Linear SVM classiﬁers; one trained
on lexical features (word n-grams) and one trained on syntactic features (PoS n-grams). The ﬁnal
prediction for a document to be in Flemish Dutch or Netherlandic Dutch is made by the classiﬁer
that outputs the highest probability for one of the two labels. This conﬁdence vote approach
outperforms a meta-classiﬁer on the development data and on the test data.

1

Introduction

Discriminating between Dutch and Flemish in Subtitles (DFS) is a shared task at the VarDial evaluation
campaign 2018. The task aims at identifying language variety in written Dutch texts, speciﬁcally sub-
titles from movies and television, and classifying them as either Netherlandic Dutch or Flemish Dutch
(Zampieri et al., 2018).

Although DFS is organized for the ﬁrst time at VarDial, it adheres to the workshop’s general themes
of closely related languages and language varieties. The long-running discriminating between similar
languages (DSL) shared task has been organized previously, and has yielded lessons about distinguishing
similar languages and language varieties (Zampieri et al., 2017).

Since the Netherlands and Flanders adhere to the same standard language (Dutch), the task at hand
is one of language variety identiﬁcation rather than similar language identiﬁcation. This distinction is
important because differences between language varieties are often less obvious than differences between
different languages, however related they are (Goutte et al., 2016). Still, native Dutch speakers from the
Netherlands or Belgium mostly have no problem coming up with anecdotal or typical differences, and
linguistic work has previously formulated the most important distinctions.

There are plenty of practical applications for identifying Dutch language variety. It can for example
extend current work on Author Proﬁling (AP) for Dutch by estimating a speaker’s origin. In data selec-
tion, researchers might only be interested in texts written by either Dutch or Belgian authors and use an
automated system to make the distinction. For the purposes of theoretical linguists, a machine learning
system that outputs feature weights might indicate differences between the language varieties that have
not been systematically studied before.

This paper will describe the submissions of the Computational Linguistics & Psycholinguistics
(CLiPS) research center of the University of Antwerpen to the DFS shared task for VarDial 2018. As
we alluded to in the introduction we will draw lessons from previous work that aimed at discriminating
similar languages and language varieties, and more theoretical linguistic work on differences between
Netherlandic Dutch and Flemish Dutch.

This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/

ProceedingsoftheFifthWorkshoponNLPforSimilarLanguages,VarietiesandDialects,pages191–198SantaFe,NewMexico,USA,August20,2018.1912 Related work

Eleven teams participated in the previous edition of the DSL shared task (Zampieri et al., 2017). The task
involved determining the language or language variety for written news excerpts. It featured ten different
languages with some having two or three varieties, making for a set of fourteen possible classes. For a
multiclass classiﬁcation problem with a random baseline at 10 percent, the performance of most systems
was very good. The best system yielded a weighted F-score of .927 (Bestgen, 2017).

There are only slight differences with regards to performance and overall approach in the top-
performing teams. One trend is the use of word and character n-grams as the most discriminating fea-
tures, which is also commonly the case for the task of Native Language Identiﬁcation (NLI) (Tetreault
et al., 2017). In choosing a classiﬁcation algorithm, four out of the ﬁve top teams opted to use linear
support vector machines in their setup, which also echoes techniques used in NLI. The system paper
describing the best submission (Bestgen, 2017) again underlines this methodological similarity, having
drawn inspiration from entries in NLI shared tasks.

Alongside the main DSL task there were two speciﬁc tasks of language variety identiﬁcation in 2017,
namely Arabic Dialect Identiﬁcation (ADI) and German Dialect Identiﬁcation (GDI). The goal in both
tasks is to identify a native language dialect in speech transcripts. For the Arabic variant, participants had
access to transcripts and accompanying acoustic features from a multi-dialectal speech corpus and were
tasked to discriminate between ﬁve Arabic dialects. A new dataset was developed speciﬁcally for the GDI
task. The set contains transcribed interviews of one of four variants of Swiss German. The transcriptions
use Schwyzert¨utschi Dial¨aktschrift which contains phonetic properties of language varieties (Tetreault et
al., 2017).

The mentioned tasks on dialect identiﬁcation are most closely related to the newly proposed DFS task,
in that they deal with language variety, but the provided data in ADI and GDI is based on spoken data
and provides some phonetic insight into these transcripts (either through separate acoustic Vectors or a
rich phonetic transcript) whereas the DFS data does not.

We can, however, mention some promising approaches in the previous tasks using ensembles or meta-
classiﬁers with a large variety of word and character n-gram features that yield top performances in ADI
(Malmasi and Zampieri, 2017a) (second place) and GDI (Malmasi and Zampieri, 2017b) (ﬁrst place).
Malmasi and Zampieri use combinations of base classiﬁers to approach both tasks. In the ﬁrst ensemble
method, each base classiﬁer votes for its most probable label output and the label with the most votes
serves as the system’s ﬁnal output. The second ensemble method averages the probability outputs of
all base classiﬁers and the class label with the highest averaged probability serves as the system’s ﬁnal
output. The third variant, and most accurate approach, uses a Random Forest algorithm to meta-classify
the probability distribution output of the base classiﬁers. The use of basic n-gram features with meta-
methods seems to yield promising results regardless of the language varieties concerned.

The DFS data set and task are both closely related to and directly motivated by van der Lee and
van den Bosch (2017), who offer the ﬁrst comprehensive study of the automatic distinction between
Dutch and Flemish. The authors explore lesser studied techniques, some of which apply speciﬁcally
to the distinction between Netherlandic Dutch and Flemish Dutch. Syntactic features, of which part-
of-speech n-grams yield the most important patterns, perform well even without combining them with
word n-grams. Another important takeaway is that for their dataset, different feature groups are best
combined using a meta-classiﬁer. The conﬁguration which combines all feature groups and trains a
separate classiﬁcation on their output probability distribution yields the highest F-score at 92 percent.

3 Data and methodology

In this section, we describe the characteristics of the provided data set and how this inﬂuences our
methodology. We further select features and techniques that we deem important to try for the proposed
task on the basis of the work discussed in the previous section.

192Misery

Ajuin
Ui
Miserie
Ellende

Translation Word form Language variety Occurrences in BEL Occurrences in DUT
Onion
0
53
0
170
0
25
30
51

Microwave Microgolf
Magnetron
Gsm
Mobieltje

Flemish
Netherlandic
Flemish
Netherlandic
Flemish
Netherlandic
Flemish
Netherlandic

6
46
12
138
1
6
112
15

Cell phone

Table 1: Excerpt of the lists of typical Flemish words and typical Dutch words with similar meanings.
The frequencies of the words in the provided training set indicate the usefulness of the word list features
over word n-grams because some words are very uncommon. This table somewhat overstates the dif-
ferences between the Flemish and Netherlandic synonyms. For some words with similar meaning in the
Wikipedia list there were no evident occurrence differences.

3.1 Data
The provided training data consists of 300,000 Dutch subtitles of which half were produced for Dutch
television and half were produced for Flemish television (receiving the labels DUT and BEL respec-
tively). A development set which contained 250 DUT-documents and 250 BEL-documents was also
provided.

The subtitles originate from movies, television programs and documentaries but unlike the SUBTIEL
corpus used in van der Lee and van den Bosch (2017), a single document does not represent as single
movie or a single episode of a series. Rather, a document contains several (varying from one to thirty)
lines and between thirty to 150 words.

We note two important implications from this prior data limitation: there is an expected performance
drop compared to the work done by van der Lee and van den Bosch (2017) since each document contains
fewer words, and some macro-features, notably document length features should not capture much in
terms of language variety.

3.2 Features
We consider a top-down approach for our selection of features. Commonly stated language differences
between Netherlandic Dutch and Flemish Dutch are in speciﬁc word choices (see Table 1) and word
order choices. The latter is often a difference in placing ﬁnite verbs before versus after auxiliary verbs.
To see how word occurrence may differ we take an excerpt from the list of differences between Dutch
and Flemish from Wikipedia (2018) and count occurrences of words in the training documents (Table
1). Using word n-grams with tf-idf weighting should let the classiﬁer capture these and more subtle
differences in occurrence patterns. We use unigrams up to trigrams to partly learn distinctive patterns of
word order.

We also try lemma 1-3 grams. These features should be less important for capturing word order since
removing inﬂection also means losing most morpho-syntactical information of the patterns. However,
they can be useful for grouping words with diverse inﬂectional forms. We use Frog (van den Bosch et
al., 2007) for ﬁnding the lemma of a given word.

Frog was also used to give detailed part-of-speech tags for our PoS n-gram features. We are interested
in ﬁnding patterns of three or more words to capture distinctions in word type order between the target
language varieties. Frog tags adhere to the CGN tagset (Van Eynde, 2004) and display a wide array of
morpho-syntactic information. Verbs receive tags for their tense, whether it is singular or plural, ﬁnite
or an inﬁnitive, for example. We experimented with the level of detail in tagging to ﬁnd the optimal
conﬁguration, but we found that more detailed tags consistently boost performance and pattern scarcity
does not occur until we extend to 7-grams. Our ﬁnal implementation relies on PoS patterns from trigrams
up to 6-grams.

193For our last group of features we were informed by work in Native Language Identiﬁcation (Malmasi
and Dras, 2017; Tetreault et al., 2017). Function words can feed important (and currently missing)
stylometric information to our classiﬁer. For our implementation of function word trigrams we remove
words that are neither articles, pronouns, conjunctions or auxiliary verbs and construct patterns for the
remaining words (regardless of the number of gap words). As with the other n-gram feature sets we
apply tf-idf weighting on the word counts.

3.3 Classiﬁcation methods
As a simpliﬁcation we only consider the Linear Support Vector Machine (SVM) as a base classiﬁer. The
consideration is that SVMs have been shown to work well when using large feature sets and when making
binary distinctions in the data. The algorithm is also fast to train and accurate in its default conﬁguration,
at least in its scikit-learn (Pedregosa et al., 2011) implementation, which further allows us to focus more
on combining feature sets than ﬁnding optimal algorithms and parameters.

We consider three combination approaches:

1. Default. We combine the feature vectors and use only one SVM to ﬁnd optimal separation in the

feature space.

2. Highest conﬁdence vote. Each of the feature vectors is fed into their own base classiﬁer. Since
each feature group is intended to capture a separate quality of the language varieties, which might be
present or absent in the document, the classiﬁer which outputs the highest probability for a certain
label decides the ﬁnal label.

3. Meta-classiﬁer. Probability outputs of each of the base-classiﬁers serve as input for a meta-
classiﬁer. The meta-classiﬁer should learn regularities in probability distribution such as in-balance
between the labels and which of the feature sets makes less accurate predictions. We use Linear
Discriminant Analysis as our meta-classiﬁer here, which worked best in the case of NLI (Malmasi
and Dras, 2017).

4 Results
Each of the feature groups are ﬁrst tested separately on the development set to see their individual con-
tribution to the classiﬁcation results. We then test combinations of feature groups in each of the meta-
classiﬁer setups and motivate our selection of the three entries submitted to the shared task evaluation.

4.1 Feature group performance
Table 2 shows the F-scores of the individual feature
groups. Although the linguistic information cap-
tured in the feature groups is different, performance
of the feature groups is mostly similar. Only the
function word n-grams perform very poorly with
55 percent F-score. We think this partly pertains to
our operationalization of this feature set. It would
be better to use stricter checks on what composes
a function word, and to use placeholders to signal
presence of other words. There can also be a limit
on the number of words that may be skipped to form function word n-grams. Two function words that
appear in separate sentences are not suited to be used as a bigram compared to two function words that
have only one word between them. For further tests, we left out the function word n-grams because we
expect them to harm performance in any conﬁguration.

Feature group N-gram range F-score
0.689
Words
0.672
Lemmas
PoS
0.688
0.552
Function words

Table 2: Performance of individual features on
the development set.

1-3
1-3
1-6
1-3

The content features (word n-grams) perform the best, but this is closely followed by the syntactic
features (PoS n-grams). This again stresses the need for a suitable method to combine classiﬁers. Lemma
n-grams perform slightly worse than word n-grams and since the feature groups capture somewhat similar
language variety, it is important to consider if this feature group really contributes anything.

1944.2 Combining feature groups
Table 3 compares the proposed classiﬁcation combinations. A combination of the word and PoS n-
grams features outperforms other combinations in the conﬁdence vote and meta-classiﬁer setting. No
single classiﬁcation combination consistently outperforms the others, which motivated us to make a
submission for each of the proposed classiﬁcation combinations.

Feature groups

Words + Lemmas
Words + PoS
Lemmas + PoS
All groups

Classiﬁcation combination
Default Conﬁdence vote Meta-classiﬁer
0.684
0.706
0.684
0.692

0.672
0.710
0.694
0.697

0.689
0.693
0.680
0.698

Table 3: Combinations of feature groups using one of the three proposed methods. Meta-classiﬁcation
boosts performance in cases when the PoS feature group is included. The best combination of fea-
ture groups corresponds to individual feature group testing and our motivation to have features capture
linguistic information. Conﬁdence voting works best in this case, yielding 71 percent F-score on the
development set.

4.3 Test set performance
The performances of our submissions show the same slight differences on the test data (see Table 4) as on
the development data. Conﬁdence voting again outperforms the others combination methods, followed
by the meta-classiﬁer and default vector combinations. Overall, the results on the test set are a lot lower
than on the development set. There is no obvious way in which the test documents differ from those
in the training or development set. The length of the documents is similar, and the distribution of the
labels remained the same. In future attempts, it would be an improvement to evaluate the system during
development using (embedded) cross-validation to better predict results on unseen data.

5 Discussion and Conclusion
In the following section we do a more in-depth discussion of the results and the different submissions.
We ﬁrst show which patterns in the features were captured by the system and why we believe they are
distinctive for the two varieties of Dutch. We then critically discuss the submitted system and which
improvements could be made.

5.1 Feature importance
Tables 5 and 6 show the most important features for both Flemish and Netherlandic Dutch by feature
group as learnt by the base classiﬁers. The lexical differences contain mostly frequent interjections. This
also explains why this table only shows unigrams.

Some unexpected results are ‘ele’, ‘enten’, ‘ent’ and ‘ine’ for Flemish Dutch. It turns out that these are
sufﬁxes that follow the special ‘¨e’ or ‘¨ı’ characters. These characters were not processed correctly in the
provided data, leaving a whitespace in their stead. Words containing these character and these sufﬁxes
such as ‘ﬁnanci¨ele’, ‘cli¨enten’, ‘pati¨ent’ or ‘coca¨ıne’ were more common in Flemish subtitles.

Submission no. Class. combination F-score
0.629
#1
0.636
#2
#3
0.627

Meta-classiﬁer
Conﬁdence vote
Default

Table 4: Test scores show the same pattern observed in the development set but our performance is
consistently lower on the test set.

195No. BEL
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

ele
enten
da‘s
ent
allee,
komaan,
ine
vanop
sami
amai,

DUT
oke.
oke,
he?
masterchef
text
he.
grayson.
eh,
eh.
he,

No. BEL
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

DUT
VVZ VV PP
VVD VVD PP
PP(‘Er‘) PP(ﬁn) SENT
VV PP(‘ie‘) SENT
VVD VVN VVD
FW SENT SENT
IN PP SENT

IN NP SYM
NP PP$ N
NP SYM SENT
SENT NP PP$
SENT PP(‘U‘) IN
IN NP PP$
PP$ NNS SENT
SENT SYM(‘Da’s‘) JJ UH(‘O‘) UH(‘Ja‘)
IN NP SYM SENT
IN PP(‘dat‘) SENT

RB(‘Zo‘) UH(‘Ja‘) SENT
NN NN VV

Table 5: Most important word n-
gram features. We ﬁnd expected val-
ues and some that are particular to
the subtitle data set. Future work
could use intensive preprocessing to
limit observed artifacts such as mis-
formatting and genre-speciﬁc infor-
mation.

Table 6: Most important part-of-speech n-gram features. Since
we start with trigrams, these are more informative for analysing
speciﬁc word orders. They are expressed in the Penn Treebank
tagset to promote reader comprehensibility, but direct transla-
tions were not always possible. In cases where a tag referred to
mostly one speciﬁc word, this word is included in brackets. We
ﬁnd new distinctive linguistic patterns that were not expected
a priori.

‘Sami’ for Flemish and ‘Masterchef’ and ‘Grayson’ are also surprising terms in the list of top most
distinctive features. These artifacts of the training data indicate that some television content was more
common for one of the language varieties. For any shared task such artifacts pose a dilemma as removing
them, for example through named entity recognition, would be more faithful to the task of language
variety identiﬁcation, but could potentially harm the competitive performance. We decided to use any
information available in the subtitles to train a competitive system.

The part-of-speech n-grams (Table 6) are also interesting to analyze and contain some new patterns
that we did not expect a priori. In section 3.2 we mentioned the order of ﬁnite and auxiliary verbs but
we do not ﬁnd this as one of the most important features. Instead, a common pattern in Flemish Dutch
(in features 2, 4, 6 and 7) is using possessive pronouns after proper nouns whereas Netherlandic Dutch
tends to use a possessive ‘s’ in such cases. This was also found in van der Lee and van den Bosch (2017).
Examples from the training set include ‘Coryn haar moeder’ and ‘Tom zijn karakter’ for Flemish versus
‘Henry‘s auto’ and ‘Anne‘s verdwijning’ for Netherlandic.

Our ﬁndings also echo some lexical characteristics for Flemish. The speciﬁc proper noun ‘U’ is more
common, as well as two-word contraction ‘Da’s’ which is tagged as a special ‘incomplete’ sign in our
tagset (Van Eynde, 2004). The Netherlandic features show similar ﬁndings. Multiword interjections such
as ‘Oh ja’, ‘Zo ja’ and ‘Maar ja’ are strong indicators for the label, but are sometimes mistagged. The
personal pronoun ‘ie’ is indicative for Netherlandic as is ‘er’.

On a syntactic level we ﬁnd an interesting use of verbs in features 1, 2 and 5 for Netherlandic Dutch.

It is not uncommon to see three consecutive verbs such as in the training sentence:

“Na alle verderf die ik had veroorzaakt was Lana mijn hoop op verlossing.”
(‘After all the misery I had caused her, Lana was my hope for salvation.’)

In Flemish Dutch, commas are regularly placed before the last verb making this construction less com-
mon. This may be part or all of the reason why van der Lee and van den Bosch (2017) ﬁnd the use of
commas to be indicative of Flemish Dutch. An example training sentence is:

“De priester die me had opgeleid, was teleurgesteld.”

(‘The priest who trained me was dissappointed.’).

The syntactic patterns we extracted from just the top ten most distinctive features show interesting
differences between Flemish Dutch and Netherlandic Dutch. In future work these differences could be

196explicitely modelled for the task of NLI. We generally see that the tags that we used and the range of our
n-grams are sometimes too detailed to capture the differences we found upon later inspection. Future
work could beneﬁt from an iterative approach to its feature design.

5.2 Conclusion

Our second submission, which implemented the conﬁdence vote combination of classiﬁers, yielded the
best results. Overall it achieved third place out of twelve at the DFS shared task. The ﬁrst place submis-
sion achieved an f-score of 66 percent, which is considerably higher than our own result.

In this particular task setup, we think the conﬁdence vote outperformed the meta-classiﬁer because
clues of language variety can vary greatly between the short documents in the data set. One document
may contain a syntactic pattern typical of Flemish Dutch whilst containing words that are predominantly
used in Netherlandic Dutch. A meta-classiﬁer can learn that the probabilities that are output by the
word n-gram classiﬁer are generally more accurate than those output by the PoS n-gram classiﬁer. The
conﬁdence vote approach allows this general logic to be overruled in cases that are easier to predict for
one of either classiﬁer; the n-gram classiﬁer is ignored in documents with distinctive syntactic patterns.
In any n-
gram conﬁguration, performance can be boosted by leaving very common or very uncommon patterns
out of the feature. Very common uni and bigrams may not be informative and needlessly inﬂuence the
classiﬁcation while very uncommon patterns are hard to generalize and may lead to overﬁtting on the
training set.

Our n-gram feature groups performed well on this task but could further be improved.

The part-of-speech n-grams were a useful feature group for discriminating Flemish Dutch from
Netherlandic Dutch. The level of detail in the part-of-speech tags is something to critically analyze
in future work. As we showed with a manual inspection of the most import PoS-patterns, the important
differences between the language varieties on a syntactic level did not require as much detail in the tags
to be captured. Because of the speciﬁcity of the tags, some of the found patterns also overlap with the
lexical features. The pattern ‘SENT SYM(incomplete) JJ’ (PoS feature eight for Flemish) almost always
refers to ‘Da’s’ which was already captured using the word n-grams.

The function words for modelling stylometric aspects did not work with our operationalization. As
noted, our implementation which skipped over arbitrarily long sequences of words to form function
word n-grams was not optimal. It can be interesting to use function word co-occurrence as a feature, but
different distances between these words should not be treated as the same pattern. We suggest the use of
placeholders to signal the number of skips, or to limit the length of the skipped sequence.

Given time we would have used cross-validation. It would have given us more accurate performance

assessments during development and would have allowed optimization of hyperparameters.

We only tried Linear Discriminant Analysis for the meta-classiﬁer due to time constraints, but other
options are available and might have performed better. Our result, which shows conﬁdence voting out-
performing the meta-classiﬁer consistently should always be seen in the context of this speciﬁc task and
with the speciﬁc meta-classiﬁer we used. There are several other ways to combine classiﬁers using some
form of averaging or voting that can also be explored.

In conclusion, we used a very directed approach to test features based on linguistic knowledge of
both language varieties and found that these features added important discriminating information to the
system. We also tried several classiﬁer combinations, and found that a conﬁdence voting algorithm
outperformed a meta classiﬁer in this speciﬁc task.

This project further yields a number of directions for future work to be explored for similar tasks or for
providing a more extensive analysis of the differences between Dutch language varieties. We encourage
such work to use our system, which is available from the CLiPS GitHub 1.

1https://github.com/clips/vardial-dfs

197References
Yves Bestgen. 2017. Improving the character ngram model for the dsl task with bm25 weighting and less fre-
quently used feature sets. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and
Dialects (VarDial), pages 115–123.

Cyril Goutte, Serge L´eger, Shervin Malmasi, and Marcos Zampieri. 2016. Discriminating similar languages:

Evaluations and explorations. arXiv preprint arXiv:1610.00031.

Shervin Malmasi and Mark Dras. 2017. Native language identiﬁcation using stacked generalization. arXiv

preprint arXiv:1703.06541.

Shervin Malmasi and Marcos Zampieri. 2017a. Arabic dialect identiﬁcation using ivectors and asr transcripts. In
Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
178–183.

Shervin Malmasi and Marcos Zampieri. 2017b. German dialect identiﬁcation in interview transcriptions.

In
Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
164–169.

Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12:2825–2830.

Joel Tetreault, Jill Burstein, Claudia Leacock, and Helen Yannakoudakis. 2017. Proceedings of the 12th work-
shop on innovative use of nlp for building educational applications. In Proceedings of the 12th Workshop on
Innovative Use of NLP for Building Educational Applications.

Antal van den Bosch, Bertjan Busser, Sander Canisius, and Walter Daelemans. 2007. An efﬁcient memory-based

morphosyntactic tagger and parser for dutch. LOT Occasional Series, 7:191–206.

Chris van der Lee and Antal van den Bosch. 2017. Exploring lexical and syntactic features for language variety
identiﬁcation. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects
(VarDial), pages 190–199, 4.

Frank Van Eynde. 2004. Part of speech tagging en lemmatisering van het corpus gesproken nederlands. KU

Leuven.

Wikipedia. 2018. Lijst van verschillen tussen het nederlands in nederland, suriname en vlaanderen — Wikipedia,

the free encyclopedia. [Online; accessed 03-May-2018].

Marcos Zampieri, Shervin Malmasi, Nikola Ljubeˇsi´c, Preslav Nakov, Ahmed Ali, J¨org Tiedemann, Yves Scherrer,
In Proceedings of the Fourth
and No¨emi Aepli. 2017. Findings of the vardial evaluation campaign 2017.
Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages 1–15, Valencia, Spain, April.
Association for Computational Linguistics.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, Suwon Shon, James Glass, Yves Scherrer, Tanja
Samardˇzi´c, Nikola Ljubeˇsi´c, J¨org Tiedemann, Chris van der Lee, Stefan Grondelaers, Nelleke Oostdijk, Antal
van den Bosch, Ritesh Kumar, Bornini Lahiri, and Mayank Jain. 2018. Language Identiﬁcation and Mor-
phosyntactic Tagging: The Second VarDial Evaluation Campaign. In Proceedings of the Fifth Workshop on
NLP for Similar Languages, Varieties and Dialects (VarDial), Santa Fe, USA.

198
Learning Comment Controversy Prediction in Web Discussions

Using Incidentally Supervised Multi-Task CNNs

Nils Rethmeier, Mark H¨ubner, Leonhard Hennig

German Research Center for Artiﬁcial Intelligence (DFKI), Germany

firstname.lastname@dfki.de

Abstract

Comments on web news contain controver-
sies that manifest as inter-group agreement-
Tracking such rapidly evolving
conﬂicts.
controversy could ease conﬂict resolution or
journalist-user interaction. However, this pre-
supposes controversy online-prediction that
scales to diverse domains using incidental
supervision signals instead of manual label-
ing.
To more deeply interpret comment-
controversy model decisions we frame predic-
tion as binary classiﬁcation and evaluate base-
lines and multi-task CNNs that use an auxil-
iary news-genre-encoder. Finally, we use abla-
tion and interpretability methods to determine
the impacts of topic, discourse and sentiment
indicators, contextual vs. global word inﬂu-
ence, as well as genre-keywords vs. per-genre-
controversy keywords – to ﬁnd that the models
learn plausible controversy features using only
incidentally supervised signals.
Introduction

1
Online discussion comments are exchanged in
parallel, creating redundancy that prohibits dis-
cussions from developing beyond a superﬁcial
stage of conﬁrming previously held opinions.
Instead, Mahyar et al. (2017) recently demon-
strated that focusing users on controversial com-
comments that cause inter-group
ments – i.e.
agreement-conﬂicts (Dori-Hacohen et al., 2015)
– helps speed up inter-group consensus ﬁnding
leading to improved group decisions. However,
their system (ConsensUS) uses manual contro-
versy labels which can not capture rapidly evolv-
ing comment-controversy at scale or over diverse
domains. Hence,
to fully automate comment-
controversy prediction systems we contribute the
following solutions to a number of challenges.
(I) We extend controversy prediction to comment-
level, and to German news discussions. We eval-
uate topic, sentiment and discourse importance

(Cramer, 2011) and analyze whether models plau-
sibly capture controversy aspects using explain-
ability methods (see Sec. 5.3). (II) We use com-
ment vote-agreement to create an incidentally su-
pervised (Roth, 2017) controversy signal as seen
in Figure 1. Structural (output feature) signals like
genre, are predicted by a sub-encoder (see Sec. 4)
rather than required as input. (III) Sentiment and
discourse input feature creation work on any tok-
enizable language (see Sec. 3).

Figure 1: A comment is assumed controversial if
its up and down votes show no clear 2/3 majority
decision.

2 Related Research

Since predicting user agreement-conﬂicts upon
web news comments is a special case of contro-
versy prediction, we list in the following related
works that: (a) learn to predict controversy, using
(b) incidental supervision, and (c) work on online
(news) discussions. Chen et al. (2016) visualized
controversial words using dissimilarities in pro vs.
contra argument embeddings. Garimella et al.
(2017) identiﬁed controversial topics using bipar-
tite Twitter follower-graphs, while Dori-Hacohen
and Allan (2015) proposed an incidentally super-
vised binary classiﬁcation to predict controversial
topics via Wikipedia tags. Jang et al. (2016) used
language modeling to predict controversial doc-
uments, based on earlier hypotheses by Cramer
(2011): “that language in news discussions is a
good indicator of controversy”. Choi et al. (2010)
focused on using sentiment polarity indicators and

Proceedingsofthe9thWorkshoponComputationalApproachestoSubjectivity,SentimentandSocialMediaAnalysis,pages316–321Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguisticshttps://doi.org/10.18653/v1/P17316subtopics, i.e. topically related phrases of nouns.
Vote-based learning signals have been exploited
by both Pool and Nissim (2016); Basile et al.
(2017) who predict the sentiment distributions of
news outlets or ﬁnd controversial news pieces us-
ing Facebook-article emoticon-votes.
Instead of
predicting controversial topics (articles), we pre-
dict controversial comments, hence putting the fo-
cus on users (commentators) as curators of contro-
versial content.

3

Incidental Supervision Signals

Controversy signal: We use comment vote-
agreement ratios and news tags as incidental su-
pervision signals (Roth, 2017) to label comments
as controversial and by genre. Comments without
a clear 2/3 majority of either agreeing (up) or dis-
agreeing (down) votes are considered controver-
sial – i.e. of conﬂicted agreement. The ratio is cal-
culated as r = min(up, down)/max(up, down).
Ratios below 0.5 mark a 2/3 majority. Ratios
above 0.5 mark conﬂicted agreement. We re-
duce labeling noise via two noise margins:
(a)
controversial comments must have a vote-ratio
r > 0.6 and (b) that both the up-votes (group)
and down-votes (group) should each have more
than 2 votes. Article Genre signal: Predicting
controversy without context structure is difﬁcult,
hence we use article-genre (topic) prediction as
an incidental structure signal. The data contains
15 genres – some of which are noisy mixes of
others. However, to keep preprocessing general,
we use genres ”as-is”. Corpus: We collected
comments and the above training signals for ev-
ery article published by the Austrian newspaper
DerStandard.at in 2015. Each article has a
news genre tag and user comments, that in turn
receive up and down votes. The corpus contains
813k comments, from which we extracted 8.9k
controversial and 12.6k non-controversial com-
ments after removing duplicates and short com-
ments with less than ﬁve words. Text prepro-
cessing:
is source agnostic without language-
speciﬁc NLP. We remove noise like low-frequency
words. We create special tokens for discourse
(repeated punctuation) and reactionary sentiment
(emoticons) by categorizing emoticons into four
(non-overlapping) types using a Wikipedia emoti-
con list1, see Table 1. We keep stop words, as they

1https://en.wikipedia.org/wiki/List_

of_emoticons

often overlap with discourse markers (see Sec. 5).
Compounds are separated with a $comp$ token.
Finally, we pre-trained word2vec (Mikolov et al.,
2013) embeddings on 3.35M preprocessed article
and comment sentences to cover standard German
and mixed (non)dialect.

Pattern
URL
happy
sad
skeptical
unserious
rep. punct.
compounds word $comp$ word Go-Fan

Replacement
$url$
$happy$
$sad$
$skeptical$
$unserious$
$.$, $,$, $?$, $!$

Example
web.de
:) :D
:(
:S, :/
:P ;p
... !!!

Table 1: Text normalization reduces vocabulary
noise and creates input features.

4 Models
Baselines: As baselines we use Multinomial
Naive Bayes (MNB) and Regularized Logistic Re-
gression (LR) trained on TF or TFIDF Bag-of-
Ngrams. FastText (FT) (Joulin et al., 2016) is
trained on embedding 1-3grams.

Single / Multi-task CNNs
: We also use con-
volutional neural nets (CNN) as they are widely
used in text classiﬁcation. Below, we describe how
we modiﬁed the single-task model (ST) by Kim
(2014) to create a multi-task architecture (MT)
as follows. ST: A CNN that predicts comment-
controversy only. It uses a deeper classiﬁer, input-
token dropout, custom word2vec embeddings and
trains on comment, controversy label pairs via a
binary cross-entropy – see Controversy CNN in
Figure 2. MT: This model adds a genre-encoder to
the ST. The encoder predicts multi-class genre via
categorical cross-entropy and softmax on genre
labels.
Its penultimate activation map is fed to
the ST’s controversy classiﬁer, to provide genre
plus controversy features – see red downward ar-
row entitled genre encoding in Figure 2. The two
losses are trained as a weighted sum. Thus, genre
features are not required when predicting on new
data.

MT modiﬁcations: Since feature extraction
module design is central to CNNs, we evaluate
a range of different design choices. We separate
extraction modules into three categories from left
to right: convolution methods, activation schemes,

317and pooling mechanisms as seen in the upper and
middle parts of Figure 2. White boxes are mod-
ules, dashed/dotted lines are module-combination
options. Modules are marked by author, or with *
for our own modiﬁcations. Module details are as
follows:

Figure 2: CNN modiﬁcations. Upper white box
classiﬁes genre to encode it, lower one classiﬁes
controversy. Colored rectangles are layers and op-
erations as per the legend.

Conv: Kim (2014). CReLU Appends negated
activations before applying ReLU (Shang et al.,
2016). PosNeg Conv* (PNC): Learns separate
convolutions for negated and positive embedding-
activations, to extend CReLU. ReLU: (Hahnloser
et al., 2000). Swish: Self-attention multiplying
inputs x by their sigmoid σ(x) (Ramachandran
et al., 2018). Squeeze and Excite (SE): Bottle-
necked multi-layer attention that learns convolu-
tion ﬁlter importances (Hu et al., 2018). MaxPool:
(LeCun et al., 1998). Max(SPool)*: Appends per-
ﬁlter Standard Deviation Pooling (SPool) to Max-
Pool, to preserve variance info. In the next section
we evaluate the most successful combinations.

5 Results and Discussion

We evaluate on 8.9k controversial and 12.6k non-
controversial comments that each belong to ex-
actly one genre. We created 5 randomly sampled
(stratiﬁed) folds – 4 folds for cross validation (CV)
and 1 as holdout set. MNB, LR, FT, Conv+ReLU
(ST) only predict controversy. The MT models
jointly predict controversy + genre and are tested
for various modiﬁcation combos. Finally, we in-
vestigate models decision semantics and feature
type importances via ablation studies.

5.1 Baselines: MNB, LR, FT
In Table 2 we list F1, area under the ROC curve
(AU C) and accuracy (Acc) controversy prediction
results on the holdout test set. We see that FastText
is the best baseline2. Optimal hyperparameters
from 4-fold CV were: word-embedding 1-3gram
with 128 dimensional w2v embeddings for FT,
and TFIDF 1+2grams with a maximum document-
frequency of 100% and a minimum term fre-
quency of 2 for MNB and LR.

5.2 ST, MT CNNs
Stopwords and punctuation are kept as they con-
tain discourse and sentiment features – see sec.
5.3 for details. Low-frequency words are re-
placed with a pre-trained unknown word token
(UNK). Conv+ReLU (ST): The controversy-only
CNN outperformed FT at optimal CV parame-
ters of: 1-5gram, global max pooling, 128 ﬁl-
ters and 1k classiﬁer widths. More ﬁlters or a 4k
width decreased CV and test performance. Stan-
dard dropout (Hinton et al., 2012) and Batch Nor-
malization (Ioffe and Szegedy, 2015) decreased
performance, while 20% token-dropout (Gal and
Ghahramani, 2016) led to consistent
improve-
ment. Conv+ReLU (MT): Adding a genre-task
network to ST improved performances by 2 –
4 points each, despite working on halved hy-
per parameters – i.e. MTs performed best us-
ing only 64 ﬁlters and 512 classiﬁer units, giv-
ing less model parameters than the ST, especially
since increasing ST’s parameters hurt its perfor-
mance. MT modiﬁcations: Since some modiﬁca-
tions underperformed we only list combinations
that are top-3 in one of the measures. Notice-

2An always-controversial predictor gives F1 = 58%,
Acc = 42% and sample weighted class average F1 =
24%. A always-non-controversial predictor gives F1 = 42%,
Acc = 58% and F1 = 43%). Neither is useful in practice.

318MaxPoolμAvgPoolConcat-Invert input signlin. Conv. n-gramnRReLUMultiplyGenre encodingRσStdPoolRSSqueeze & Excite2018 [Hu]μR  CReLU 2016[Shang]R-Swish 2018[Ramachandran]SReLU 2000[Hahnloser]RGenre Encoder CNNMTCNNControversy CNNactivateconv-olveactivateconv-olvepoolpoolRRRREmbeddingInputToken dropout1...5N-gram Conv.2014 [Kim]PosNeg Conv *[this work]1-1.........DropoutSoftmaxSigmoidFully con. R/SMaxPool2014 [Kim]MaxStdPool *[this work]σably, the MT+PNC+SPool+Swish variant signiﬁ-
cantly improved AU CROC and Acc over the sim-
pler Conv+ReLU (MT) model, which produced
the best F1. Overall, we see that adding more inci-
dental supervision signals beats adding advanced
network modules.

Model
MNB
LR
FT
Conv+ReLU (ST)
Conv+ReLU (MT↓)
PNC+CReLU
PNC+SPool+Swish
Conv+SE+ReLU

AU C
59.84
62.92
65.06
68.25
72.12
72.06
72.28
71.91

F1
55.72
58.14
60.57
62.03
64.48
63.40
64.21
63.93

Acc
57.44
60.12
63.82
66.42
68.37
68.72
68.82
68.76

Table 2: Holdout performances for the controver-
sial class (y=1). Baselines: top 3. ST: middle,
MT: last 4 – only module combinations with top-
3 performance in one measure are listed as: best,
2nd best, 3rd best.

5.3 Feature-type ablation

We ablated sentiment, discourse and topical fea-
tures (Choi et al., 2010; Cramer, 2011). Then, we
re-tuned the Conv+ReLU (MT) on the 4, now ab-
lated, CV folds to measured test set performance
changes as follows. Three sentiment ablations:
(1) polarity words (sent ws by Waltinger (2010)),
(2) repeated punctuation (punct.), and (3) emoti-
cons (emotes) as mentioned in sec. 3. Discourse:
Removal of German discourse markers (DiMLex)
(Stede and Umbach, 1998). Topic: Noun removal
as in Choi et al. (2010) to represent topical indi-
cators. Figure 3 shows the relative percentual per-
formance drop per ablation. Thus, for controversy
topic was the most important, fol-
prediction:
lowed by discourse markers3 and sentiment with
repeated punctuation and emoticons being impact-
ful style/sentiment features. Polarity words affect
prediction, but are not language independent.

3Markers overlap with a stop word list in approximately
49% of occurrences in our dataset. Stop words: http://
www.ranks.nl/stopwords/german.

Figure 3: Relative controversy prediction per-
formance drop in % for removal of: sentiment
(blues), discourse (orange) and topic/nouns (red).

5.4 Per-word impacts
Inspired by explainability methods (Li et al., 2016;
Arras et al., 2017) we also measured the contro-
versy prediction-score change when replacing a
token with a class neutral UNK token4.

Figure 4: DE → EN Per-token controversy im-
pacts: Red is important for controversy. Blue low-
ers the controversy score. Last paragraph: context
dependent word inﬂuence of the word Windows.

In Figure 4 we colored per-token score drops
(red) or increases (blue) for German-to-English
word-by-word translations on test set comments.
We show examples by ablation types as de-
scribed in section 5.3. As before, nouns and
discourse markers increase controversy, while, ex-
pectedly, an (#unserious) ;) emoticon is strongly
counter-indicative of controversy. Repeated punc-
tuation, like $.$ or $?$, also impacts prediction.
Finally, the model learned context dependent con-

4Removing tokens would create unusual n-grams, and

hence wrong results.

319sent ws     punct.    emotes   discourse   topical5101520   ROC246   F1510  Acc10.3610.0010.3715.9512.815.545.264.975.755.1311.397.906.776.456.75Discourse or punctuation ($):Because it not_a UNK country but a dictatorship is . What UNK Putin of human_rights and peace $.$ .  Had you the UNK or are you vaccinated $?$ ?  ii ii   Emoticons:They employ the same word_choice :((    .          a.    Was easily UNK the tradition UNK ? ;) Context dependent word influence:Interestingly , if one something negative against ⏎Windows posts will one instantly_be with UNK ⏎bombarded . But 2 years were we by Microsoft marketing ⏎ and by Microsoft fan_boys UNK how cool yet not ⏎Windows 8 and 8 .1 is . domestic politics

kp¨o
p¨uhringer
sp¨o
state elections
federal level
genre

afd
fp¨o
gr¨unen
parties
faymann
+ controversy

international politics
poroschenko
separatists
putin
arabs
israelis
+ controversy

ceaseﬁre
mariupol
rebels
hamas
air raid
genre

economy

panorama

bonds
rbi
hedge funds
budget
credits
genre

tsipras
troika
syriza
greece
varoufakis
+ controversy

entry
battery
property dmg.
passage
tents
genre

pegida
prejudices
refugee policy
hate-monger
antisemitism
+ controversy

Table 3: Token importances in descending order. On the left genre: most important genre tokens. On the
right (+ controversy): most controversial tokens per genre. Tokens are sorted by mean positive impact
on genre and genre+controversy predictions.

troversy polarity for the word Windows, with has
both strong positive and negative polarity.

5.5 Token impacts on genre and controversy
To generate keywords for controversy and genre
vs. controversy-per-genre, we averaged UNK
token-replacement prediction-impacts over all oc-
currences of a token ti and calculated its im-
pact mean µ(impacts(ti)) and standard deviation
σ(impacts(ti)), similar to how Horn et al. (2017)
extract topic keywords.
Controversy keywords:
In Table 4 we divided
tokens into infrequent (top half) and common to-
kens (lower half). Infrequent tokens have over 10
occurrences, frequent ones at least 200.

(a) 0 con
”
.
;
–
possibly
.
-
?
”
with

(b) ↑↓ con
pkk
kurds
crimea
tsipras
israelis
eu
usa
#unser.#
#happy#
$.$

(c) ↑ con
separatists
putin
pegida
israelis
hamas
eu
usa
country
people
austria

(d) ↓ con
yet
thx
has
ain’t
yeah
have
#happy#
#unser.#
anyway
from

σ(impacts(token))
µ(impacts(token))
(a) No impact := smallest σ(impacts) ≈ 0 top.
(b) Impactful := largest σ(impacts) top.
(c) Pro controv. := most positive µ(impacts) top.
(d) Contra cont. := most negative µ(impacts) top.

increase it or (d) generally decrease it. We see
that, standard punctuation has no impact on
controversy (a), but repeated punctuation, emotes
and political terms do (b). Expectedly, political
terms generally increase controversy (c), while
colloquialisms and friendly emotes lower it (d).

Genre vs. controversy-per-genre keywords:
We examined mean token impacts µ(impacts)
on genre classiﬁcation vs. per-genre controversy
in Table 3 for the four most
interesting gen-
res. The domestic politics genre is dominated
by established Austrian parties or generic politi-
cal terms, while right-wing, left-wing and liberal
parties characterize domestic controversy. The in-
ternational genre shows mostly war related terms.
Its controversy focuses on the 2015 Ukraine and
middle east conﬂicts. Keywords for the econ-
omy genre are general ﬁnance terms, whereas the
Greek debt crisis dominates genre controversy.
The panorama genre focuses on refugee-related
terms, where the related right-wing issues caused
controversy in 2015.

6 Conclusion

We proposed a fully automated,
incidentally
supervised, multi-task approach for comment-
controversy prediction and showed that it suc-
cessfully captures contextual controversy seman-
tics despite only using minimal, language inde-
pendent, preprocessing and feature creation. In the
future, we aim to extend data collection to study
controversy drift over time.

Table 4: Controversy impacts for seldom (upper
half) and frequent token (lower half).

Acknowledgements

(a) not
The tokens impact controversy either:
at all, (b) positively or negatively, (c) generally

This work was supported by the German Fed-
eral Ministry of Education and Research (BMBF)
through the project DEEPLEE (01IW17001).

320References
Leila Arras, Franziska Horn, Gr´egoire Montavon,
Klaus-Robert M¨uller, and Wojciech Samek. 2017.
”What is relevant in a text document?”: An inter-
pretable machine learning approach. PloS one 12
(2017).

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Bag of Tricks
arXiv preprint

and Tomas Mikolov. 2016.
for Efﬁcient Text Classiﬁcation.
arXiv:1607.01759 (2016).

Yoon Kim. 2014. Convolutional Neural Networks for

Sentence Classiﬁcation. In EMNLP.

Angelo Basile, Tommaso Caselli, and Malvina Nissim.
2017. Predicting Controversial News Using Face-
book Reactions.. In CLiC-it.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proc. IEEE 86, 11 (1998).

Wei-Fan Chen, Fang-Yu Lin, and Lun-Wei Ku. 2016.
WordForce: Visualizing Controversial Words in De-
bates. In COLING.

Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng.
Identifying Controversial Issues and Their

2010.
Sub-topics in News Articles. In PAISI.

Peter A Cramer. 2011. Controversy as news discourse.

Vol. 19. Springer Science & Business Media.

Shiri Dori-Hacohen and James Allan. 2015. Auto-
mated Controversy Detection on the Web. In ECIR.

Shiri Dori-Hacohen, Elad Yom-Tov, and James Allan.
2015. Navigating Controversy as a Complex Search
Task. In SCST@ECIR.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In NIPS.

Venkata Rama Kiran Garimella, Gianmarco De Fran-
cisci Morales, Aristides Gionis, and Michael Math-
ioudakis. 2017. Reducing Controversy by Connect-
ing Opposing Views. In WSDM.

Richard HR Hahnloser, Rahul Sarpeshkar, Misha A
Mahowald, Rodney J Douglas, and H Sebastian Se-
ung. 2000. Digital selection and analogue ampli-
ﬁcation coexist in a cortex-inspired silicon circuit.
Nature 405, 6789 (2000).

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012.
Improving neural networks by
preventing co-adaptation of feature detectors. CoRR
(2012).

Franziska Horn, Leila Arras, Grgoire Montavon,
Klaus-Robert Mller, and Wojciech Samek. 2017.
Discovering topics in text datasets by visualizing rel-
evant words. CoRR (2017).

Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-
IEEE Conference on Com-

Excitation Networks.
puter Vision and Pattern Recognition (2018).

Sergey Ioffe and Christian Szegedy. 2015. Batch Nor-
malization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift. In ICML.

Myungha Jang, John Foley, Shiri Dori-Hacohen, and
James Allan. 2016. Probabilistic Approaches to
Controversy Detection. In CIKM.

Jiwei Li, Will Monroe, and Daniel Jurafsky. 2016. Un-
derstanding Neural Networks through Representa-
tion Erasure. CoRR (2016).

Narges Mahyar, Weichen Liu, Sijia Xiao,

Jacob
Browne, Ming Yang, and Steven Dow. 2017. Con-
sensUs: Visualizing Points of Disagreement for
Multi-Criteria Collaborative Decision Making. In
CSCW Companion.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient estimation of word
arXiv preprint

frey Dean. 2013.
representations in vector space.
arXiv:1301.3781 (2013).

Chris Pool and Malvina Nissim. 2016.

Distant
supervision for emotion detection using Face-
book reactions. In Proceedings of the Workshop
on Computational Modeling of People’s Opin-
ions, Personality, and Emotions in Social Me-
dia (PEOPLES). The COLING Organizing Com-
mittee, Osaka, Japan. http://aclweb.org/
anthology/W16-4304

Prajit Ramachandran, Barret Zoph, and Quoc V Le.
ICLR

2018. Searching for activation functions.
(2018).

Dan Roth. 2017. Incidental Supervision: Moving be-

yond Supervised Learning. In AAAI.

Wenling Shang, Kihyuk Sohn, Diogo Almeida, and
Honglak Lee. 2016. Understanding and improving
convolutional neural networks via concatenated rec-
tiﬁed linear units. In International Conference on
Machine Learning.

Manfred Stede and Carla Umbach. 1998. DiMLex:
A lexicon of discourse markers for text generation
and understanding. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2. Association for Computational Linguis-
tics.

Ulli Waltinger. 2010. GERMANPOLARITYCLUES:
A Lexical Resource for German Sentiment Anal-
ysis. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC). electronic proceedings, Valletta, Malta.

321
Implicit and Explicit Aspect Extraction in Financial Microblogs

Thomas Gaillat, Bernardo Stearns, Gopal Sridhar,

Ross McDermott, Manel Zarrouk

Insight Centre for Data Analytics NUI Galway

Brian Davis

Maynooth University

brian.davis@mu.ie

firstname.surname@insight-centre.org

Abstract

This paper focuses on aspect extraction
which is a sub-task of Aspect-based Sen-
timent Analysis. The goal is to report an
extraction method of ﬁnancial aspects in
microblog messages. Our approach uses
a stock-investment taxonomy for the iden-
tiﬁcation of explicit and implicit aspects.
We compare supervised and unsupervised
methods to assign predeﬁned categories at
message level. Results on 7 aspect classes
show 0.71 accuracy, while the 32 class
classiﬁcation gives 0.82 accuracy for mes-
sages containing explicit aspects and 0.35
for implicit aspects.

Introduction

1
Sentiment Analysis (SA) in the ﬁnancial domain
has shown a growing interest in recent years. Ac-
quiring an insight into the public opinion of rel-
evant and valuable economic signals can give a
competitive edge and allow more informed invest-
ment decisions to be executed. Microblog plat-
forms such as Twitter and StockTwits, are cen-
tral to determining these economic signals (Bollen
et al., 2011; Zhang et al., 2011). Investors share
their opinions about stocks, companies and prod-
ucts, and these contents are valuable for whomever
is interested in predicting market trends. Research
in the area of SA tries to shed some light on this
problem. Its purpose is to identify opinions and
sentiments that are directed towards entities such
as stocks and companies or towards the attributes,
or aspects, of these entities.

The authors are involved in SSIX1 (Davis et al.,
2016), a project focused on SA in ﬁnancial mar-
kets.
It currently offers sentiment scores for

1Social Sentiment IndeX is a platform dedicated to SA in

ﬁnancial microblogs. Available at https://ssix-project.eu/

stocks and companies and intends to provide ﬁner-
grained SA by including aspects. In order to con-
duct Aspect-Based SA in this project, the ﬁrst
step is to identify aspects in microblog messages,
which is the focus of this paper.
As stated in SemEval-2015,

the problem in
Aspect-based SA can be divided into three sub-
tasks, i.e. aspect category identiﬁcation, Opin-
ion Target Expression (OTE) extraction and sen-
timent polarity assignment (Pontiki et al., 2015).
In this paper, we focus on the ﬁrst sub-task of as-
pect category assignment. There have been two
types of approaches to conduct this subtask.
In
the ﬁrst type, aspect words are extracted and clus-
tered (Qiu et al., 2011; Chen and Liu, 2014; Shu
et al., 2016; Poria et al., 2016).
In the second
type, predeﬁned aspects categories are assigned
to entity-attribute pairs at sentence level (Pontiki
et al., 2015). The ﬁrst type of approaches tar-
gets explicit aspects while the second one also in-
cludes implicit aspects, i.e. aspects that are not
explicitly mentioned in the text strings (Liu, 2012,
p. 77). Using predeﬁned aspects corresponds to
the project requirements but most approaches deal
with hotel, restaurant and product-related data. To
the best of our knowledge none of them use a cor-
pus of annotated aspects in the ﬁnancial domain.
We present a method that focuses on the as-
pect category identiﬁcation of implicit and ex-
plicit aspects. The originality of our work is
to evaluate different aspect category identiﬁcation
approaches based on a predeﬁned taxonomy of
stock-investment aspects. Work is carried out on a
limited data set with a view to expanding it should
results be satisfactory. Our approach relies on us-
ing a corpus of annotated messages to build sev-
eral types of models based on distributional se-
mantics and supervised learning methods. Also
original is that our work focuses on the stock-
investment domain as it is to be added to the SSIX

ProceedingsoftheFirstWorkshoponEconomicsandNaturalLanguageProcessing,pages55–61Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics55platform. The remainder of this paper is divided
as follows. Section 2 covers related work. Section
3 gives details about the corpus that was used. In
Section 4 the different models are described. Re-
sults are presented in Section 5, followed by the
conclusion in Section 6

2 Related Work

Available methods in aspect category identiﬁca-
tion can be divided into supervised and unsuper-
vised approaches. Unsupervised approaches in-
clude a number of lexicon-based strategies rely-
ing on i) frequency measures used with association
measures such as Point-wise Mutual Information
(PMI) to link words with lexicon entries (Popescu
and Etzioni, 2005; Long et al., 2010), ii) syntactic
relations to relate core sentiment words, expressed
by adjectives, to target aspect words expressed by
nouns (Liu et al., 2016; Fang and Huang, 2012;
Jo and Oh, 2011; Brody and Elhadad, 2010; Chen
and Liu, 2014), and iii) on word association mea-
sures for topic extractions and clustering methods
(Fang and Huang, 2012; Jo and Oh, 2011; Brody
and Elhadad, 2010; Chen and Liu, 2014). All
these methods rely on lexicons to search for ex-
plicit words linked to aspects.

Supervised approaches rely on Machine Learn-
ing (ML) algorithms that are trained on classi-
ﬁed instances of aspects prior to performing clas-
siﬁcation of new instances. Many studies have
proposed different types of Conditional Random
Fields (CRF) models (Jakob and Gurevych, 2010;
Mitchell et al., 2013; Shu et al., 2016; Cruz et al.,
2014; Poria et al., 2016) that distinguish aspects
from non-aspects in text sequences.
In parallel,
other methods apply aspect category identiﬁcation
on the basis of predeﬁned aspects linked to Entity
(E) and Attribute (A) pairs (Pontiki et al., 2015,
2014). The current SemEval framework requires
the extraction of explicit mentions of E and of all
mentions of A (implicit and explicit)(Pontiki et al.,
2015).

With respect to the implicit / explicit distinc-
tion, traditional approaches have focused on ex-
plicit aspects (Liu et al., 2016; Schouten et al.,
2018), hence relying on word occurrences to deter-
mine aspects. Other, more novel, methods have fo-
cused on identifying implicitly-referred-to aspects
(Pontiki et al., 2015). (Dosoula et al., 2016) de-
veloped an implicit feature algorithm that uses co-
occurrences to assign implicit aspects at sentence

level in online restaurant reviews.

Our framework is similar to SemEval-2015
Task 12 (Pontiki et al., 2015) insofar as we used
predeﬁned categories of aspects (A) for stocks
considered as entities (E). Likewise, our approach
includes the extraction of aspects that are not nec-
essarily mentioned in messages. The difference is
that we use a two-level aspect taxonomy for coarse
and ﬁne-grained characterization, which gives 32
ﬁne-grained classes as opposed to the 9 classes
of the laptop data set of SE-2015 task 12 for in-
stance. We also conduct category identiﬁcation at
message level without creating E/A pairs. For the
requirements of the project, we use a speciﬁc ﬁ-
nancial aspect taxonomy. Albeit applied to a dif-
ferent domain, results show higher or equivalent
F1-Scores depending on the granularity.

3 Corpus

The approach relies on a corpus of messages spe-
cialised in stock trading2. Microblog messages
were posted by stock traders who share investment
ideas and intelligence. The data set is described in
Table 1.

Aspect type
All types
Implicit aspects
Explicit aspects

Number of messages
368
218
150

Table 1: Number of implicit and explicit mes-
sages in the data set

3.1 Taxonomy of Stock-Investment Aspects
As a preliminary step to aspect identiﬁcation, a ﬁ-
nancial expert deﬁned a taxonomy of trading as-
pects (See Appendix). They were grouped on the
basis of hypo/hypernym relations following a gen-
eral to more speciﬁc hierarchy. The ﬁnal taxon-
omy consists in an aspect class dominating an as-
pect sub-class. No related terms, nor synonyms,
were added to these subclasses. There are 7 aspect
classes, e.g. User Action, Asset Direction and 32
aspect subclasses, e.g. User Action>Buying Inten-
tion. Aspect classes do not include the same num-
ber of subclasses. For instance, the User Action
class includes 5 aspect subclasses while the User
Outlook class includes 2 aspect subclasses. The

2The dataset

is available at https://bitbucket.org/ssix-

project/stock-investment-aspect-extraction

56taxonomy is used i) to compute the semantic relat-
edness between taxonomy labels and textual can-
didates (DSM approach. See Section 4.1) and ii)
to relate message features with taxonomy classes
(Supervised-learning approach. See Section 4.2.

3.2 Annotation Scheme
The messages were manually classiﬁed by one ﬁ-
nancial expert according to the afore-mentioned
taxonomy by matching aspect classes and sub-
classes with messages. Annotation includes the
message ID and the OTE that substantiates the se-
lected class. The following example is a JSON-
type extract of the ﬁrst message classiﬁed as User
Outlook > Negative Outlook.

{ "ID": 1,

"User Outlook",

"Negative Outlook",

"AspectClass":
"Aspect":
"OTE": "Could easily see $AMZN
drop 200 points after hours
tomorrow",
"Message":
$AMZN drop 200 points after hours
tomorrow after earnings"
}

"Could easily see

4 Building a Classiﬁcation Model

This section focuses on the method used to build
different models for the aspect extraction task.
The task of the classiﬁer is to assign (i) aspect
classes and (ii) subclasses to messages. In this sec-
tion, we present the two approaches. The ﬁrst one
applies a distributional semantics model, while the
second one is based on several Machine Learning
algorithms.

4.1 Distributional Semantics Model (DSM)
This approach relies on word embeddings for
the computation of semantic relatedness with
Word2vec (Mikolov et al., 2013). Word embed-
dings fall in the category of distributional seman-
tics methods in which the meaning of a word is
related to the distribution of words around it (Ju-
rafsky and Martin, 2009, p.659-665).

Word2vec, in its skip-gram architecture, is such
a model and was trained on the Google news cor-
pus. The vector values are the weights computed
by the hidden layer of a Neural Network trained
on a corpus. The Word2vec skip-gram model al-
lows to ﬁnd words that appear frequently together,
and infrequently in other contexts (Mikolov et al.

2013).

The task of identifying aspects can be formu-
lated as mapping textual elements of messages to
their most related aspect class label in the taxon-
omy. There are two steps: extracting candidates
and computing relatedness with the classes.
4.1.1 Extracting Candidates
After preprocessing (tokenisation and Part-of-
Speech (POS) tagging) The extraction of candi-
dates relies on rule-based heuristics using morpho-
syntactic patterns to select relevant Noun Phrases
and Verb Phrases including modiﬁers such as ad-
verbs, adjectives and present participles. The pur-
pose is to capture ﬁne-grained senses of these
phrases. Example (1) illustrates the extraction of
the item declining revenue.

1) $MCD with declining revenue for a good

while

In example (1) only declining revenue is ex-
tracted. This segment is semantically relevant for
the classiﬁcation as Revenue Down, while the re-
mainder of the NP does not procure any informa-
tion regarding the type of aspect.
4.1.2 Computing Semantic Relatedness
Computing semantic relatedness consists of com-
paring vectors of candidates with vectors of as-
pect subclasses. First, multi-word candidates or
labels are combined into single vectors to obtain
pairs of candidate-aspect vectors. The method is
the sum of the vectors of multi-word expressions.
To compute relatedness between vectors, we use
the Indra implementation (Freitas et al., 2016) of
the cosine similarity metric. The system computes
cosine similarity for all possible pairwise combi-
nations of tokens in each message. We retain the
pair with the highest score.

4.2 Supervised Learning Models
This approach relies on training several machine-
learning models. Building the classiﬁer consists in
a multi-class supervised classiﬁcation task.
4.2.1 Feature Engineering
After preprocessing (tokenisation, accent removal,
lower-casing and POS tagging), messages were
converted into vectors including the following fea-
tures:

• Bag of Words (BoW) - They are used to cre-
ate a numerical representation of the vocabu-
lary of messages. We use three types of statis-

57tics (binary count, frequency count and tf-idf)
applied on n-gram clusters.

• Part of Speech - PoS are used to create a nu-
merical representation of the POS present in
each message. This representation is based
on the Penn Treebank POS tagset(Marcus
et al., 1993).

• Numericals - These are used to create a
representation of ﬁnancial values mentioned
in the messages such as percentages, ratios,
stock prices and amounts (e.g. $55).

• Predicted sentiment of entity- The senti-
ment predicted3 on the ﬁnancial entities in-
cluded in the messages that may contain as-
pects.
It is a continuous value on a [-1;1]
range.

4.2.2 Machine-Learning Algorithms and

Optimization

A number of Machine Learning Python-based
models were tested. Two methods are based on
decision trees with XGboost (Chen and Guestrin,
2016) and Random Forests (Breiman, 2001). We
also used Support Vector Machines (Vapnik, 2000)
and Conditional Random Fields (Lafferty et al.,
2001). Each of these methods use the same vector
representation created in the feature engineering
phase.

In order to ﬁnd the best hyper-parameters for
the tested models, we used the Particle Swarm
Optimization (PSO) method. This method was
appropriate due to the fact that hyper-parameters
are numbers, mostly in a continuous space. PSO
(Kennedy and Eberhart, 1995) was applied using
100 particles (speciﬁc hyper-parameter conﬁgura-
tions) during 100 iterations, using same weights
for velocity, particle best and global best. For each
particle position, the average accuracy in 10-fold
cross validation was calculated.

4.3 Model Selection, Validation and

Evaluation

Choosing the best classiﬁer is done in two stages.
Firstly, a model selection procedure helps select
the best model among the DSM and ML mod-
els. All models were tested with 10-fold cross-
validation whereby the dataset is divided in ten
parts. Each part is used as a test set once in the ten

3with the use of the SSIX FinSentiA Sentiment Analyser

(Gaillat et al., 2018).

iterations of the process. Secondly, the selected
model is validated by using the leave-one-out op-
tion, meaning that the training is conducted on all
instances except one. The process is repeated until
all instances have been used as a test instance.

In the model selection stage we computed
global accuracy for 32 classes. In the validation
stage, we used F1-Score for 7 and 32 classes to
measure the effects of the coarse and ﬁne-grained
annotation levels. The annotated corpus described
in Section 3 was used for training and testing. In
the DSM approach, 172 initially annotated mes-
sages were used as test set.

5 Results and Discussion

In the model selection stage all of the approaches
show different results as shown in Table 2.

Model

DSM (baseline)
ML Methods

Accuracy Standard
deviation
-
0.425

Xgboost
Random Forest

0.5689
0.5435
SVC 0.449
CRF 0.431

0.046
0.038
0.027
0.052

Table 2: Model selection stage: Accuracy for each
model for the 32 aspect classiﬁcation task

Xgboost was selected and validation showed re-
sults (see Table 3) in line with the best scores ob-
tained in SemEval-2015 Task 12.

Table 4 shows the accuracy for message clas-
siﬁcation according to the implicit or explicit na-
ture of the 32 aspects. The distinction between
implicit and explicit aspect messages shows that
explicit aspects are well classiﬁed while implicit
aspects are only correctly handled in about 35%
of cases. This suggests that the classiﬁer lacks sig-
niﬁcant features to identify implicit aspects. The
size of the data set appears to be a limitation but
the size of sentences may also impair the classiﬁer
by adding noise to the data. Using aspect-relevant
OTEs as a BoW feature could help address this
point.

6 Conclusion and Future Work

In this paper, we have reported on a series of ex-
periments in the domain of Aspect Extraction. The
experiments focused on the sub-task of aspect cat-

58Model
Xgboost
(32 classes)
Xgboost
(7 classes)

Acc

F1-Score P

R

0.565

0.49

0.52

0.49

0.712

0.71

0.70

0.71

Table 3: Model validation stage: Accuracy, F1-
Score, Precision (P) and Recall (R) for the 32 and
7 aspect classiﬁcation task

Aspects Acc
0.351
Implicit
Explicit
0.826

F1-Score P
0.32
0.8

0.28
0.84

R
0.28
0.8

Table 4: Accuracy according to messages includ-
ing 32 implicit and explicit aspects

egory identiﬁcation in the domain of stock invest-
ments. A taxonomy was used to identify prede-
ﬁned aspects in microblog messages. A distri-
butional semantics model and several supervised
learning methods were used for the task.

Results show that explicit aspect identiﬁcation
performs well, but implicit aspect identiﬁcation re-
mains an issue that can be tackled with larger data
set and improved feature engineering. Despite the
size of the training data set, results suggest that
more efforts can be invested in the development of
a larger data set.

7 Appendix
Taxonomy of stock-investment aspects

– Moving Lower
– Breakout
– New High
– Trending Higher
– Trending Lower
– Trending Sideways

• Asset Behaviour

– Oversold
– Overbought
– Overvalued
– Undervalued
– Short Squeeze
– Selling Pressure

• Financial Results
– Earnings Beat
– Earnings Miss
– Revenue Up
– Revenue Down
– Proﬁt Warning

• Analyst Ratings

– Buy Recommendation
– Sell Recommendation
– Rating Upgrade
– Rating Downgrade

• User Action

– Buying Intention
– Selling Intention
– Bought
– Sold
– Shorting
• User Outlook

– Positive Outlook
– Negative Outlook

• Insider Activity

– Insider Selling
– Insider Buying

• Asset Direction

– Moving Higher

References
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.

Leo Breiman. 2001. Random Forests. Machine Learn-

ing, 45(1):5–32.

Samuel Brody and Noemie Elhadad. 2010. An Un-
supervised Aspect-sentiment Model for Online Re-
views. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 804–812, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A
Scalable Tree Boosting System. In Proceedings of
the 22Nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD
’16, pages 785–794, New York, NY, USA. ACM.

59Zhiyuan Chen and Bing Liu. 2014. Topic Modeling
Using Topics from Many Domains, Lifelong Learn-
In Proceedings of the 31st In-
ing and Big Data.
ternational Conference on International Conference
on Machine Learning - Volume 32, ICML’14, pages
II–703–II–711, Beijing, China. JMLR.org.

WSDM ’11, pages 815–824, New York, NY, USA.
ACM.

Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing (2nd Edition). Prentice-
Hall, Inc., Upper Saddle River, NJ, USA.

Ivan Cruz, Alexander F. Gelbukh, and Grigori Sidorov.
2014. Implicit Aspect Indicator Extraction for As-
pect based Opinion Mining. Int. J. Comput. Linguis-
tics Appl., 5:135–152.

James Kennedy and Russel Eberhart. 1995.

Parti-
In , IEEE International
cle swarm optimization.
Conference on Neural Networks, 1995. Proceedings,
volume 4, pages 1942–1948 vol.4.

Brian Davis, Keith Cortis, Laurentiu Vasiliu, Adaman-
tios Koumpis, Ross Mcdermott, and Siegfried Hand-
schuh. 2016. Social Sentiment Indices Powered by
X-Scores. In ALLDATA 2016 , The Second Interna-
tional Conference on Big Data, Small Data, Linked
Data and Open Data, Lisbon, Portugal. Interna-
tional Academy, Research, and Industry Association
( IARIA ).

Nikoleta Dosoula, Roel Griep, Rick den Ridder, Rick
Slangen, Ruud van Luijk, Kim Schouten, and Flav-
ius Frasincar. 2016. Sentiment Analysis of Multi-
ple Implicit Features per Sentence in Consumer Re-
view Data. In Databases and Information Systems
(DB&IS), Riga, Latvia. Springer.

Lei Fang and Minlie Huang. 2012. Fine Granular As-
In
pect Analysis Using Latent Structural Models.
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers
- Volume 2, ACL ’12, pages 333–337, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Andr´e Freitas, Siamak Barzegar, Juliano Efson Sales,
Siegfried Handschuh, and Brian Davis. 2016. Se-
mantic Relatedness for All (Languages): A Com-
parative Analysis of Multilingual Semantic Related-
ness Using Machine Translation. In Eva Blomqvist,
Paolo Ciancarini, Francesco Poggi, and Fabio Vi-
tali, editors, Knowledge Engineering and Knowl-
edge Management: 20th International Conference,
EKAW 2016, Bologna, Italy, November 19-23, 2016,
Proceedings, pages 212–222. Springer International
Publishing, Cham.

Thomas Gaillat, Annanda Sousa, Manel Zarrouk, and
Brian, Davis. 2018. FinSentiA: Sentiment Analy-
sis in English Financial Microblogs. In Proceedings
of the TALN-CORIA 2018, Rennes, France. Revue
TAL.

Niklas Jakob and Iryna Gurevych. 2010. Extracting
Opinion Targets in a Single- and Cross-domain Set-
ting with Conditional Random Fields. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
pages 1035–1045, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Yohan Jo and Alice H. Oh. 2011. Aspect and Sen-
timent Uniﬁcation Model for Online Review Anal-
In Proceedings of the Fourth ACM Interna-
ysis.
tional Conference on Web Search and Data Mining,

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Label-
In Proceedings of the Eigh-
ing Sequence Data.
teenth International Conference on Machine Learn-
ing, ICML ’01, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers, San Rafael,
Calif.

Qian Liu, Bing Liu, Yuanlin Zhang, Doo Soon Kim,
and Zhiqiang Gao. 2016.
Improving Opinion As-
pect Extraction Using Semantic Similarity and As-
In Thirtieth AAAI Conference
pect Associations.
on Artiﬁcial Intelligence, Phoenix, Arizona USA.
AAAI Press.

Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
Review Selection Approach for Accurate Feature
Rating Estimation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ’10, pages 766–774, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient Estimation of Word Represen-
tations in Vector Space. ArXiv: 1301.3781.

Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open Domain
In Proceedings of the 2013
Targeted Sentiment.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1643–1654, Seattle, Wash-
ington, USA. ACL.

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
SemEval-2015 Task 12: Aspect Based Sentiment
In Proceedings of the 9th International
Analysis.
Workshop on Semantic Evaluation (SemEval 2015),
pages 486–495, Denver, USA. The Association for
Computational Linguistics.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou,
Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:

60Aspect Based Sentiment Analysis. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), pages 27–35, Dublin, Ire-
land. Association for Computational Linguistics and
Dublin City University.

Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews.
In Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ’05, pages 339–346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Soujanya Poria, Erik Cambria, and Alexander Gel-
bukh. 2016. Aspect extraction for opinion min-
ing with a deep convolutional neural network.
Knowledge-Based Systems, 108(Supplement C):42–
49.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion Word Expansion and Target Extrac-
tion Through Double Propagation. Comput. Lin-
guist., 37(1):9–27.

K. Schouten, O. van der Weijde, F. Frasincar, and
R. Dekker. 2018.
Supervised and Unsupervised
Aspect Category Detection for Sentiment Analysis
IEEE Transactions on
with Co-occurrence Data.
Cybernetics, 48(4):1263–1275.

Lei Shu, Bing Liu, Hu Xu, and Annice Kim. 2016. Su-
pervised Opinion Aspect Extraction by Exploiting
Past Extraction Results. CoRR, abs/1612.07940.

Vladimir Vapnik. 2000.

Learning Theory, 2 edition.
and Statistics. Springer-Verlag, New York.

The Nature of Statistical
Information Science

Xue Zhang, Hauke Fuehres, and Peter A. Gloor. 2011.
Predicting Stock Market Indicators Through Twitter
I hope it is not as bad as I fear. Procedia - Social
and Behavioral Sciences, 26(Supplement C):55–62.

61
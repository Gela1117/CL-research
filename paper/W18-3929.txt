HeLI-based Experiments in Swiss German Dialect Identiﬁcation

Tommi Jauhiainen
University of Helsinki

@helsinki.fi

Heidi Jauhiainen

University of Helsinki

@helsinki.fi

Krister Lind´en

University of Helsinki

@helsinki.fi

Abstract

In this paper we present the experiments and results by the SUKI team in the German Dialect
Identiﬁcation shared task of the VarDial 2018 Evaluation Campaign. Our submission using HeLI
with adaptive language models obtained the best results in the shared task with a macro F1-
score of 0.686, which is clearly higher than the other submitted results. Without some form of
unsupervised adaptation on the test set, it might not be possible to reach as high an F1-score with
the level of domain difference between the datasets of the shared task. We describe the methods
used in detail, as well as some additional experiments carried out during the shared task.
Introduction

1
The ﬁfth VarDial workshop (Zampieri et al., 2018) included for the second time a shared task for German
Dialect Identiﬁcation (GDI). The varieties of German were from the areas of Basel, Bern, Lucerne, and
Zurich. These varieties are considered dialects of Swiss German (gsw) by the ISO-639-3 standard (Lewis
et al., 2013). For the ﬁrst time the GDI shared task included a separate track for unknown language
detection.

We have used the HeLI method and its variations in the shared tasks of the three previous VarDial
workshops (Jauhiainen et al., 2015a; Jauhiainen et al., 2016; Jauhiainen et al., 2017a). The HeLI method
is robust and competes with the other state-of-the-art language identiﬁcation methods. For the current
workshop we wanted to try out some variations and possible improvements to the original method. We
submitted two different runs on the four-way classiﬁcation track and in the end we did not submit any
runs on the unknown language detection track.

2 Related Work
The differences between deﬁnitions of dialects and languages are not usually clearly deﬁned, at least not
in terms which would be able to help us automatically decide whether we are dealing with languages
or dialects. Also the methods used for dialect identiﬁcation are most of the time exactly the same as
for general language identiﬁcation. Language identiﬁcation of close languages and dialects is one of the
remaining challenges of language identiﬁcation research. For a recent survey on language identiﬁcation
and the methods used in the ﬁeld, the reader is referred to an article by Jauhiainen et al. (2018).

2.1 German dialect identiﬁcation
The German dialect identiﬁcation has earlier been considered by Scherrer and Rambow (2010), who
used a lexicon of dialectal words. Hollenstein and Aepli (2015) experimented with a perplexity based
language identiﬁer using character trigrams. They reached an average F-score of 0.66 on sentence level
between 5 German dialects.

The results of the ﬁrst shared task on German dialect identiﬁcation are described by Zampieri et al.
(2017). Ten teams submitted results on the task utilizing a variety of machine learning methods used

This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/.

ProceedingsoftheFifthWorkshoponNLPforSimilarLanguages,VarietiesandDialects,pages254–262SantaFe,NewMexico,USA,August20,2018.254for language identiﬁcation. We report the results in Table 1. The methods used are listed in the ﬁrst
column, used features in the second column, best reached weighted F1-score in the third column, and the
describing article in the fourth column.

Method
Ensemble of 9 individual SVMs
BM25 weighted SVM
Conditional Random Fields
SVM + SGD ensemble
Kernel Ridge Regression
Linear SVM
Cross Entropy
Perplexity
Naive Bayes with TF-IDF
LSTM NN

Features used
char. n-grams 1-8 and words
char. n-grams 1-5
char. n-grams, preﬁxes, sufﬁxes...
n-grams 1-8
n-grams 3-6
char. n-grams and words
char. n-grams up to 25 bytes
words

characters or words

Weighted F1

0.662
0.661
0.653
0.639
0.637
0.626
0.614
0.612
0.605
0.263

Reference

(Malmasi and Zampieri, 2017)

(Bestgen, 2017)

(Clematide and Makarov, 2017)

(Ionescu and Butnaru, 2017)
(C¸ ¨oltekin and Rama, 2017)

(Hanani et al., 2017)
(Gamallo et al., 2017)

(Barbaresi, 2017)

Table 1: The weighted F1-scores using different methods on the 2017 GDI test set.

2.2 Unknown language detection
Unknown languages are languages with which the language identiﬁer has not been trained. Especially
in a real-world situation it is always possible to encounter unknown languages. If the language identi-
ﬁcation method used produces a comparable score for different texts, it is possible to try thresholding.
In thresholding, we ﬁnd a score under (or over) which our prediction of the language is so poor, that
we label it as unknown. Suzuki et al. (2002) describes a language identiﬁcation method which was
originally designed for identifying the language of web pages crawled from the internet. They had a pre-
determined threshold for each language known by the identiﬁer and if the threshold was not reached the
web page was categorized as junk. In the Finno-Ugric Languages and the Internet project (Jauhiainen et
al., 2017b), we have also experimented with unknown language (or just junk) detection in order to cope
with pages written in languages not known to our identiﬁer. The method we use in production is based
on a language set identiﬁcation method (Jauhiainen et al., 2015b), which determines the languages used
on a page. The method is simply a threshold for the number of languages: if too many languages are
found in a piece of text, the text is categorized as junk. In the production environment our threshold is
currently 10 languages on one web page. The production threshold has been empirically determined for
the parameters used with the language set identiﬁcation method and for the number of languages known
by the identiﬁer.

2.3 Language model adaptation
Language model adaptation was used by Chen and Liu (2005) for identifying the language of speech. In
the system built by Chen and Liu (2005), the speech is ﬁrst run through Hidden Markov Model-based
phone recognizers (one for each language), which tokenize the speech into sequences of phones. The
probabilities of those sequences are calculated using corresponding language models and the most prob-
able language is selected. An adaptation routine is then used so that each of the phonetic transcriptions
of the individual speech utterances is used to calculate probabilities for words t, given a word n-gram
history of h as in Equation 1.

Pa(t|h) = λPo(t|h) + (1 − λ)Pn(t|h),

(1)

where Po is the original probability calculated from the training material, Pn the probability calculated
from the data being identiﬁed, and Pa the new adapted probability. λ is the weight given to original
probabilities. This adaptation method resulted in decreasing the error rate in a three-way identiﬁcation
between Chinese, English, and Russian by 2.88% and 3.84% on an out-of-domain (different channels)
data, and by 0.44% on in-domain (same channel) data.

Zhong et al. (2007) also used language model adaptation with language adaptation of speech. They

evaluated three different conﬁdence measures and the best faring measure is deﬁned as follows:

255C(gi, M ) =

1
n

[log(P (M|gi)) − log(P (M|gj))],

(2)

where M is the sequence to be identiﬁed, n the number of frames in the utterance, gi the best identiﬁed
language, and gj the second best identiﬁed language. The two other evaluated conﬁdence measures
were clearly inferior. Although the C(gi, M ) measure performed the best of the individual measures, a
Bayesian classiﬁer based ensemble using all the three measures gave slightly higher results. Zhong et al.
(2007) use the same language adaptation method as Chen and Liu (2005), using the conﬁdence measures
to set the λ for each utterance.

3 Test setup

The dataset used in the shared task consists of manual transcriptions of speech utterances by speakers
from different areas: Bern, Basel, Lucerne, and Zurich. The transcriptions are written entirely in lower-
cased letters. Samardˇzi´c et al. (2016) describe the ArchiMob corpus, which is the source for the shared
task dataset. Zampieri et al. (2017) describe how the training and test sets were extracted from the Archi-
Mob corpus for the 2017 shared task. The sizes of the training and development sets can be seen in
Table 2. The ﬁrst track of the shared task was a standard four-way language identiﬁcation between the
four German dialects present in the training set.

Variety
Bern (BE)
Basel (BS)
Lucerne (LU)
Zurich (ZH)

Training Development
32,447
30,770
32,955
32,714

8,471
11,116
9,966
9,039

Table 2: The sizes in words of the datasets distributed for the 2018 GDI shared task.

This year the GDI task also included a second track for unknown dialect detection. The unknown
dialect was not included in the training or the development sets, but it was present in the test set. The test
set was identical for both tracks, but the lines containing unknown dialect were ignored when calculating
the scores for the ﬁrst track.

4 Basic HeLI method, run 1 on track 1

We ﬁrst presented the HeLI method, originally published by Jauhiainen (2010), at the VarDial 2016
(Jauhiainen et al., 2016). To make this article more self-contained, we present the full description of the
method as it is used in the best submitted run for the GDI shared task. The survey by Jauhiainen et al.
(2018) uses the same uniﬁed notation to deﬁne the features and methods used for language identiﬁcation.
This description differs from the original mostly in that we are leaving out the cut-off value c for the size
of the language models as using all the available material was always the best option. When we are
not using the cut-off value, no derived corpus C(cid:48) consisting of the used features is generated. The ﬁnal
submissions were done with a system using only lowercased character 4-grams, so we present the method
without the back-off function. For the complete description of the HeLI method see our VarDial 2016
article (Jauhiainen et al., 2016).

4.1 Description of the HeLI method using only 4-grams of characters
The goal is to correctly guess the language g ∈ G for each of the lines in the test set. In the method,
each language g is represented by a lowercased character 4-gram language model. The training data is
tokenized into words using non-alphabetic and non-ideographic characters as delimiters and the words
are lowercased. The relative frequencies of character 4-grams are calculated inside the words, so that
the preceding and the following space-characters are included. The 4-grams are overlapping, so that for
example a word with three characters include two character 4-grams. Then we transform the relative
frequencies into scores using 10-based logarithms.

256The corpus containing only the n-grams of the length 4 in the language models is called C4. The
domain dom(O(C4)) is the set of all character n-grams of length 4 found in the models of any language
(u) are calculated similarly for all n-grams u ∈ dom(O(C4)) for each language
g ∈ G. The values vC4
g, as shown in Equation 3.

g

 − log10

p

(cid:16) c(C4

(cid:17)

g ,u)
lC4
g

vC4

g

(u) =

, if c(C4
, if c(C4

g , u) > 0
g , u) = 0,

(3)

g , u) is the number of n-grams u found in the corpus of the language g and lC4

where c(C4
is the total
number of the n-grams of length 4 in the corpus of language g. These values are used when scoring the
words while identifying the language of a text. The word t is split into overlapping 4-grams of characters
i , where i = 1, ..., lt − 4. lt is the length of the word in characters, including the preceding and the
u4
following space-characters. Each of the n-grams u4

If the n-gram u4

i is not
found in any of the models, it is simply discarded. We deﬁne the function dg(t, 4) for counting n-grams
in t found in a model in Equation 4.

i is found in dom(O(C4

i is then scored separately for each language g.
g )), the values in the models are used. If the n-gram u4

g

dg(t, 4) =

i ∈ dom(O(C4))

, if u4
, otherwise.

(4)

When all the n-grams of the size 4 in the word t have been processed, the word gets the value of the

average of the scored n-grams u4

i for each language, as in Equation 5.

vg(t, 4) =

vC4

g

(u4
i )

, if dg(t, 4) > 0,

(5)

lt−4(cid:88)

(cid:26) 1

0

i=1

lt−4(cid:88)

1

dg(t, 4)

i=1

where dg(t, 4) is the number of n-grams u4
g )). If all of the n-grams
of the size 4 were discarded, dg(t, 4) = 0, a word gets the penalty value p for every language, as in
Equation 6.

i found in the domain dom(O(C4

vg(t, 0) = p

(6)

The mystery text is tokenized into words using the non-alphabetic and non-ideographic characters as
delimiters. The words are lowercased. After this, a score vg(t, 4) is calculated for each word t in the
mystery text for each language g. The whole line M gets the score Rg(M ) equal to the average of the
scores of the words vg(t, 4) for each language g, as in Equation 7.

(cid:80)lT (M )

Rg(M ) =

i=1 vg(ti, 4)

lT (M )

(7)

where T (M ) is the sequence of words and lT (M ) is the number of words in the line M. Since we
are using negative logarithms of probabilities, the language having the lowest score is returned as the
language with the maximum probability for the mystery text.

4.2 Experiments on the development set and results on the test set
The training dataset was completely written in lowercase so we used only lowercased versions of the
language models. First we tested the effect of not using all the data in the language models with varying
the cut-off parameter c on the development set. The largest language model was the character 6-gram
model for the Basel-area dialect with 16,947 different 6-grams. The results using optimized penalty
values for each c are presented in Table 3. The results would seem to indicate that the recall starts to
decline in an increasing manner as soon as some of the material from the language models is left out. This

257is something we have noticed before in settings where the training data is of very good quality. If we are
using the identiﬁer in a production setting with, for example, Wikipedia-derived language models, some
of the models include so much noice that not using the complete models improves the results (Jauhiainen
et al., 2017b). The optimal penalty value is also clearly tied to the maximum size of the used language
models.

Lowercased words Lowercased nmax

8
8
8

Penalty p Cut-off c
16,947
15,000
10,000

5.1
4.9
4.8

Recall
64.47%
64.41%
64.19%

yes
yes
yes

Table 3: Basic HeLI method results on the development set with varying c.

We decided to use all the available data and then optimized the used language models and the penalty
value p. The results on the development set with different model combinations can be seen in Table 4.
The penalty value p presented in the third column was the optimal one for each conﬁguration. The HeLI
method using character n-grams from one to four attained the best recall 65.97%.

Lowercased words Lowercased nmax

no
no
no
no
no
yes
no
no
no

4
5
6
8
7
8
3
2
1

Penalty p

5.8
5.4
5.3
5.4
5.1
5.1
5.7
5.7
5.6

Recall
65.97%
65.39%
64.79%
64.56%
64.51%
64.47%
62.80%
53.43%
33.13%

Table 4: Basic HeLI method results on the development set using different language model combinations.

We also experimented with leaving out the lower order n-grams. The results of these experiments on
the development set can be seen in Table 5. To our surprise, the best results were attained using only the
4-grams of characters, which means that the backoff function of the HeLI method is not used at all. The
recall on the development set was 66.10%. We also re-tested using lower cut-offs c, but leaving off any
material in the language models only made the results worse again.

Lowercased words Lowercased n-gram range

not used
not used
not used
not used

4 - 4
1 - 4
2 - 4
3 - 4

Penalty p

5.8
5.8
5.8
5.8

Recall
66.10%
65.97%
65.97%
65.97%

Table 5: Basic HeLI method results on the development set with different n-gram ranges.

We decided to use the character 4-grams and the penalty value of 5.8 for the ﬁrst run. We added the
development data to the training data and generated new models. The system attained a recall of 63.97%
on the test set, which was somewhat less than what we had seen with the development set.

5 HeLI with language model adaptation, run 2 on track 1

While experimenting with the basic HeLI method we created a test setting to detect the difference of
using out-of-domain and in-domain training data. For each language, we divided the development set in
two halves. We experimented with adding the ﬁrst half of the development data (we call it dv-dv) to the
training data (tr) of each language, creating new language models and testing them on the second half
of the development data (dv-tst). The recalls on the second half of the development set (dv-tst) using
the combined tr and dv-dv for training were much better than the recalls on the ﬁrst half (dv-dv) using
just tr for training. The recalls can be seen in the fourth column of Table 6. In the heading of the table,
the training data used is indicated in parenthesis after the test data. We decided to try to test dv-tst also

258without including dv-dv in the training set in order to see if dv-tst was for some reason generally easier
to identify than dv-dv. The second half of the development data, dv-tst, turned out to be a little bit easier
to identify than the ﬁrst half, dv-dv, using the original models generated from just the training data as
can be seen in the ﬁfth column of Table 6. Another hypothesis is that the development data is from an
another source than the training data and the ﬁrst half introduces a great number of new words which are
relevant to the second half. Also we wanted to know if just 15.9% increase in training text amount could
generate this much better recall. In order to test this hypothesis we removed the same amount of lines
as was in dv-dv from the training data, marked as tr-sz(dv-dv) in the table, and inserted the dv-dv lines
instead. The recall percentages from those tests are in the ﬁnal column of Table 6 and they suggest that
the development data is indeed from a somewhat different domain than the training data and the identiﬁer
actually performs better when some of the original training data is removed.

It can also be seen from the results of the experiments that the best models for in-domain experiments
were word and character n-grams from one to ﬁve and for the out-of-domain they were character n-grams
from one to four or just 4-grams. This would then indicate that if the domain of the language to be tested
is the same or similar to the one that the models have been created from, the models could use longer
character n-grams and words, if not, then using just character n-grams is a better strategy.

n-gram range Words

1 - 8
1 - 7
1 - 6
1 - 5
1 - 4
1 - 3
1 - 2

1
-

1 - 8
1 - 7
1 - 6
1 - 5
1 - 4

1 - 3
1 - 2

4

1

dv-dv (tr)
63.65%
63.82%
64.16%
64.59%
64.76%
65.28%
65.11%
63.95%
63.61%
63.65%
63.73%
64.21%
65.15%
65.45%
65.28%
61.89%
52.88%
33.18%

dv-tst (tr+dv-dv)

78.65%
78.95%
78.91%
79.55%
79.25%
78.74%
78.35%
78.01%
77.53%
78.74%
78.91%
78.95%
79.21%
78.35%
78.14%
73.20%
60.87%
36.34%

dv-tst (tr)
65.46%
65.89%
66.02%
65.98%
66.07%
66.67%
66.37%
65.29%
64.86%
65.59%
65.42%
65.21%
65.76%
66.58%
67.35%
63.70%
54.08%
33.08%

dv-tst (tr-sz(dv-dv)+dv-dv)

79.55%
79.68%
79.90%
80.41%
79.94%
79.77%
79.17%
78.57%
78.01%
79.47%
79.51%
79.55%
79.94%
79.25%
78.57%
73.97%
61.60%
37.5%

yes
yes
yes
yes
yes
yes
yes
yes
yes
no
no
no
no
no
no
no
no
no

Table 6: Baseline HeLI recalls using different combinations of training and development sets.

What we learned from these experiments with the basic HeLI method is that, if we would be able to
somehow incorporate well identiﬁed sentences into the original models it might introduce crucial new
word or character n-gram vocabulary. We decided to try always adding the character 4-grams from the
most conﬁdently identiﬁed sentence to the language model of the respective language and re-identifying
the rest, always marking the best identiﬁed sentence as not needing to be identiﬁed again. This process is
recursive and it runs until all the sentences except the last one are used for language modelling. In order
to decide which sentence is most conﬁdently identiﬁed, we need a conﬁdence score. As a conﬁdence
measure CM, we used the difference between the scores of the best Rg(M ) and the second best Rh(M )
identiﬁed language for each line. Later we found that basically the same conﬁdence measure was earlier
proposed by Zhong et al. (2007). In our case it is calculated using the Equation 8:

CM (Cg, M ) = Rh(M ) − Rg(M )

(8)

where M is the line containing the mystery text. It could be beneﬁcial to end the recursive adaptation
before all the sentences are exhausted, if the conﬁdence score is reliable enough. However, we did not
have time to experiment with a cut-off value for the conﬁdence score before the submissions were due.
The identiﬁer with language model adaptation reached 77.99% recall on the development set with
the same language models (character 4-gram) and penalty value (5.8) which we used with the basic
HeLI method in run 1. It was an increase of 12.71% on top of the recall of the basic HeLI method.

259We suspected that using higher order n-grams or words could produce even better results, but we did
not have time to test this theory. We added the development data to the training data, generated new
language models and submitted our run 2. The second run reached a recall of 69.19% on the test set,
an increase of 5.22%. The macro F1-score attained on the run 2 was 0.6857. The results are very good
considering that there was an unknown dialect within the actual test set and all the lines in the unknown
dialect were incorrectly incorporated into some of the language models. The ﬁnal results compared with
the best results submitted by other teams are shown in Table 7.

System
HeLI with adaptive language models, run 2
benf
saﬁna
taraka rama
The basic HeLI method, run 1
LaMa
XAC
GDI classiﬁcation
dkosmajac
Random Baseline

F1 (macro)
0.6857
0.6464
0.6449
0.6398
0.6386
0.6374
0.6336
0.6203
0.5909
0.2521

Table 7: Results compared with the other submitted runs. Our submitted results are bolded.

6 Experiments with unknown language detection, track 2

The basic HeLI method always maps the mystery text M into one of the languages it has been trained
with. The 2015 Discriminating Between Similar Languages shared task included an unknown category
which contained several a priori unknown languages. One of the methods we used in 2015 was using a
threshold for the score Rg(M ) to detect the unknown language. In order to assess the suitability of using
the threshold score with the German dialects, we compared the range of the scores when g was correctly
or incorrectly identiﬁed using the character 4-gram language models on the development set. The score
ranges can be seen in Table 8, where the line with correct identiﬁcations is bolded. The lower the score,
the better the mystery text ﬁts the language. The scores ranged from 1.28 to 4.56 when the dialect was
correctly identiﬁed, with most of the scores higher than the lower ranges of the incorrect identiﬁcations.
The fact that the worst absolute score (4.56) was attained with a correct identiﬁcation drove us to the
conclusion that simply using the score as a cut-off would not be a quick solution to the unseen language
problem. Due to time restrictions, we did not pursue this investigation further. We were also unable to
test the language set based thresholding method we are using in the production environment. In the end,
we did not submit any results to the unknown language detection track.

Correct language

ZH
ZH
ZH
ZH

Identiﬁed language

ZH
LU
BS
BE

1.28
2.37
1.71
2.26

4.56
4.23
4.21
3.95

Lowest score Highest score

Table 8: Score ranges when trying to identify the dialect from Zurich area.

7 Conclusions

The macro F1-score attained by the basic HeLI method is within 0.0078 score difference to the best ﬁve
results submitted by the other teams. Unsupervised language model adaptation improved on the recall
of the basic HeLI-method by 5.22%. The score difference between our run using the adaptive language
models and the second best submitted run is 0.0393. Language model adaptation would seem to be
especially usable in situations where the training material can be expected to be from a different domain
than the material to be identiﬁed. The adaptation method proved to be very robust as it performed well
even with the unknown language present in the test set.

260Acknowledgments
This research was partly conducted with funding from the Kone Foundation Language Programme (Kone
Foundation, 2012).

References
Adrien Barbaresi. 2017. Discriminating between Similar Languages using Weighted Subword Features. In Pro-
ceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages 184–
189, Valencia, Spain.

Yves Bestgen. 2017. Improving the Character Ngram Model for the DSL Task with BM25 Weighting and Less
Frequently Used Feature Sets. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
and Dialects (VarDial), pages 115–123, Valencia, Spain.

C¸ a˘gri C¸ ¨oltekin and Taraka Rama. 2017. T¨ubingen system in VarDial 2017 shared task: experiments with language
identiﬁcation and cross-lingual parsing. In Proceedings of the Fourth Workshop on NLP for Similar Languages,
Varieties and Dialects (VarDial), pages 146–155, Valencia, Spain.

Yingna Chen and Jia Liu. 2005. Language Model Adaptation and Conﬁdence Measure for Robust Language
Identiﬁcation. In Proceedings of International Symposium on Communications and Information Technologies
2005 (ISCIT 2005), volume 1, pages 270–273, Beijing, China.

Simon Clematide and Peter Makarov. 2017. CLUZH at VarDial GDI 2017: Testing a Variety of Machine Learning
In Proceedings of the Fourth Workshop on NLP for

Tools for the Classiﬁcation of Swiss German Dialects.
Similar Languages, Varieties and Dialects (VarDial), pages 170–177, Valencia, Spain.

Pablo Gamallo, Jose Ramom Pichel, and I˜naki Alegria. 2017. A Perplexity-Based Method for Similar Languages
Discrimination. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects
(VarDial), pages 109–114, Valencia, Spain.

Abualsoud Hanani, Aziz Qaroush, and Stephen Taylor. 2017. Identifying dialects with textual and acoustic cues.
In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
93–101, Valencia, Spain.

Nora Hollenstein and No¨emi Aepli. 2015. A Resource for Natural Language Processing of Swiss German Di-
alects. In Proceedings of the International Conference of the German Society for Computational Linguistics
and Language Technology (GSCL 2015), pages 108–109, University of Duisburg-Essen, Germany.

Radu Tudor Ionescu and Andrei Butnaru. 2017. Learning to Identify Arabic and German Dialects using Mul-
tiple Kernels. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects
(VarDial), pages 200–209, Valencia, Spain.

Tommi Jauhiainen, Heidi Jauhiainen, and Krister Lind´en. 2015a. Discriminating Similar Languages with Token-
Based Backoff. In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages,
Varieties and Dialects (LT4VarDial), pages 44–51, Hissar, Bulgaria.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2015b. Language Set Identiﬁcation in Noisy Synthetic
Multilingual Documents. In Proceedings of the Computational Linguistics and Intelligent Text Processing 16th
International Conference, CICLing 2015, pages 633–643, Cairo, Egypt.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2016. HeLI, a Word-Based Backoff Method for Lan-
In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and

guage Identiﬁcation.
Dialects (VarDial3), pages 153–162, Osaka, Japan.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2017a. Evaluating HeLI with Non-Linear Mappings.
In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
102–108, Valencia, Spain.

Tommi Jauhiainen, Krister Lind´en, and Heidi Jauhiainen. 2017b. Evaluation of Language Identiﬁcation Methods
Using 285 Languages. In Proceedings of the 21st Nordic Conference on Computational Linguistics (NoDaLiDa
2017), pages 183–191, Gothenburg, Sweden. Link¨oping University Electronic Press.

Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lind´en. 2018. Automatic Lan-

guage Identiﬁcation in Texts: A Survey. arXiv preprint arXiv:1804.08186.

261Tommi Jauhiainen. 2010. Tekstin kielen automaattinen tunnistaminen. Master’s thesis, University of Helsinki,

Helsinki.

Kone Foundation. 2012. The Language Programme 2012-2016. http://www.koneensaatio.ﬁ/en.

M. Paul Lewis, Gary F. Simons, and Charles D. Fennig, editors. 2013. Ethnologue: Languages of the world,

seventeenth edition. SIL International, Dallas, Texas.

Shervin Malmasi and Marcos Zampieri. 2017. German Dialect Identiﬁcation in Interview Transcriptions.

In
Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages
164–169, Valencia, Spain.

Tanja Samardˇzi´c, Yves Scherrer, and Elvira Glaser. 2016. ArchiMob–A corpus of spoken Swiss German.

Proceedings of the Language Resources and Evaluation (LREC), pages 4061–4066, Portoroz, Slovenia.

In

Yves Scherrer and Owen Rambow. 2010. Word-based Dialect Identiﬁcation with Georeferenced Rules. In Pro-
ceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages
1151–1161, Massachusetts, USA. Association for Computational Linguistics.

Izumi Suzuki, Yoshiki Mikami, Ario Ohsato, and Yoshihide Chubachi. 2002. A Language and Character Set De-
termination Method Based on ngram Statistics. ACM Transactions on Asian Language Information Processing
(TALIP), 1(3):269–278.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubeˇsi´c, Preslav Nakov, Ahmed Ali, J¨org Tiedemann, Yves Scherrer,
and No¨emi Aepli. 2017. Findings of the VarDial Evaluation Campaign 2017. In Proceedings of the Fourth
Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), Valencia, Spain.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, Suwon Shon, James Glass, Yves Scherrer, Tanja
Samardˇzi´c, Nikola Ljubeˇsi´c, J¨org Tiedemann, Chris van der Lee, Stefan Grondelaers, Nelleke Oostdijk, Antal
van den Bosch, Ritesh Kumar, Bornini Lahiri, and Mayank Jain. 2018. Language Identiﬁcation and Mor-
phosyntactic Tagging: The Second VarDial Evaluation Campaign. In Proceedings of the Fifth Workshop on
NLP for Similar Languages, Varieties and Dialects (VarDial), Santa Fe, USA.

Shan Zhong, Yingna Chen, Chunyi Zhu, and Jia Liu. 2007. Conﬁdence measure based incremental adaptation for
online language identiﬁcation. In Proceedings of International Conference on Human-Computer Interaction
(HCI 2007), pages 535–543, Beijing, China.

262
EmotionX-JTML: Detecting emotions with Attention

Johnny Torres

ESPOL University / Guayaquil, Ecuador

jomatorr@espol.edu.ec

Abstract

Chandler

This paper addresses the problem of auto-
matic recognition of emotions in text-only
conversational datasets for the EmotionX
challenge. Emotion is a human character-
istic expressed through several modalities
(e.g., auditory, visual, tactile), therefore,
trying to detect emotions only from the
text becomes a difﬁcult task even for hu-
mans. This paper evaluates several neural
architectures based on Attention Models,
which allow extracting relevant parts of
the context within a conversation to iden-
tify the emotion associated with each ut-
terance. Empirical results the effective-
ness of the attention model for the Emo-
tionPush dataset compared to the baseline
models, and other cases show better results
with simpler models.

1

Introduction

With technology increasingly present in people’s
lives, human-machine interaction needs to be as
natural as possible, including the recognition of
emotions. Emotions are an intrinsic characteristic
of humans, often associated with mood, tempera-
ment, personality, disposition or motivation (Aver-
ill, 1980). Moreover, emotions are inherently mul-
timodal, as such, we perceived them in great detail
through vision or speech (Jain and Li, 2011).

Detecting emotions from text poses particular
difﬁculties. For instance, an issue that arises from
working with conversational text data is that the
same utterance (message) can express different
emotions depending on its context. The table 1
illustrate the issue with some utterances express-
ing different emotions with the same word from
the challenge datasets (Chen et al., 2018).

I guess it must’ve been some
movie I saw. (Neutral)

Chandler What do you say? (Neutral)

Monica Okay! (Joy)

Chandler Okay! Come on! Let’s go! All

right! (Joy)

Rachel Oh okay, I’ll ﬁx that to. What’s her

e-mail address? (Neutral)

Ross Rachel! (Anger)

Rachel All right, I promise. I’ll ﬁx this. I
swear. I’ll-I’ll- I’ll-I’ll talk to her.
(Non-neutral)
Ross Okay! (Anger)
Rachel Okay. (Neutral)

Table 1: Two dialogs from Friends TV scripts.
The word “Okay!” denote different emotions de-
pending of the context.

Despite improvements with neural architec-
tures, given an utterance in a conversation with-
out any previous context, it is not always obvious
even for human beings to identify the emotion as-
sociated. In many cases, the classiﬁcation of ut-
terances that are too short is hard. For instance,
the utterance ’Okay’ can be either an Agreement
or indicative of Anger, for such cases the context
plays an essential role at disambiguation. There-
fore, using context information from the previous
utterances in a conversation ﬂow is a crucial step
for improving DA classiﬁcation.

In this paper, we explore the use of AMs to learn
the context representation, as a manner to differ-
entiate the current utterance from its context as
well as a mechanism to highlight the most relevant
information while ignoring unnecessary parts for
emotion classiﬁcation. We propose and compare
different neural-based methods for context repre-
sentation learning by leveraging a recurrent neu-

ProceedingsoftheSixthInternationalWorkshoponNaturalLanguageProcessingforSocialMedia,pages56–60,Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics56ral network architecture with LSTM (Hochreiter
and Schmidhuber, 1997) or gated recurrent units
(GRUs) (Chung et al., 2014) in combination with
AMs.

2 Related Work

The identiﬁcation of emotions is an essential task
for understanding natural language and building
conversational systems. Previous works on recog-
nizing emotion in text documents consider three
categories: keyword-based, learning-based, and
hybrid recommendation approaches (Kao et al.,
2009).

In recent years,

learning methods based on
neural architectures have achieved great success.
Emotion recognition can be framed as a sen-
tences classiﬁcation task and has been addressed
using various traditional statistical methods, such
as Markov Models (HMM) (Stolcke et al., 2000),
conditional random ﬁelds (CRF) (Zimmermann,
2009) and support vector machines (SVM) (Hen-
derson et al., 2012). Recent work has shown
advances in text classiﬁcation using deep learn-
ing techniques, such as convolutional neural net-
works (CNN) (Kalchbrenner and Blunsom, 2013;
Lee and Dernoncourt, 2016), recurrent neural net-
works (RNNs) (Lee and Dernoncourt, 2016; Ji
et al., 2016) and short-term long memory models
(LSTM) (Shen and Lee, 2016).

Recent previous works have suggested utilizing
context as possible prior knowledge for utterance
classiﬁcation (Lee and Dernoncourt, 2016; Shen
and Lee, 2016). Contextual information from
preceding utterances has been found to improve
the classiﬁcation performance, but it depends on
the speciﬁc aspect of the dataset Ortega and Vu
(2017). These works highlight that such informa-
tion should be differentiable from the current ut-
terance information; otherwise, the contextual in-
formation could have a negative impact.

Attention mechanisms (AMs) introduced by
Bahdanau et al. (2014) have contributed to sig-
niﬁcant improvements in many natural language
processing tasks,
for instance machine trans-
lation (Bahdanau et al., 2014), sentence clas-
siﬁcation (Shen and Lee, 2016) and summa-
rization (Rush et al., 2015), uncertainty detec-
tion (Adel and Sch¨utze, 2016), speech recogni-
tion (Chorowski et al., 2015), sentence pair model-
ing (Yin et al., 2015), question-answering (Golub
and He, 2016), document classiﬁcation (Yang

Figure 1: Label distribution of the datasets in the
challenge.

et al., 2016) and entailment (Rockt¨aschel et al.,
2015) . AMs let the model decide what parts of
the input to pay attention to according to the rele-
vance of the task.

3 Data

Conversational datasets with utterance informa-
tion are accessible such as movies,
television
scripts or chat records. Although, despite the im-
portance of emotion detection in conversational
systems, most datasets do not have emotion tags,
so it is not possible to use such data directly to
train models to identify emotions.

The EmotionX challenge provides two anno-
tated datasets with emotions tags. The ﬁrst, de-
noted Friends, contains the scripts of seasons 1
to 9 of Friends TV shows1. The second, denoted
EmotionPush, consist of private conversations be-
tween friends on Facebook Messenger collected
by the appEmotionPush (2016).

Each utterance in the datasets has the same for-
mat: the user, the message, and the emotion label.
The labels are one of six primary emotions anger,
disgust, fear, happiness, sadness, surprise, and
neutral deﬁned in (1987). EmotionPush dataset
has more skewed label distribution than Friends
dataset as shown in Fig.1.

Both Friends and EmotionPush datasets contain
1,000 dialogues. The length distribution of utter-
ances in EmotionPush dataset is much shorter than
the length of those of TV show scripts (10.67 vs.
6.84). The EmotionPush dataset is anonymized to
hide users’ details such as names of real people,
locations, organizations, and email addresses. Ad-

1Scripts of seasons 1-9 of “Friends”: http://www.

livesinabox.com/friends/scripts.shtml

57102103angerdisgustfearjoyneutralnon-neutralsadnesssurpriseFriendsEmotionPushu(t − 1)

u(t)

u(t − 2)

word
embeddings...

convolution layer

pooling layer

Pt−2

Pt−1

Pt−3

Context Attention Method

Softmax

Context representation

Figure 2: An overview of the architecture of the
model based on Attention for classifying emotions
in the conversation context.

ditional steps were applied to ensure the privacy of
users as described in the dataset paper (Chen et al.,
2018).

4 Model

The architecture of the model considers two main
parts: the CNN-based utterance representation and
the attention mechanism for context representation
learning. The Figure 2 shows an overview of the
model. The model feeds the context representa-
tion into a softmax layer which outputs the pos-
terior of each context utterances given the current
utterance.

4.1 Utterance Representation
The proposed architecture uses CNNs for the rep-
resentation of each utterance. For the emotion
classiﬁcation task, the input matrix represents an
utterance and its context (i.e.,n previous utter-
ances). Each column of the matrix stores the em-
beddings of the corresponding word, resulting in
d dimensional input matrix M ∈ RM×d. The
weights of the word embeddings use the 300-
dimensional GloVe Embeddings pre-trained on
Common Crawl data (Pennington et al., 2014).

The model performs a discrete 1D convolution
on an input matrix with a set of different ﬁlters of
width |f| across all embedding dimensions d, as
described by the following equation:

(w∗f )(x, y) =

d(cid:88)

|f|/2(cid:88)

i=1

j=−|f|/2

w(i, j)·f (x−i, y−j)

(1)

After the convolution, the model applies a max
pooling operation that stores only the highest ac-
tivation of each ﬁlter. Additionally, the model ap-
plies ﬁlters with different window sizes 3-5 (multi-
windows), which span a different number of input
words. Then, the model concatenates all feature
maps to one vector which represents the current
utterance and its context.

4.2 Attention Layer
The model applies an attention layer to different
sequences of input vectors, e.g., representations of
consecutive utterances in a conversation. For each
of the input vectors u(t − i) at time step t − i in
a conversation, the model computes the attention
weights for the current time step t as follows:

(cid:80)
exp(f (u(t − i)))
0<j<m exp(f (u(t − j))

αi =

(2)

where f is the scoring function. In the model, f is
the linear function of the input u(t − i)
f (u(t − i)) = W T u(t − i)

(3)

(cid:88)

where W is a trainable parameter. The out-
put attentive u after the attention layer is the
weighted sum of the input sequence.

attentive u =

αiu(t − i)

(4)

i

4.3 Context Modeling
This paper evaluates different methods to learn the
context representation using AMs.

Max This method applies max-pooling on top of
the utterance representations which spans all the
contexts and the embedding dimension.

Input This method applies the attention mecha-
nism directly on the utterance representations. The
weighted sum of all the utterances represents the
context information.

GRU-Attention This method uses a sequential
model with GRU cells on top of the utterance rep-
resentations to learn the relationship between the
context and the current utterance over time. The
output of the hidden layer of the last state is the
context representation.

58NB

CNN

CNN-BiLSTM

GRU-Attention

Friends
EmotionPush∗
Friends
EmotionPush∗
Friends
EmotionPush∗
Friends
EmotionPush∗

WA UWA Neu
51.4
54.9
68.7
67.3
59.2
64.3
80.8
71.5
63.9
74.7
77.4
87.0
85.2
57.1
78.2
91.4

57.4
57.3
45.2
41.7
43.1
39.4
33.4
46.8

Joy
57.5
76.2
60.2
46.9
61.8
60.3
46.0
65.7

Sad
50.0
87.5
41.2
43.7
45.9
28.7

-

29.9

Fea
-
-

21.9
0.0
12.5
0.0
3.1
-

Ang
100.0

-

46.6
27.0
46.6
32.4
45.1

-

Sur
76.3
100.0
61.5
53.8
51.0
40.9
51.8
58.3

Non
36.8
26.7
20.6
40.0
8.8
26.7
30.0
47.1

Table 2: Weighted and unweighted accuracy on Friends and EmotionPush

5 Experiments

For the experiments, neural architectures apply an
end-to-end learning approach, i.e., with minimum
text preprocessing. For cross-validation, the split-
ting strategy divides them by the dialogues, similar
to (Chen et al., 2018).

The challenge evaluates the performance us-
ing the metrics weighted accuracy (WA) and un-
weighted accuracy (UWA), as deﬁned in equations
5 and 6.

(cid:88)

l∈C

1
|C|

WA =

UWA =

slal

(cid:88)

l∈C

al

(5)

(6)

where al denotes the accuracy of emotion class
l and sl denotes the percentage of utterances in
emotion class l.

The Table 2 shows the experimental results in-
cluding baselines for the emotion detection task.
This paper evaluated a Multinomial Naive Bayes
(NB) model and the proposed Attention Model
(AM). Surprisingly, NB model outperforms neu-
ral models for UWA metric in both datasets with
57.4% and 57.3%. This result could be related to
the size of the dataset since neural architectures
take advantage of learning on large-scale datasets.
The attention model performs well on the Emo-
tionPush dataset but fails to improve on the
Friends datasets for WA metric. Further evaluation
of the results as depicted in the Fig. 3, show that
the label imbalance for neutral emotion affects the
predictions of other labels.

Figure 3: Confusion matrix for the results of At-
tention Model on the Friend dataset.

6 Conclusions and Future Work

This paper presents a neural attention model for
the EmotionX challenge. Attention models take
advantage of the context information in conversa-
tional datasets for recognizing emotions. The re-
sults obtained through several experiments outper-
formed the baseline methods in some metrics in
the emotionPush dataset and was less effective on
the Friends dataset.

Despite the promising results with Attention
Models, the model struggles to accurately detect
ambiguous utterances in the Friend dataset due to
the label imbalance and the small scale of it. As
such, large-scale conversational corpus with anno-
tated data becomes crucial for pushing the fron-
tiers in emotion recognition.

Attention methods have the potential to provide
improved accuracy in detecting emotions in con-
versational datasets, and future work can explore
additional strategies for Attention Models.

59References
Heike Adel and Hinrich Sch¨utze. 2016. Exploring dif-
ferent dimensions of attention for uncertainty detec-
tion. arXiv preprint arXiv:1612.06549 .

James R Averill. 1980. A constructivist view of emo-
tion. In Theories of emotion, Elsevier, pages 305–
339.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473 .

Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo,
Lun-Wei Ku, et al. 2018. Emotionlines: An emotion
corpus of multi-party conversations. arXiv preprint
arXiv:1802.08379 .

Jan K Chorowski, Dzmitry Bahdanau, Dmitriy
Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
2015. Attention-based models for speech recogni-
tion. In Advances in neural information processing
systems. pages 577–585.

Edward Chao-Chun Kao, Chun-Chieh Liu, Ting-Hao
Yang, Chang-Tai Hsieh, and Von-Wun Soo. 2009.
Towards text-based emotion detection a survey and
In Information Manage-
possible improvements.
ment and Engineering, 2009. ICIME’09. Interna-
tional Conference on. IEEE, pages 70–74.

Ji Young Lee and Franck Dernoncourt. 2016.

Se-
quential short-text classiﬁcation with recurrent and
arXiv preprint
convolutional neural networks.
arXiv:1603.03827 .

Daniel Ortega and Ngoc Thang Vu. 2017. Neural-
based context representation learning for dialog act
classiﬁcation. arXiv preprint arXiv:1708.02561 .

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Alexander M Rush, Sumit Chopra, and Jason We-
A neural attention model for ab-
arXiv preprint

ston. 2015.
stractive sentence summarization.
arXiv:1509.00685 .

Paul Ekman, Wallace V Friesen, Maureen O’sullivan,
Anthony Chan,
Irene Diacoyanni-Tarlatzis, Karl
Heider, Rainer Krause, William Ayhan LeCompte,
Tom Pitcairn, Pio E Ricci-Bitti, et al. 1987. Uni-
versals and cultural differences in the judgments of
facial expressions of emotion. Journal of personal-
ity and social psychology 53(4):712.

David Golub and Xiaodong He. 2016. Character-level
question answering with attention. arXiv preprint
arXiv:1604.00727 .

Matthew Henderson, Milica Gaˇsi´c, Blaise Thomson,
Pirros Tsiakoulis, Kai Yu, and Steve Young. 2012.
Discriminative spoken language understanding us-
ing word confusion networks. In Spoken Language
Technology Workshop (SLT), 2012 IEEE. IEEE,
pages 176–181.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Anil K Jain and Stan Z Li. 2011. Handbook of face

recognition. Springer.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse relation language models. arXiv
preprint arXiv:1603.01913 .

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584 .

Sheng-syun Shen and Hung-yi Lee. 2016. Neural at-
tention models for sequence classiﬁcation: Analysis
and application to key term extraction and dialogue
act detection. arXiv preprint arXiv:1604.00077 .

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Shih-Ming Wang, Chun-Hui Li, Yu-Chun Lo, Ting-
Hao K Huang, and Lun-Wei Ku. 2016. Sensing
emotions in text messages: An application and de-
arXiv preprint
ployment study of emotionpush.
arXiv:1610.04758 .

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
pages 1480–1489.

Wenpeng Yin, Hinrich Sch¨utze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
arXiv preprint arXiv:1512.05193 .

Matthias Zimmermann. 2009. Joint segmentation and
classiﬁcation of dialog acts using conditional ran-
dom ﬁelds. In Tenth Annual Conference of the In-
ternational Speech Communication Association.

60
 
 
 

 

Detecting Simultaneously Chinese Grammar Errors 

Based on a BiLSTM-CRF Model 

Yajun Liu, Hongying Zan, Mengjie Zhong, Hongchao Ma 
College of Information and Engineering, Zhengzhou University 

liuyajun_gz@163.com, iehyzan@zzu.edu.cn 
1837361628@qq.com, ma-hc@foxmail.com  

 

 

Abstract 

In  the  process  of  learning  and  using 
Chinese, many learners of Chinese as 
foreign 
language(CFL)  may  have 
grammar errors due to negative migra-
tion of their native languages. This pa-
per  introduces  our  system  that  can 
simultaneously diagnose four types of 
grammatical  errors  including  redun-
dant  (R),  missing  (M),  selection  (S), 
disorder  (W)  in  NLPTEA-5  shared 
task.  We  proposed  a  Bidirectional 
LSTM CRF neural network (BiLSTM-
CRF)  that  combines  BiLSTM  and 
CRF  without  hand-craft  features  for 
Chinese Grammatical Error Diagnosis 
(CGED).  Evaluation  includes  three 
levels, which are detection level, iden-
tification  level  and  position  level.  At 
the  detection  level  and  identification 
level,  our  system  got  the  third  recall 
scores, and achieved good F1 values. 

Introduction 

1 
With the rapid development of China’s economy, 
“Chinese Fever” has been set off in the world and 
more foreigners begin to learn Chinese. Writing is 
an  important  part  of  Chinese  learning,  and  the 
grammar is the basis of writing. In the process of 
writing and communicating with each other using 

Chinese,  learners  of  Chinese  as  foreign  lan-
guage(CFL)  may  have  grammar  errors  due  to 
negative migration of their native languages. 

Traditional  learning  methods  for  CFL  rely  on 
heavily manual work to point out grammar errors, 
which costs a lot of time and labor. In order to re-
duce  the  workload  of  manual  identification,  it  is 
necessary  to  explore  effective  methods  for  Chi-
nese  Grammatical  Error  Diagnosis  (CGED).  In 
the field of natural language processing, CGED is 
a great challenge because of the flexibility and ir-
regularity in Chinese, so a series of CGED evalua-
tion tasks are arranged. 

The  CGED  evaluation  tasks  provided  a  plat-
form for many researchers to study the automatic 
detection  of  Chinese  grammatical  errors.  The 
CGED  2018  evaluation  task  defines  Chinese 
grammatical  errors  as  four  categories:  redun-
dant(R),  selection  (S),  missing(M),  disorder(W). 
As shown in Table 1, the example sentences cor-
responding to each error are given. 

In  this  paper,  we  regarded  the  CGED  2018 
shared task as a character-based sequence labeling 
task.  We  proposed  a  Bidirectional  LSTM 
CRF(BiLSTM-CRF)  neural  network  that  com-
bines LSTM and CRF for sequence labeling with-
out  any  hand-craft  features.  Firstly,  we  use 
BiLSTM network to learn the information in the 
sentence and extract features, then we utilize CRF 
for  sequence  labeling  to  complete  automatically 
Chinese grammatical errors detection. 

Error Type 
R(Redundant) 
W(Word Order) 

M(Missing) 

S(Selection) 

Error Sentence 
时间是无价之宝的。 

你采取几种方法应该帮助他们。 

任何婴儿心都是白纸似的清白。 

大家都知道吸烟是害健康的。 

Correct Sentence 
时间是无价之宝。Time is priceless. 
你应该采取几种方法帮助他们。 
You should take several steps to help them. 
任何婴儿的心都是白纸似的清白。 
Any baby's heart is white innocence. 
大家都知道吸烟是损害健康的。 
Everyone knows that smoking is harmful to health. 

Table 1: The examples given.

 

 

 

Proceedingsofthe5thWorkshoponNaturalLanguageProcessingTechniquesforEducationalApplications,pages188–193Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics188 

 
 
 

The rest of this paper is organized as follows: 
Section  2  briefly  introduces  related  work  in  this 
field. Section 3 introduces the model that we pro-
posed. Section 4 discusses experiments and results 
analysis,  including  data  preprocessing,  hyperpa-
rameters  and  experiment  results.  Finally,  conclu-
sion and prospects are arranged. 

2  Related Work 
Automatic detection of grammatical errors is one 
of the most important tasks in the field of natural 
language  processing.  Researchers  have  already 
done a lot of work in the field of English gram-
matical  errors  diagnosis.  For  example,  Helping 
Our Own (HOO) is a series of shared tasks in cor-
recting  textual  errors  (Dale  and  Kilgarriff,2011; 
Dale  et  al.,  2012).  The  CoNLL2013  and 
CoNLL2014 shared tasks (Ng et al., 2013; Ng et 
al.,  2014)  focused  on  grammatical  error  correc-
tion, and many approaches were proposed, such as 
based N-gram language model methods (Hdez et 
al., 2014), statistical machine translation methods 
(Felice  et  al.,  2014),  machine  learning  methods 
(Wang et al., 2014), etc. 

Compared with English, the study for Chinese 
grammatical errors diagnosis started later. The re-
searchers  also  proposed  many  methods,  such  as 
statistical  learning  methods  (Chang  et  al.,  2012), 
ruled-based  methods  (Lee  et  al.,  2013),  and  hy-
brid-based  model  methods  (Lee  et  al.,  2014). 

However, due to the lack of corpora and the limi-
tations of technology, the research progress is lim-
ited  greatly.  The  CGED  shared  tasks  (Yu  et  al., 
2014;  Lee  et  al.,  2015,  2016;  RAO  et  al.,  2017) 
provided researchers with a good platform to pre-
sent  their  work.  In  CGED2016  shared  task,  a 
CRF-based model achieved good precision (Liu et 
al., 2016) and a model based on CRF+LSTM get 
good results (Zheng et al., 2016). In CGED 2017, 
researchers  used  some  features  such  as  part  of 
speech,  collocation  words,  N-gram  etc.,  and  put 
forward the BiLSTM+CRF model to train models 
for each error type respectively, then analyzed the 
errors  by  model  fusion,  finally  made  great  pro-
gresses  for  CGED  (Xie  et  al.,  2017;  Liao  et  al., 
2017). 

In this paper, we propose a bidirectional LSTM 
CRF Neural Network (BiLSTM-CRF) for CGED. 
The model is described as follows: 

(1)  Different  from  the  previous  methods  that 
train  models  for  each  error  type,  in  our  system, 
only one model is trained for all error types, and 
multiple  error  types  are  predicted  at  the  same 
time. 

(2) Our model captures sentence-level features 
based on the  powerful  long-term  memory ability 
of BiLSTM and uses CRF for sequence labeling. 
(3)  The  model  only  learns  from  word  infor-

mation without any handcraft features. 

Fig 1 The proposed BiLSTM-CRF model.

3  Model  
In this paper, we regard Chinese Grammatical er-
rors diagnosis as the sequence labeling task based 
on character level, and the tag sets are R (Redun-
dant),  S  (Selection),  M  (Missing),  W  (Word  Or-
der), C (Correct). The BiLSTM-CRF model pre-
sented in this paper is shown in Figure 1, which 
includes  Embedding  Layer,  BiLSTM  Layer  and 
CRF layer. 

(1) Embedding Layer: transforms the index of 

word into word vector. 

(2)  BiLSTM  Layer:  learns  the  information  of 

each word and extracts features from sentence. 

(3) CRF Layer: decodes and produces labels for 

words. 
3.1  Embedding Layer 
Embedding  Layer  aims  to  transform  words  into 
distributed representations which capture syntactic 

 

 

 

189 

Given a sentence S, then we can describe it as 

sequence  of  words, and  each  word  is  derived 

and  semantic  meanings  of  words.  Therefore,  we 
use  word  embeddings  to  represent  words  in  the 
sentence. 

S(cid:3404)(cid:4668)w(cid:2869),w(cid:2870),w(cid:2871),…,w(cid:3041)(cid:2879)(cid:2869),w(cid:3041)(cid:4669), which contains a 
from  a  vocabulary V.  Words  are  represented  by 
distributional  vectors w∈(cid:1844)(cid:3031)  which  are  drawn 
from a word embedding matrix W	∈(cid:1844)|(cid:2906)	|(cid:3400)(cid:3031). Af-
X(cid:3404)(cid:4668)(cid:1876)(cid:2869),(cid:1876)(cid:2870),(cid:1876)(cid:2871),…,(cid:1876)(cid:3041)(cid:2879)(cid:2869),(cid:1876)(cid:3041)(cid:4669). 

ter Embedding Layer, then we can get X:  

3.2  BiLSTM Layer 
Due to the powerful long-term memory ability of 
LSTM, LSTM based neural networks, which have 
access to both past and future contexts, are proven 
to be effective in sequence labeling task. The hid-
den states in bidirectional LSTM can capture both 
past  and  future  context  information  and  accom-
plish sequence labeling for each token.  

Basically,  a  LSTM  unit  is  composed  of  three 
multiplicative gates which control the proportions 
of information to forget and to pass on to the next 
time  step.  Three  components  composite 
the 
LSTM-based recurrent neural networks: one input 
gate 
 with  corresponding  weight  matrix 

(cid:1861)(cid:3047)
(cid:1849)(cid:4666)(cid:3051)(cid:3036)(cid:4667),(cid:1849)(cid:4666)(cid:3035)(cid:3036)(cid:4667),(cid:1849)(cid:4666)(cid:3030)(cid:3036)(cid:4667),(cid:1854)(cid:4666)(cid:3036)(cid:4667);  one  forget  gate (cid:1858)(cid:3047) with 
weight  matrix (cid:1849)(cid:4666)(cid:3051)(cid:3033)(cid:4667),(cid:1849)(cid:4666)(cid:3035)(cid:3033)(cid:4667),(cid:1849)(cid:4666)(cid:3030)(cid:3033)(cid:4667),(cid:1854)(cid:4666)(cid:3033)(cid:4667) ;  one 
output  gate (cid:1867)(cid:3047) with  corresponding  weight  matrix 
(cid:1849)(cid:4666)(cid:3051)(cid:3042)(cid:4667),(cid:1849)(cid:4666)(cid:3035)(cid:3042)(cid:4667),(cid:1849)(cid:4666)(cid:3030)(cid:3042)(cid:4667),(cid:1854)(cid:4666)(cid:3042)(cid:4667). Formally, the formulas 
(1) to update an LSTM unit at time (cid:1872) are: 
(cid:1861)(cid:3047)(cid:3404)(cid:2026)(cid:4666)(cid:1849)(cid:4666)(cid:3051)(cid:3036)(cid:4667)(cid:1876)(cid:3047)(cid:3397)(cid:1849)(cid:4666)(cid:3035)(cid:3036)(cid:4667)(cid:1860)(cid:3047)(cid:2879)(cid:2869)(cid:3397)(cid:1849)(cid:4666)(cid:3030)(cid:3036)(cid:4667)(cid:1855)(cid:3047)(cid:2879)(cid:2869)
(cid:3397)(cid:1854)(cid:4666)(cid:3036)(cid:4667)(cid:4667)	
(cid:1858)(cid:3047)(cid:3404)(cid:2026)(cid:4666)(cid:1849)(cid:4666)(cid:3051)(cid:3033)(cid:4667)(cid:1876)(cid:3047)(cid:3397)(cid:1849)(cid:4666)(cid:3035)(cid:3033)(cid:4667)(cid:1860)(cid:3047)(cid:2879)(cid:2869)(cid:3397)(cid:1849)(cid:4666)(cid:3030)(cid:3033)(cid:4667)(cid:1855)(cid:3047)(cid:2879)(cid:2869)
(cid:3397)(cid:1854)(cid:4666)(cid:3033)(cid:4667)(cid:4667)	
(cid:1873)(cid:3047)(cid:3404)(cid:1872)(cid:1853)(cid:1866)(cid:1860)(cid:4666)(cid:1849)(cid:4666)(cid:3051)(cid:3030)(cid:4667)(cid:1876)(cid:3047)(cid:3397)(cid:1849)(cid:4666)(cid:3035)(cid:3030)(cid:4667)(cid:1860)(cid:3047)(cid:2879)(cid:2869)
(cid:3397)(cid:1849)(cid:4666)(cid:3030)(cid:3030)(cid:4667)(cid:1855)(cid:3047)(cid:2879)(cid:2869)(cid:3397)(cid:1854)(cid:4666)(cid:3030)(cid:4667)(cid:4667)	
(cid:1855)(cid:3047)(cid:3404)(cid:1861)(cid:3047)⨀(cid:1873)(cid:3047)(cid:3397)(cid:1858)(cid:3047)⨀(cid:1855)(cid:3047)(cid:2879)(cid:2869)	
(cid:1867)(cid:3047)(cid:3404)(cid:2026)(cid:4666)(cid:1849)(cid:4666)(cid:3051)(cid:3042)(cid:4667)(cid:1876)(cid:3047)(cid:3397)(cid:1849)(cid:4666)(cid:3035)(cid:3042)(cid:4667)(cid:1860)(cid:3047)(cid:2879)(cid:2869)(cid:3397)(cid:1849)(cid:4666)(cid:3030)(cid:3042)(cid:4667)(cid:1855)(cid:3047)
(cid:3397)(cid:1854)(cid:4666)(cid:3042)(cid:4667)(cid:4667)		
(cid:1860)(cid:3047)(cid:3404)(cid:1867)(cid:3047)⨀(cid:1872)(cid:1853)(cid:1866)(cid:1860)	(cid:4666)(cid:1855)(cid:3047)(cid:4667) 
where σ is the element-wise sigmoid function and 
⨀ is the element-wise product. (cid:1876)(cid:3047) is the input vec-
tor at time (cid:1872), and (cid:1860)(cid:3047) is the hidden state vector stor-
(cid:1872). 
is a sequence X of word vectors from Embedding 
Layer,  where X(cid:3404)(cid:4668)(cid:1876)(cid:2869),(cid:1876)(cid:2870),(cid:1876)(cid:2871),…,(cid:1876)(cid:3041)(cid:2879)(cid:2869),(cid:1876)(cid:3041)(cid:4669) .  The 
as h(cid:3404)(cid:4668)(cid:1860)(cid:2869),(cid:1860)(cid:2870),(cid:1860)(cid:2871),…,(cid:1860)(cid:3041)(cid:2879)(cid:2869),(cid:1860)(cid:3041)(cid:4669).  Each  final  hid-

output of the BiLSTM Layer is a sequence of the 
hidden states for each input word vectors, denoted 

ing all the useful information at (and before) time 

Mathematically, the input of the BiLSTM layer 

(1) 

 
 
 

den  state  is  the  concatenation  of  the  forward (cid:1860)(cid:3047)(cid:4652)(cid:4652)(cid:4652)(cid:1318) 
and  backward (cid:1860)(cid:3047)(cid:3382)(cid:4652)(cid:4652)(cid:4652) hidden  states,  then  we  can  get 
(cid:1860)(cid:3047) : (cid:1860)(cid:3047)(cid:4652)(cid:4652)(cid:4652)(cid:1318)(cid:3404)(cid:1864)(cid:1871)(cid:1872)(cid:1865)(cid:3435)(cid:1876)(cid:3047),(cid:1860)(cid:3047)(cid:2879)(cid:2869)
(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:1318)(cid:3439), (cid:1860)(cid:3047)(cid:3382)(cid:4652)(cid:4652)(cid:4652)(cid:3404)(cid:1864)(cid:1871)(cid:1872)(cid:1865)(cid:3435)(cid:1876)(cid:3047),(cid:1860)(cid:3047)(cid:2878)(cid:2869)
(cid:3382)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:4652)(cid:3439) 
(cid:1860)(cid:3047)(cid:3404)(cid:4670)(cid:1860)(cid:3047)(cid:4652)(cid:4652)(cid:4652)(cid:1318),(cid:1860)(cid:3047)(cid:3382)(cid:4652)(cid:4652)(cid:4652)(cid:4671) 

as 

be 

can 

Layer 

described 

The sequence of hidden states in the BiLSTM 

3.3  CRF Layer 
Since there are many syntactic constraints in natu-
ral language sentences, the relationship among ad-
jacent  tags  is  very  important  for  CGED  shared 
task.  If  we  simply  transfer  directly  the  hidden 
states of BiLSTM Layer to a Softmax layer for tag 
prediction,  it  is  possible  to  break  the  syntactic 
constraints and it is difficult to consider the corre-
lation  among  adjacent  tags.  Conditional  random 
field (CRF) is the most commonly used method in 
structural prediction, and its basic idea is to use a 
series  of  potential  functions  to  approximate  the 
conditional  probability  of  the  output  label  se-
quence for the input word sequence. 

quences. So we can use the hidden state sequence 
to get the conditional probability of the output se-
quence, and the conditional probability is: 

h(cid:3404)
(cid:4668)(cid:1860)(cid:2869),(cid:1860)(cid:2870),(cid:1860)(cid:2871),…,(cid:1860)(cid:3041)(cid:2879)(cid:2869),(cid:1860)(cid:3041)(cid:4669),  then  we  treat  it  as  the 
that (cid:1877)(cid:3404)(cid:4668)(cid:1877)(cid:2869),(cid:1877)(cid:2870),(cid:1877)(cid:2871),…,(cid:1877)(cid:3041)(cid:2879)(cid:2869),(cid:1877)(cid:3041)(cid:4669) ,  where (cid:1877)(cid:2919)∈(cid:1851) 
and (cid:1851) represents  the  set  of  all  possible  label  se-
(cid:1868)(cid:4666)(cid:1877)|(cid:1860);(cid:1849),(cid:1854)(cid:4667)
(cid:3404) ∏ (cid:1857)(cid:1876)(cid:1868)	(cid:4666)(cid:1849)(cid:3052)(cid:3284)(cid:3127)(cid:3117),(cid:3052)(cid:3284)
(cid:1860)(cid:3397)(cid:1854)(cid:3052)(cid:3284)(cid:3127)(cid:3117),(cid:3052)(cid:3284)(cid:4667)
(cid:3041)(cid:3036)(cid:2880)(cid:2869)
(cid:3021)
∑
∏ (cid:1857)(cid:1876)(cid:1868)	(cid:4666)(cid:1849)(cid:3052)(cid:3284)(cid:3127)(cid:3117)(cid:4594)
(cid:1860)(cid:3397)(cid:1854)(cid:3052)(cid:3284)(cid:3127)(cid:3117)(cid:4594)
,(cid:3052)(cid:3284)(cid:4594)(cid:4667)
Where (cid:1849),(cid:1854) is the two weight matrices, and the 
(cid:3021)
(cid:3041)(cid:3036)(cid:2880)(cid:2869)
(cid:3052)(cid:4594)∈(cid:3026)
,(cid:3052)(cid:3284)(cid:4594)
vector for the given label pair (cid:4666)(cid:1877)(cid:2919),(cid:1877)(cid:2920)(cid:4667). At the same 
(cid:1838)(cid:4666)(cid:1849),(cid:1854)(cid:4667)(cid:3404) (cid:3533)(cid:1864)(cid:1867)(cid:1859)(cid:1868)(cid:4666)(cid:1877)(cid:3036)|(cid:1860)(cid:3036);(cid:1849),(cid:1854)(cid:4667)

time, in order to train the CRF Layer, we use the 
classical maximum conditional likelihood estima-
tion to train our model. The final log-likelihood of 
the weight matrix is as follows: 

input to the CRF Layer. The output of CRF Layer 
is our final prediction label sequence, we can see 

subscription indicates that we extract the weight 

(3) 
Finally, the Viterbi algorithm is used to train the 

(cid:4666)(cid:3035)(cid:3284),(cid:3052)(cid:3284)(cid:4667)

(2) 

 

 

CRF Layer and decode the optimal output se-
quence. 

4  Experiments and Results Analysis 
In this paper, based on the CGED series evalua-
tions, we adopted the dataset of CGED 2016 and 
CGED 2018 shared tasks as out training dataset, 
then we manually deleted some incorrect sentenc-

 

 

 

190 
 
 

 

es in the training set and rebuilt the dataset. The 
CGED 2017 test set was selected as the validation 
set and the CGED 2018 test set was used as the 
test set. We selected BiLSTM-CRF model for 
CGED 2018 shared task. This part mainly in-
cludes data preprocessing, parameter settings, re-
sults analysis on the validation set and the test set. 
4.1  Data Preprocessing  
Since the CGED evaluation task involves identifi-
cation of incorrect boundary positions, word seg-
mentation  may  cause  the  misalignment  between 
the  end  points  of  words  and  corresponding  error 
intervals. At the same time, it may also result in 
overlapping problems among multiple types of er-

rors. Therefore, in this paper we employed charac-
ters for Chinese grammatical error diagnosis. Dif-
ferent from previous methods that trained models 
for  each  error  type,  only  one  model  which  can 
identity  simultaneously  four  types  of  errors  is 
trained in our system. 

Using previous data preprocessing method (Liu 
et  al.,  2016),  we  extracted  correct  sentences  and 
wrong sentences from the corpus according to the 
manual annotation, and then respectively marked 
characters  with  the  corresponding  labels  that  in-
clude  redundant(R),  missing(M),  selection(S), 
disorder(W),  correct  (C).  we  give  some  prepro-
cessing examples that are shown in Table 2. 

Error sentence:          他们是不但我父母，而且是人生的先辈。 
Correction sentence: 他们不但是我父母，而且是人生的导师。 

(They are not only my parents but also mentors in life.) 

Manual annotation: (3,5) W (16,17) S 
Preprocessing results： 
他/C 们/C 是/W 不/W 但/W 我/C 父/C 母/C，/C 而/C 且/C 是/C 人/C 生/C 的/C 先/S 辈/S。/C 
他/C 们/C 不/C 但/C 是/C 我/C 父/C 母/C，/C 而/C 且/C 是/C 人/C 生/C 的/C 导/C 师/C。/C 

Table 2: The examples of data preprocessing. 

Methods 

False Positive Rate 
Precision 
Recall 
F1-Score 
Precision 
Recall 
F1-Score 
Precision 
Recall 
F1-Score 

Detection Level 

Identification Level 

Position Level 

CRF 
0.1881 
0.7514 
0.3093 
0.4382 
0.6328 
0.1763 
0.2758 
0.3913 
0.0658 
0.1126 

BiLSTM-CRF 
0.9643 
0.6016 
0.9481 
0.7361 
0.3375 
0.32 
0.3285 
0.0015 
0.0009 
0.0011 

Table 3: The results on the validation set. 

4.2  Parameter Settings  
In this paper, word vector is randomly initialized, 
and  word  vector  dimension  is  50.  Here  is  the 
overview of optimized parameters: 
· Word vector dimension  
· Hidden size 
· Adam learning rate  
· Epoch 
4.3  Experiments Results  
In this paper, we use two different models to con-
duct  experiments  respectively,  which  are  CRF 
model (M1) and BiLSTM-CRF model (M2).  

50 
50 
0.001 
300 

CRF model: The CRF model adds a variety of 
grammatical features such as bigram and trigram 
features. The selection of features directly affects 
the performance of the model. Therefore, this ex-
periment adopts  the feature  length of 7 and uses 
bigram and trigram to extract features.  

BiLSTM-CRF  model:  The  BiLSTM-CRF 
model combines LSTM and CRF for sequence la-
beling. Firstly, we use BiLSTM network to learn 
information  in  the  sentence  and  extract  features, 
then  we  utilize  CRF  for  sequence  labeling  to 
complete automatically CGED shared work.  

The results on the validation set: The valua-
tion  set  used  in  this  paper  is  the  test  set  in  the 
CGED2017 shared task. Two different models are 

 

 

 

191 

used to conduct experiments on the valuation set, 
results are shown in Table 3. 

From Table 3, we can see that CRF model has 
lower  False  Positive  Rate  (FPR)  than  BiLSTM-
CRF model, and CRF model achieves better pre-
cision performance at the detection level and the 
identification  level,  because  that  CRF  model  has 
more  features  information  such  as  bi-gram,  tri-
gram.  However,  CRF  model  and  BiLSTM-CRF 
model are not good at position level. We think that 
our models are short of identification of position 
boundary. Next, we will focus on the position lev-
el by adding character position features. 

The results on the test set: The test set is the 
test  set  in  the  CGED  2018  shared  task.  We  sub-
mitted  only  one  result  in  this  task.  The  Table  4 
lists the result Run1 we submitted and the test re-
sult based on CRF model. 

At the error detection level and error identifica-
tion level, our system achieves a third recall rate 
and  gets  a  good  F1  value.  However,  our  system 

Detection Level 

Identification Level 

Position Level 

Methods 

False Positive Rate 
Precision 
Recall 
F1-Score 
Precision 
Recall 
F1-Score 
Precision 
Recall 
F1-Score 

 
 
 

has a poor performance at the error position level 
and FPR. Since our system recognizes four types 
of errors at the same time, increasing the difficulty 
of recognition, it is easier to identify a correct sen-
tence as an error sentence, it results in lower FPR 
performance on the test set. In addition, our sys-
tem  is  based  on  character  level,  although  the 
BiLSTM  network  has  a  powerful  long-term 
memory function, the lack of word collocation in-
formation also results in lower position level effi-
ciency. Another reason for low position level effi-
ciency is that tag does not distinguish among loca-
tions. For example, 
Error: 我/C朋/C友/C的/C努/C力/C真/C是/C
可/S看/S的/C。/C 
Correction: 我朋友的努力真是有效的。 
(My friend’s efforts are really effective) 
In this sentence, “可看” should be corrected as “有
效”. There was no distinction in two “/S”, so we 
think it leads to lower position level efficiency. 

CRF 
0.0851 
0.8506 
0.3449 
0.4908 
0.7373 
0.17 
0.2763 
0.5037 
0.0615 
0.1096 

Run1 
0.9309 
0.5441 
0.9179 
0.6926 
0.3144 
0.6266 
0.4187 
0.0078 
0.0189 
0.0110 

Table 4: The results on the test set.

5  Conclusion 
On  the  basis  of  CGED  series  evaluation  tasks, 
this  paper  proposes  a  neural  network  model 
based  on  BiLSTM-CRF,  which  is  used  for  Chi-
nese grammatical error detection. It has good ef-
fect at the detection level and identification level, 
especially the high recall rate. But it has low per-
formance at the position level. Next, we will add 
some  external  features,  such  as  parts  of  speech, 
character  position  features  and  collocation  fea-
tures to improve the performance of our system. 
References  
Chang, Ru-Yng, Chung-Hsien Wu, and Philips Kokoh 
Prasetyo.  2012.  Error  diagnosis  of  Chinese  sen-
tences  using  inductive  learning  algorithm  and  de-
testing  mechanism. ACM 
composition-based 
Transactions on Asian Language Information Pro-
cessing (TALIP), 11(1), 3. 

Dale, Robert, and Adam Kilgarriff. 2011, September. 
Helping our own: The HOO 2011 pilot shared task. 
In Proceedings of the 13th European Workshop on 
Natural  Language  Generation (pp.  242-249).  As-
sociation for Computational Linguistics. 

Dale, Robert, Ilya Anisimoff. 2012, June. HOO 2012: 
A  report  on  the  preposition  and  determiner  error 
correction  shared  task.  In Proceedings  of  the  Sev-
enth  Workshop  on  Building  Educational  Applica-
tions  Using  NLP (pp.  54-62).  Association  for 
Computational Linguistics. 

Felice,  Mariano,  et  al.  2014.  Grammatical  error 
correction using hybrid systems and type filtering. 
In Proceedings  of  the  Eighteenth  Conference  on 
Computational  Natural  Language  Learning: 
Shared Task (pp. 15-24). 

Gaoqi,  R.  A.  O.,  et  al.  2017.  IJCNLP-2017  Task  1: 
Chinese  Grammatical  Error  Diagnosis.   Proceed-
ings of the IJCNLP 2017, Shared Tasks, 1-8. 

 

 

 

192 

Hdez,  S.  David,  and  Hiram  Calvo.  2014.  CoNLL 
2014  shared  task:  Grammatical  error  correction 
with a syntactic n-gram language model from a big 
corpora.  In Proceedings of  the  Eighteenth Confer-
ence  on  Computational  Natural  Language  Learn-
ing: Shared Task (pp. 53-59). 

Lee,  Lung-Hao,  et  al.  2013,  November.  Linguistic 
rules based Chinese error detection for second lan-
guage  learning.  In Work-in-Progress  Poster  Pro-
ceedings  of  the  21st  International  Conference  on 
Computers in Education (ICCE-13) (pp. 27-29). 

for 

Lee, Lung-Hao, et al. 2014. A sentence judgment sys-
detection. 
tem 
In Proceedings of COLING 2014, the 25th Interna-
tional  Conference  on  Computational  Linguistics: 
System Demonstrations (pp. 67-70). 

grammatical 

error 

Lee,  Lung-Hao,  et  al.  2015. Overview  of  the  NLP-
TEA  2015  Shared  Task  for  Chinese  Grammatical 
Error Diagnosis. Proceedings of the 2nd Workshop 
on  Natural  Language  Processing  Techniques  for 
Educational  Applications (NLPTEA'15),  Beijing, 
China, 31 July, 2015, pp. 1-6. 

Lee, Lung-Hao, et al. 2016. Overview of nlp-tea 2016 
shared  task  for  chinese grammatical  error  diagno-
sis. In Proceedings of the 3rd Workshop on Natural 
Language  Processing  Techniques  for  Educational 
Applications (NLPTEA2016) (pp. 40-48). 

Liao,  Quanlei,  et  al.  2017.  YNU-HPCC  at  IJCNLP-
2017 Task 1: Chinese Grammatical Error Diagno-
sis  Using  a  Bi-directional  LSTM-CRF  Mod-
el. Proceedings of the IJCNLP 2017, Shared Tasks, 
73-77. 

Liu, Yajun, et al. 2016. Automatic Grammatical Error 
Detection  for  Chinese  based  on  Conditional  Ran-
dom Field. In Proceedings of the 3rd Workshop on 
Natural Language Processing Techniques for Edu-
cational Applications (NLPTEA2016) (pp. 57-62). 
Ng, Hwee Tou, et al. 2013. The CoNLL-2013 Shared 
Task 
Correc-
tion. Seventeenth  Conference  on  Computational 
Natural  Language  Learning:  Shared  Task(pp.1-
12). 

Grammatical 

Error 

on 

on 

Ng, Hwee Tou, et al. 2014. The CoNLL-2014 shared 
correction. 
task 
In Proceedings  of  the  Eighteenth  Conference  on 
Computational  Natural  Language  Learning: 
Shared Task (pp. 1-14). 

grammatical 

error 

Wang,  Peilu,  Zhongye  Jia,  and  Hai  Zhao.  2014. 
Grammatical error detection and correction using a 
single maximum entropy model. In Proceedings of 
the Eighteenth Conference on Computational Nat-
ural Language Learning: Shared Task (pp. 74-82). 
Xie, Pengjun. 2017. Alibaba at IJCNLP-2017 Task 1: 
Embedding Grammatical Features into LSTMs for 

 

 
 
 

 

Grammatical 

Diagnosis 
Chinese 
Task. Proceedings  of  the  IJCNLP  2017,  Shared 
Tasks, 41-46. 

Error 

Yu,  Liang-Chih,  Lung-Hao  Lee,  and  Li-Ping  Chang. 
2014,  November.  Overview  of  grammatical  error 
diagnosis  for  learning  Chinese  as  a  foreign  lan-
guage. In Proceedings of the 1st Workshop on Nat-
ural  Language  Processing  Techniques  for  Educa-
tional Applications (pp. 42-47). 

Zheng,  Bo,  et  al.  2016.  Chinese  Grammatical  Error 
Diagnosis  with  Long  Short-Term  Memory  Net-
works.  In Proceedings  of  the  3rd  Workshop  on 
Natural Language Processing Techniques for Edu-
cational Applications (NLPTEA2016) (pp. 49-56). 

 

193
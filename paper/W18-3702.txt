 

Syntactic and Lexical Approaches to Reading Comprehension  

Henry Lin 

esotericbubba@hotmail.com 

Hackley School, Tarrytown, New York 

 
 

 
 
 
 

Abstract 

Teaching  reading  comprehension  in  K  – 
12  faces a number  of  challenges. Among 
them are identifying the portions of a text 
that  are  difficult  for  a  student,  compre-
hending  major  critical  ideas,  and  under-
standing  context-dependent  polysemous 
words. We present a simple, unsupervised 
but  robust  and  accurate  syntactic  method 
for  achieving  the  first  objective  and  a 
modified  hierarchical  lexical  method  for 
the  second  objective.  Focusing  on  pin-
pointing troublesome sentences instead of 
the  overall  readability  and  on  concepts 
central  to  a  reading,  we  believe  these 
methods  will  greatly  facilitate  efforts  to 
help students improve reading skills. 

Introduction 

1 
Teaching  reading  comprehension  and  readability 
research are related but also different. Readability 
research generally focuses on ranking the difficult 
level  of  a  passage  while  reading  comprehension 
education more directly aims at helping students 
read better. 

Although readability metrics offer a good indi-
cation of a passage’s difficulty level, a more use-
ful  approach  for  teaching  comprehension  is  to 
pick out those difficult sentences for specific, tar-
geted  learning.    Although  vocabulary  is  an  im-
portant factor in making a sentence difficult, it al-
so  often  happens  that  a  sentence,  either  with  no 
unknown words or  after all the  words have been 
looked up, is still difficult to understand. The fol-
lowing  is  an  example  from  a  6th  grade  history 
reading:  

“Nor  have  legitimate  grounds  ever  failed  a 
prince  who  wished  to  show  colorable  excuse  for 
the non-fulfillment of his promise.”1 
                                                   
1 Niccolo Machiavelli, The Prince, Chapter XVII. 

 

Even  though  the  main  idea  was  more  or  less 
clear, sentences like this were, in general, difficult 
for 6th graders. 

Sufficient  background  and vocabulary  are  two 
prerequisites of reading success, but beyond these 
two,  what  textual  features  are  there  that  make  a 
sentence hard? This is one question this paper ad-
dresses.  The second question is how to help stu-
dents understand all major critical ideas in a read-
ing because in a passage, in addition to the main 
idea,  there  are  major  supporting  details  that  are 
crucial to comprehension.  For example, in Martin 
Luther  King  Jr.’s  Beyond  Vietnam  speech,  the 
main  idea  is  to  oppose  the  war  in  Vietnam  and 
there are  four major  reasons given.   Understand-
ing  these  four  reasons  is  as  integral  to  the  pas-
sage’s comprehension as the main idea.  The third 
question  we  address  is  how  to  help  students  un-
derstand  in-context  polysemous  words. Together, 
this paper makes the following contributions: 
•  A  set  of  simple  and  accurate  statistics  that 
identifies, within a passage, the sentences that 
are challenging. 

•  A set of interesting  findings about the stand-

ardized reading tests. 

•  A  modified  hierarchical  lexical  clustering 

method to find critical concepts in a reading. 
•  A  word2vec  application  for  selecting  in-

context meaning of a word. 

2  Previous Work 
One focus of the previous NLP work on accessing 
text difficulties is  readability  ranking.  For  exam-
ple, Lexile (Lennon, 2004), Flesch-Kincaid (Kin-
caid,  1975),  Dale-Chall  (Dale,  1948),  Coleman-
Liau (Coleman, 1975), and SMOG (McLaughlin, 
1969) largely rely on words and sentence length. 
Since one or two long sentences or difficult words 
do not necessarily make a passage difficult, those 
systems  give  rankings  for  an  entire  passage  or  a 

Proceedingsofthe5thWorkshoponNaturalLanguageProcessingTechniquesforEducationalApplications,pages11–19Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics11book  and  are  not  aimed  at  pinpointing  difficult 
sentences. 

Recently,  Pitler  et.  al.  (2008),  Peterson  et.  al. 
(2009),  Kate  et.  al.  (2010),  Feng  (2010),  and 
Dascalu  et.  al.  (2013)  addressed  the  readability 
problem using supervised data and a richer set of 
linguistic features. However, their systems still fo-
cus on giving a readability score of the overall ar-
ticle,  not  individual  sentences  from  which  stu-
dents  can  improve  their  reading  comprehension. 
Pitler et. al. (2008) and Tanaka-Ishii et. al. (2010) 
also built comparators to decide relative difficulty 
between two sentences.  Both and Tanaka-Ishii et. 
al.  (2010)  especially  make  heavy  use  of  lexical 
features. All these models also require supervised 
data and vocabulary acquisition. 

Works  by  François  et.  al.  (2014),  Siddharthan 
et.  al.  (2014),  and  Vajjala  et.  al.  (2014)  have  fo-
cused  on  sentence  simplification  instead  of  sen-
tence selection for the purpose of teaching reading 
comprehension.  This paper provides a simple and 
robust  method  for  identifying  difficult  sentences 
in a reading passage. We incorporate some of the 
standard  features  seen  in  previous  work  such  as 
tree depth, but we  also  devise new  features such 
as abstract appositives.  While much of the previ-
ous research has made use of both lexical and syn-
tactic  features,  our  focus is on  an in-depth study 
on syntax phenomena  that  contribute to sentence 
complexity. 

In addition to individual sentences that are hard 
to read, scattered concepts are also challenging to 
a reader.  An author often develops a critical idea 
in  several  paragraphs  using  paraphrases,  syno-
nyms, and related ideas. When a reader cannot see 
the  relation  among  these  words  and  phrases,  he 
will have difficulty grasping that concept.  For this 
problem, we propose a word2vec-based (Mikolov, 
2013)  modified  hierarchical  clustering  model  to 
find clusters of concepts in a reading passage. 

3  The Syntactic Features 
We present a set of simple and robust features able 
to identify the difficult sentences in a reading.  We 
show the efficacy of these features in a series of 
tests on grade-level readings. 
3.1  The Features 
Figures 1a – 1f depict each feature in action.  In 
the  figure,  each  rectangular  box  describes  what 
the feature is and how the feature is determined. 

 

3.2  Feature Performance 
Our  goal  is  to  find  candidate  sentences  that  are 
challenging for a young reader. This task is diffi-
cult  to  evaluate  for  two  reasons:  the  lack  of  la-
beled  data  at  sentence  level  and  probably  more 
importantly, the lack of a methodology for creat-
ing such a dataset.  The creation of supervised da-
ta involves judgment from a young reader (under 
16 years of age). First, young children often can-
not  articulate  what  they  find  difficult.  Second, 
they sometimes think they understand a sentence 
while they don’t. An attempt was made at a local 
tutoring center for children 11-16. Fifty-two chil-
dren  were  given  a  grade-level  passage  and  an 
above-grade  passage  (e.g.  a  hard  SAT  passage).  
They  were  asked  to  pick  out  the  sentences  they 
didn’t understand.  For both passages, more than 
80%  of  the  children  either  said  they  understood 
everything  or  they  found  the  passage  hard  but 
couldn’t  tell  where  the  difficulties  were.  They 
were then given multiple-choice questions.  Fewer 
than 5% of the children who claimed they under-
stood everything scored perfectly on the test.  For 
more than 50% of the mistakes made, more than 
half the  children claimed that it was not because 
they  didn’t  understand  the  passage  but  because 
they were careless.  This attempt showed that hu-
man judgment from a young reader is hard to ob-
tain.  Secondly, an approximation of difficulty via 
test performance is problematic. Perhaps, a possi-
ble approach is to convene expert reading teachers 
and ask them to, based on their field experiences, 
rank  each  sentence’s  difficulty  level  for  each 
grade. This  would  require  these teachers to have 
intimate knowledge of how children process sen-
tences.  For  these  reasons,  we  first  evaluate  the 
features by measuring how  well they  correspond 
to the changes in reading levels. We then use the 
features to rank the difficulty of each sentence and 
perform a qualitative assessment. 
     For the first part of the evaluation, we look for 
data that  correlate  well  with grade levels. Repre-
sentative grade-level readings are not easy to col-
lect because readers in each grade vary greatly in 
their  reading  abilities2.  We  thus  use  passages  in 
standardized tests. In this section, we present data 
from passages on the New York State ELA tests, 
which  are  annual  tests  given  to  students  from 
grades  3  to  8.  For  high  school  reading  data,  we 
                                                   
2 For example, according to Lexile, the range for 7th grade 
reading is 300L to 1330L, a difference between Three Billy-
Goats Gruff (340L) and Understanding Hume (1290L). 

12use  the  SAT  test,  a  national  test  for  high  school 
students. Thus, the data represent standard reading 
levels of grades 3 to high school. We first run the 

Stanford parser (Manning et. al.,2014).  We then col-
lect statistics of the nine features on each sentence. 
The  data  statistics  and  feature  performance  are

(b) Interruption 

(d) Inversion and Negation 

(a) Delay, NPVP Pairs, and Depth 

 

(c) Parallelism 

 

 

 

(e) Abstract Appositive 

Figure 1. Syntactic Features 

(f) PP Fronting 

presented in Table  1 and  Figures  2a-2c. p-values 
of  t-test  at  α=0.05  are  shown  in Tables  2a  –  2c. 
For example, the increase in Delay from Grade 5 

to Grade 6 is 95% statistically significant (p-value 
0.003 < 0.05 in Table 2a).  All significant changes 
are in bold. While the general trend is increasing 

 

 

 

 

13through grades, sometimes decreases are observed 
in two adjacent grades. Many of the decreases are  
statistically  insignificant  such  as  the  decrease  in 
Delay from G3 to G4 with p-value of 0.13.    
     It is noticeable that in grades 3 – 12, standard 
readings  contain  virtually  none  of  the  more  spe-
cialized features of 1c-1f.  These features are more 
prominent in older and more mature readings such 
as those in 19th-century literature.  In section 5, we 
use only features in 1a and 1b.   

 

#Sentences 
Grade  Test Year 
975 
2006 – 10 
3 
1,729 
2006 – 10 
4 
1,131 
2006 – 10 
5 
1,145 
6 
2006 – 10 
1.296 
2006 – 10 
7 
2006 – 10 
8 
1,636 
2009,12, 16  1,397 
9+ 

Table 1. Data Statistics 

#Tokens 
9,967 
20,533 
14,972 
17,306 
20,256 
26,812 
35,415 

Depth 
3.47e-11 
1.48e-7 
0.002 
0.011 
0.68 

2.26e-55 

Interruption 

0.20 
0.008 
0.04 
0.58 
0.05 
3.10e-6 

Pair NP-VP 

1.76e-11 
0.035 
0.002 
0.011 
0.68 

Delay 
0.13 
0.48 
0.003 
0.38 
0.59 
2.64e-9 

1.09e-38 
Table 2a. p-values 

Inversion  Parallel 

0.10 
0.25 
0.31 
0.08 
0.83 

0.61 
0.015 
0.31 
0.08 
0.83 

1.80e-14 

1.80e-14 
Table 2b. p-values 

 

 

Negation  Abstract 
Appositive 

0.07 
0.45 
0.28 
0.06 
0.30 

0.008 
0.08 
0.33 
0.76 
0.35 
9.52e-12 
0.87 
Table 2c. p-values 

PP 

Fronting 

0.10 
0.83 
0.75 
0.14 
0.24 
7.68e-9 

 

Grade 
3à4 
4à5 
5à6 
6à7 
7à8 
8à9+ 

Grade 
3à4 
4à5 
5à6 
6à7 
7à8 
8à9+ 

Grade 

3à4 
4à5 
5à6 
6à7 
7à8 
8à9+ 

 

15

10

5

0

3.5
3.3
3.1
2.9
2.7
2.5
2.3
2.1
1.9
1.7
1.5
1.3
1.1
0.9
0.7
0.5
0.3
0.1
-0.1

Depth

3

4

6

5

7
Figure 2a. Depth 

8

9+

3

4

5

6

7

8

9+

Delay
Interruption
Negative

Pairs of NP-VP
Parallel

Figure 2b. Delay, NPVP, Interruption, Parallel, 

and Negation 

 

0.04

0.03

0.02

0.01

0

5

6

7

8

9+

Abstract appositive

3

4
Inversion

PP fronting

Figure 2c. Inversion, Abstract Appositive, and PP 

Fronting 

 

 

 

 
 

 

14Top  3 
by both 

Notes: 

One or two of the tradespeople even darted 
out  of  their  shops,  and  went  a  little  way 
down the street before me, that they might 
turn,  as  if  they  had  forgotten  something, 
and pass me face to face – on which occa-
sions I don't know whether they or I made 
the worse pretence; they of doing it, or I of 
not seeing it. 
Specific  features  are  PP  fronting  (itali-
cized) and one parallel phrase (underlined). 

Table 3. Sentence Ranking Example 

4  The Lexical Approach 
We now turn to finding critical ideas in a reading.  
Our  concern  is  to  find  related  and  paraphrased 
words that contribute to the same idea.  
4.1  An Example 
We distinguish critical ideas from the main idea of 
a reading.   Critical ideas are any ideas that the au-
thor develops to some extent.  A crude definition 
is  that  a  critical  idea  is  an  idea  that  the  author 
mentions more  than once. They may  or may not 
be the main idea, but they should all contribute to 
the  main  idea.    In  the  following  short  passage, 
there is one main idea and several critical ideas. 

“Black holes are the most efficient engines of de-
struction known to humanity. Their intense gravity is 
a one-way ticket to oblivion, and material spiraling 
into  them  can  heat  up  to  millions  of  degrees  and 
glow  brightly.  Yet,  they  are  not  all-powerful.  Even 
supermassive  black  holes  are  minuscule  by  cosmic 
standards. They typically account  for  less than one 
percent of their galaxy's mass. Accordingly, astron-
omers  long  assumed  that  supermassive  holes,  let 
alone their smaller cousins, would have little effect 
beyond  their  immediate  neighborhoods.  So  it  has 
come as a surprise over the past decade that black 
hole  activity  is  closely  intertwined  with  star  for-
mation  occurring  farther  out  in  the  galaxy.”  (SAT 
2009 Practice Test) 

The  main  idea  is  the  last  sentence  of  the  pas-
sage,  but  the  many  critical  ideas  that  the  author 
develops  are:  “black  holes”,  “destruction”,  and 
“intertwined with star formation”.  
4.2  Finding Critical Ideas 

The word2vec model (Mikolov, 2013) has been 
a widely used statistical model for encoding word 
meanings.  We use a modified hierarchical cluster-

     Next we rank the sentences. Each sentence has 
a  vector  of  nine  feature  scores.   Although  many 
different  weighing  schemes  are  possibilities,  we 
take the simple approach of uniform weights.  We 
compare the top-3 most difficult sentences ranked 
by  the nine  features  to  those  ranked  by  sentence 
length and tree depth.  For lower-grade texts, there 
is almost no difference in the order.  But for more 
complex  passages,  more  significant  differences 
start to show.  Through this exercise, we also find 
a  qualitative  value  of  the  nine  features.    Even 
when the rankings by our nine features agree with 
the length-based rankings, we can point out more 
specifically what makes these sentences difficult.  
These  specifics  are  shown  as  Notes  in  Table  3.  
We believe the ability to locate these syntax phe-
nomena for students should be helpful in improv-
ing their reading skills. 
 
Rank 
Top  1 
by both 

Sentence 

Deeming  that  a  serene  and  unconscious 
contemplation  of  him  would  best  beseem 
me, and would be most likely to quell his 
evil mind, I advanced with that expression 
countenance, and was rather congratulating 
myself on my success, when suddenly the 
knees  of  Trabb's  boy  smote  together,  his 
hair  uprose,  his  cap  fell  off,  he  trembled 
violently in every limb, staggered out into 
the road, and crying to the populace, "Hold 
me!” 

Top  2 
by 
length 
and 
depth 

Notes:   Specifically,  in  addition  to  a  depth  of  17 
levels, two long  delay  (underlined), and a 
parallel phrase (double underlined). 
Words cannot state the amount of aggrava-
tion  and  injury  wreaked  upon  me  by 
Trabb's boy, when, passing abreast of me, 
he  pulled  up  his  shirt  collar,  twined  his 
side-hair,  stuck  an  arm  akimbo,  and 
smirked extravagantly by, wriggling his el-
bows and body, and drawling to his attend-
ants,  "Don't  know  yah,  don't  know  yah, 
'pon  my soul don't know yah!" 
The disgrace attendant on his immediately 
afterwards taking so crowing and pursuing 
me across the  bridge  with crows, as from 
an  exceedingly  dejected  fowl  who  had 
known  me when I  was a  blacksmith, cul-
minated the disgrace with which  I  left  the 
town, and was, so to speak, ejected by it in-
to the open country. 
a  long  interruption  of  18  words  (under-
lined),  one  parallel  phrase  (“crowing  and 
pursuing”,  double  underline),  and  one  PP 
fronting (“with which”, italicized).   

Top  2 
by  nine 
features 

Notes:  

 

15ing algorithm using word2vec3 as a representation 
of each word.  First, cosine distances are comput-
ed on every word pair in the passage (after remov-

ing  stopwords),  resulting  in  an !×!  matrix 

where n is the number of words.  Unlike the tradi-
tional hierarchical clustering where the end result 
is a tree structure, our clustering is more flat and 
does not build a hierarchy.  The linking criteria are 
two: (1) the distance between two words must ex-
ceed  a  minimum  and  (2)  the  distance  between  a 
word and an existing cluster must exceed a mini-
mum  percentage  of  the  best  pair  in  the  cluster.  
The algorithm is in Figure 3. 
 

 

 

 

Figure 3. Word2Vec Modified Clustering 

 

5  Applications,  Experiments  and  Re-

sults 

In addition to identifying troublesome sentences, 
there are many other useful things possible with 
these features. Interesting experiments include 
comparing tests across many dimensions such as 
across geography and across standards. 

 

                                                   
3 This is the Google News word2vec at 
https://github.com/mmihaltz/word2vec-GoogleNews-
vectors 

 

5.1  State Difference? 

The  National  Assessment  of  Educational 
Progress, or NEAP  offers reading assessments to 
4th  and  8th  graders  nationwide.  In  2015,  all  52 
states participated. A state may score higher than 
another  state  for  a  variety  of  reasons,  economic, 
political, etc. In this experiment, we’re interested 
in seeing if there might be any meaningful corre-
lation at all between a state’s NAEP score and the 
difficulty level of its state ELA4 tests. To this end, 
we  select  Massachusetts,  the  top-ranking  state 
whose NAEP score of 235 is considerably higher 
than the national average of 221, and compare its 
state ELA passages to those of New York whose 
score is 223. The data comparison is shown in Ta-
ble 4a. The metrics are shown in Tables 4b and 4c 
where p-values are at 95% and the bold values in-
dicate  statistical  significance.  Again,  the  more 
specialized feature ‘Inversion’ is not a significant 
factor in 4th and 8th grade readings5. 

 

Grade 
NY 4th  
MA 4th 
NY 8th 
MA 8th 

Sentences 

1,729 
1,093 
1,636 
908 

Table 4a. NY and MA ELA Passages 

Metric 
Delay 

Interruption 
Pairs NP VP 

Depth 

NY 4th  MA 4th 
2.083 
1.551 
0.180 
0.527 
1.765 
1.484 
8.662 
7.723 
0.002 
0.002 

Inversion 

0.80 
Table 4b. NY and MA 4th grade comparison 

Interruption 
Pairs NP VP 

Metric 
Delay 

p-value 
0.016 
1.71e-6 
5.46e-7 
1.26e-5 
0.46 
Table 4c. NY and MA 8th grade comparison 

NY 8th  MA 8th 
2.613 
2.110 
0.557 
1.116 
2.074 
1.778 
9.809 
9.114 
0.004 
0.007 

Depth 

Inversion 

 
It’s  interesting  to  see  that  for  both  4th  and  8th 
grades,  there  is  a  progression  of  text  difficulty 
from  NY’s  ELA  tests  to  MA’s  ELA  tests.  There 
are  many  reasons,  both  educational  and  non-
educational, that come into play to influence one 
                                                   
4 English Language Arts 
5 At the time of the paper, only the 4th and 8th grade ELA 
from Massachusetts tests are publically available online. 

Words 
20,533 
16,593 
26,812 
17,594 

p-value 
9.26e-5 
3.54e-7 
7.68e-11 
1.85e-15 

16state’s performance.  Perhaps this could be a first 
step  in  better  understanding  the  impact  of  in-
creased level of difficulty on student reading per-
formance.  
 
5.2  SAT or ACT? 
The SAT and the ACT are standardized tests col-
lege-bound juniors and seniors take. One common 
section in both tests is the Reading section where 
students are given passages to read and multiple-
choice  questions to  answer. Students  and  parents 
have long wondered which test is easier. A simple 
online search of  “SAT  reading vs. ACT  reading” 
yields many comparisons. The question of which 
test is easier depends on many factors such as tim-
ing, question types, and so on. What this paper is 
concerned  with  is  not  necessarily  the  simple 
yes/no answer to the question of which test is eas-
ier,  but  rather  with  comparing  the  passages  on 
each reading test. From a simple survey at a local 
test preparation center, students who choose ACT 
all report that the ACT passages are more straight-
forward  than  those  on  the  SAT,  and  those  who 
take the SAT  report that  some SAT passages  are 
harder to read, specifically in genres such as pre-
1900  fictions  and  history.  This  fact  does  not  di-
rectly lead to  a  judgment  of  which test  is  easier, 
simply that the ACT passages are easier to read6. 
To test this hypothesis and to quantify how much 
easier  or  harder  the  reading  passages  differ  on 
each test, we collect passages from both tests and 
run the feature analysis on them.  The data infor-
mation is presented in Table 5a. 

 

Test 

SAT 

ACT 

Year of 

Test 

Number 
of pas-
sages 

Number of 

words 

26,862 

40 

2015 – 16 
Official 
Practice 
2015 – 17 
Official Re-
leased Tests 
Table 5a. SAT and ACT Passage Data 

40 

28,752 

 
 
 
                                                   
6 Independent of the level of the passages, the questions can 
still be hard. Therefore, the level of passages is but one fac-
tor among many that a student takes into account in decid-
ing which test to take. 

 

Feature 
Delay 

Interruption 
Pairs NP-VP 

Depth 

Inversion 

SAT 
3.364 
1.552 
2.502 
11.403 
0.009 

ACT 
2.570 
1.214 
2.068 
10.264 
0.008 

Table 5b. SAT and ACT 

p-value 
0.0006 
0.014 
2.92e-12 
1.92e-12 
0.728 

 

 

Feature 
Delay 

Interruption 
Pairs NP VP 

Depth 

SAT 
2.397 
1.349 
0.893 
2.179 
0.031 

ACT 
1.248 
0.841 
0.425 
1.490 
0.021 

Inversion 
Table 5c. SAT and ACT Standard Deviation 

The results of the  analysis  are shown in Table 
5b.  ACT  passages  score  uniformly  lower  than 
those on the SAT with majority of the difference 
being statistically significant. Table 5c shows that 
the standard deviations of the SAT are higher, in-
dicating  that  the  SAT  passages  have  more  varia-
tions. The two excerpts from each test in Table 6 
give a qualitative view of the phenomenon where 
* indicates an example of increased complexity. 
 

ACT Hu-
manities 

ACT Sci-

ence 

* SAT Hu-
manities:  

SAT Sci-
ence 

In 2008, the prodigiously gifted bass-
ist, singer, and composer Esperanza 
Spalding released her major-label de-
but. Esperanza, which she recorded as 
a twenty-three-year-old instructor at 
the Berklee College of Music. 
Pikas, a diminutive alpine-dwelling 
rabbit relative. are unique among al-
pine mammals in that they gather up 
vegetation throughout summer—
including ﬂowers, grasses, leaves, ev-
ergreen needles, and even pine cones 
– and live off the hay pile throughout 
winter, rather than hibernating or 
moving downslope. 
But of all relations, that between men 
and women, being the nearest and 
most intimate, and connected with the 
greatest number of strong emotions, 
was sure to be the last to throw off the 
old rule, and receive the new; for, in 
proportion to the strength of a feeling 
is the tenacity with which it clings to 
the forms and circumstances with 
which it has even accidentally become 
associated … 
Nearly a half-century ago, Peter Higgs 
and a handful of other physicists were 

17likely context-independent choice is “purchasing” 
and that  is  what the  baseline model  chooses.   In 
the  given  passage,  however,  the  enclosing  sen-
tence  is  “According  to  [this  thesis],  television 
consumption  leads  above  all  to  moral  dangers.” 
After  adding up  all the  vectors  of the  contextual 
words, the correct answer “viewing” surfaces and 
the context-model is able to answer that question 
correctly.    This  model  makes  concrete  what  the 
English  teachers  have  meant  when  they  instruct 
the students to look at the context.  It also repre-
sents nicely the idea that the meaning of a word is 
selected by its surrounding words (the context).   

6  Conclusion and Future Work 

We  present  a  set  of  straightforward  and  novel 
features to identify difficult sentences in a reading 
passage. In our experiments, the features correlate 
well with the actual grade of each text. We are al-
so able to quantify and make more concrete of the 
differences  between  Common  Core  and  pre-
Common  Core  standards,  and  between  different 
states. In the future, we hope to not only put all in 
an application for real use but also to incorporate 
general-purpose lexical features to further enhance 
reading  comprehension  education.   Secondly,  we 
intend  to  continue  to investigate using  word2vec 
as  a stepping stone to distributed meaning  repre-
sentation.    For  example,  extend  critical  ideas  to 
multi-word  phrases  and  tackle  reading  compre-
hension questions such as those on the SAT.  

 

Acknowledgments  
This work would not have been possible without 
the patient and generous help from my mentor Dr. 
Scott McCarley from IBM Research. 
T 
 

 

trying to understand the origin of a 
basic physical feature: mass. You can 
think of mass as an object’s heft or, a 
little more precisely, as the resistance 
if offers to having its motion changed. 
Table 6. SAT and ACT Passage Difference Examples 
 
5.3  Automatic Vocabulary Response 

It  is  labor  intensive  to  manually  evaluate 
the  efficacy  of  the  word2vec-based  lexical  ap-
proach.    While  we  annotate  data  for  further  re-
search, we meanwhile evaluate the idea on vocab-
ulary questions on the 8 released SAT official tests 
(CollegeBoard,  2009).    These  vocabulary  ques-
tions ask the meaning of a word in the context of a 
given passage.  The majority of the choices con-
sist of one word each.  Our baseline approach is to 
measure the vector cosine score between the word 
in  question  and  the  words  in  each  choice.    The 
choice with the greatest similarity score is chosen 
as the answer.  When a choice has more than one 
word, we first remove the function words and then 
take the average of the vector scores.   
     We then apply a contextual word2vec model to 
the  questions.    For  each  word  in  a  vocabulary 
question, we locate the sentence that the word oc-
curs in  and  add  up  the  vectors of  all the  content 
words in that sentence. The resultant vector is then 
compared to each choice in the vocabulary ques-
tion.  Table  7 shows that  the  context model out-
performs baseline significantly.  This  experiment 
shows  the  power  of  combining  context  and  a 
computable  meaning  representation  such  as  the 
word2vec. 
 

28 Vocabulary Questions from 8 official SAT tests 

Method 
Baseline 
Context 

Num. Correct 

5 
20 

Accuracy 
17.86% 
71.43% 

Table  7.  Word2Vec-based  Vocabulary  Perfor-
mance 
 

One  reason  the  baseline  performs  poorly  is 
that almost all words tested in the SAT vocabulary 
questions  are  polysemous.    The  word2vec  is 
trained  on  mostly  news  data  which  biases  the 
meaning of a word toward a typical news-oriented 
meaning.  For example, the word ‘consumption’, 
without  context,  is  most  intuitively  associated 
with consumer and commerce. In this question, of 
the  five  choices,  “destruction”,  “viewing”,  “ero-
sion”,  “purchasing”,  and  “obsession”,  the  most 

 

18Mikolov T., Ilya Sutskever, Kai Chen, Greg Corrado, 
and  Jeffrey  Dean.  2013  Distributed  Representa-
tions  of  Words  and  Phrases  and  their  Composi-
tionality. In NIPS, pages 3111–3119 

Petersen, S.E., Ostendorf, M. 2009. A machine learn-
ing  approach  to  reading  level  assessment.  Com-
puter Speech and Language, 23: 89-106. 

Pitler, Emily and Ani Nenkova. 2008. Revising Read-
ability:  A  Unified  Framework  for  Predicting  Text 
Quality. Conference on Empirical Methods in Nat-
ural Language Processing, Honolulu, Hawaii. 

Siddharthan A., Angrosh  Mandya.  2014.  Hybrid text 
simplification  using  synchronous  dependency  gram-
mars  with  hand-written  and  automatically  harvested 
rules.  In  Proceedings  of  the  14th  Conference  of  the 
European Chapter of the ACL, Gothenburg, Sweden. 
Tanaka-Ishii,  K.,  Tezuka,  S.,  and  Terada,  H.  2010. 
Sorting texts by readability.  Computational Linguis-
tics, 36(2), pp. 203-227 
Vajjala Sowmya, Detmar Meurers 2014. Assessing the 
relative reading level of sentence pairs for text simpli-
fication. In Proceedings of the 14th Conference of 
the  European  Chapter  of  the  ACL,  Gothenburg, 
Swede.

References 
Coleman,  Meri;  and  Liau,  T.  L.  1975.  A  computer 
readability formula designed for machine scoring, 
Journal  of Applied  Psychology, Vol. 60,  pp. 283–
284.  

CollegeBoard,  2009.  The  Official  SAT  Study  Guide. 

2nd edition, Macmillan Publishing 

Common  Core. Common  Core  State  Standards  2010 
National  Governors  Association  Center  for  Best 
Practices,  Council of  Chief  State  School  Officers, 
Washington D.C.  

Dale  E,  Chall  J.  1948.  A  Formula  for  Predicting 
Readability. Educational Research Bulletin. 27. pp. 
11–20. 

Dascalu,  M.,  P. Dessus,  S. Trausan-Matu,  M.  Bianco, 
and A. Nardy. 2013. Readerbench, an environment for 
analyzing text complexity and reading strategies. In Ar-
tificial  Intelligence  in  Education,  pages  379–388. 
Springer 
 
Feng, Lijun. 2010. Automatic Readability Assessment. 
Dissertation Thesis, City University of New York, 
NY. 

François, T. et Bernhard, D. (eds.),  2014. Recent Ad-
vances  in  Automatic  Readability  Assessment  and 
Text Simplification. In International Journal of Ap-
plied Linguistics (Special issue), 165:2 

Kate,  Rohit.  J.,  Luo,  X.,  Patwardhan,  S.,  Franz,  M., 
Florian, R., Mooney, R. J., Welty, C. 2010. Learn-
ing  to  predict  readability  using  diverse  linguistic 
features.  In  the  23rd  International  Conference  on 
Computational Linguistics, p. 546–554 

Kincaid,  J.P.,  Fishburne,  R.P.,  Rogers,  R.L.,  and 
Chissom, B.S. 1975. Derivation of new readability 
formulas  (automated  readability  index,  fog  count, 
and flesch reading ease formula) for Navy enlisted 
personnel. Research Branch Report 8–75. Chief of 
Naval Technical Training: Naval Air Station Mem-
phis. 

Lennon,  Colleen  and  Hal  Burdick.  2004.  The  Lexile 
Framework  as an Approach  to Reading  Measure-
ment 
Success. 
https://cdn.lexile.com/cms_page_media/135/The%
20Lexile%20Framework%20for%20Reading.pdf 

and 

Manning, Christopher D., Mihai S., John Bauer, Jen-
ny Finkel, Steven J. Bethard, and David McClosky. 
2014.  The  Stanford  CoreNLP  Natural  Language 
Processing Toolkit. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational 
Linguistics: System Demonstrations, pp. 55-60. 

McLaughlin,  G.  Harry.  1969.  SMOG  Grading  —  a 
New Readability Formula. Journal of Reading. 12 
(8): 639–646. 

 

19
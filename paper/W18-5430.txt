Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not:

Testing whether Word Embeddings Can Tell

Pia Sommerauer and Antske Fokkens

Computational Lexicology and Terminology Lab

Vrije Universiteit Amsterdam

De Boelelaan 1105 Amsterdam, The Netherlands

pia.sommerauer@vu.nl, antske.fokkens@vu.nl

Abstract

This paper presents an approach for inves-
tigating the nature of semantic information
captured by word embeddings. We propose
a method that extends an existing human-
elicited semantic property dataset with gold
negative examples using crowd judgments.
Our experimental approach tests the ability of
supervised classiﬁers to identify semantic fea-
tures in word embedding vectors and com-
pares this to a feature-identiﬁcation method
based on full vector cosine similarity. The idea
behind this method is that properties identi-
ﬁed by classiﬁers, but not through full vec-
tor comparison are captured by embeddings.
Properties that cannot be identiﬁed by either
method are not. Our results provide an initial
indication that semantic properties relevant for
the way entities interact (e.g. dangerous) are
captured, while perceptual information (e.g.
colors) is not represented. We conclude that,
though preliminary, these results show that our
method is suitable for identifying which prop-
erties are captured by embeddings.
Introduction

1
Word embeddings are widely used in NLP and
have been shown to boost performance in a large
selection of tasks ranging from morphological
analysis to sentiment analysis (Lazaridou et al.,
2013; Socher et al., 2013; Zhou and Xu, 2015,
among many others). Despite a number of dif-
ferent approaches to evaluation, our understand-
ing of what type of information is represented by
the vectors remains limited. Most approaches fo-
cus on full-vector comparison which treat vectors
as points in a space (Yaghoobzadeh and Sch¨utze,
2016), which are evaluated by performance on
semantic similarity or relatedness test sets and
analogy questions (Mikolov et al., 2013; Turney,
2012). Previous work, however, has shown that
high performance does not necessarily mean that

vectors actually contain the information required
to solve the task (Rogers et al., 2017; Linzen,
2016). Better understanding of the kind of seman-
tic information captured by word embeddings can
increase our understanding of how they help im-
prove downstream tasks. In general, understand-
ing what information is present in (often promi-
nent) input embeddings forms an essential com-
ponent of gaining deeper understanding of the na-
ture of information and manner in which it travels
through the hidden layers of a neural network.

In this paper, we propose a method that inves-
tigates what kind of semantic information is en-
coded in vectors using a human-elicited dataset
of semantic properties. We compare the output
of supervised classiﬁers to an approach based on
full-vector comparison that cannot access individ-
ual dimensions. The assumptions behind this ap-
proach are that (1) both full-vector comparison
and the supervised classiﬁer will perform well
on identifying semantic properties that correlate
highly with general similarity; (2) the classiﬁer
will outperform full-vector analysis on properties
that are reﬂected by the context, but shared among
a diverse set of entities and (3) that neither ap-
proach will perform well on properties that are not
represented directly or indirectly in the text. The
last two outcomes can indicate whether a semantic
property is encoded in embeddings (2) or not (3).
The main contribution of this paper lies in the
new method and corpus it proposes. To our knowl-
edge, this is the ﬁrst approach that aims at identify-
ing whether speciﬁc semantic properties are cap-
tured by individual dimensions or complex pat-
terns in the vector. In addition, we provide speciﬁc
hypotheses as to which properties are captured
well by which method and test them using our
approach.1 Our general hypothesis states that se-

1The hypotheses and code for our experiments can

Proceedingsofthe2018EMNLPWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages276–286Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics276mantic properties that are relevant for the way en-
tities interact with the world are well represented
(e.g. functions of objects, activities entities are fre-
quently involved in), whereas properties of rela-
tively little consequence for the way entities inter-
act with the world are not (e.g. perceptual proper-
ties such as shapes and colors, which either have
no function or highly diverse functions). Though
preliminary due to the complexity of the task, re-
sults indicate that these tendencies hold. More-
over, the overall outcome shows that the method
and data are complementary to existing intrinsic
evaluation methods.

The rest of this paper is structured as follows.
We discuss related work in Section 2. Our method
is outlined in Section 3. Section 4 presents our ex-
periments and results. We ﬁnish with a critical dis-
cussion and overview of future work in Section 5.

2 Related work

Intrinsic evaluation of word embeddings has pri-
marily focused on two main tasks:
identifying
general semantic relatedness or similarity and the
so-called analogy task, where word embeddings
have been shown to be able to predict missing
components of analogies of the type A is to B as
C is to D (Mikolov et al., 2013; Turney, 2012).
Furthermore, most intrinsic evaluation methods
take full vectors into consideration. The famous
examples P aris − F rance + Italy ≈ Rome
or king − man + woman ≈ queen evoke the
suggestion that embeddings can capture semantic
properties. The task has, however, been criticized
substantially (Linzen, 2016; Gladkova and Drozd,
2016; Gladkova et al., 2016; Drozd et al., 2016,
among others).

Gladkova et al. (2016) follow an observation
in Levy and Goldberg (2014) on the large differ-
ences in performance on different categories in the
Google analogy set (Mikolov et al., 2013). They
provide a new, more challenging, analogy dataset
that improves existing sets on balance (capturing
more semantic categories) and size. Linzen (2016)
points out more fundamental problems including
the observation that the target vector in the anal-
ogy task can often be found by simply taking the
vector closest to the source. Drozd et al. (2016)
show that classiﬁers picking out the target word
from a set of related terms outperform the stan-

be found at: https://cltl.github.io/semantic_
space_navigation

dardly applied cosine addition or multiplication
methods. Though also boosted by the aforemen-
tioned proximity bias, these results indicate that
standard methods of solving analogies miss infor-
mation that is captured by embeddings. Rogers
et al. (2017) conclude that the analogy evaluation
does not reveal if word embedding representations
indeed capture speciﬁc semantic properties.

On top of that, an embedding may capture spe-
ciﬁc semantic properties in ways that are not anal-
ogous to semantic properties of related categories.
Analogy methods assume that semantic properties
stand in analogous relation to each other based on
the information provided by the context, but there
is no reason why (e.g.) things made of wood and
things made of plastic result in (combinations of)
embedding dimensions that are similar enough to
stand in a parallel relation to each other. Our set-
up can determine whether the properties are rep-
resented without supposing such structures by tar-
geting semantic properties directly rather than in
relation to other concepts.

Several approaches have attempted to derive
properties collected in property norm datasets
from the distribution in naturally occurring texts
(Kelly et al., 2014; Baroni et al., 2010; Barbu,
2008). Whereas these approaches yield indica-
tions about the potential of distributional models,
they do not go beyond full-vector proximity on a
low-dimensional SVD model or context words in a
transparent, high-dimensional count model. Their
focus lies on detecting informative contexts. We
follow the idea behind this approach and make a
human-elicited property dataset that is created in
the same tradition, but larger. Our approach goes
beyond the previous work in two ways: ﬁrst, we
add gold negative examples which allows us to
go beyond testing for salient properties. Second,
we compare full vector proximity to the outcome
of a classiﬁer which allows us to verify whether
the property is captured for entities that share the
property, but are not similar otherwise.

A few other studies go beyond full vec-
tor comparisons, moving towards the interpreta-
tion of word embedding dimensions. Tsvetkov
et al. (2015, 2016) evaluate word embeddings
by measuring the correlation between word em-
bedding vectors and count vectors representing
co-occurrences of words with WordNet super-
senses. While they show that their results have
a higher correlation with results obtained from

277extrinsic evaluations than standardly used intrin-
sic evaluations, they do not provide insights into
what kind of semantic information is represented
well. Yaghoobzadeh and Sch¨utze (2016) decom-
pose distributional vectors into individual linguis-
tic aspects by means of a supervised classiﬁca-
tion approach to test which linguistic phenomena
are captured by embeddings. They test their ap-
proach on an artiﬁcially created corpus and do not
provide insights into speciﬁc semantic knowledge.
Faruqui et al. (2015) transform learned embedding
matrices into sparse matrices to make them more
interpretable, which is complementary to our ap-
proach.

Previous studies provide (indicative) support for
the hypothesis that embeddings lack information
people get from other modalities than language.
Fagarasan et al. (2015) present a method to ground
embedding models in perceptual information by
mapping distributional spaces to semantic spaces
consisting of feature norms. Several approaches to
boosting distributional models with visual infor-
mation show that the additional information im-
proves the performance of word embedding vec-
tors (Roller and Schulte im Walde, 2013; Lazari-
dou et al., 2014). Whereas this indicates that word
embedding models lack visual information, it does
not show to what extent different types of prop-
erties are encoded. The method proposed in this
paper is, to the best of our knowledge, the ﬁrst
approach speciﬁcally designed to identify what
semantic knowledge is captured in word embed-
dings. We are not aware of earlier work that pro-
vides explicit hypotheses about the kind of infor-
mation we expect to learn from distributional vec-
tors, making this the ﬁrst attempt to conﬁrm these
hypotheses experimentally.

3 Method

The core of our evaluation consists of testing
whether nearest neighbors and classiﬁers are ca-
pable of identifying which embeddings encode a
given semantic property. We ﬁrst describe the
dataset and then present the procedure we ap-
ply. We complete this section with our hypotheses
about the outcome of our evaluation.

3.1 Extended CSLB Data
We use the Centre for Speech, Language and the
Brain concept property norms dataset (Devereux
et al., 2014, henceforth CSLB). This dataset fol-

lows the tradition of the sets created by McRae
et al. (2005); Vinson and Vigliocco (2008) and
used in Kelly et al. (2014); Baroni et al. (2010);
Barbu (2008) and is the largest available seman-
tic property dataset we are aware of. In the col-
lection process, human subjects were given con-
crete and mostly monosemous concepts and asked
to provide a set of semantic features. Polysemous
concepts were disambiguated. Properties were
elicited by cues such as has, is, does and made of.
An empty slot was provided to ﬁll in other rela-
tions. The dataset comprises 638 annotated con-
cepts, each of which was presented to 30 partici-
pants. Properties listed by at least two participants
are included in the published set.

We select features associated with at least 20
concepts. In an exploratory experiment, we count
all concepts for which the target feature is listed as
positive examples and all other concepts as nega-
tive examples. However, the fact that people did
not list a property does not necessarily mean that
a given concept is a negative example of it. For
instance: falcon is described by is a bird, but not
by is an animal.

For proper evaluation, the CSLB dataset should
be extended with veriﬁed negative examples. We
apply two methods to add both positive and (veri-
ﬁed) negative properties to CSLB. First, we select
properties that necessarily imply the target prop-
erty (e.g. is a bird implies is an animal) or nec-
essarily exclude the target property (e.g. is food
almost certainly excludes has wheels). We both
manually inspect the extended sets of positive and
negative examples per selected property to ex-
clude remaining noise independently, resolving
disagreements after discussion.2

The resulting dataset has the disadvantage that
negative examples largely consist of the same
speciﬁc categories, e.g. negative examples of
has wheels are food, animals and plants. Based on
these examples, we cannot tell whether the classi-
ﬁer performs well because embeddings encode the
property of having wheels or because it can distin-
guish vehicles from food, animals and plants. We
therefore need to expand the dataset so that it in-
cludes diverse negative and positive examples and
preferably positive and negative examples that are
closely related in semantic space.

Ultimately, we want to verify and increase the

2All annotations, guiding principles as well as notes about
resolving discussions can be found at https://cltl.
github.io/semantic_space_navigation.

278entire dataset and distinguish between things that
always or typically have a property (e.g. bike
has wheels, banana is yellow), things that can
have a property (e.g. bikini - is pink, plate -
made of metal) and things that do normally not
have a property (e.g. grape - does kill, beer -
is pink). We set up a crowdsourcing task in which
we ask participants whether a property applies to
a word. Possible answers are yes, mostly, possibly
and no.

This crowdsourcing method has currently been
applied to a selection of property-concept pairs
that were labeled as false-positives by at least one
of our approaches in the initial setup.
In addi-
tion, we extend the property-concept pairs given
to crowd workers by collecting the nearest neigh-
bors of the property centroid and a number of seed
words. We aim at (1) identifying negative exam-
ples that have a high cosine similarity to positive
examples in the dataset and (2) including a broader
variety of words. This nearest-neighbors strategy
explicitly aims at collecting words that are highly
similar to positive examples of a property but are
not associated with it. For instance, in order to ex-
tend the concept set for the property has wheels,
we used the seed words car, sledge, and ship.3

In the experiments reported in this paper, we
only consider properties that clearly apply to a
concept as positive examples (the yes and mostly
cases) and properties that clearly do not apply as
negative examples, leaving disputable cases and
the cases that possibly apply for future work. We
manually checked cases of disagreement in the
crowd data and selected or removed data based on
these criteria.4

3.2 Classiﬁcation approaches
We use the pretrained Word2vec model based on
the Google News corpus.5 The underlying ar-
chitecture is a skip-gram with negative sampling
model (Mikolov et al., 2013), which learns word
vectors by predicting the context given a word.

The overall goal is to investigate whether word
vectors capture speciﬁc semantic properties or not.
We start from the assumption that classiﬁers can

3The details about our selection and full lists of seed

words are provided with our code (see link in Footnote 1).

4Some difference in judgment are clearly the result of lack
of knowledge (e.g. not knowing a something is an animal).
The original outcome of the crowd and ﬁnal resulting test are
provided on the github repository associated with this paper.

5https://code.google.com/archive/p/

learn properties that are represented in the embed-
ding in a binary classiﬁcation task. We apply su-
pervised classiﬁcation to see whether a logistic re-
gression classiﬁer or a neural network are capable
of distinguishing embeddings of words that have
a speciﬁc semantic property from those which do
not. Speciﬁcally, we use embedding vectors cor-
responding to words associated or not associated
with a semantic target-property (i.e. positive and
negative examples) as input for a binary classiﬁer
and test whether the classiﬁer can learn to distin-
guish embeddings of words that have the property
from those who do not. However, word embed-
dings also capture semantic similarity. If a prop-
erty is shared by similar entities (e.g. most animals
with a beak are birds), the classiﬁers may perform
well because of this similarity rather than identi-
fying the actual property. We therefore compare
the performance of classiﬁers to the performance
of an approach based on full vector similarity. If
only the classiﬁers score well, this provides an in-
dication that the embedding captures the property.
If both methods perform poorly this could mean
that the property is not captured.6

Supervised classiﬁcation
As the datasets are limited in size, we evaluate
by applying a leave-one-out approach. We em-
ploy two different supervised classiﬁers, which we
expect to differ in performance. As a ‘vanilla’
approach, we use a logistic regression classiﬁer
with default settings as implemented in SKlearn.
This type of classiﬁer is also used in Drozd et al.
(2016) to detect words of similar categories in an
improved analogy model.

In addition, we use a basic neural network.
Meaningful properties may not always be encoded
in individual patterns, but rather arise from a com-
bination of activated dimensions. This is not cap-
tured well by a logistic regression model, as it can
only react to individual dimensions. In contrast,
the neural network can learn from patterns of di-
mensions. We use a simple multi-layer perceptron
(as implemented in SKlearn7) with a single hid-
den layer. We calculate the number of nodes in the
hidden layer as follows: (number of input dimen-
sions + number of output dimensions) * 1/3. The

6Given the size and balance of our dataset as well as the
lack of ﬁne-tuning, we remain careful not to draw ﬁrm con-
clusions at this point.

7http://scikit-learn.org/stable/index.

word2vec/

html

279pretrained Google News vectors have 300 dimen-
sions, resulting in a hidden layer of 100 nodes. We
use the recommended settings for small datasets.
No parameter tuning was conducted so far due to
the limited size of the datasets and the use of a
leave-one-out evaluation strategy. We present the
runs of several models, as the neural network can
react to the order in which the examples are pre-
sented as well as the randomly assigned vectors for
initialization. While the performance of the model
could be optimized further by experimenting with
the settings, we ﬁnd that the set-up presented here
already outperforms the logistic regression classi-
ﬁer in many cases.

Full vector similarity
To show that supervised classiﬁcation can go be-
yond full vector comparison in terms of cosine
similarity, we compare the performance of the
classiﬁers to an n-nearest neighbors approach. We
calculate the centroid vector of all positive exam-
ples in the training set. The training set consists
of all positive examples in the leave-one-out split
except for the one we are testing on. We then con-
sider its n-nearest neighbors measured by their co-
sine distance to the centroid as positive examples.
We vary n between 100 and 1,000 in steps of 100.
We report the performance of the optimal number
of neighbors for each property (which varies per
property). In future work, we will add more ﬁne-
grained steps and investigate the performance of
a classiﬁer using the cosine similarity of words to
the centroid as a sole feature.

Variety approximation
The performance of the approaches outlined above
depends on to the variety of words associated with
a property. We approximate this variety by calcu-
lating the average cosine similarity of words as-
sociated with a property to one-another. This is
done by averaging over the cosine similarities be-
tween all possible pairs of words. A high average
cosine similarity means that the words associated
with a concepts tend to be close to each other in
the space, which should mostly apply to words as-
sociated with taxonomic categories. In contrast, a
low average cosine means a high diversity, which
should largely apply to general descriptions.

3.3 Speciﬁc hypotheses
We select a number of properties for closer inves-
tigation based on the clean and extended dataset

described in Section 3.1. We ﬁrst formulated the
hypotheses independently, before discussing and
specifying them.8 Table 1 summarizes the agreed
upon expectations. The hypotheses can be catego-
rized in the following way:
Sparse Textual Evidence
We select properties of which we expect that tex-
tual evidence is too sparse to be represented by
distributional vectors. The properties is black,
is yellow, is red and made of wood have little im-
pact on the way most entities belonging to that
class interact with the world. We expect that the
only textual evidence indicating them are individ-
ual words denoting the properties themselves (e.g.
red, black, wooden)9 and it is unclear how often
they are mentioned explicitly. It may, however, be
the case that certain subcategories in the datasets
are learned regardless of this sparsity, because they
happen to coincide with more relevant taxonomic
categories such as red fruits.
Fine-grained Distinctions in Larger Categories
We expect that a supervised classiﬁer may be able
to make more ﬁne-grained distinctions between
examples of the same category when these dif-
ferences are relevant for the way they interact
with the world. We select two properties that in-
troduce crucial distinctions in larger categories:
has wheels and is found in seas. The former ap-
plies to a sub-group of vehicles and may be ap-
parent in certain behaviors and contexts only ap-
plying to these vehicles (rolling, street, etc). The
latter applies to animals, plants and other entities
found in water, but it is unclear whether textual ev-
idence is enough to distinguish between seawater
and fresh water.
Mixed Groups
We expect that a supervised machine learning ap-
proach can ﬁnd positive examples of a property
that are not part of the most common class in the
training set. For instance, the majority of posi-
tive examples for is dangerous and does kill refer
to weapons or dangerous animals. We expect the
classiﬁer to (1) ﬁnd positive examples from less
well represented groups and (2) be able to distin-
guish between positive and negative examples of
a well-represented category (e.g. rhino v.s. hippo

8Details can be found on the paper’s github repository.
9In the case of made of wood, the evidence may be a bit
broader, as it might be indicated by different types of wood
occurring in the context of furniture.

280Property
is an animal
is food
is dangerous
does kill
is used in cooking
has wheels
is found in seas
is black
is red
is yellow
made of wood

learnable property
yes
yes
yes
yes
yes
possibly
possibly
no
no
no
no

Table 1: Hypotheses about whether selected semantic
properties can be learned by a supervised classiﬁer

for killing). For the property is used in cooking,
the example words refer to food items as well as
utensils. We expect that classiﬁers can distinguish
between cooking-related utensils and other tools.

Polysemy
We expect that machine learning can recognize
vector dimensions indicating properties applying
to different senses of a word, whereas the nearest-
neighbors approach simply assigns the word to its
dominant class. For instance, we expect that word
vectors that can be used to describe animals as
well as food (e.g. chicken, rabbit or turkey) record
evidence of both contexts, but end up closer to one
of the categories. A supervised machine learning
approach should be able to ﬁnd the relevant di-
mensions regardless of the cosine similarity to one
of the groups and classify the word correctly. We
test this by training on a set of monosemous words
(animals and food items) and test on a set of poly-
semous and monosemous examples.

4 Experimental set-up and results
4.1 Concept diversity vs performance
We ﬁrst investigate the relation between perfor-
mance and diversity of concepts associated with
a property on the full, noisy dataset using a leave-
one-out approach. Table 2 shows a selection of
the f1-scores achieved on properties in the CSLB
dataset in relation to the average cosine similar-
ity of the associated words. A high average co-
sine similarity means that the concepts overall
have similar vector representations and can thus
be seen as having a low diversity. The results
of the Spearman Rank correlation clearly indicate
that scores achieved by nearest neighbors correlate
more strongly with the average cosine than the two
supervised classiﬁcation approaches.

0.22

0.22

0.22
0.23
0.24
0.24
0.24
0.25
0.25
0.25
0.26
...
0.3
0.31
0.32

cos
0.15
0.15
0.16
0.16
0.16
...
0.2
0.21
0.21
0.21
0.21
0.22

feature
is heavy
is strong
is thin
is hard
is expensive
...
is black
is electric
is dangerous
is colourful
is brown
has a handle
handles
has a seat
seats
does smell
is smelly
made of glass
has a point
does protect
is yellow
is soft
is red
is fast
is tall
is a tool
...
is a weapon
is green
has a
blade blades
is worn
has wheels
is found
in kitchens
does ﬂy
has a tail
is an animal
is eaten edible
has four legs
is a vehicle
does eat
...
has a beak
made of cotton
has roots
is a mammal
does grow
is a plant
has leaves
...
has pips seeds
is juicy
is a vegetable
is played
does play
does make music 0.55
spearman-r

0.33
0.33
0.33
0.33
0.34
0.34
0.34
...
0.37
0.37
0.37
0.37
0.37
0.37
0.37
...
0.47
0.5
0.52
0.53

0.32
0.32
0.33

f1-neigh
0.15
0.13
0
0.15
0
...
0.29
0.48
0.53
0.14
0.13
0.44

f1-lr
0.17
0.13
0.05
0.08
0.28
...
0.23
0.5
0.57
0.25
0.22
0.57

f1-net
0.21
0.34
0.1
0.26
0.37
...
0.24
0.69
0.59
0.32
0.33
0.58

type
op
e
vp
op
e

vp
vp
e
vp
vp
p

0.43

0.3

0.48

p

0.08

0.15

0.29
0.38
0.38
0.22
0.12
0.34
0.3
0.43
0.5
...
0.74
0.45
0.68

0.47
0.82
0.56

0.57
0.53
0.64
0.37
0.67
0.76
0.68
...
0.63
0.68
0.3
0.69
0.52
0.43
0.41
...
0.5
0.71
0.78
0.9

0.89
0.72

0
0.23
0.26
0
0
0.13
0.31
0.57
0.51
...
0.56
0.45
0.65

0.86
0.83
0.73

0.76
0.68
0.76
0.88
0.66
0.69
0.71
...
0.83
0.56
0.65
0.85
0.81
0.63
0.71
...
0.08
0.48
0.75
0.98

0.95
0.52

op

vp
p
f
vp
op
vp
vp
vp
t

t
vp
p

f
p
e

f
p
t
f
p
t
f

p
vp
p
t
e
t
p

p
op
t
f

f

0.37

0.28
0.47
0.37
0.23
0.16
0.27
0.48
0.65
0.47
...
0.63
0.45
0.74

0.9
0.87
0.76

0.76
0.69
0.78
0.85
0.66
0.79
0.68
...
0.87
0.64
0.72
0.86
0.81
0.64
0.78
...
0.46
0.56
0.81
0.98

0.92
0.59

Table 2: Performance of different approaches in rela-
tion to the average cosine similarity of words associ-
ated with a property (cos). The last row shows the
Spearman Rank correlation between f1-scores and av-
erage cosine similarity. Property types are listed un-
der type (p = part, vp = visual-perceptual, op = other-
perceptual, e = encyclopaedic, f = functional, t = taxo-
nomic).

281Property
full does kill
crowd does kill
full has wheels
full is black
full is dangerous
crowd is dangerous
full is found in seas
crowd is found in seas
full is red
full is used in cooking
full is yellow
full made of wood
full is an animal test
full is an animal train
full is food test
full is food train

pos
101
67
79
42
177
131
83
47
29
142
24
87
37
166
37
97

neg
69
49
349
89
104
84
72
28
80
61
68
282
20
77
20
146

Table 3: Class distribution in dataset consisting of the
clean datasets derived from the CSLB set and the ad-
ditional crowd judgments (marked full ). For some
properties, we included the dataset consisting of crowd-
judgments only, as it is more balanced across seman-
tic categories than the full set (marked crowd ). For
all properties, a leave-one-out approach was applied to
evaluation except for is animal and is food.

4.2 Outcome Speciﬁc Hypotheses
We carry out further experiments on a small ex-
tended and clean subset, consisting of carefully se-
lected negative examples from the CSLB dataset
and crowd annotations validated by the authors.
The distribution of positive and negative exam-
ples per property is shown in Table 3. For some
properties, the sets derived from the CSLB norms
alone have an imbalanced distribution of nega-
tive examples over semantic categories, as they
were selected by means of logical exclusion (e.g.
concepts listed under has wheels have been se-
lected as negative examples of is food). Therefore,
we add the more balanced but smaller datasets
created by crowd-judgments only where enough
judgments have been collected. We created addi-
tional sets for words part of the food-animal pol-
ysemy to test whether supervised classiﬁers can
successfully predict semantic properties of vari-
ous senses of polysemous words. In the following
sections, we will outline the most striking results.
Most results conﬁrm, but some contradict our ini-
tial hypotheses.

Table 4 shows the f1-scores on the full clean
datasets. As hypothesized, the color properties
is yellow and is red perform low in all approaches,
with slightly better results yielded by supervised
learning.

The properties involved in functions and activi-

ties or with high impact on the interaction of enti-
ties with the world all perform highly in the clas-
siﬁcation approaches. For does kill, is dangerous
and is used in cooking,
there is a large differ-
ence between the best nearest neighbors approach
and the best classiﬁcation approach (between 60
and 19 points), indicating that the classiﬁcation
approaches are able to infer more information
from individual dimensions than is provided by
full vector similarity. The property is dangerous
has, as can be expected, a particularly high diver-
sity of associated words (comparable to the col-
ors). Has wheels and is found in seas can be ex-
pected to have high correlations with other tax-
onomic categories (ﬁsh and water animals, vehi-
cles), which is reﬂected in the lower diversity and
comparatively high nearest neighbor performance.
Cases contradicting our expectations are the vi-
sual properties is black and made of wood. Both
have comparatively high classiﬁcation perfor-
mance with a big difference to the nearest neigh-
bor results. Most likely, this is due to a cat-
egory bias in the negative examples.
For in-
stance, a large portion of the negative examples
for is made of wood consist of animals and food.
In the dataset for is black, a large proportion of the
positive examples consists of animals. A classiﬁer
can perform highly by simply learning to distin-
guish these two categories from the rest.

The biases in semantic classes mentioned above
partially result from the way we generated the neg-
ative examples from the original CSLB dataset.
This means that a classiﬁer may learn to distin-
guish two semantic categories rather than being
able to ﬁnd vector dimensions indicative of the tar-
get property. We therefore also present selected
results on crowd-only datasets shown in Table 4,
which do not have this bias. It can be observed
that for all three properties,10 the performance
of the classiﬁcation approaches drops marginally,
whereas it rises for nearest neighbors.

We investigate the outcome on a number of
individual examples to gain more insights into
whether the subtle differences hypothesized in
Section 3 hold. Since we only formulate a general
hypothesis for Sparse Textual Evidence, we do not
dive deeper into the results for that category here.

10We only included properties for which we had enough

positive and negative examples in our set

282Fine-Grained Category Distinctions
The full clean has wheels dataset includes a num-
ber of instances for which the classiﬁers can make
more ﬁne-grained distinctions than nearest neigh-
bors. As hypothesized, classiﬁers,
in contrast
to nearest-neighbors, can recognize that neither
sled nor a skidoo have wheels, but a unicycle
a limousine, a train, carriage, an ambulance, a
porsche do. Another ﬁne-grained distinction can
be identiﬁed in the is found in seas crowd-only
set: Sculpin is correctly identiﬁed as a seawater
ﬁsh by all classiﬁers but not by nearest-neighbors.

Mixed Groups
Whereas nearest neighbors predominantly identify
weapons as is dangerous in the crowd-only set,
the classiﬁers go beyond this category. The neu-
ral network approach correctly identiﬁes that imi-
tation pistol, imitation handgun, and screwdriver
are negative examples of is dangerous. Further-
more, no animals are labeled as dangerous based
on proximity to the centroid, but the classiﬁers
are able to distinguish between some dangerous
and non-dangerous animals (e.g. rhinoceros is la-
beled positive, while giraffe and zebra are labeled
as negative). All three classiﬁers recognize that
meth, cocaine and oxycodone are considered dan-
gerous substances, despite the fact that they are
far away from the centroid of dangerous things.
Of the only two disease-like concepts, Hepatitis C
and allergy, the former is recognized by all clas-
siﬁers and the latter only by logistic regression.
The performance on the smaller, but also weapon-
dominated does kill crowd-only set is comparable,
but the variety of atypical cases is lower. Among
the only two disease-related items, dengue is iden-
tiﬁed by all classiﬁers and dengue virus only by
the neural network.

In the crowd-only is found in seas set, seabird
and gannet are correctly labeled as positive, even
though positive examples almost exclusively con-
sist of ﬁsh or underwater-animals, whereas the
negative examples encompass a vast variety of an-
imals, including bird and some freshwater ﬁsh.

Polysemy
For polysemy between food and animals (Table 4),
we observe that when trained on pure animal and
food words and tested on polysemous animal and
food words, the classiﬁers perform highly with a
large difference to nearest neighbors. For food
versus pure animal words, the classiﬁer perfor-

av-cos
0.23
0.37

property
full is yellow
full is used in
cooking
full is black
0.19
full is red
0.23
full is dangerous
0.24
crowd is dangerous 0.26
full has wheels
0.38
full is found in seas 0.44
crowd is found
0.50
in seas
0.27
full does kill
crowd does kill
0.30
full made of wood 0.17
0.37
full is food test
full is an
0.37
animal test

neigh
0.19
0.29

0.35
0.36
0.58
0.61
0.90
0.87
0.87

0.67
0.70
0.14
0.00
0.52

lr
0.47
0.98

0.75
0.51
0.88
0.86
0.96
0.97
0.94

0.83
0.82
0.84
0.36
0.88

net1
0.64
0.98

0.77
0.54
0.88
0.86
0.96
0.98
0.96

0.86
0.84
0.85
0.36
0.88

net2
0.64
0.98

0.77
0.52
0.87
0.86
0.95
0.98
0.96

0.82
0.80
0.85
0.36
0.88

Table 4: F1 scores achieved by logistic regression (lr)
two runs of a neural net classiﬁer (net1 and net2 and the
n-best nearest neighbors evaluated with leave-one-out
on the full datasets (marked as full and the crow-only
sets (marked as crowd ).

.

mance is much lower. We expect the extremely
low nearest neighbor performance to be due to the
fact that the centroid is calculated over pure food
items (without a single animal-related item, not
even culinary meat terms such as pork or beef )
which is far away from the animal-region in the
space. Despite the classiﬁers outperforming near-
est neighbors, the outcome does not conﬁrm our
original hypotheses. We expected that the classi-
ﬁers could identify that edible animals have both
animal properties and food properties, but upon in-
spection of the results, the classiﬁers only identi-
ﬁed entities with a predominant animal sense cor-
rectly as animals and those with a predominant
food sense correctly as food.

5 Discussion & Future Work

The experiments presented in this approach have
several limitations. First, our semantic datasets
are still limited in size. Second, the implication
method we applied to generate negative exam-
ples led to biases for some properties where most
negative examples belong to a small set of (tax-
onomic) classes. Third, no parameter tuning has
been carried out so far. Careful parameter tun-
ing would ensure that the best possible classiﬁca-
tion approaches are chosen and that the obtained
results truly exploit the informative power of the
embeddings. Due to the limited size of the dataset
and the leave-one-out approach to evaluation, this

283has not been possible in this preliminary study.
Fourth, the experiments presented here only con-
cern a small subsection of semantic properties too
limited to draw general conclusions.

Despite these limitations, our results provide
preliminary insights that lead us to conclude that
the overall idea behind our methods works and
opens up promising directions for future work.
We ﬁrst aim to address the limitations of the cur-
rent dataset. We intend to incorporate other sets
designed for similar insights, such as the analo-
gies presented in Drozd et al. (2016) and the Se-
mEval 2018 discriminative property set (Krebs
et al., 2018). In addition, we plan to extend and
reﬁne the sets with crowd annotations asking for
graded judgments (e.g. a property can mostly or
possibly apply) and exploit these judgments in fu-
ture experiments.

Once we created a bigger and more balanced
dataset, we can carry out experiments on differ-
ent train and test splits in order to overcome the
limitations of the leave-one-out evaluation. Fur-
thermore, we will apply careful parameter tuning
on a development set in order to ensure our results
are representative of the information captured by
the embeddings. The increased size of the set will
allow us to conduct more experiments that take the
distributions of semantic categories in the splits
into account as was done for the polysemy set.
This way, we ensure that we do not train on ex-
amples that belong to the same semantic category
as the ones in the test set.

Going beyond the method introduced in this pa-
per, we plan on investigating the type of informa-
tion encoded in linguistic context by testing which
properties can be learned from textual context di-
rectly. In addition, applying the method presented
by Faruqui et al. (2015) may provide stronger indi-
cations about the information represented by word
embedding dimensions. Adding these to exper-
iments allows us to trace which information is
provided by the context and what ends up being
present in word embeddings.

6 Conclusion

The main contribution of this paper is that it in-
troduces a new method aimed at investigating the
kind of semantic information captured by word
embedding vectors. We have taken the ﬁrst steps
towards constructing a dataset suitable for this
investigation on the basis of an existing dataset

of human-elicited semantic properties. We intro-
duced a set of hypotheses concerning which se-
mantic properties are captured by embeddings and
presented exploratory experiments verifying them.
The current results are limited by the size and
balance of our dataset, as discussed in detail in the
previous section. Nevertheless, we can report pre-
liminary insights based on our experiments. We
show that classiﬁers, in particular neural networks,
can identify which entities have a speciﬁc property
in cases where this does not follow from general
similarity or the overall semantic class the entity
belongs to. This can be seen as a ﬁrst indication
that (some) semantic properties are encoded in in-
dividual (patterns of) vector dimensions, which
can be identiﬁed.

The results on the extended datasets partly con-
ﬁrm that visual properties are not well represented
by embeddings, while properties relating to func-
tion (e.g. cooking, having wheels) and interac-
tions with other entities (e.g. being dangerous or
killing) tend to be represented well. Some of
these indications could be the result of the bias
in our current dataset, but others have been con-
ﬁrmed on the smaller crowd-only sets for proper-
ties with enough available data (is dangerous and
does kill). Further evidence is provided by the full
dataset for has wheels which encompasses a large
group of vehicles to which the property does not
apply. In addition, we support these indications by
qualitative insights through examples of the kinds
of distinctions made by the classiﬁers, but not the
nearest neighbor approach. Results achieved for
polysemous words and two visual properties cur-
rently do not conﬁrm our hypotheses.
Acknowledgements
This research is funded by the PhD in the Human-
ities Grant provided by the Netherlands Organi-
zation of Scientiﬁc Research (Nederlandse Organ-
isatie voor Wetenschappelijk Onderzoek, NWO)
PGW.17.041 awarded to Pia Sommerauer and
NWO VENI grant 275-89-029 awarded to Antske
Fokkens. We would like to thank anonymous re-
viewers for feedback and Piek Vossen for feedback
and discussion that helped improve this paper. All
remaining errors are our own.

References
Eduard Barbu. 2008. Combining methods to learn
feature-norm-like concept descriptions. In Proceed-

284ings of the ESSLLI Workshop on Distributional Lex-
ical Semantics, pages 9–16.

Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
science, 34(2):222–254.

Barry J. Devereux, Lorraine K. Tyler, Jeroen Geertzen,
and Billi Randall. 2014. The centre for speech, lan-
guage and the brain (cslb) concept property norms.
Behavior research methods, 46(4):1119–1127.

Aleksandr Drozd, Anna Gladkova, and Satoshi Mat-
suoka. 2016. Word embeddings, analogies, and ma-
chine learning: Beyond king-man+ woman= queen.
In COLING, pages 3519–3530.

Luana Fagarasan, Eva Maria Vecchi, and Stephen
Clark. 2015. From distributional semantics to fea-
ture norms: grounding semantic models in human
In Proceedings of the 11th Inter-
perceptual data.
national Conference on Computational Semantics,
pages 52–57.

Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris
Dyer, and Noah A. Smith. 2015. Sparse overcom-
plete word vector representations. In Proceedings of
ACL.

Anna Gladkova and Aleksandr Drozd. 2016. Intrinsic
evaluations of word embeddings: What can we do
better? In Proceedings of the 1st Workshop on Eval-
uating Vector-Space Representations for NLP, pages
36–42.

Anna Gladkova, Aleksandr Drozd, and Satoshi Mat-
suoka. 2016. Analogy-based detection of morpho-
logical and semantic relations with word embed-
dings: what works and what doesn’t. In Proceedings
of the NAACL Student Research Workshop, pages 8–
15.

Colin Kelly, Barry Devereux, and Anna Korhonen.
2014. Automatic extraction of property norm-like
data from large text corpora. Cognitive Science,
38(4):638–682.

Alicia Krebs, Alessandro Lenci, and Denis Paperno.
2018. Semeval-2018 task 10: Capturing discrimi-
native attributes. In Proceedings of The 12th Inter-
national Workshop on Semantic Evaluation, pages
732–740.

Is this a wampimuk?

Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014.
cross-modal map-
ping between distributional semantics and the visual
In Proceedings of the 52nd Annual Meet-
world.
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1403–
1414.

Angeliki Lazaridou, Eva Maria Vecchi, and Marco Ba-
roni. 2013. Fish transporters and miracle homes:
How compositional distributional semantics can

In Proceedings of the 2013 Con-
help np parsing.
ference on Empirical Methods in Natural Language
Processing, pages 1908–1913.

Omer Levy and Yoav Goldberg. 2014. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of the eighteenth conference on com-
putational natural language learning, pages 171–
180.

Tal Linzen. 2016. Issues in evaluating semantic spaces
In Proceedings of the 1st
using word analogies.
Workshop on Evaluating Vector-Space Representa-
tions for NLP, pages 13–18.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547–559.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, volume 13,
pages 746–751.

Anna Rogers, Aleksandr Drozd, and Bofang Li. 2017.
The (too many) problems of analogical reasoning
with word vectors. In Proceedings of the 6th Joint
Conference on Lexical and Computational Seman-
tics (* SEM 2017), pages 135–148.

Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal lda model integrating textual, cognitive
In Proceedings of the 2013
and visual modalities.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1146–1157.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.

Yulia Tsvetkov, Manaal Faruqui, and Chris Dyer. 2016.
Correlation-based intrinsic evaluation of word vec-
tor representations. In Proceedings of the 1st Work-
shop on Evaluating Vector-Space Representations
for NLP, pages 111–115.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In Proc. of EMNLP.

Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artiﬁcial Intelligence Research, 44:533–
585.

David P. Vinson and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set of
objects and events. Behavior Research Methods,
40(1):183–190.

285Yadollah Yaghoobzadeh and Hinrich Sch¨utze. 2016.
Intrinsic subspace evaluation of word embedding
representations. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
236–246.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
In Proceedings of the 53rd Annual Meet-
works.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
volume 1, pages 1127–1137.

286
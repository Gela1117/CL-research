Whitepaper on NEWS 2018 Shared Task on Machine Transliteration

Nancy Chen†, Xiangyu Duan‡, Min Zhang‡, Rafael E. Banchsη, Haizhou Li(cid:63)

†Singapore University of Technology and Design, Singapore

‡Soochow University, China 215006

ηNanyang Technological University, Singapore
(cid:63)National University of Singapore, Singapore

Abstract

Transliteration is deﬁned as the phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is a necessary subtask in many applica-
tions, such as machine translation, cor-
pus alignment, cross-language IR, infor-
mation extraction and automatic lexicon
acquisition. All such systems call for high-
performance transliteration, which is the
focus of the shared task in NEWS 2018.

1 Task Description

The objective of the Shared Task on Named En-
tity Transliteration at NEWS 2018 is to promote
machine transliteration research by providing a
common benchmarking platform for the research
community to evaluate state-of-the-art approaches
to this problem. The task is to develop machine
transliteration and/or back-transliteration systems
in one or more of the provided language pairs.

For each language pair, training and develop-
ment data sets containing source and target name
pairs are released for participating teams to train
their systems. At the evaluation time, test sets
of source names only will be released, on which
participants are expected to produce a ranked list
of transliteration and/or back-transliteration can-
didates in the target language. The results will be
automatically evaluated by using the same metrics
used in previous editions of the shared task.

This year’s shared task focuses mainly on “stan-
dard” submissions, i.e. output results from sys-
tems that have been trained only with the data pro-
vided by the shared task organizing team. This
will ensure that all results for the same task are
comparable across the different systems. Partici-
pants may submit several “standard” runs for each
of the task they participate in. Those participants
interested in submitting “non-standard” runs, i.e.

output results from systems that use additional
data during the training phase, still will be able
to do so. However such runs will be evaluated and
reported separately.

2

Important Dates

Train/Development data release
Test data release
Results Submission Due
Task (short) Papers Due
Acceptance Notiﬁcation
Camera-Ready Deadline
Workshop Date

12 March 2018
07 May 2018
14 May 2018
21 May 2018
28 May 2018
04 June 2018
20 July 2018

3 Participation

1. Registration (12 March 2018).

Prospec-
tive participants are to register through the
NEWS 2018 website by requesting the
datasets from 12 March onwards.

2. Train/Development Data (12 March 2018).
Registered participants are to obtain train and
development data from the shared task regis-
tration form and/or the designated copyright
owners of databases. All registered partici-
pants are required to participate in the evalu-
ation of at least one language pair, submit the
results, prepare a short paper and attend the
workshop at ACL 2018.

3. Test Data (07 May 2018). The test data
would be released on 07 May 2018, and the
participants have a maximum of 7 days to
submit their results to the competition site.
NEWS 2018 shared task will be run on Co-
daLab. Participants need to create a codalab
account and register into the NEWS 2018
competition in order to be able to submit their
system results. Only “standard” runs will be

ProceedingsoftheSeventhNamedEntitiesWorkshop,pages47–54Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics47processed this year. According to this, par-
ticipants are required to use only the train-
ing and development data provided within the
shared task to train their systems.

Participants can submit several runs for each
individual language pair at the competition
site. However, the total number of submis-
sions per language pair will be limited to
a maximum of 3 submissions per day, with
a total maximum of 15 submissions during
the whole period of the competition. From
all submissions done to each individual lan-
guage pair, each participant must select one
to be posted on the leaderboard. Results on
the leaderboard (by the last day of the shared
task on 14 May 2018) will constitute the ﬁnal
ofﬁcial results of the shared task.

Each submission must be saved in a ﬁle
named ”results.xml” and submitted into the
system in a ”.zip” compressed ﬁle format.
Each ”results.xml” ﬁle can contain up to 10
output candidates in a ranked list for each
corresponding input entry in the test ﬁle (re-
fer to Appendix B for more details on ﬁle for-
mating and naming conventions).

Those participants interested in submitting
“non-standard” runs, i.e.
transliteration re-
sults from systems that use additional data
during the training phase, still will be able
to do so. However such runs will be evalu-
ated and reported separately (please contact
the organizers).

4. Results (14 May 2018). Leaderboard results,
as on 14 May 2018, will be considered the
ofﬁcial evaluation results of the NEWS 2018
shared task. These results will be published
on the workshop website and proceedings.

Note that only the scores (evaluation metrics)
of the participating systems on each language
pair will be published, and no explicit refer-
ence to the participating teams will be pro-
vided. Furthermore, all participants should
agree on not to reveal identities of other par-
ticipants in any of their publications unless
permission from the other respective partici-
pants is granted. By default, all participants
remain anonymous in published results. Par-
ticipating teams are allowed to reveal only
their own identity in their publications.

5. Shared Task Short Papers (21 May 2018).
Each participant is required to submit a 4-
page system paper (short paper) describing
their system, the used approach, submissions
and results. Peer reviews will be conducted
to improve paper quality and readability and
make sure the authors’ ideas and methods can
be understood by the workshop participants.

We are aiming at accepting all system papers,
and selected ones will be presented orally in
the workshop. All participants are required
to register and attend the workshop to present
their work. All paper submission and reviews
will be managed electronically through https:
//www.softconf.com/acl2018/NEWS/.

4 Language Pairs

The different evaluation tasks within the NEWS
2018 shared task focus on transliteration and/or
back-transliteration of personal and place names
from a source language into a target language as
summarized in Table 1. This year, the shared task
offers 19 evaluation tasks, including 9 translitera-
tion tasks, 6 back-transliteration tasks and 4 hybrid
tasks. NEWS 2018 will release training, devel-
opment and testing data for each of the language
pairs. Within the 19 evaluation tasks, NEWS 2018
includes the 14 tasks that were evaluated in the
previous year editions. In such cases, the training
and development datasets are augmented versions
of the previous year ones. New test dataset will be
used in NEWS 2018 evaluations.

The names given in the training sets for Thai
(T-EnTh & B-ThEn), Persian (T-EnPe & B-PeEn),
Chinese (T-EnCh & B-ChEn), Hebrew (T-EnHe
& B-HeEn), Vietnamese (T-EnVi), Japanese (T-
EnJa) and Korean (T-EnKo) are Western names
and their respective transliterations.

The training sets in the Persian (T-PeEn & B-
EnPe) tasks are names of Persian origin. The train-
ing set in the English to Japanese Kanji (B-JnJk)
task consists only of native Japanese names. The
training set in the Arabic to English (T-ArEn) task
consists only of native Arabic names. Finally, the
training sets for the English to Indian languages
Hindi (M-EnHi), Tamil (M-EnTa), Kannada (M-
EnKa) and Bangla (M-EnBa) tasks consist of a
mix of both Indian and Western names.

48Name origin Source script Target script

Type of Task

Western
Western
Western
Western
Western
Western
Western
Mixed
Mixed
Mixed
Mixed
Western
Western
Western
Japanese
Western
Arabic
Persian
Persian

English
Thai
English
Persian
English
Chinese
English
English
English
English
English
English
Hebrew
English
English
English
Arabic
Persian
English

Transliteration
Back-transliteration
Transliteration
Back-transliteration
Transliteration
Back-transliteration
Transliteration
Mixed trans/back
Mixed trans/back
Mixed trans/back
Mixed trans/back
Transliteration
Back-transliteration

Thai
English
Persian
English
Chinese
English
Vietnamese
Hindi
Tamil
Kannada
Bangla
Hebrew
English
Japanese Katakana Transliteration
Japanese Kanji
Korean Hangul
English
English
Persian

Back-transliteration
Transliteration
Transliteration
Transliteration
Back-transliteration

Dataset Size

Train Dev
1000
30781
1000
27273
1000
13386
15677
1000
1000
41318
1000
32002
500
3256
12937
1000
1000
10957
1000
10955
1000
13623
1000
10501
9447
1000
1000
28828
1000
10514
1000
7387
31354
1000
1000
6000
11204
1000

Task ID

Test
1000 T-EnTh
1000 B-ThEn
1000 T-EnPe
1000 B-PeEn
1000 T-EnCh
1000 B-ChEn
500
T-EnVi
1000 M-EnHi
1000 M-EnTa
1000 M-EnKa
1000 M-EnBa
1000 T-EnHe
1000 B-HeEn
1000 T-EnJa
1000 B-JnJk
1000 T-EnKo
1000 T-ArEn
1000 T-PeEn
1000 B-EnPe

Table 1: Source and target languages for the shared task on transliteration.

5 Standard Datasets
Training Data (Parallel)

Paired names between source and target lan-
guages; size 3K – 41K.
Training data is used for training a basic
transliteration system.

Development Data (Parallel)

Paired names between source and target lan-
guages; size 1K (500 for T-EnVi).
Development data is in addition to the train-
ing data, which is used for ﬁne-tuning the
system parameters, in case of need. Partici-
pants are allowed to use it as part of the train-
ing data for their ﬁnal submissions.

Testing Data

Source names only; size 1K (500 for T-EnVi).
This is a held-out set, which will be used for
evaluating the quality of the transliterations.

Participants will need to obtain licenses from
the respective copyright owners of the different
datasets and/or agree to the terms and conditions
of use that are given on the downloading web-
site (Li et al., 2004; MSRI, 2010; CJKI, 2010).
NEWS 2018 will provide the contact details for
each dataset group.

The data would be provided in Unicode UTF-
8 encoding, in XML format. The results are ex-
pected to be submitted in UTF-8 encoding also in

XML format. The required XML format details
are available in the Appendix A.

Note that name pairs are distributed as-is, as
provided by the respective creators. While the
datasets are mostly manually checked, there may
be still inconsistencies (that is, non-standard us-
age, region-speciﬁc usage, errors, etc.) or incom-
pleteness (that is, not all right variations may be
covered). The participants are allowed to use any
method of their preference to further clean up the
data provided:

• For any participant conducting a manual
clean up, we appeal that such data be pro-
vided back to the organizers for redistribution
to all the participating groups in that language
pair. Such sharing beneﬁts all participants!

• If automatic clean up were used, such clean
up will be considered part of the system im-
plementation, and hence it is not required to
be shared with all participants.

All participants are required to use only the
dataset (parallel names) provided by the shared
task organizers for training their systems. This
“standard” submission procedure will ensure a fair
evaluation in term of score comparison across the
different systems. Those participants wanting to
additionally evaluate “non-standard” runs need to
contact the organizers

496 Evaluation Metrics

As in previous editions of the shared task, the qual-
ity of the submitted results will be evaluated by us-
ing the following 4 metrics. Each individual name
result might include up to 10 output candidates in
a ranked list.

Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the ﬁrst correct transliteration
in the ranked list is accepted as a correct hit.

The following notation is further assumed:
N : Total number of names
(source

ni

ri,j

ci,k

Ki

:

words) in the test set.
: Number of reference transliterations
for i-th name in the test set (ni ≥ 1).
j-th reference transliteration for i-th
name in the test set.
k-th candidate transliteration (system
output) for i-th name in the test set
(1 ≤ k ≤ 10).

:

: Number of candidate transliterations
produced by a transliteration system.

1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate. It measures correct-
ness of the ﬁrst transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.

N(cid:88)

(cid:26) 1 if ∃ ri,j : ri,j = ci,1;

(cid:27)

0 otherwise

i=1

ACC =

1
N

2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.

Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:

LCS(c, r) =

1
2

(|c| + |r| − ED(c, r))

(2)

where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum value, is taken for calculation. If the
best matching reference is given by

ri,m = arg min

j

(ED(ci,1, ri,j))

(3)

then Recall, Precision and F-score for i-th word
are calculated as follows:

Ri =

Pi =

Fi = 2

LCS(ci,1, ri,m)

|ri,m|

LCS(ci,1, ri,m)
|ci,1|
Ri × Pi
Ri + Pi

(4)

(5)

(6)

• The length is computed in distinct Unicode

characters.

• No distinction is made among different char-
acter types of a language (e.g. vowel vs. con-
sonants vs. combining diereses etc.)

3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/M RR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.

(cid:26) minj

(cid:27)

1

j if ∃ri,j, ci,k : ri,j = ci,k;

0 otherwise

M RR =

1
N

N(cid:88)

i=1

RRi

(7)

(8)

4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available.
If all of
the references are produced, then the MAP is 1.
Let’s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by

(cid:32) ni(cid:88)

N(cid:88)

1
ni

i

k=1

(cid:33)

M APref =

1
N

num(i, k)

(9)

RRi =

(1)

507 Paper Format
Paper submissions to NEWS 2018 should follow
the ACL 2018 paper submission policy, includ-
ing paper format, blind review policy and title and
author conventions. Full papers (research papers)
must be in two-column format without exceeding
eight (8) pages of content plus two (2) extra pages
for references and short papers (research and
shared task papers) must also be in two-column
format without exceeding four (4) pages content
plus two (2) extra pages for references. Submis-
sion must conform to the ofﬁcial ACL 2018 style
guidelines. For details, please refer to the ACL
2018 website: http://acl2018.org/call-for-papers/.

8 Contact Us
If you have any questions about the share task and
the datasets, please contact any of the workshop
organizers. Contact information is available at the
NEWS 2018 website http://workshop.colips.org/
news2018/contact.html

References
[CJKI2010] CJK Institute. 2010. http://www.cjk.org/.

[Li et al.2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159–166, Barcelona, Spain.

[MSRI2010] MSRI. 2010. Microsoft Research India.

http://research.microsoft.com/india.

[AILab2018] Artiﬁcial Intelligence Laboratory (AILab)
2018. Ho Chi Minh City University of Science
(VNU-HCMUS). https://www.ailab.hcmus.edu.vn/

[Cao et al.2010] Nam X. Cao, Nhut M. Pham, Quan H.
Vu. 2010. Comparative analysis of transliteration
techniques based on statistical machine translation
and joint-sequence model. In Proc. Symposium on
Information and Comunication Technology, pages
59–63, ACM.

[Ngo et al.2015] Hoang Gia Ngo, Nancy F. Chen,
Nguyen Binh Minh, Bin Ma, Haizhou Li. 2015.
Phonology-Augmented Statistical Transliteration
for Low-Resource Languages. Interspeech, 2015.

51A Appendix: Data Formats

• File Naming Conventions:

NEWS18 Z-XXYY trn.xml
NEWS18 Z-XXYY dev.xml

– Z: Type of task (T: transliteration, B:

back-transliteration, M: mixed)

– XX: Source Language
– YY: Target Language

• File formats:

All data will be made available in XML for-
mats as illustrated in Figure 1.

• Data Encoding Formats:

The data will be in Unicode UTF-8 encod-
ing ﬁles without byte-order mark, and in the
XML format speciﬁed.

B Appendix: Submission of Results

• File Naming Conventions:

Each submission must be saved in a ﬁle
named ”results.xml” and submitted into the
NEWS 2018 CodaLab competition in a ”.zip”
compressed ﬁle. Each ”results.xml” ﬁle can
contain up to 10 output candidates in a ranked
list for each corresponding input entry in the
test ﬁle.

• File formats:

All data will be provided in XML formats as
illustrated in Figure 2.
• Data Encoding Formats:

The results are expected to be submitted in
UTF-8 encoded ﬁles without byte-order mark
only, and in the XML format speciﬁed.

52<?xml version = "1.0" encoding = "UTF-8"?>

<TransliterationCorpus

CorpusFormat = "UTF-8"
CorpusID = "[task_id]"
CorpusSize = "[total_number_of_names_in_file]"
CorpusType = "[Training|Development]"
NameSource = "[name_origin]"
SourceLang = "[source_language]"
TargetLang = "[target_language]">

<Name ID="1">

<SourceName>[source_name_1]</SourceName>
<TargetName ID="1">[target_name_1_1]</TargetName>
<TargetName ID="2">[target_name_1_2]</TargetName>
...
<TargetName ID="n">[target_name_1_n]</TargetName>

</Name>

<Name ID="2">

<SourceName>[source_name_2]</SourceName>
<TargetName ID="1">[target_name_2_1]</TargetName>
<TargetName ID="2">[target_name_2_2]</TargetName>
...
<TargetName ID="k">[target_name_2_k]</TargetName>

</Name>

...
<!-- rest of the names to follow -->
...

</TransliterationCorpus>

Figure 1: Example of training and development data format.

53<?xml version="1.0" encoding="UTF-8"?>

<TransliterationTaskResults

SourceLang = "[source_language]"
TargetLang = "[target_language]"
GroupID = "[your_institution_name]"
RunID = "[your_submission_number]"
RunType = "Standard"
Comments = "[your_comments_here]"
TaskID = "[task_id]">

<Name ID="1">

<SourceName>[test_name_1]</SourceName>
<TargetName ID="1">[your_system_result_1_1]</TargetName>
<TargetName ID="2">[your_system_result_1_2]</TargetName>
...
<TargetName ID="10">[your_system_result_1_10]</TargetName>

</Name>

<Name ID="2">

<SourceName>[test_name_2]</SourceName>
<TargetName ID="1">[your_system_result_2_1]</TargetName>
<TargetName ID="2">[your_system_result_2_2]</TargetName>
...
<TargetName ID="10">[your_system_result_2_10]</TargetName>

</Name>

...
<!-- All names in test corpus to follow -->
...

</TransliterationTaskResults>

Figure 2: Example of submission result format.

54
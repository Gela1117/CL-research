SIRIUS-LTG: An Entity Linking Approach to Fact Extraction and

Veriﬁcation

Farhad Nooralahzadeh and Lilja Øvrelid

Department of Informatics
University of Oslo, Norway

{farhadno,liljao}@ifi.uio.no

Abstract

This article presents the SIRIUS-LTG sys-
tem for the Fact Extraction and VERiﬁcation
(FEVER) Shared Task.
It consists of three
components: 1) Wikipedia Page Retrieval:
First we extract the entities in the claim, then
we ﬁnd potential Wikipedia URI candidates
for each of the entities using a SPARQL query
over DBpedia 2) Sentence selection: We in-
vestigate various techniques i.e. Smooth In-
verse Frequency (SIF), Word Mover’s Dis-
tance (WMD), Soft-Cosine Similarity, Cosine
similarity with unigram Term Frequency In-
verse Document Frequency (TF-IDF) to rank
sentences by their similarity to the claim. 3)
Textual Entailment: We compare three models
for the task of claim classiﬁcation. We apply a
Decomposable Attention (DA) model (Parikh
et al., 2016), a Decomposed Graph Entail-
ment (DGE) model (Khot et al., 2018) and a
Gradient-Boosted Decision Trees (TalosTree)
model (Sean et al., 2017) for this task. The
experiments show that the pipeline with sim-
ple Cosine Similarity using TFIDF in sentence
selection along with DA model as labelling
model achieves the best results on the devel-
opment set (F1 evidence: 32.17, label accu-
racy: 59.61 and FEVER score: 0.3778). Fur-
thermore, it obtains 30.19, 48.87 and 36.55
in terms of F1 evidence, label accuracy and
FEVER score, respectively, on the test set. Our
system ranks 15th among 23 participants in the
shared task prior to any human-evaluation of
the evidence.

1

Introduction

The Web contains vast amounts of data from many
heterogeneous sources, and the harvesting of in-
formation from these sources can be extremely
valuable for several domains and applications such
as, for instance, business intelligence. The vol-
ume and variety of data on the Web are increas-
ing at a very rapid pace, making their use and
processing increasingly difﬁcult. A large volume

of information on the Web consists of unstruc-
tured text which contains facts about named enti-
ties (NE) such as people, places and organizations.
At the same time, the recent evolution of pub-
lishing and connecting data over the Web dubbed
”Linked Data” provides a machine-readable and
enriched representation of many of the world’s en-
tities, together with their semantic characteristics.
These structured data sources are a result of the
creation of large knowledge bases (KB) by dif-
ferent communities, which are often interlinked,
as is the case of DBpedia (Lehmann et al., 2015)
1, Yago 2 (Suchanek et al., 2007) and FreeBase 3
(Bollacker et al., 2008). This characteristic of the
Web of data empowers both humans and computer
agents to discover more concepts by easily navi-
gating among the datasets, and can proﬁtably be
exploited in complex tasks such as information re-
trieval, question answering, knowledge extraction
and reasoning.

Fact extraction from unstructured text is a task
central to knowledge base construction. While
this process is vital for many NLP applications,
misinformation (false information) or disinforma-
tion (deliberately false information) from unreli-
able sources, can provide false output and mislead
the readers. Such risks could be properly managed
by applying NLP techniques aimed at solving the
task of fact veriﬁcation, i.e., to detect and discrim-
inate misinformation and prevent its propagation.
The Fact Extraction and VERiﬁcation (FEVER)
shared task 4 (Thorne et al., 2018) addresses both
problems. In this work, we introduce a pipeline
system for each phase of the FEVER shared task.
In our pipeline, we ﬁrst identify entities in a given
claim, then we extract candidate Wikipedia pages
for each of the entities and the most similar sen-

1 dbpedia.org
2 www.mpi-inf.mpg.de/yago/
3 www.freebase.com
4 www.fever.ai

ProceedingsoftheFirstWorkshoponFactExtractionandVERiﬁcation(FEVER),pages119–123Brussels,Belgium,November1,2018.c(cid:13)2018AssociationforComputationalLinguistics119tences are obtained using a textual similarity mea-
sure. Finally, we label the claim with regard to ev-
idence sentences using a textual entailment tech-
nique.
2 System description
In this section, we describe our system which con-
sists of three components which solve the three
following tasks: Wikipedia page retrieval, sen-
tence selection and textual entailment.

2.1 Wiki-page Retrieval
Each claim in the FEVER dataset contains a single
piece of information about an entity that its orig-
inal Wikipedia page describes. Therefore we ﬁrst
extract entities using the Stanford Named Entity
Recognition (StanfordNER) (Finkel et al., 2005).
We observe that StanfordNER is sometimes un-
able to extract entity names in the claim due to
limited contextual information like in example 1
below:
Example 1 A View to a Kill is an
action movie.
NER:[]
Noun-Phrases:[A View to a Kill, an action
movie]

To tackle this problem, we also extract noun
phrases using the parse tree of Stanford CoreNLP
(Manning et al., 2014) and the longest multi-word
expression that contains words with the ﬁrst letter
in upper case. This enables us to provide a wide
range of potential entities for the retrieval process.
We then retrieve a set of Wikipedia page candi-
dates for an entity in the claim using a SPARQL
(Prud’hommeaux and Seaborne, 2008) query over
DBpedia, i.e. the structured version of Wikipedia.
The SPARQL query aids the retrieval process
by providing a list of candidates to the subsequent
system components, particularly when the claim
is about ﬁlm, song, music album, bands and etc.
Listing 1 shows the query employed for the entity
Meteora in Example 2 below, which outputs the
resulting Wikipedia pages (Pages):
Example 2 Meteora is not a rock
album.
Entity:[Meteora]
Pages:[’Meteora’, ’Meteora (album)’,
’Meteora Monastary’, ’Meteora (Greek
monasteries)’, ’Meteora (film)’]

The query retrieves all the Wikipedia pages

which contains the entity mention in their title.

prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#>
prefix fn:<http://www.w3.org/2005/xpath-functions/#>

SELECT DISTINCT ?resource

WHERE {?resource rdfs:label ?s.
?s <bif:contains> ’"Meteora"’.

FILTER (lang(?s) ="en")
FILTER ( fn:string-length
(fn:substring-after(?resource,
"http://dbpedia.org/resource/"))>1)
FILTER (regex(str(?resource),
"http://dbpedia.org/resource")
&& !regex(str(?resource),
"http://dbpedia.org/resource/File:")
&& !regex(str(?resource),
"http://dbpedia.org/resource/Category:")
&& !regex(str(?resource),
"http://dbpedia.org/resource/Template:")
&& !regex(str(?resource),
"http://dbpedia.org/resource/List")
&& !regex(str(?resource), "(disambiguation)")

)

}

Listing 1: SPARQL query to extract Wikipedia
page candidates for entity mention (e.g. Meteora)

2.2 Sentence Selection
Given a set of Wikipedia page candidates, the sim-
ilarity between the claim and the individual text
lines on the page is obtained. We here experiment
with several methods for computing this similar-
ity:

Cosine Similarity using TFIDF: Sentences
are
ranked by unigram TF-IDF similarity to the claim.
We modiﬁed the fever-baseline code to consider
the candidate list from the Wiki-page retrieval
components.

Soft-Cosine Similarity: Following the work of
Charlet and Damnati (2017), we measure the sim-
ilarity between the candidate sentences and the
claim. This textual similarity measure relies on
the introduction of a relation matrix in the classi-
cal cosine similarity between bag-of-words. The
relation matrix is calculated using the word2vec
representations of words.

Word Mover’s Distance(WMD): The WMD distance
”measures the dissimilarity between two text doc-
uments as the minimum amount of distance that
the embedded words of one document need to
’travel’ to reach the embedded words of another
document” (Kusner et al., 2015). The word2vec
embeddings is used to calculate semantic dis-
tances of words in the embedding space.

Smooth Inverse Frequency (SIF): We also create
sentence embeddings using the SIF weighting
scheme (Arora et al., 2017) for a claim and candi-

120date sentences. Then we calculate the cosine sim-
ilarity measure between these embedding vectors.

2.3 Entailment
The previous component provides the most sim-
ilar sentences as an evidence set for each claim.
In this component, the aim is to ﬁnd out whether
the selected sentences enable us to classify a claim
as being either SUPPORTED, REFUTED or NOT
ENOUGH INFO. In cases where multiple sen-
tences are selected as evidence, their strings are
concatenated prior to classiﬁcation. If the set of
selected sentences is empty for a speciﬁc claim,
due to the failure in ﬁnding related Wiki-page, we
simply assign NOT ENOUGH INFO as an entail-
ment label. In order to solve the entailment task
we experiment with the use of several existing
textual entailment systems with somewhat differ-
ent requirements and properties. We follow the
instruction from the Git-Hub repositories of the
three following models and investigate their per-
formances in the FEVER textual entailment sub-
task:

Decomposable Attention (DA) model (Parikh et al., 2016):
We used the publicly available DA model 5 which
is trained on the FEVER shared task dataset. We
asked the model to predict an inference label for
each claim based on the evidence set which is pro-
vided by the sentence selection component.

Decomposed Graph Entailment (DGE) model: Khot
et al. (2018) propose a decomposed graph entail-
ment model that uses structure from the claim to
calculate entailment probabilities for each node
and edge in the graph structure and aggregates
them for the ﬁnal entailment computation. The
original DGE model 6 uses Open IE (Khot et al.,
2017) tuples as a graph representation for the
claim. However, it is mentioned that the model can
use any graph with labeled edges. Therefore, we
provide a syntactic dependency parse tree using
the Stanford dependency parser (Manning et al.,
2014) which outputs the Enhanced Universal De-
pendencies representation (Schuster and Manning,
2016) as a graph representation for the claim.

Gradient-Boosted Decision Trees model: We also ex-
periment with the TalosTree model 7 (Sean et al.,

5https://github.com/sheffieldnlp/

fever-baselines

6https://github.com/allenai/scitail
7https://github.com/Cisco-Talos/fnc-1

Similarity
Cosine Similarity using TFIDF
Soft-Cosine Similarity
Word Movers Distance (WMD)
Smooth
Frequency
(SIF)

Inverse

Evidence
Precision Recall
67.24
65.53
59.29
50.33

21.14
19.50
18.24
15.19

F1
32.17
30.05
27.90
23.33

FEVER Baseline

-

-

17.18

Table 1: Evidence extraction results on develop-
ment set

2017) which was the winning system in the Fake
News Challenge (Pomerleau and Ra, 2017). The
TalosTree model utilizes text-based features de-
rived from the claim and evidences, which are
then fed into Gradient Boosted Trees to predict
the relation between the claim and the evidences.
The features that are used in the prediction model
are word count, TF-IDF, sentiment and a singular-
value decomposition feature in combination with
word2vec embeddings.

3 Experiments
3.1 Dataset
The shared-task (Thorne et al., 2018) provides an
annotated dataset of 185,445 claims along with
their evidence sets. The shared-task dataset is di-
vided into 145,459 , 19,998 and 19,998 train, de-
velopment and test instances, respectively. The
claims are generated from information extracted
from Wikipedia. The Wikipedia dump (version
June 2017) was processed with Stanford CoreNLP,
and the claims sampled from the introductory sec-
tions of approximately 50,000 popular pages.

3.2 Evaluation
In this section we evaluate our system in the two
main subtasks of the shared task: I) evidence ex-
traction (wiki-page retrieval and sentence selec-
tion) and II) Entailment. Since, the scoring for-
mula in the shared-task considers only the ﬁrst 5
predicted sentence evidences, we choose 5-most
similar sentences in the sentence selection phase
(Section 2.2).
3.2.1 Evidence Extraction
Initially, the impact of different similarity mea-
sures in sentence selection is evaluated. Table 1
shows the results of the various similarity mea-
sures described in section 2 for the evidence ex-
traction subtask on the development set. The re-

121Label
NOT ENOUGH INFO
REFUTES
SUPPORTS
NOT ENOUGH INFO
REFUTES
SUPPORTS
NOT ENOUGH INFO
REFUTES
SUPPORTS

F1

Precision Recall
10.00
60.00
82.00
5.00
30.00
92.00
1.00
42.00
92.00

46.00
61.00
46.00
41.00
62.00
38.00
28.00
66.00
40.00

Model

DA

DGE

TalosTree

FEVER baseline*

F1 Label Accuracy FEVER Score

17.00
60.00
59.00
8.00
41.00
54.00
3.00
51.00
55.00

50.61

37.78

42.24

30.31

44.93

51.37

31.54

31.27

Table 2: Pipeline performance on the dev set with the sentence selection module. (*) In the FEVER
baseline the label accuracy uses the annotated evidence instead of evidence from the evidence extraction
module.

sults suggest that the simple cosine similarity us-
ing TF-IDF is the best performing method for the
sentence selection component when compared to
the other similarity techniques. With an F1-score
of 32.17 it clearly outperforms the Soft-Cosine
Similarity (F1 30.05), WMD (F1 27.90) and SIF
(F1 23.22) measures. This component clearly also
outperforms the FEVER baseline for this subtask
(F1 17.18).

3.2.2 Entailment
This component is trained on pairs of annotated
claims and evidence sets from the FEVER shared-
task training dataset. We here train two different
models i.e. DGE and TalosTree and we utilize
the pre-trained DA model. We evaluate classiﬁ-
cation accuracy on the development set, assum-
ing that the evidence sentences are extracted in the
evidence extraction phase with the best perform-
ing setup. The results are presented in Table 2
and show that the NOT ENOUGH INFO class is
difﬁcult to detect for all three models. Further-
more, the DA model achieves the best accuracy
and FEVER score compared to the others. We also
observe that the label accuracy has a signiﬁcant
impact on the total FEVER score.

3.2.3 Final System
The ﬁnal system pipeline is established with the
SPARQL query and cosine similarity using TFIDF
in the evidence extraction module, and using the
decomposable attention model for the entailment
subtask. Table 3 depicts the ﬁnal submission re-
sults over the test set using our system.

Similarity
Our System
FEVER
Baseline

Evidence

FEVER
P
Score
19.19 70.72 30.19 48.87 36.55

Acc.

F1

R

-

-

18.26 48.84 27.45

Table 3: Final system pipeline results over test set.

4 Conclusion

We present our system for the FEVER shared
task to extract evidence from Wikipedia and ver-
ify each claim w.r.t.
the obtained evidence. We
examine various conﬁgurations for each compo-
nent of the system. The experiments demonstrate
the effectiveness of the TF-IDF cosine similarity
measure and decomposable attention on both the
development and test datasets.

Our future work includes: 1) to implement a
semi-supervised machine learning method for ev-
idence extraction , and 2) to investigate different
neural architectures for the veriﬁcation task.

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
In Proceedings of the 2008
human knowledge.
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA. ACM.

122Sebastian Schuster and Christopher D. Manning. 2016.
Enhanced english universal dependencies: An im-
proved representation for natural language under-
standing tasks. In LREC.

Baird

and

news

Talos

Sean,

targets

Sibley Doug,

Pan Yuxi.
disinformation with
2017.
fake
http:
//blog.talosintelligence.com/2017/
06/talos-fake-news-challenge,
Ac-
cessed: 2018-07-01.

challenge

victory.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, WWW ’07, pages 697–
706, New York, NY, USA. ACM.

James

Thorne,

Andreas Vlachos,

Christos
Christodoulopoulos,
and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
veriﬁcation. In NAACL-HLT.

Delphine Charlet and Geraldine Damnati. 2017. Sim-
bow at semeval-2017 task 3: Soft-cosine semantic
similarity between questions for community ques-
tion answering. In Proceedings of the 11th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2017), pages 315–319. Association for Computa-
tional Linguistics.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005.
Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2017.
Answering complex questions using open informa-
tion extraction. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 311–316.
Association for Computational Linguistics.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. In AAAI.

Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-
ian Q. Weinberger. 2015. From word embeddings to
document distances. In Proceedings of the 32Nd In-
ternational Conference on International Conference
on Machine Learning - Volume 37, ICML’15, pages
957–966. JMLR.org.

Jens Lehmann, Robert

Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, and Christian Bizer. 2015. DB-
pedia - a large-scale, multilingual knowledge base
extracted from wikipedia. Semantic Web Journal,
6(2):167–195.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Ankur Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.
Association for Computational Linguistics.

Dean Pomerleau and Delip Ra. 2017. Fake news chal-
lenge. http://fakenewschallenge.org/,.

Eric Prud’hommeaux and Andy Seaborne. 2008.
SPARQL Query Language for RDF.
W3C
Recommendation. http://www.w3.org/TR/
rdf-sparql-query/.

123
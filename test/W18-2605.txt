DuReader: a Chinese Machine Reading Comprehension Dataset from

Real-world Applications

Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang,

Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang

{hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01,

wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com

Baidu Inc., Beijing, China

Abstract

leaves more opportunity for

This paper introduces DuReader, a new
large-scale, open-domain Chinese ma-
chine
reading comprehension (MRC)
dataset, designed to address real-world
MRC. DuReader has three advantages
(1) data
over previous MRC datasets:
sources: questions and documents are
based on Baidu Search and Baidu Zhi-
dao1; answers are manually generated.
(2) question types:
it provides rich
annotations
for more question types,
especially yes-no and opinion questions,
that
the
research community. (3) scale: it contains
200K questions, 420K answers and 1M
documents;
is the largest Chinese
MRC dataset so far. Experiments show
that human performance is well above
current state-of-the-art baseline systems,
leaving plenty of room for the community
to make improvements.
To help the
community make these improvements,
both DuReader2 and baseline systems3
have been posted online. We also organize
a shared competition to encourage the
exploration of more models. Since the
release of the task, there are signiﬁcant
improvements over the baselines.
Introduction

it

1
The task of machine reading comprehension
(MRC) aims to empower machines to answer
questions after reading articles (Rajpurkar et al.,

1Zhidao (https://zhidao.baidu.com)

the
community-based question answering

is

largest Chinese
(CQA) site in the world.

2http://ai.baidu.com/broad/download?

dataset=dureader

3https://github.com/baidu/DuReader

2016; Nguyen et al., 2016).
In recent years, a
number of datasets have been developed for MRC,
as shown in Table 1. These datasets have led to
advances such as Match-LSTM (Wang and Jiang,
2017), BiDAF (Seo et al., 2016), AoA Reader (Cui
et al., 2017), DCN (Xiong et al., 2017) and R-
Net (Wang et al., 2017). This paper hopes to
advance MRC even further with the release of
DuReader, challenging the community to deal
with more realistic data sources, more types of
questions and more scale, as illustrated in Tables
1-4. Table 1 highlights DuReader’s advantages
over previous datasets in terms of data sources and
scale. Tables 2-4 highlight DuReader’s advantages
in the range of questions.

Ideally, a good dataset should be based on ques-
tions from real applications. However, many ex-
isting datasets have been forced to make vari-
ous compromises such as: (1) cloze task: Data
is synthesized missing a keyword. The task is
to ﬁll in the missing keyword (Hermann et al.,
2015; Cui et al., 2016; Hill et al., 2015).
(2)
multiple-choice exams: Richardson et al. (2013)
collect both ﬁctional stories and the corresponding
multiple-choice questions by crowdsourcing. Lai
et al. (2017) collect the multiple-choice questions
from English exams. (3) crowdsourcing: Turkers
are given documents (e.g., articles from the news
and/or Wikipedia) and are asked to construct ques-
tions after reading the documents(Trischler et al.,
2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017).
The limitations of the datasets lead to build
datasets based on queries that real users submit-
ted to real search engines. MS-MARCO (Nguyen
et al., 2016) is based on Bing logs (in English),
and DuReader (this paper) is based on the logs
of Baidu Search (in Chinese). Besides question
sources, DuReader complements MS-MARCO
and other datasets in the following ways:

question types: DuReader contains a richer in-

ProceedingsoftheWorkshoponMachineReadingforQuestionAnswering,pages37–46Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics37Dataset

CNN/DM (Hermann et al., 2015)

HLF-RC (Cui et al., 2016)

CBT (Hill et al., 2015)
RACE (Lai et al., 2017)

MCTest (Richardson et al., 2013)
NewsQA (Trischler et al., 2017)
SQuAD (Rajpurkar et al., 2016)
SearchQA (Dunn et al., 2017)
TrivaQA (Joshi et al., 2017)

NarrativeQA (Koˇcisk`y et al., 2017)
MS-MARCO (Nguyen et al., 2016)

DuReader (this paper)

Lang
EN
ZH
EN
EN
EN
EN
EN
EN
EN
EN
EN
ZH

#Que.
#Docs
1.4M 300K
28K
100K
108
688K
50K
870K
500
2K
10K
100K
100K
536
6.9M
140K
660K
40K
1.5K
46K
200K1
100K
1M
200k

News

Fairy/News

Source of Docs

Source of Que.
Synthetic cloze
Synthetic cloze
Synthetic cloze Children’s books
English exam
Crowdsourced
Crowdsourced
Crowdsourced

English exam
Fictional stories

CNN
Wiki.

QA site

Web doc.

Trivia websites Wiki./Web doc.
Crowdsourced
Book&movie

User logs
User logs

Web doc.

Web doc./CQA

Answer Type
Fill in entity
Fill in word
Multi. choices
Multi. choices
Multi. choices
Span of words
Span of words
Span of words

Span/substring of words

Manual summary
Manual summary
Manual summary

Table 1: DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu
Search & Baidu Zhidao; answers are manually generated, (2) question types, and (3) scale: 200k questions, 420k answers and 1M
documents (largest Chinese MRC dataset so far). The next three tables address advantage (2).

1 Number of unique documents

ventory of questions than previous datasets. Each
question was manually annotated as either Entity,
Description or YesNo and one of Fact or Opin-
ion.
In particular, it annotates yes-no and opin-
ion questions that take a large proportion in real
user’s questions. Prior work has largely empha-
sized facts, but DuReader are full of opinions as
well as facts. Much of the work on question an-
swering involves span selection, methods that an-
swer questions by returning a single substring ex-
tracted from a single document. Span selection
may work well for factoids (entities), but it is less
appropriate for yes-no questions and opinion ques-
tions (especially when the answer involves a sum-
mary computed over several different documents).
document sources: DuReader collects docu-
ments from the search results of Baidu Search as
well as Baidu Zhidao. All the content in Baidu
Zhidao is generated by users, making it different
from the common web pages. It is interesting to
see if solutions designed for one scenario (search)
transfer easily to another scenario (question an-
swering community). Additionally, previous work
provides only a single paragraph (Rajpurkar et al.,
2016) or a few passages (Nguyen et al., 2016) to
extract or generate answers, while DuReader pro-
vides multiple full documents (that contains a lot
of paragraphs or passages) for each question to
generate answers. This will raise paragraph selec-
tion (i.e. select the paragraphs likely containing
answers) an important challenge as shown in Sec-
tion 4.

data scale: The ﬁrst release of DuReader con-
tains 200K questions, 1M documents and more
than 420K human-summarized answers. To the
best of our knowledge, DuReader is the largest

Chinese MRC dataset so far.

2 Pilot Study

What types of question queries do we ﬁnd in the
logs of a search engine? A pilot study was per-
formed to create a taxonomy of question types.
We started with a relatively small sample of 1000
question queries, selected from a single day of
Baidu Search logs.

The pilot helped us to agree on the following
taxonomy of question types. Each question was
manually annotated as:

• either Fact or Opinion, and
• one of: Entity, Description or YesNo
Regarding to Entity questions, the answers are
expected to be a single entity or a list of entities.
While the answers to Description questions are
usually multi-sentence summaries. The Descrip-
tion questions contain how/why questions, com-
parative questions that comparing two or more ob-
jects, and the questions that inquiring the mer-
its/demerits of goods, etc. As for YesNo questions,
the answers are expected to be an afﬁrmative or
negative answers with supporting evidences. Af-
ter the deep analysis of the sampled questions, we
ﬁnd that whichever the expected answer type is,
a question can be further classiﬁed into Fact or
Opinion, depending on whether it is about asking
a fact or an opinion. Table 2 gives the examples of
the six types of questions.

The pilot study helped us identify a number of
important issues. Table 3 shows that all six types
of question queries are common in the logs of
Baidu Search, while previous work has tended to
focus on fact-entity and fact-description questions.
As shown in Table 3, fact-entity questions account

38Entity

Fact
iphone哪天发布
On which day will iphone be released Top 10 movies of 2017

Opinion
2017最好看的十部电影

Description 消防车为什么是红的
Why are ﬁretrucks red
39.5度算高烧吗
Is 39.5 degree a high fever

YesNo

丰田卡罗拉怎么样
How is Toyota Carola
学围棋能开发智力吗
Does learning to play go improve intelligence

Table 2: Examples of the six types of questions in Chinese (with glosses in English). Previous datasets
have focused on fact-entity and fact-description, though all six types are common in search logs.

Fact Opinion
23.4%

Entity
8.5%
Description 34.6% 17.8%
YesNo
7.5%
Total

Total
31.9%
52.5%
8.2%
15.6%
66.2% 33.8% 100.0%

Table 3: Pilot Study found that all six types
of question queries are common in search logs.
Previous MRC datasets have emphasized span-
selection methods. Such methods are appropriate
for fact-entity and fact-description. Opinions and
yes-no leave big opportunities (about 33.8% and
15.6% of the sample, respectively).

for a relatively small fraction (23.4%) of the sam-
ple. Fact-descriptions account for a larger fraction
of the sample (34.6%). From this Table, we can
see that opinions (33.8%) are common in search
logs. Yes-No questions account for 15.6%, with
one half about fact, another half about opinion.

Previous MRC datasets have emphasized span-
selection methods. Such methods are appropriate
for fact-entity and fact-description, but it is prob-
lematic when the answer involves a summary of
multiple sentences from multiple documents, es-
pecially for Yes-no and opinion questions. This
requires methods that go beyond currently popu-
lar methods such as span selection, and leave large
opportunity for the community.

3 Scaling up from the Pilot to DuReader
3.1 Data Collection and Annotation
3.1.1 Data Collection
After the successful completion of the pilot study,
we began work on scaling up the relatively small
sample of 1k questions to a more ambitious col-
lection of 200k questions.
The DuReader is a sequence of 4-tuples: {q, t,
D, A}, where q is a question, t is a question type, D

is a set of relevant documents, and A is an answer
set produced by human annotators.

Before labeling question types, we need to col-
lect a set of questions q from search logs. Accord-
ing to our estimation, there are about 21% ques-
tion queries in search logs. It would take too much
time, if human annotators manually label each
query in search logs. Hence, we ﬁrst randomly
sample the most frequent queries from search logs,
and use a pre-trained classiﬁer (with recall higher
than 90%) to automatically select question queries
from search logs. Then, workers will annotate the
question queries selected by the classiﬁer. Since
this annotation task is relatively easy, each query
was annotated by one worker. The experts will
further review all the annotations by workers and
correct them if the annotation is wrong. The accu-
racy of workers’ annotation (judged by experts) is
higher than 98%.

Initially, we have 1M frequent queries sam-
pled from search logs. The classiﬁer automati-
cally selected 280K question queries. After human
annotation, there are 210K question queries left.
Eventually, we uniformly sampled 200K questions
from the 210K question queries.

We then collect the relevant documents, D, by
submitting questions to two sources, Baidu Search
and Baidu Zhidao. Note that the two sources are
very different from one another; Zhidao contains
user-generated content and tends to have more
documents relevant to opinions. Since the two
sources are so different from each another, we de-
cided to randomly split the 200k unique questions
into two subsets. The ﬁrst subset was used to pro-
duce the top 5 ranked documents from one source,
and the second subset was used to produce the top
5 ranked documents from the other source.

We also believe that it is important to keep the
entire document unlike previous work which kept
a single paragraph (Rajpurkar et al., 2016) or a few

39Fact Opinion
Entity
14.4% 13.8%
Description 42.8% 21.0%
YesNo
5.1%
Total

Total
28.2%
63.8%
2.9%
8.0%
60.1% 39.9% 100.0%

Table 4: The distribution of question types in
DuReader is similar to (but different from) the Pi-
lot Study (Table 3), largely because of duplicates.
The duplicates were removed from DuReader (but
not from the Pilot Study) to reduce the burden on
the annotators.

passages (Nguyen et al., 2016). In this case, para-
graph selection (i.e. select the paragraphs likely
containing answers) becomes critical to the MRC
systems as we will show in Section 4.

Documents are parsed into a few ﬁelds includ-
ing title and main content. Text has been tokenized
into words using a standard API.4

3.1.2 Question Type Annotation
As mentioned above, annotators labeled each
question in two passes. The ﬁrst pass classiﬁed
questions into one of three types: Entity, Descrip-
tion and YesNo questions. The second pass classi-
ﬁed questions as either Fact or Opinion.

Statistics on these classiﬁcations are reported in
Table 4. Note that these statistics are similar to
those reported for the pilot study (Table 3), but dif-
ferent because duplicates were removed from Ta-
ble 4 (but not from Table 3). We don’t want to bur-
den the annotators with lots of copies of the most
frequent questions, hence we kept unique ques-
tions in DuReader. That said, both tables agree
on a number of important points. As pointed out
above, previous work has tended to focus on fact-
entity and fact-description, while leaves large op-
portunity on yes-no and opinion questions.

3.1.3 Answer Annotation
Crowd-sourcing was used to generate answers.
Turkers were given a question and a set of relevant
documents. He/she was then asked to write down
answers in his/her own words by reading and sum-
marizing the documents.
If no answers can be
found in the relevant documents, the annotator was
asked to give an empty answer. If more than one
answer can be found in the relevant documents,
the annotator was asked to write them all down.

4http://ai.baidu.com/tech/nlp/lexical

In some cases, multiple answers were merged into
a single answer, when it was determined that the
multiple answers were very similar to one another.
Note that the answers to Entity questions and
YesNo questions are more diverse. The answers to
the Entity questions include both the entities and
the sentences containing them. See the ﬁrst ex-
ample in Table 5. The bold words (i.e. green,
gray, yellow, pink) are the entity answers to the
question, and the sentences after the entities are
the sentence containing them. The answers to the
YesNo questions include the opinion types (Yes, No
or Depend) as well as the supporting sentences.
See the last example in Table 5. The bold words
(i.e. Yes and Depend) are the opinion types by
following the supporting sentences. The second
example shows that a simple yes-no question isn’t
so simple. The answer can be almost anything,
including not only Yes and No, but also Depends,
depending on context (supporting sentences).

3.1.4 Quality Control
Quality control is important because of the size
of this project: 51, 408 man-hours distributed over
about 800 workers and 52 experts.

We have an internal crowdsourcing platform
and annotation guidelines to annotate data. When
annotating answers, workers are hired to create the
answers and experts are hired to validate the an-
swer quality. The workers will be hired if they
pass an examine on a small dataset. The accuracy
of workers’ annotation should be higher than 95%
(judged by the experts). Basically, there are three
rounds for answer annotations:
(1) the workers
will give the answers to the questions after read-
ing the relevant documents. (2) the experts will re-
view all answers created by the workers, and they
will correct the answers if they consider that the
answers are wrong. The accuracy (judged by the
experts) of answers by the workers is around 90%.
(3) The dataset is divided into 20 groups according
to the workers and experts who annotate the data.
5% of data will be sampled from each group. The
sampled data in each group will be further checked
again by other experts.
If the accuracy is lower
than 95%, the corresponding workers and the ex-
perts need to revise the answers again. The loop
will end until the overall accuracy reaches 95%.

3.1.5 Training, Development and Test Sets
In order to maximize the reusability of the dataset,
we provide a predeﬁned split of the dataset into

40
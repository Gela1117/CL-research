Comparison of NLP Approaches for Vaccination Behaviour Detection

Shot Or Not:

Aditya Joshi1 Xiang Dai1,2 Sarvnaz Karimi1

Ross Sparks1 C´ecile Paris1 C Raina MacIntyre3

1CSIRO Data61, Sydney, Australia, 2University of Sydney, Sydney, Australia

3The University of New South Wales, Sydney, Australia

{aditya.joshi,dai.dai,sarvnaz.karimi}@csiro.au

{ross.sparks,cecile.paris}@csiro.au, r.macintyre@unsw.edu.au

Abstract

Vaccination behaviour detection deals with
predicting whether or not a person re-
ceived/was about to receive a vaccine. We
present our submission for vaccination be-
haviour detection shared task at the SMM4H
workshop. Our ﬁndings are based on three
prevalent text classiﬁcation approaches: rule-
based, statistical and deep learning-based. Our
ﬁnal submissions are: (1) an ensemble of sta-
tistical classiﬁers with task-speciﬁc features
derived using lexicons, language processing
tools and word embeddings; and, (2) a LSTM
classiﬁer with pre-trained language models.
Introduction

1
Public opinion about vaccines is diverse. Most
people support vaccination, but some of these peo-
ple do not receive vaccination. On the other hand,
people who are vaccinated may also have concerns
regarding the safety or efﬁcacy of vaccines.
In
other words, a person’s stance towards vaccines
(referred to as ‘vaccine hesitancy’) is distinct from
whether or not they received a vaccine shot (re-
ferred to as ‘vaccination behaviour’). While au-
tomatic detection of vaccine hesitancy has been
explored in the past, computational approaches to
detect vaccination behaviour have been limited.
Towards this, our paper deals with vaccination be-
haviour detection (SMM4H shared task #4). Vac-
cination behaviour and vaccine hesitancy can to-
gether help to understand penetration of vaccina-
tion programmes and the trust that communities
place in large-scale vaccination programmes (Holt
et al., 2016).

Vaccination behaviour detection is the task of
predicting whether or not a given piece of text
refers to a person receiving or intending to receive
a vaccine. For example, the tweet ‘I took the vac-
cine this morning, feeling great!’ is positive be-
cause the speaker reports having received the vac-

cine. On the contrary, ‘Vaccines drastically reduce
risks of infection’ is negative because the tweet de-
scribes vaccines but does not report a vaccine be-
ing administered.

Past work in vaccination behaviour detection
uses n-grams as features of a statistical classi-
ﬁer (Skeppstedt et al., 2017; Huang et al., 2017).
However, alternatives to n-grams have shown
promise in several Natural Language Processing
(NLP) tasks. Therefore, we compare three typical
NLP approaches for vaccination behaviour detec-
tion: rule-based, statistical and deep learning tech-
niques. Our submissions to the shared task use
statistical and deep learning-based text classiﬁca-
tion. The systems are trained on a concatenation
of the training and the validation set. The work re-
ported in this paper ranked ﬁrst among nine teams,
as communicated by the shared task committee.
2 Approaches
In this section, we describe the three approaches
that we employ for vaccination behaviour detec-
tion: Statistical, rule-based and deep learning-
based.

2.1 Statistical Approach
Our statistical approach uses an ensemble of three
classiﬁers: logistic regression, support vector ma-
chine with both using LIBLINEAR (Fan et al.,
2008), and random forest using scikit-learn (Pe-
dregosa et al., 2011). We use the following non-
default parameters: (a) Positive misclassiﬁcation
cost is set to 3 in logistic regression; (b) 100 esti-
mators in random forest. Majority voting is used
to combine predictions from the classiﬁers, i.e., a
tweet must be predicted as positive by at least two
classiﬁers for it to be predicted as positive by the
ensemble.

The random forest classiﬁer uses unigrams as
features. The features for logistic regression and

Proceedingsofthe3rdSocialMediaMiningforHealthApplications(SMM4H)Workshop&SharedTask,pages43–47Brussels,Belgium,October31,2018.c(cid:13)2018AssociationforComputationalLinguistics43Description
Unigrams and bigrams in the tweet

Feature
N-grams
Special Characters @ and # which indicate user mentions and hashtags, ! and ?
POS
Negation
Word Similarity

Number of words of each POS tag
Presence of negation words
Maximum value of similarity of words in the tweet and words
indicating administration/reception of a vaccine
Average of word vectors of the words in the tweet
Number of characters and words
Number of words of each emotion category

Sentence Vector
Length
Emotion

Type
Boolean
Boolean
Count
Count
Real

Real
Count
Count

Table 1: Features of the statistical approach.

support vector machine are summarised in Table 1.
These features are:

1. Uni/Bigrams: Boolean;
2. Special Characters: A feature each indicat-

ing four special characters ?, #, @, !

3. POS: Count of each POS tag using NLTK
POS tagger (Bird and Loper, 2004). This fea-
ture follows the intuition that presence of cer-
tain POS tags such as verbs may serve as sig-
nals;

4. Negation: Presence of a negation word. This
is to serve as a negation feature where, al-
though the act of receiving a vaccine is men-
tioned, the negation word changes the output
class;

5. Word Similarity: For each word, we obtain
similarity with ‘receive, ‘get’ and ‘take’, and
use the highest similarity as this feature. We
use pre-trained embeddings from Mikolov
et al. (2013). This is to allow presence of
words related to the act of receiving to be
used as a signal for prediction;

6. Sentence Vector: A sentence vector is com-
puted as an average of word vectors using
GloVe embeddings (Pennington et al., 2014);

7. Length: Number of characters and words;
8. Emotion: Word counts of each emotion cat-
egory as given by SenticNet (Cambria et al.,
2014).

The

combination

classiﬁcation costs
experimentally validated.

of

classiﬁers,

mis-
and features has been

2.2 Rule-based Approach
Since vaccination behaviour detection may appear
to be only about detecting administration of a vac-
cine, we implement a na¨ıve method to detect vac-
cination behaviour. Our rule-based approach looks
for words indicating ‘receive’ (without negation)
to predict vaccination behaviour as follows:

1. If a tweet contains one among the words
‘give’, ‘take’, ‘taking’, ‘gave’, ‘giving’, ‘get’,
‘getting’, ‘took’, ‘receive’ or ‘received’ and
no negation word, predict the tweet as posi-
tive.

2. Else, predict the tweet as negative.

2.3 Deep Learning-based Approach
We experiment with ﬁve typical deep learning-
based models:

1. Sentence Vector: 200 dimensions; Logistic

Regression. (SV)

2. Dense Neural Network: 64 dimensions, 1

inter. layer + 5 epochs (DNN)

3. BiLSTM: GloVe840B + 3 epochs + 50 lstm

units + 0.25 dropout (BiLSTM)

4. CNN: GloVe840B + 5 epochs + 50 ﬁlters + 2

ﬁlter length + 0.75 dropout (CNN)

5. LSTM-LM: We pre-train a language model
on the training dataset with a 3-layer LSTM.
We then build a softmax layer on top of
this pretrained LSTM, and ﬁne-tune the neu-
ral network with supervision (Howard and
Ruder, 2018).

All models are implemented using Tensor-
Flow (Abadi et al., 2016). The parameters are ex-
perimentally determined.

44Approach
Skeppstedt et al. (2017)
Huang et al. (2017)
Statistical
Rule-based
SV
DNN
BiLSTM
CNN
LSTM-LM

F-Score Accuracy
76.84
77.64
80.75
40.48
77.87
78.74
78.30
78.40
80.87

87.01
87.65
88.97
64.91
87.39
87.66
87.30
87.60
88.94

Table 2: 10-fold cross-validation results (%) on the
training dataset.

3 Experimental Setup
The shared task provided three labeled datasets
of tweets for evaluation: a training dataset (5751
tweets of which 1692 are positive), a validation
dataset (1215 tweets of which 306 are positive)
and a test dataset (161 tweets, labels undisclosed).
two past works as base-
lines (Skeppstedt et al., 2017; Huang et al., 2017).
The two baselines use n-grams as features of sta-
tistical classiﬁer.

We re-implement

4 Results
We present our results in six parts. We ﬁrst de-
scribe the performances on the training, validation
and test sets. Then, to understand the components
contributing to the performance, we perform ad-
ditional evaluation: (a) impact of the size of the
training set on the performance; (b) impact of data
source from which language model is trained in
case of the deep learning approach; and (b) impact
of the features on the performance of the statistical
approach. Finally, we present an analysis of errors
made by our system.

4.1 Performance on Training Set
The performance of our methods using 10-fold
cross-validation is shown in Table 2. The perfor-
mance of the re-implementation of baselines are
comparable to the original papers. The low val-
ues in case of the rule-based approach highlight
that vaccination behaviour detection is not a trivial
task of detecting words that indicate administra-
tion of a vaccine. The best F-scores are achieved
by the statistical approach (80.75%) and LSTM-
LM (80.87%). This is an improvement of 3-4%
over the baseline.

F-Score Accuracy
Approach
Statistical
86.06
LSTM-LM 88.74

85.71
89.44

Table 3: Performance (%) on the test dataset.

Statistical LSTM-LM

20%
40%
60%
80%
100%

73.59
75.17
79.26
80.54
81.56

77.69
78.58
78.95
79.52
80.43

Table 4: F-scores (%) of the two best-performing ap-
proaches for varying size of the training set.

4.2 Performance on Validation and Test Sets
The statistical approach achieves an average F-
score of 81.56%, while LSTM-LM achieves
80.43% on the validation set. Similarly, the per-
formance of our methods on the test dataset is in
Table 3. We obtain a F-score of 86.06% with the
statistical approach and 88.74% with the LSTM-
LM on the test set of 161 instances.

Impact of Size of the Training Set

4.3
To analyse the impact of the training set size on
the resultant performance, we show the F-scores
for the two best approaches for varying sizes of the
training set in Table 4. ‘20%’ indicates that 20% of
the training set was used to train the system while
the validation set was used for evaluation. We ob-
serve that when training on a small size of labeled
data, LSTM-LM performs much better than sta-
tistical model. This shows the beneﬁt of transfer
learning that it can utilize knowledge learned from
unlabeled data to train a model with a small num-
ber of labeled instances.

4.4

Impact of Language Model Source in
LSTM-LM

A pre-trained language model represents knowl-
edge learned from source data that is applied to
a classiﬁer. To understand if the domain of this
source data has an impact on the performance of
the resultant classiﬁer, we compare how effective
different domains are for vaccination behaviour
detection. We compare three datasets in Table 6.
The SMM4H dataset is the training dataset for
the task while WikiText-103 (Merity et al., 2016)

45Feature
POS
Special characters
Negation
Word similarity
Sentence vector
Length
Emotion

∆F-score (%)

1.16
0.97
0.66
0.15
0.20
0.39
0.33

Table 5: Degradation in F-scores (%) of the statistical
approach when each of the features is removed.

Figure 2: Sources of errors in false negatives.

Source data
WikiText-103
IMDB
SMM4H

# of tokens

101M
17M
884K

F-score

80.84 (± 0.37)
81.15 (± 0.83)
80.43 (± 0.67)

Table 6: F-scores (%) of the LSTM-LM when language
model is pretrained on different source data.

and IMDB (Maas et al., 2011) are datasets from
wikipedia and a movie review corpus respectively.
The latter are signiﬁcantly larger than the SMM4H
dataset. However, they only result in a marginally
higher performance.

4.5

Impact of Features in the Statistical
Approach

To understand how the features contribute to the
statistical approach, we conduct ablation tests.
The degradation in F-score when each of the fea-
tures is removed is in Table 5. The positive values
in all ﬁelds validate the value of the proposed fea-
tures. The highest degradation is observed in case
of POS-based features.

4.6 Error Analysis
We analyse incorrectly predicted instances from
the validation set. About 50% of errors have ﬁrst
or second person pronouns. Nearly 44% of false

Figure 1: Sources of errors in false positives.

negatives have negative sentiment about ﬂu shots
because of actual or expected, unpleasant side-
effects. The ratio of false negatives to false pos-
itives is 1.40. An analysis of 50 random false pos-
itives and 50 random false negatives are shown in
Figures 1 and 2 respectively. The label ‘Unsure’
indicates that the error could not be assigned to
any of the other categories. Some incorrectly clas-
siﬁed instances for the different error sources are:
• Negative opinion but no claim whether they
would take it, as in the case of ‘Getting a ﬂu
vaccine after reading this article is crazy!’.
• Mentions of taking a ﬂu shot without express-
ing sentiment, such as ‘Flu shots for hubby
and daughter... check.’.

• Took it or about to take it and expressed
favourable opinion about shots, as in the case
of the tweet ‘We’re headed to the @Brigham-
Womens ﬂu shot clinic! Getting vaccinated is
good for you and your community.’.

5 Conclusions

We evaluate three text classiﬁcation approaches
for the task of vaccination behaviour detection.
The rule-based approach considers simple pres-
ence of words, the statistical approach uses an
ensemble of classiﬁers and task-speciﬁc features
while the deep learning approaches employ ﬁve
neural models. On comparing the three ap-
proaches, we observe that an ensemble of statis-
tical classiﬁers using task-speciﬁc features and a
deep learning model using pre-trained language
model and LSTM classiﬁer obtain comparable
performance for vaccination behaviour detection.
Our ﬁndings in the error analysis which show
that vaccine hesitancy often conﬂicts with vacci-
nation behaviour detection, will be helpful for fu-
ture work.

46Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In EMNLP, pages 1532–1543,
representation.
Doha, Qatar.

Maria Skeppstedt, Andreas Kerren, and Manfred Stede.
2017. Automatic detection of stance towards vacci-
nation in online discussion forums. In Proceedings
of the International Workshop on Digital Disease
Detection using Social Media, pages 1–8, Taipei,
Taiwan.

References
Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorﬂow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283, Savannah, GA.

Steven Bird and Edward Loper. 2004. NLTK: The nat-
ural language toolkit. In ACL, page 31, Barcelona,
Spain.

Erik Cambria, Daniel Olsher, and Dheeraj Rajagopal.
2014. SenticNet 3: A common and common-sense
knowledge base for cognition-driven sentiment anal-
ysis. In AAAI, Quebec, Canada.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classiﬁcation. Journal of
machine learning research, 9(Aug):1871–1874.

D Holt, Fredric Bouder, C Elemuwa, G Gaedicke,
A Khamesipour, B Kisler, S Kochhar, R Kutalek,
W Maurer, P Obermeier, et al. 2016. The impor-
tance of the patient voice in vaccination and vaccine
safetyare we listening? Clinical Microbiology and
Infection, 22:146–153.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 328–339.

Xiaolei Huang, Michael C Smith, Michael J Paul,
Dmytro Ryzhkov, Sandra C Quinn, David A Bro-
niatowski, and Mark Dredze. 2017. Examining pat-
terns of inﬂuenza vaccination in social media.
In
AAAI Joint Workshop on Health Intelligence, pages
542–546, San Francisco, CA.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–150.

Stephen Merity, Caiming Xiong, James Bradbury, and
Pointer sentinel mixture

Richard Socher. 2016.
models. CoRR abs/1609.07843.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.

Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in Python. Journal of machine
learning research, 12(Oct):2825–2830.

47
Texar: A Modularized, Versatile, and Extensible Toolbox

for Text Generation

Zhiting Hu*, Zichao Yang, Haoran Shi, Bowen Tan, Tiancheng Zhao,

Junxian He, Xiaodan Liang, Wentao Wang, Xingjiang Yu, Di Wang, Lianhui Qin,

Xuezhe Ma, Hector Liu, Devendra Singh, Wangrong Zhu, Eric P. Xing

zhitingh@cs.cmu.edu*, Carnegie Mellon University, Petuum Inc.

Abstract

We introduce Texar, an open-source
toolkit aiming to support the broad set of
text generation tasks. Different from many
existing toolkits that are specialized for
speciﬁc applications (e.g., neural machine
translation), Texar is designed to be highly
ﬂexible and versatile. This is achieved by
abstracting the common patterns under-
lying the diverse tasks and methodolo-
gies, creating a library of highly reusable
modules and functionalities, and enabling
arbitrary model architectures and vari-
ous algorithmic paradigms. The features
make Texar particularly suitable for tech-
nique sharing and generalization across
different text generation applications. The
toolkit emphasizes heavily on extensibil-
ity and modularized system design, so that
components can be freely plugged in or
swapped out. We conduct extensive exper-
iments and case studies to demonstrate the
use and advantage of the toolkit.

Introduction

1
Text generation spans a broad set of natural lan-
guage processing tasks that aim at generating nat-
ural language from input data or machine rep-
resentations. Such tasks include machine transla-
tion (Bahdanau et al., 2014; Brown et al., 1990),
dialog systems (Williams and Young, 2007; Ser-
ban et al., 2016), text summarization (Hovy and
Lin, 1998; See et al., 2017), article writing (Wise-
man et al., 2017), text paraphrasing and manipula-
tion (Madnani and Dorr, 2010; Hu et al., 2017a),
image captioning (Vinyals et al., 2015b; Karpa-
thy and Fei-Fei, 2015), and more. Recent years
have seen rapid progress of this active area in both
academia and industry, especially with the adop-

tion of modern deep learning approaches in many
of the tasks. On the other hand, considerable re-
search efforts are still needed to improve relevant
techniques and enable real-world practical appli-
cations.

The variety of text generation tasks share many
common properties and goals, e.g., to generate
well-formed, grammatical and readable text, and
to realize in the generation the desired information
inferred from inputs. To this end, a few key mod-
els and algorithms are increasingly widely-used to
empower the different applications, such as neural
encoder-decoders (Sutskever et al., 2014), atten-
tions (Bahdanau et al., 2014; Luong et al., 2015b),
memory networks (Sukhbaatar et al., 2015), adver-
sarial methods (Goodfellow et al., 2014; Hu et al.,
2017b; Lamb et al., 2016), reinforcement learn-
ing (Ranzato et al., 2015; Bahdanau et al., 2016),
as well as some optimization techniques, data pre-
processing and result post-processing procedures,
evaluations, etc.

It is therefore highly desirable to have an open-
source platform that uniﬁes the development of the
diverse yet closely-related applications, backed
with clean and consistent implementations of the
core algorithms. Such a uniﬁed platform enables
reuse of common components and functionali-
ties, standardizes design, implementation, and ex-
perimentation, fosters reproducible research, and
importantly, encourages technique sharing among
different text generation tasks, so that an algorith-
mic advance originally developed for a speciﬁc
task can quickly be evaluated and potentially gen-
eralized to many other tasks.

Though a few remarkable open-source toolkits
have been developed, they have been largely de-
signed for one or few speciﬁc tasks, especially
neural machine translation (Britz et al., 2017;
Klein et al., 2017; Neubig et al., 2018) and dialog
related algorithms (Miller et al., 2017). This pa-

ProceedingsofWorkshopforNLPOpenSourceSoftware,pages13–22Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics13per introduces Texar, a general-purpose text gen-
eration toolkit that aims to support most of the
popular applications in the ﬁeld, by providing re-
searchers and practitioners a uniﬁed and ﬂexible
framework for building their models. Texar is built
upon TensorFlow1, a popular deep learning plat-
form. Texar emphasizes on three key properties,
namely, versatility, modularity, and extensibility.
• Versatility: Texar contains a wide range
of features and functionalities for 1) arbi-
trary model architectures as a combination
of encoders, decoders, discriminators, mem-
ories, and many other modules; and 2) dif-
ferent modeling and learning paradigms such
as sequence-to-sequence, probabilistic mod-
els, adversarial methods, and reinforcement
learning. Based on these, both workhorse and
cutting-edge solutions to the broad spectrum
of text generation tasks are either already in-
cluded or can be easily constructed.

• Modularity: Texar is designed to be highly
modularized, by decoupling solutions to di-
verse tasks into a set of highly reusable mod-
ules. Users can construct their model at a high
conceptual level just like assembling LEGO
bricks. It is convenient to plug in or swap out
modules, conﬁgure rich options of each mod-
ule, or even switch between distinct model-
ing paradigms. For example, switching be-
tween maximum likelihood learning and re-
inforcement learning involves only minimal
code changes. Modularity makes Texar use-
ful for fast prototyping, parameter tuning,
and model experimentation.

• Extensibility: The toolkit provides interfaces
of multiple functionality levels, ranging from
simple Python-like conﬁguration ﬁles to full
library APIs. Users of different needs and ex-
pertise are free to choose different interfaces
for appropriate programmability and internal
accessibility. The library APIs are fully com-
patible with the native TensorFlow interfaces,
which allows a seamless integration of user-
customized modules, and enables the toolkit
to take advantage of the vibrant open-source
community by easily importing any external
components as needed.

1https://www.tensorﬂow.org

Furthermore, Texar puts much emphasis on
well-structured high-quality code of uniform de-
sign patterns and consistent styles, along with
clean documentations and rich tutorial examples.
In the following, we provide details of the
toolkit structure and design. To demonstrate the
use of the toolkit and its advantages, we perform
extensive experiments and cases studies, including
generalizing the state-of-the-art machine transla-
tion model to multiple text generation tasks, in-
vestigating different algorithms for language mod-
eling, and implementing composite neural archi-
tectures beyond conventional encoder-decoder for
text style transfer. All are easily realized with the
versatile toolkit.

Texar is under Apache license 2.0, and will
be released very soon. Please check out http:
//www.cs.cmu.edu/˜zhitingh for the re-
lease progress.

2 Structure and Design

In this section, we ﬁrst provide an overview of the
toolkit on its design principles and overall struc-
tures. We then present the detailed structure of
Texar with running examples to demonstrate the
key properties of the toolkit (sec 2.2-2.4).

Figure 1 shows the stack of main modules and
functionalities in Texar. Building upon the lower
level deep learning platform (TensorFlow), Texar
provides a comprehensive set of building blocks
for model construction, training, evaluation, and
prediction. Texar is designed with the goals of ver-
satility, modularity, and extensibility in mind. In
the following, we ﬁrst present the design princi-
ples that lead to the goals (sec 2.1), and describe
the detailed structure of Texar with running exam-
ples to demonstrate the properties of the toolkit
(sec 2.2-2.4).

2.1 The Design of Texar
The broad variation of the many text generation
tasks and the fast-growing new models and algo-
rithms have posed unique challenges to design-
ing a versatile toolkit. We tackle the challenges
through proper decomposition of the whole exper-
imentation pipeline, extensive sets of modules to
assemble freely, and user interfaces of varying ab-
stract levels.
Pipeline Decomposition We begin with a high-
level decomposition of model construction and
learning pipeline. A deep neural model is typically

14Figure 1: The stack of main modules and functionalities in Texar.

Figure 2: Example various model architectures in recent text generation literatures. E denotes encoder,
D denotes decoder, C denotes classiﬁer (i.e., binary discriminator). (a) The canonical encoder-decoder,
sometimes with attentions A (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Vaswani
et al., 2017), or copy mechanisms (Gu et al., 2016; Vinyals et al., 2015a; Gulcehre et al., 2016); (b)
Variational encoder-decoder (Bowman et al., 2015; Yang et al., 2017); (c) Encoder-decoder augmented
with external memory (Sukhbaatar et al., 2015; Bordes et al., 2016); (d) Adversarial model using a binary
discriminator C, with or without reinforcement learning (Liang et al., 2017; Zhang et al., 2017; Yu et al.,
2017); (e) Multi-task learning with multiple encoders and/or decoders (Luong et al., 2015a; Firat et al.,
2016); (f) Augmenting with cyclic loss (Hu et al., 2017a; Goyal et al., 2017); (g) Learning to align with
adversary, either on samples y or hidden states (Lamb et al., 2016; Lample et al., 2017; Shen et al., 2017).

learned with the following abstract procedure:

maxθ L(fθ, D)

(1)

where (1) fθ is the model that deﬁnes the model
architecture and the intrinsic inference procedure;
(2) D is the data; (3) L is the losses to optimize;
and (4) max denotes the optimization and learn-
ing procedure. Note that the above can have mul-
tiple losses imposed on different parts of compo-
nents and parameters (e.g., generative adversarial
networks (Goodfellow et al., 2014)). Texar is de-
signed to properly decouple the four elements, and
allow free combinations of them through uniform
interfaces. Such design has underlay the strong
modularity of the toolkit.

In particular, the decomposition of model ar-
chitecture and inference (i.e., fθ) from losses

and learning has greatly improved the clean-
ness of the code structure and the opportuni-
ties for reuse. For example, a sequence decoder
can focus solely on performing different decod-
ing (inference) schemes, such as decoding with
ground truths, and greedy, stochastic, or beam-
search decoding, etc. Different
learning algo-
rithms then use different schemes as a subrou-
tine in the learning procedure—for example, max-
imum likelihood learning uses decoding with
ground truths (Mikolov et al., 2010), a policy gra-
dient algorithm can use stochastic decoding (Ran-
zato et al., 2015), and an adversarial learning
can use either the stochastic decoding for policy
gradient-based updates (Yu et al., 2017) or the
Gumbel-softmax reparameterized decoding (Jang
et al., 2016) for direct gradient back-propagation.

15ApplicationsModel templates + Conﬁg ﬁlesEvaluationModelsDataEncoderPredictionTrainingLibrary APIsArchitecturesLossesDecoderClassiﬁerMemory...MLEAdvRewardRegMonoTextParallelTextDialogNumericalTrainerOptimizerRL Agentlr decay/grad clip...EDA𝑥𝑦EDPrior𝑥𝑦𝑧EDM𝑥𝑦CED𝑥𝑦𝑧ED𝑥𝑦C0/1E1E2E3D1D2D3𝑦1𝑦2𝑦3𝑥1𝑥2𝑥3𝑥ED1D2C𝑦1𝑦20/1(a)(b)(d)(c)(e)(f)(g)Figure 3: The catalog of a subset of modules for model construction and learning. Other modules, such as
memory network modules, and those for evaluation and prediction, are omitted due to space limitations.
More new modules are continuously added to the library.

With uniﬁed abstractions, the decoder and the
learning algorithms need not know the implemen-
tation details of each other. This also enables con-
venient switch between different learning algo-
rithms for the same model, by simply changing
the inference scheme and connecting to the new
learning module, without adapting the model ar-
chitecture (see sec 2.3 for the example).

Modules Readily to Assemble The fast evolu-
tion of modeling and learning methodologies in
the research ﬁeld has led to sophisticated models
that go beyond the canonical (attentive) sequence-
to-sequence alike paradigms and introduce many
new composite architectures. Figure 2 summarizes
several model architectures recently used in the lit-
erature for different tasks. To versatilely support
all these diverse approaches, we break down the
complex models and extract a set of frequently-
used modules (e.g., encoders, decoders,
classifiers, etc). Figure 3 shows the catelog
of a subset of modules. Crucially, Texar allows
free concatenation between these modules in or-
der to assemble arbitrary model architectures.
Such concatenation can be done by directly in-
terfacing two modules, or through an interme-
diate connector module that provides gen-
eral, highly-usable functionalities of shape trans-
formation, reparameterization (e.g., (Kingma and
Welling, 2013; Jang et al., 2016)), sampling, and
others.

User Interfaces
It is critical for the toolkit to be
ﬂexible enough to allow construction of the sim-
ple or advanced models, while at the same time
providing proper abstractions to relieve users from
overly concerning about low-level implementa-
tions. To this end, Texar provides two types of user
interfaces with different abstract levels: 1) Python-
style conﬁguration ﬁles that instantiate pre-deﬁned
model templates, and 2) a set of intuitive library
APIs called in Python code. The former is simple,
clean, straightforwardly understandable for non-
expert users, and is widely adopted by other toolk-
its (Britz et al., 2017; Neubig et al., 2018; Klein
et al., 2017), while the latter allows maximal ﬂexi-
bility, full access to internal states, and essentially
unlimited customizability. Examples are provided
in the following section.

2.2 Assemble Arbitrary Model Architectures
Figure 4 shows an example of specifying an atten-
tive sequence-to-sequence model through either
the YAML conﬁguration ﬁle (left panel), or sim-
ple Python code (right panel), respectively.

• The conﬁguration ﬁle passes hyperparame-
ters to the model template which instantiates
the model for subsequent training and eval-
uation (which are also conﬁgured through
YAML). Text highlighted in blue in the ﬁg-
ure speciﬁes the names of modules to use.
Module hyperparameters follow the module

16DecoderBasicRNNDecoderAttentionRNNDecoderTransformerDecoderBahdananAttnLuongAttnMonotonicAttnEncoderUnidirectionalRNNEncoderBidirectionalRNNEncoderTransformerEncoderHierarchicalRNNEncoderConvEncoderClassifier/DiscriminatorRNNClassifierConvClassifierHierarchicalClassifierDataMonoTextPairedText...MultiAlignedDialogLossMLE	LossAdversarial	LossCross-entropyKLDBinary	adversarial	loss…AgentPolicy	Gradient	AgentQ-learning	AgentActor-critic	AgentNumericalConnectorMLPTransformerStochastic...ReparameterizedStochasticConcatForward…OptimizationOptimizerLearning	Rate	Decay…Adam/SGD/…Piecewise/Exp/…Model	architectureModel	lossTrainerDataFigure 4: Two ways of specifying an attentive sequence-to-sequence model. Left: Snippet of an example
YAML conﬁguration ﬁle of the sequence-to-sequence model template. Only those hyperparameters that
the user concerns are speciﬁed explicitly in the particular ﬁle, while the remaining many hyperparameters
can be omitted and will take default values. Right: Python code assembling the sequence-to-sequence
model, using the Texar library APIs. Modules are created as Python objects, and then can be called as
functions to perform the main logic (e.g., decoding) of the module. (Other code such as optimization is
omitted.)

names as children in the conﬁguration hier-
archy. Note that most of the hyperparameters
have sensible default values, and users only
have to specify a small subset of them. Hy-
perparameters taking default values can be
omitted in the conﬁguration ﬁle.

• The library APIs offer high-level function
calls. Users are enabled to efﬁciently build
desired pipelines at a high conceptual level,
without worrying too much about the low-
level implementations. Power users are also
given the option to access the full internal
states for native programming and low-level
manipulations.

2.3 Plug-in and Swap-out Modules
Texar builds a shared abstraction of the broad
set of text generation tasks, and creates highly
reusable modules. It is thus very convenient to
switch between different application contexts, or
change from one modeling paradigm to another,
by simply plugging in/swapping out a single or
few modules, or even merely changing a conﬁgu-
ration parameter, while keeping other parts of the
modeling and training pipeline agnostic.

Figure 5 illustrates an example of switching
between three major learning paradigms of an
RNN decoder, i.e., maximum-likelihood based su-
pervised learning, adversarial learning, and rein-
forcement learning, using the library APIs. Local
modiﬁcation of only few lines of code is enough
to achieve such change. In particular, the same
decoder is called with different decoding modes
(e.g., greed train and greedy infer), and
discriminator or reinforcement learning agent is
added when needed, with simple API calls.

The convenient module replacement can be
valuable for fast exploration of different algo-
rithms for a speciﬁc task, or quick experimentation
of an algorithm’s generalization on different tasks.

2.4 Customize with Extensible Interfaces
With the aim of supporting the rapidly advancing
research area of text generation, Texar emphasizes
heavily on extensibility, and allows easy addition
of customized or external modules through various
interfaces, without editing the Texar codebase.

With the YAML conﬁguration ﬁle, users can di-
rectly insert their own modules by providing the
Python importing path to the module. For exam-
ple, to use a externally implemented RNN cell in

17    1# Read data 2dataset = PairedTextData(data_hparams) 3data = DataIterator(dataset).get_next() 4 5# Encode 6embedder = WordEmbedder(dataset.vocab_size, emb_dim) 7encoder = UnidirectionalRNNEncoder(hparams=cell_hparams) 8enc_outputs, _ = encoder( 9  embedder(data['source_text_ids']), data['source_length']) 10 11# Decode 12decoder = AttentionRNNDecoder( 13  memory=enc_outputs, attn_type='LuongAttention', hparams=cell_hparams) 14outputs, length, _ = decoder( 15  embedder(data['target_text_ids']), data['target_length']-1, mode='greedy_train') 16 17# Loss 18loss = sequence_sparse_softmax_cross_entropy( 19  labels=data['target_text_ids'][:,1:], logits=outputs.logits, seq_length=length) 1source_embedder: WordEmbedder 2  dim: 300 3encoder: UnidirectionalRNNEncoder 4  rnn_cell: 5    type: BasicLSTMCell 6      num_units: 300 7    num_layers: 1 8    dropout: 9      output_dropout: 0.5 10      variational_recurrent: True 11target_embedder: WordEmbedder 12  dim: 300 13decoder: AttentionRNNDecoder 14  rnn_cell: 15    type: BasicLSTMCell 16      num_units: 300 17    num_layers: 1 18  attention: 19    type: LuongAttention 20connector: ZeroConnector  
NICT Self-Training Approach to Neural Machine Translation

at NMT-2018

Kenji Imamura and Eiichiro Sumita

National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
fkenji.imamura,eiichiro.sumitag@nict.go.jp

Abstract

This paper describes the NICT neural ma-
chine translation system submitted at the
NMT-2018 shared task. A characteristic
of our approach is the introduction of self-
training. Since our self-training does not
change the model structure, it does not in-
ﬂuence the efﬁciency of translation, such
as the translation speed.
The experimental
results showed that
the translation quality improved not only
in the sequence-to-sequence (seq-to-seq)
models but also in the transformer models.
Introduction

1
In this study, we introduce the NICT neural trans-
lation system at the Second Workshop on Neural
Machine Translation and Generation (NMT-2018)
(Birch et al., 2018). A characteristic of the sys-
tem is that translation qualities are improved by
introducing self-training, using open-source neu-
ral translation systems and deﬁned training data.

The self-training method discussed herein is
based on the methods proposed by Sennrich et al.
(2016a) and Imamura et al. (2018), and they are
applied to a self training strategy. It extends only
the source side of the training data to increase
variety. The merit of the proposed self-training
strategy is that it does not inﬂuence the efﬁciency
of the translation, such as the translation speed,
because it does not change the model structure.
(However, the training time increases due to an in-
crease in the training data size.)

The proposed approach can be applied to any
translation method. However, we want to con-
ﬁrm on which model our approach is practically
effective. This paper veriﬁes the effect of our self-
training method in the following two translation
models:

(cid:15) Sequence-to-sequence (seq-to-seq) models
(Sutskever et al., 2014; Bahdanau et al.,
2014) based on recurrent neural networks
(RNNs). Herein, we use OpenNMT (Klein
et al., 2017) as an implementation of the seq-
to-seq model.
(cid:15) The transformer model proposed by Vaswani
et al.
(2017). We used Marian NMT
(Junczys-Dowmunt et al., 2018) as an imple-
mentation of the transformer model.

The remainder of this paper is organized as fol-
lows. Section 2 describes the proposed approach.
Section 3 describes the details of our system. Sec-
tion 4 explains the results of experiments, and Sec-
tion 5 concludes the paper.

2 Self-training Approach
2.1 Basic Flow
The self-training approach in this study is based
on a method proposed by Imamura et al. (2018).
Their method extends the method proposed by
Sennrich et al. (2016a) that a target monolingual
corpus is translated back into source sentences and
generates a synthetic parallel corpus. Then, the
forward translation model is trained using the orig-
inal and synthetic parallel corpora. The synthetic
parallel corpus contains multiple source sentences
of a target sentence to enhance the encoder and at-
tention. The diversity of the synthetic source sen-
tences is important in this study. Imamura et al.
(2018) conﬁrmed that the translation quality im-
proved when synthetic source sentences were gen-
erated by sampling, rather than when they were
generated by n-best translation.

Although Imamura et al. (2018) assumed the us-
age of monolingual corpora, it can be modiﬁed to
a self-training form by assuming the target side of
parallel corpora as monolingual corpora. In fact,

Proceedingsofthe2ndWorkshoponNeuralMachineTranslationandGeneration,pages110–115Melbourne,Australia,July20,2018.c(cid:13)2018AssociationforComputationalLinguistics110To control the diversity of generated sentences,
the synthetic source generation in this paper intro-
duces an inverse temperature parameter 1=(cid:28) into
the softmax function.

∑
yt (cid:24) Pr(yjy<t; x)1=(cid:28)
y′ Pr(y′jy<t; x)1=(cid:28)

(2)

Figure 1: Flow of Self-training

they proposed such self-training strategy and con-
ﬁrmed the effect on their own corpus.

Figure 1 shows the ﬂow of self-training. The

procedure is summarized as follows.

1. First, train the back-translator that translates
the target language into the source using orig-
inal parallel corpus.

2. Extract the target side of the original paral-
lel corpus, and translate it into the source
language (synthetic source sentences) using
the above back-translator. During back-
translation, it not only generates one sentence
but also generates multiple source sentences
per target sentence using a sampling method.

3. Construct the synthetic parallel corpus mak-
ing pairs of the synthetic source sentences
and their original target sentences. If we de-
ﬁne the number of synthetic source sentences
per target sentence as N,
the size of the
synthetic parallel corpus becomes N-times
larger than the original parallel corpus.

4. Train the forward translator, which translates
the source to the target, using a mixture of the
original and synthetic parallel corpora.

In this study, we modify the method proposed
by Imamura et al. (2018) to improve the efﬁciency
of the training while maintaining the diversity of
the source sentences.

2.2 Diversity Control
Imamura et al. (2018) generates synthetic source
sentences by sampling. The sampling is based on
the posterior probability of an output word as fol-
lows.

yt (cid:24) Pr(yjy<t; x);

(1)
where yt, y<t, and x denote the output word se-
quence at time t, history of the output words, and
input word sequence, respectively.

If we set the inverse temperature parameter 1=(cid:28)
greater than 1.0, high probability words become
preferable, and if we set it to inﬁnity, the sampling
becomes identical to the argmax operation. On the
contrary, if we set it less than 1.0, diverse words
will be selected, and the distribution becomes uni-
form if we set it zero.

2.3 Dynamic Generation
A problem in the research proposed by Imamura
et al. (2018) is that the training time increases
(N + 1)-times with an increase in the training data
size. To alleviate this problem, we introduce dy-
namic generation that uses different synthetic par-
allel sets for each epoch (Kudo, 2018). Speciﬁ-
cally, a synthetic parallel sentence set, which con-
tains one synthetic source sentence per target sen-
tence, is used for an epoch of the training. By
changing the synthetic parallel sentence set for
each epoch, we expect a similar effect to using
multiple source sentences in the training.

For implementation, we do not embed the dy-
namic generation in the training program but per-
form it ofﬂine. Multiple synthetic source sen-
tences were generated in advance, whose number
N is 20 this time, and N synthetic parallel sets are
constructed. During training, a synthetic set is se-
lected for each epoch using round-robin schedul-
ing, and learns the model using the synthetic set
and the original corpus. We can use the same
learning rates because the sizes of the original and
synthetic sets are the same. Using the dynamic
generation, the size of the training data is restricted
to double of the original parallel corpus. 1

The training procedure is summarized as fol-

lows.

1. First, train the back-translator using the orig-

inal parallel corpus.

2. Translate the target side of the original cor-
pus into N synthetic source sentences per tar-
get sentence using the above back-translator.

1Although the size is restricted double, the training time

takes more than double because of late convergence.

111Original Parallel CorpusSynthetic Source SentencesSynthetic Parallel Corpus(Forward) TranslatorSource →TargetBack-TranslatorTarget →SourceTranslationTest SentenceTrainingTrainingTarget SentencesNote that the sampling method described in
Section 2.2 is used for the generation.

3. Make N sets of the synthetic parallel sen-
tences by pairing the synthetic source sen-
tences generated in Step 2 and the target side
of the original parallel corpus.

4. Train the forward translator. In each epoch,
select one synthetic parallel set, and train the
model using the mixture of the synthetic and
original parallel sets.

3 Applied Systems

In this paper, we apply the proposed self-training
approach to two translator types; the seq-to-seq
model (Sutskever et al., 2014; Bahdanau et al.,
2014) implemented by OpenNMT (LUA version)
(Klein et al., 2017) and the transformer model
(Vaswani et al., 2017) implemented by Marian
NMT (Junczys-Dowmunt et al., 2018). Table 1
summerizes the system description.

3.1 Back-Translator
The back-translator used herein is OpenNMT,
which employs an RNN-based seq-to-seq model.
The training corpus for the back-translation is
preprocessed using the byte-pair encoding (BPE)
(Sennrich et al., 2016b). For each language, 16K
subword types were independently computed. The
model was optimized using the stochastic gradient
descent (SGD) whose learning rate was 1.0.

For the back-translation, we modiﬁed Open-
NMT to generate synthetic source sentences by
sampling. This time, we generated three types
of synthetic source sentences changing the inverse
temperature parameter 1=(cid:28) to 1.0, 1.2, and 1.5.

3.2 Forward Translator 1: Transformer

Model

The ﬁrst forward translator is Marian NMT, which
is based on the transformer model. We used this
system for the submission. The settings were al-
most identical to the base model of Vaswani et al.
(2017). 2 The vocabulary sets were equal to those
of the back-translator for the original parallel cor-
pus. For the synthetic source sentences, we di-
rectly used subword sequences output from the
back-translator.

2We

referred

the

described
https://github.com/marian-nmt/marian-

settings

at
examples/tree/master/transformer.

Marian NMT performs the length normalization

using the following equation.

∑
t log Pr(ytjy<t; x)

llnorm(yjx) =

;

T W P

(3)
where llnorm(yjx), W P , and T denote the log-
likelihood normalized by the output length, word
penalty, and number of output words, respectively.
If we set the word penalty greater than 0.0, long
hypotheses are preferred. The setting of the word
penalty will be further discussed in Section 4.1.

3.3 Forward Translator 2: Seq-to-Seq Model
The other forward translator used herein is Open-
NMT based on the seq-to-seq model. The settings
were almost the same as the back-translator. SGD
was used for the optimization, but the learning rate
was set to 0.5 because all target sentences appear
twice in an epoch.

∑

At the translation, we translated the source sen-
tence into 10-best, and the best hypothesis was se-
lected using the length reranking based on the fol-
lowing equation (Oda et al., 2017).
llbias(yjx) =

log Pr(ytjy<t; x) + W P (cid:1) T;
(4)
where llbias(yjx) denotes the log-likelihood bi-
ased by output length. Although this formula dif-
fers from Equation 3, there is an equivalent ef-
fect that long hypotheses are preferred if the word
penalty W P is set to a positive value.

t

4 Experiments

In this section, we describe our results for the
NMT-2018 shared task in English-German trans-
lation. Note that the shared task uses the WMT-
2014 data set preprocessed by Stanford NLP
Group.

In our experiments, we add two baselines. One
is the model trained from the original parallel cor-
pus only. Another is the model trained using the
synthetic corpus, which contains 1-best generation
and did not use the dynamic generation, with the
original corpus. For the inverse temperature pa-
rameter 1=(cid:28), we tested 1.0, 1.2, and 1.5. This is
because the translation quality was better when the
diversity was slightly inhibited in our preliminary
experiments. Note that the submitted system was
Marian NMT (the transformer model) trained us-
ing 1=(cid:28) = 1:0 synthetic corpus.

112Preprocessing
Model

Word Embedding
Encoder
Decoder

Training

Learning Rate

Dropout
Maximum Length
Mini-batch Size

Translation
Program Arguments
(for Training)

OpenNMT
BPE 16K (independent)
Seq-to-Seq
500 units
2-layer Bi-LSTM (500 + 500 units)
2-layer LSTM (1,000 units)
SGD (14 + 6 epochs)
Back-translator: 1.0
Forward Translator: 0.5
ddrop = 0:3
80
64
Beam Width: 10
-brnn -brnn_merge concat
-rnn_size 1000
-end_epoch 20
-start_decay_at 14
-param_init 0.06
-learning_rate 0.5

Marian NMT
BPE 16K (independent)
Transformer
512 units
6-layer (dmodel = 512, df f = 2048)
6-layer (dmodel = 512, df f = 2048)
Adam (early stopping by cross-entropy)
0.0003

ddrop = 0:1
100
64
Beam Width: 6
--type transformer
--max-length 100
--mini-batch-fit --maxi-batch 1000
--early-stopping 10
--valid-freq 5000
--valid-metrics cross-entropy

perplexity translation

--beam-size 6
--enc-depth 6 --dec-depth 6
--transformer-heads 8
--transformer-postprocess-emb d
--transformer-postprocess dan
--transformer-dropout 0.1
--label-smoothing 0.1
--learn-rate 0.0003 --lr-warmup 16000
--lr-decay-inv-sqrt 16000 --lr-report
--optimizer-params 0.9 0.98 1e-09
--clip-norm 5
--sync-sgd --seed 1111
--exponential-smoothing

Table 1: Summary of our Systems

4.1 Word Penalty / Length Ratio

precision. 3

The BLEU score signiﬁcantly changes due to the
translation length (Morishita et al., 2017). For
instance, Figure 2 shows BLEU scores of our
submitted system (a) when the word penalty was
changed from 0.0 to 2.0 and (b) on various length
ratios (LRs), which indicate the ratios of the num-
ber of words of the system outputs to the reference
translations (sys=ref).

As shown in Figure 2 (a), the BLEU scores
change over 0.5 when we change the word
penalty. The penalties of the peaks are differ-
ent among the development/test sets. The BLEU
score peaks were at W P = 1:2, 0:2, and 0:5
in the newstest2013, newstest2014, and
newstest2015 sets, respectively. Therefore,
the BLEU scores signiﬁcantly depend on the word
penalty. However, as shown in Figure 2 (b), we
can see that the peaks of the BLEU scores were at
LR = 1:0 in all development/test sets. This set-
ting supports no brevity penalty and high n-gram

These results reveal that the length ratio should
be constant for fair comparison when we compare
different systems because they generate transla-
tions of different lengths. Therefore, we compare
different models and settings by tuning the word
penalty to maintain the stable length ratio on the
development set (newstest2013). In this ex-
periment, we show results of the two length ratios
based on the “original parallel corpus only” of the
transformer model. Note that the submitted sys-
tem employs the ﬁrst setting.
1. LR ≃ 0:988, which is the length ratio when
2. LR ≃ 0:973, which is the length ratio when

W P = 1:0.

W P = 0:5.

4.2 Results
Tables 2 and 3 show the results of Marian NMT
(the transformer model) and OpenNMT (the seq-
3If we tune the word penalty to make the BLEU score
the length ratios of

maximum on the newstest2013,
newstest2014 and 2015 become greater than 1.0.

113(a) Word Penalty vs. BLEU

(b) Length Ratio vs. BLEU

Figure 2: Word Penalty, Length Ratio and BLEU Scores (Marian NMT; 1=(cid:28) = 1:0)

to-seq model), respectively. These tables consist
of three information groups. The ﬁrst group shows
training results; the number of training epochs and
perplexity of the development set. The second
and third groups show the BLEU scores when the
length ratio in the development set become 0.988
and 0.973, respectively. The results of Marian
NMT were better than those of OpenNMT in all
cases. The following discussion mainly focuses
on the results of Marian NMT (Table 2), but there
was similarity in Table 3.

First, in comparison with the “original parallel
corpus only” and the “one-best without dynamic
generation,” the perplexity of the one-best case in-
creased from 4.37 to 4.43. Along with increas-
ing the perplexity, the BLEU scores of the test
sets (newstest2014 and newstest2015)
degraded to 26.19 and 28.49 when LR ≃ 0:988.
This result indicates that the self-training, which
simply uses one-best translation result, is not ef-
fective.

On the contrary, using our self-training method,
the perplexities were decreased and the BLEU
scores improved signiﬁcantly regardless of the in-
verse temperature parameters in most cases. 4 For
example, when 1=(cid:28) = 1:0, the perplexity were
decreased to 4.20, and the BLEU scores improved
to 27.59 and 30.19 on the newstest2014 and
2015, respectively, when LR ≃ 0:988. When
LR ≃ 0:973, the BLEU scores further improved,
but the improvements come from the length ratio.
The same tendency was observed in OpenNMT.
We can conclude that the proposed self-training
method is effective for the transformer and seq-

4The signiﬁcance test was performed using the multeval
tool (Clark et al., 2011) at a signiﬁcance level of 5% (p <
0:05). https://github.com/jhclark/multeval

to-seq models.

The effectiveness of the inverse temperature pa-
rameter is still unclear because the BLEU scores
were depend on the parameters.

5 Conclusions

The self-training method in this paper improves
the accuracy without changing the model struc-
ture. The experimental results show that the pro-
posed method is effective for both the transformer
and seq-to-seq models. Although our self-training
method increases training time by more than dou-
ble, we believe that it is effective for the tasks that
emphasize on translation speed because it does not
change the translation efﬁciency.

In this paper, only restricted settings were
tested. We require further experiments such as
another back-translation methodology and settings
of the inverse temperature parameters.

Acknowledgments

This work was supported by “Promotion of Global
Communications Plan — Research and Develop-
ment and Social Demonstration of Multilingual
Speech Translation Technology,” a program of the
Ministry of Internal Affairs and Communications,
Japan.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Neural machine translation by
CoRR,

Bengio. 2014.
jointly learning to align and translate.
abs/1409.0473.

Alexandra Birch, Andrew Finch, Minh-Thang Luong,
Graham Neubig, and Yusuke Oda. 2018. Findings

1142627282930310.00.51.01.52.0BLEUWord Penaltynewstest2013newstest2014newstest20152627282930310.970.980.991.001.011.021.03BLEULength Rationewstest2013newstest2014newstest2015Training

#Epoch PPL # 2013
2014
26.95
26.03
Original Parallel Corpus Only
49
26.17
One-best w/o Dynamic Generation 61
26.43 (-)
26.84 (+) 27.59 (+) 30.19 (+) 26.65 (+) 27.92 (+)
83
27.01 (+)
112
27.70 (+)
26.75 (+) 27.74 (+)
98

1=(cid:28) = 1:0
Self-Training 1=(cid:28) = 1:2
1=(cid:28) = 1:5

29.80
26.67 (+)
30.17 (+) 26.51 (+)

BLEU " (Dev. LR ≃ 0:988)
2015
29.35
28.49 (-)

BLEU " (Dev. LR ≃ 0:973)
2015
2013
29.67
25.77
28.91 (-)
25.94
30.65 (+)

27.92 (+) 30.00
28.04 (+) 30.27 (+)

2014
26.81
26.19 (-)

4.37
4.43
4.20
4.21
4.25

The bold values denote the results of the submitted system. (+) and (-) symbols denote results that are
signiﬁcantly improved and degraded from the “original parallel corpus only,” respectively.

Table 2: Results of Marian NMT (Transformer Model)

Training

#Epoch PPL # 2013
2014
22.69
23.45
20
Original Parallel Corpus Only
22.86 (-) 22.36
One-best w/o Dynamic Generation 20
23.56
20
23.59
20
20
23.58

1=(cid:28) = 1:0
Self-Training 1=(cid:28) = 1:2
1=(cid:28) = 1:5

5.58
5.75
5.34
5.46
5.41

23.15 (+)
22.95
23.13 (+)

BLEU " (Dev. LR ≃ 0:973)
2013
23.23

2015
25.96
N/A (No Word Penalty)
26.33
25.96
26.50 (+)

23.49 (+)
23.15
23.45 (+)

2015
25.70
24.35 (-)
23.38
26.03
25.71
23.38
26.32 (+) 23.55

2014
22.94

BLEU " (Dev. LR ≃ 0:988)

(+) and (-) symbols denote results that are signiﬁcantly improved and degraded from the “original parallel
corpus only,” respectively.

Table 3: Results of OpenNMT (Seq-to-Seq Model)

of the second workshop on neural machine transla-
tion and gen eration. In Proceedings of the Second
Workshop on Neural Machine Translation and Gen-
eration (NMT-2018), Melbourne, Australia.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176–181, Portland, Oregon, USA.

Kenji Imamura, Atsushi Fujita, and Eiichiro Sumita.
2018. Enhancement of encoder and attention us-
ing target monolingual corpora in neural machine
translation. In Proceedings of the 2nd Workshop on
Neural Machine Translation and Generation (NMT-
2018), Melbourne, Australia.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, Andr´e F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. arXiv preprint
arXiv:1804.00344.

Makoto Morishita, Jun Suzuki, and Masaaki Nagata.
2017. NTT neural machine translation systems at
WAT 2017. In Proceedings of the 4th Workshop on
Asian Translation (WAT2017), pages 89–94, Taipei,
Taiwan.

Yusuke Oda, Katsuhito Sudoh, Satoshi Nakamura,
Masao Utiyama, and Eiichiro Sumita. 2017. A
simple and strong baseline: NAIST-NICT neural
machine translation system for WAT2017 English-
Japanese translation task. In Proceedings of the 4th
Workshop on Asian Translation (WAT2017), pages
135–139, Taipei, Taiwan.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2016, Volume 1: Long Pa-
pers), pages 86–96, Berlin, Germany.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proceedings of ACL 2017, System Demon-
strations, pages 67–72, Vancouver, Canada.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of Advances in Neural Infor-
mation Processing Systems 27 (NIPS 2014), pages
3104–3112.

Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (ACL-2018), Melbourne, Australia.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

115
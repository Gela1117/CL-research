PICO Element Detection in Medical Text via Long Short-Term Memory

Neural Networks

Di Jin
MIT

jindi15@mit.edu

Peter Szolovits

MIT

psz@mit.edu

Abstract

literature databases.

Successful
evidence-based medicine
(EBM) applications rely on answering
clinical questions by analyzing large
In order
medical
to formulate a well-deﬁned,
focused
clinical question, a framework called
PICO is widely used, which identiﬁes
the sentences in a given medical
text
that belong to the four components:
Participants/Problem (P), Intervention (I),
Comparison (C) and Outcome (O). In
this work, we present a Long Short-Term
Memory (LSTM) neural network based
model to automatically detect PICO ele-
ments. By jointly classifying subsequent
sentences in the given text, we achieve
state-of-the-art results on PICO element
classiﬁcation compared to several strong
baseline models. We also make our
curated data public as a benchmarking
dataset so that the community can beneﬁt
from it.

Introduction

1
The paradigm of evidence-based medicine (EBM)
involves the incorporation of current best evi-
dence, such as the reports of randomized con-
trolled trials (RCTs), into decision making for pa-
tient care (Sackett, 1997). Such evidence, in-
tegrated with the physician’s own expertise and
patient-speciﬁc factors, can lead to better patient
outcomes and higher quality health care (Sackett
et al., 1996). In practice, successful EBM appli-
cations rely on answering clinical questions via
analysis of large medical literature databases such
as PubMed. And most often, a PICO framework
is used to formulate a well-deﬁned, focused clini-
cal question, which decomposes the question into

four parts: Participants/Problem (P), Intervention
(I), Comparison (C) and Outcome (O) (Richardson
et al., 1995).

Typically the analyses that underlie EBM begin
by selecting a set of potentially relevant papers,
which are then further reﬁned by human judgment
to form the evidence base on which the answer to
a speciﬁc question depends. To facilitate this se-
lection process, it would be advantageous that all
papers (or at least their abstracts) can be organized
according to the PICO foci. Unfortunately, a sig-
niﬁcant portion of the medical literature contains
either unstructured or sub-optimally structured ab-
stracts, without speciﬁcally identiﬁed PICO ele-
ments. Therefore, we would like to introduce a
method to automate the identiﬁcation of PICO el-
ements in medical abstracts in order to make pos-
sible the automated selection of possibly relevant
articles for a proposed study.

In this paper, we present a system based on arti-
ﬁcial neural networks (ANN) to tackle the issue of
extracting PICO elements in medical abstracts as
a classiﬁcation task at the sentence level. Our key
contributions are as follows:

1. Previous methods for PICO elements ex-
traction focused on shallow models such
as Naive Bayes (NB), Support Vector Ma-
chines (SVM) and Conditional Random
Fields (CRF), which are limited in modeling
capacity. To signiﬁcantly boost the perfor-
mance, we propose a Long Short-Term Mem-
ory (LSTM) based ANN model to solve this
task.

2. Most previous systems detected the PICO el-
ements one by one; thus several classiﬁers
needed to be built and trained separately,
which is sub-optimal in efﬁciency. That ap-
proach also cannot take advantage of shared
structure among the individual classiﬁers. In

ProceedingsoftheBioNLP2018workshop,pages67–75Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics67this work we extract PICO components si-
multaneously from any given medical ab-
stract.

3. In all previous works, the only dataset used
for training and test and made public is from
(Kim et al., 2011). However, this dataset
contains only 1000 abstracts, which is not
enough for a ANN based deep learning model
to obtain good generalization results. There-
fore, we curate a dataset comprising of over
tens of thousands of abstracts and make it
public as a benchmark dataset so that every-
one else can use it.

4. Instead of normally treating PICO detection
as a single sentence classiﬁcation problem,
we view it as a sequential sentence classiﬁ-
cation task, where the sequence of sentences
in an abstract is jointly predicted.
In this
way, the information from the context sen-
tences can be used to help predict the current
sentence, which does improve the classiﬁca-
tion accuracy considerably. Leveraging this
strategy, we obtain state-of-the-art PICO ele-
ments extraction accuracy, signiﬁcantly out-
performing all previous methods.

2 Related Work

In many previous user studies, the generalized
use of the PICO framework or similar schema by
clinicians has been validated for its performance
improvement on searching literature for clinical
questions (Schardt et al., 2007; Boudin et al.,
2010c; Znaidi et al., 2015). This has greatly fueled
academic interest in the development of systems
for automatic PICO element detection. Over the
last decade, the research progress for this task can
be summarized according to three aspects: mod-
els for classiﬁcation, dataset generation, and task
formulation.

Many well-known machine learning techniques
have been proposed to build stronger models for
this task,
including Nave Bayes (NB) (Huang
et al., 2013; Boudin et al., 2010a; Demner-
Fushman and Lin, 2007), Random Forest (RF)
(Boudin et al., 2010a), Support Vector Machine
(SVM) (Boudin et al., 2010a; Hansen et al., 2008),
Conditional Random Field (CRF) (Kim et al.,
2011; Chung, 2009; Chung and Coiera, 2007)
and Multi-Layer Perceptron (MLP) (Boudin et al.,
2010a; Huang et al., 2011). Also Boudin et al. in

(Boudin et al., 2010b) proposed a location-based
weighting strategy as an extension to the language
modeling approach inspired by the special distri-
bution pattern of PICO elements in medical ab-
stracts. All these models heavily rely on care-
ful selections of hand-engineered features includ-
ing lexical features such as bag of words (BOW),
stemmed words and cue-words/verbs, and seman-
tic features such as synonyms and hypernyms pro-
vided by some ontologies (e.g., WordNet). As
an important complement to this task, most re-
cent work from Dernoncourt et al. (Dernoncourt
et al., 2016) proposed the model based on cur-
rently emerging deep ANN architectures such as
LSTM for further performance boosting, as well
as to remove the need for hand-crafted features.
However, this work has not targeted to address the
issue of PICO element detection.

To generate the datasets for both training and
test, earlier works mainly relied on manual an-
notation, which resulted in small corpora on the
order of hundreds of abstracts (Demner-Fushman
and Lin, 2007; Dawes et al., 2007; Chung, 2009;
Kim et al., 2011). Afterwards, later works made
use of the structural information embedded in
some abstracts for which the authors have clearly
stated distinctive sentence headings (Boudin et al.,
2010a; Huang et al., 2011, 2013). Speciﬁcally,
some abstracts contain explicit headings such
as “PATIENTS”, “SAMPLE” or “OUTCOMES”,
which can be used to locate sentences correspond-
ing to PICO elements. In this way, tens of thou-
sands of abstracts that contain PICO elements
from PubMed can be automatically compiled as
a well-annotated dataset, which can increase the
size of dataset by two orders of magnitude.

In terms of task formulation, most previous
works focused on categorizing one PICO class at
a time using an individual classiﬁer (Boudin et al.,
2010a; Huang et al., 2013). Therefore, in order to
detect all four PICO components, one would need
to build and train four individual models, which is
inefﬁcient. Furthermore, it is hard to disambiguate
the classiﬁcation label conﬂicts between different
model predictions on the same sentence. These
limitations were resolved by working directly on
the labels of interest for EBM, allowing multi-
label classiﬁcation instead of binary and allowing
sentences that are unrelated to labels of interest
to be labeled as an ”Other” category (Kim et al.,
2011; Demner-Fushman and Lin, 2007). This is a

68more realistic setting and ought to provide better
insight into the performance we should expect for
this kind of task.

3 The Proposed Model

First we introduce our notation. We denote scalars
in italic lowercase (e.g., k), vectors in bold lower-
case (e.g., s) and matrices in italic uppercase (e.g.,
W ). Colon notations xi:j and si:j are used to de-
note the sequence of scalars (xi, xi+1, ..., xj) and
vectors (si, si+1, ..., sj).

Our model is composed of three components:
the token embedding layer, the sentence-level la-
bel inference layer, and the label sequence opti-
mization layer (Figure 1).
In the following sec-
tions they will be discussed in detail.

its corresponding vector representation. Token
representations are encoded by the column vec-
tor in the embedding matrix W word ∈ Rdw×|V |,
where dw is the dimension of the word vector and
V is the vocabulary of the dataset. Each column
∈ Rdw is the word embedding vector for
W word
the ith word in the vocabulary. To transform a cer-
tain word w into its corresponding embedding vec-
tor ew, we use the following equation:

i

ew = W wordvw,

(1)
where vw is the one hot vector of word w with
dimension of |V | that has 1 at the corresponding
index and zero in all other positions. The word
embeddings W word can be pre-trained on large
unlabeled datasets using unsupervised algorithms
such as word2vec (Mikolov et al., 2013), GloVe
(Pennington et al., 2014) and fasttext (Bojanowski
et al., 2016).

3.2 Sentence-level Label Inference Layer
This layer takes as input the embedding vector e
of each token in a sentence from the token embed-
ding layer and produces a vector r ∈ Rl to rep-
resent the probability that this sentence belongs to
each label, where l is the number of labels. To
this aim, the sequence of embedding vectors e is
ﬁrst input into a bi-directional LSTM (bi-LSTM),
which outputs a sequence of hidden states h1:N
(h ∈ Rdh) for a sentence of N words with each
hidden state corresponding to a token. To form the
ﬁnal representation vector s of this sentence, atten-
tive pooling is used, which can be described using
the following equations (Yang et al., 2016):

Figure 1: Model architecture. w: original token;
e:
token embedding; h: bi-LSTM hidden state;
s: sentence representation vector; r: sentence la-
bel probability vector; y: predicted sentence la-
bel. Replacing bi-LSTM with convolutional neu-
ral network (CNN) did not improve the results: we
therefore used bi-LSTM.

3.1 Token Embedding Layer
This layer takes as input a given sentence w com-
prising N words w = [w1, w2, ...wN ] and outputs

ui = tanh(Wshi + bs),

αi =

(cid:80)
exp(u(cid:62)
i us)
(cid:88)
j exp(u(cid:62)
αihi,

s =

,

j us)

(2)

(3)

(4)

i

where us ∈ Rds is the token level context vec-
tor used to measure the relevance or importance of
each token with respect to the whole sentence, and
Ws ∈ Rds×dh is the transformation matrix for soft
alignment.

The obtained vector s is subsequently input to a
feed-forward neural network with only one hidden
layer, which outputs the corresponding probability
vector r.

693.3 Label Sequence Optimization Layer
Each medical abstract consists of several sen-
tences with the sentence category following some
patterns, such as that the category “Results” is al-
ways followed by “Conclusion”. Such patterns
can yield better classiﬁcation performance via the
conditional random ﬁeld (CRF) algorithm. Given
the sequence of probability vectors r1:n from the
last label inference layer for an abstract of n sen-
tences, this layer outputs a sequence of labels y1:n,
where yi represents the predicted label assigned to
the ith sentence.

In order to model dependencies between subse-
quent labels, we incorporate a matrix T that con-
tains the transition probabilities between two sub-
sequent labels; we deﬁne T [i, j] as the probability
that a token with label i is followed by a token with
the label j. The score of a label sequence y1:n is
deﬁned as the sum of the probabilities of individ-
ual labels and the transition probabilities:

n(cid:88)

n(cid:88)

i=1

i=2

s(y1:n) =

ri(yi) +

T [yi−1, yi].

(5)

The score in the above equation can be trans-
formed into the probability of a certain label se-
quence by taking a softmax operation over all pos-
sible label sequences:

(cid:80)

p(y1:n) =

es(y1:n)

ˆy1:n∈Y es(ˆy1:n)

,

(6)

where Y denotes the set of all possible label se-
quences. During the training phase, the objec-
tive is to maximize the probability of the gold la-
bel sequence. While in the testing phase, given
an input sequence, the corresponding sequence of
predicted labels is chosen as the one that maxi-
mizes the score using the Viterbi algorithm (For-
ney, 1973).

4 Experiments
4.1 Dataset Preparation
The dataset used in this study1 is curated from
MEDLINE, which is a free access database
on medical articles. Speciﬁcally, we extracted
489,026 abstracts from PubMed by stating the fol-
lowing search limits: 1. Text Availability: Ab-
stract; 2. Languages: English; 3. Publication

1https://github.com/jind11/PubMed-PICO-Detection

Types: Randomized Controlled Trial (Search con-
ducted on 2017/08/28). Among them, abstracts
with structured section headings were selected for
automatic annotation of sentence category. Al-
though P, I and O headings were our detection
targets, we also annotated the other types of sen-
tences into one of the AIM (A), METHOD (M),
RESULTS (R) and CONCLUSION (C) labels to
facilitate the use of our CRF label sequence op-
timization method. Note that, although we have 7
labels in total, we only care about the detection ac-
curacy of the P, I and O labels and thus mainly dis-
cuss their performance in the following sections.
In this study, the C component was incorpo-
rated into the I category since the “COMPARI-
SON” section also refers to a kind of intervention
in an RCT. And in fact, there are very few abstracts
with comparison labels found in PubMed.

We annotated a certain section heading into one
of the 7 labels based on whether it contains the key
words that belong to the assigned label as shown in
Table 1 (section headings are only used to gener-
ate gold labels and not used for model training and
inference). In very rare cases, the section heading
of a certain sentence may contain the key words of
more than one category, in which case that sen-
tence will be assigned into multi-labels accord-
ing to Table 1. Table 2 presents a typical abstract
example with section headings annotated into the
7 labels. A total of 24,668 abstracts contain at
least one of the P/I/O labels. There are 21,198
abstracts with P-labels, 13,712 with I-labels and
20,473 with O-labels (Table 3). Note that, the ab-
stracts in PubMed follow a diversity of rhetorical
structure and only a small fraction of them contain
PICO elements based on their section headings.

4.2 Training Settings

Ten-fold cross-validation was employed to assess
the results statistically, where abstracts were ran-
domly split into 10 equal partitions. Nine of them
were used for training and the remaining one for
testing. This step repeats for ten rounds. For
each round of training, 10% of the training set
was randomly extracted as the development set for
early stopping, that is, the test set was evaluated at
the highest development set performance, which
is measured by the average F1 score of all three
P/I/O labels.

The token embeddings were pre-trained on a
large corpus combining Wikipedia, PubMed and

70Category
Aim (A)

Heading Name Key Words
Objective, Background, Purpose, Importance, Introduction, Aim, Rationale, Goal,
Context, Hypothesis

Participants (P) Population, Participant, Sample, Subject, Patient
Intervention (I)
Outcome (O)
Method (M)
Results (R)
Conclusion (C) Conclusion, Implication, Discussion, Interpretation

Intervention
Outcome, Measure, Variable, Assessment
Method, Setting, Design, Material, Procedure, Process, Methodology
Result, Finding

Table 1: Key words of section headings in structured abstracts for automatic annotation.

Heading Name

Cate.

AIMS

DESIGN

SETTING

PARTICIPANTS

INTERVENTION

A

M

M

P

I

MEASUREMENTS O

FINDINGS

CONCLUSIONS

R

C

Sentences
[...] The aims of the trial were to test for differences between
standard 1-and 0.5-mg doses (both twice daily during 8weeks) in
(1) abstinence, (2) adherence and (3) side effects.
Open-label randomized parallel-group controlled trial with 1-year
follow-up. [...]
Stop-Smoking Clinic of the Virgen Macarena University Hospital in
Seville, Spain.
The study comprised smokers (n=484), 59.5% of whom were men
with a mean age of 50.67years and a smoking history of 37.5
pack-years.
Participants were randomized to 1mg (n=245) versus 0.5mg
(n=239) and received behavioural support, which consisted of a
baseline visit and six follow-ups during 1year.
The primary outcome was continuous self-reported abstinence
during 1year, with biochemical veriﬁcation. [...] Also measured
were baseline demographics, medical history and smoking
characteristics.
Abstinence rates at 1year were 46.5% with 1mg versus 46.4% with
0.5mg [odds ratio (OR)=0.997; 95% conﬁdence interval (CI) =
0.7-1.43; P=1.0]; [...]
There appears to be no difference in smoking cessation
effectiveness between 1mg and 0.5mg varenicline, [...].

Table 2: A typical abstract example with section headings and their corresponding annotated labels. The
PMID of this abstract is 28449281.

Category Abstracts Sentences
27,695
P
24,602
I
O
32,525

21,198
13,712
20,473

Table 3: Number of times each of the categories P,
I and O appear in abstracts and in sentences in the
data.

PMC texts (Moen and Ananiadou, 2013) using the
word2vec tool2. They are ﬁxed during the training

phase to avoid over-ﬁtting3.

The model is trained using the Adam optimiza-
tion method (Kingma and Ba, 2014). For regu-
larization, dropout is applied to each layer and l2
regularization is also used. Hyperparameters were
optimized via grid search and the best conﬁgura-
tion is shown in Table 4. Code for this work is
available online4.

2https://code.google.com/archive/p/word2vec/

3http://bio.nlplab.org/
4https://github.com/jind11/LSTM-PICO-Detection

71Para. Para. Name
dw
dh
ds
bz
lr
β

Token Embed. Size
LSTM Hidden Size
Attention Vector Size
Batch Size
Learning Rate
l2 Regularization Ratio

Value
200
150
300
40
0.001
0.0001

Table 4: Hyperparameters. Batch size refers to the
number of abstracts in one batch.

5 Results and Discussion
Table 5 and 6 detail the results of classiﬁcation for
each label in terms of performance scores (pre-
cision, recall and F1) and confusion matrix, re-
spectively (for one fold).
It can be seen that
the classiﬁer is very good at predicting the la-
bels of AIM, RESULTS and CONCLUSION but
has difﬁculty in distinguishing among the labels of
PARTICIPANTS, INTERVENTION, OUTCOME
and METHOD. Indeed, the PARTICIPANTS, IN-
TERVENTION and OUTCOME sections can be
deemed as more speciﬁc aspects of the METHOD
descriptions, therefore, it is naturally more dif-
ﬁcult to tell the P/I/O elements apart from the
METHOD section. Since our main goal is to accu-
rately extract the P/I/O components from a given
abstract, we will only discuss their performance in
the following.

Cate.
A
P
I
O
M
R
C
Total

p (%)
97.7
88.5
74.9
84.5
87.0
93.3
93.8
90.1

r (%) F1 (%)
97.8
98.0
85.6
82.8
78.1
81.5
83.2
83.8
85.6
84.2
94.8
96.4
91.1
92.5
90.0
90.0

Support

3811
2722
2331
3219
5623
9236
4312
31254

Table 5: Results in terms of precision (p), recall (r)
and F-measure (F1) on the test set for each class
obtained by our model for one of the ten folds.

Table 7 compares our model against several pre-
viously widely-used baseline models. Since there
is no benchmarking dataset, we cannot compare
with published best models (this is one of the rea-
sons why we want to publish this dataset).

The ﬁrst baseline is the logistic regression (LR)
model that uses the n-gram features extracted from
the current sentence for classiﬁcation.
In this

P

P
2213
M 181
C
0
A
4
R
9
O
15
I
40

M
197
4804

6
43
21
277
278

C
5
9

3904

3
175
11
0

A
29
40
8

3743

0
20
0

R
84
30
393
6

8952
136
28

O
49
242
1
11
65
2688
142

I
145
317
0
1
14
72
1843

Table 6: Confusion matrix obtained by our model
for one of the ten folds. Rows correspond to pre-
dicted labels, and columns correspond to true la-
bels.

scenario, each sentence is predicted individually
without context information from the surrounding
sentences considered. Likewise, the second base-
line MLP ﬁrst computes the vector representation
for each sentence by taking the max pooling oper-
ation of the embeddings of all tokens in the sen-
tence, then classiﬁes the current sentence via a
neural network with three hidden layers (hidden
layer dimensions are 400, 400 and 200, respec-
tively). On the other hand, the third baseline is
a CRF model that also uses n-grams as features
(only the ﬁrst 100 tokens were used for each sen-
tence since most sentences are shorter than 100
tokens) and outputs the most probable label se-
quence for the whole abstract. Therefore, the CRF
baseline takes into account both preceding and
succeeding sentences when classifying the current
sentence.

As presented by Table 7, the LR baseline per-
forms worst, which is quite reasonable consider-
ing that it is still a very shallow model and only
uses the local sentence information. As a com-
parison, the MLP model also only considers the
features from the current sentence but performs
better than LR because its modeling capacity is
much larger. By incorporating the surrounding
sentences, the CRF baseline performs even bet-
ter than MLP system, which veriﬁes that context
information is quite useful in sequential classiﬁca-
tion problems.

Lastly but most

importantly, our proposed
model performs much better than all the baselines
for all three P/I/O labels. The advantages of our
model and the reasons for its improved perfor-
mance are summarized below:

No human-engineered features Our model
does not rely on any hand-engineered features that
require much domain experience and are quite dif-

72Models

LR
MLP
CRF

Our Model

r

P-element (%)
p
F1
67.7
66.9
75.8
77.8
82.2
79.8
85.5
87.8

68.5
74.1
77.5
83.4

r

I-element (%)
p
F1
55.3
55.6
64.9
64.3
67.8
69.0
76.7
72.7

55.0
65.9
70.3
81.3

r

O-element (%)
p
F1
66.2
65.4
75.8
73.8
76.0
76.2
83.1
81.1

67.0
77.9
76.3
85.3

Table 7: Performance in terms of precision (p), recall (r) and F-measure (F1) on the test set with several
baselines and our proposed model (average value based on 10 fold cross validation). Since the dataset
used here was introduced in this work, there is no previously published method for reference.

ﬁcult to craft.

No n-gram features Unlike many other systems
that rely heavily on n-grams, our model simply
uses the token embedding vector to represent each
token and feeds it into the recurrent neural network
(RNN) model for inference. In this way, the pre-
trained embeddings on large corpora can encode
the syntactic and semantic information of words
for better language understanding. This can also
help combat word scarcity problem. For exam-
ple, the alternatively spelled tokens “tendonitis”
and “tendinitis” are two different unigrams, how-
ever, their semantic meanings are the same, and
this similarity can be revealed by their correspond-
ing closely parallel embedding vectors.

Joint prediction Instead of predicting each sen-
tence one by one, our model classiﬁes all sen-
tences in one abstract jointly, which improves the
overall classiﬁcation performance by implying the
constraints of coherency between subsequent pre-
dicted labels. This improvement is clearly evi-
denced by Table 8.

Sequence modeling An RNN model is good at
modeling sequences such as sentences by consid-
ering the dependency between tokens, which can-
not be accounted for by context-free models such
as those using bag of words features. And the
long-term memory characteristic of LSTM model
further grants the RNN model the ability to cope
with long sentences.

Figure 2 presents an example of the transition
matrix after the model has been trained, which en-
codes the transition probability between two sub-
sequent labels. It effectively reﬂects what label is
the most likely one that should follow the current
one. For example, a sentence pertaining to the RE-
SULTS is typically followed by a sentence pertain-
ing to the CONCLUSION (1.16), which makes
sense. From this transition matrix, we can ﬁgure

Model

Full Model
-sequence optimization

P
85.5
78.2

F1 (%)

I

76.7
68.2

O
83.1
78.3

Table 8: Ablation analysis. 10 fold cross valida-
tion F1-scores are reported. “-sequence optimiza-
tion” is our model without the label sequence op-
timization layer.

out the most probable label sequence: A → M →
P → I → O → R → C, which is also consistent
with our observations.

Table 9 presents a few examples of prediction
errors that are related to P/I/O labels. This er-
ror analysis suggests that part of the model er-
ror comes from the ambiguity between some label
pairs, such as O and M, O and R, and I and M. For
example, the sentence “Plasma volume and total
body haemoglobin were determined at rest.” can
be deemed as a METHOD description in a gen-
eral sense, however, it can also be further speciﬁed
as an OUTCOME. On the other hand, a fair num-
ber of sentence labels are indeed debatable. For
instance, the sentence “Iron supplementation was
given to one group as a substitution remedy, an-
other group was given iron and folic acid and the
third group was without supplementation during
the collection period.” belongs to the PARTICI-
PANT label according to the gold standard, but it
makes more sense that it should be classiﬁed as an
INTERVENTION.

6 Conclusion

In this work we have presented an LSTM based
ANN architecture to detect the PICO elements in
medical RCT abstracts. We demonstrated that the
use of a more advanced LSTM model and jointly
predicting the classes of all sentences in a given
text can improve the overall classiﬁcation perfor-

73Sentence
The study included 16 patients who were randomized into one of three
6-month treatment protocols.
Referral service doing n-of-1 trials at the requests of community and academic
physicians.
Iron supplementation was given to one group as a substitution remedy, another
group was given iron and folic acid and the third group was without
supplementation during the collection period.
Plasma urea and creatinine concentrations and angiotensin converting enzyme
activity were measured at the start of the study and the end of each treatment
period.
Heart rate was recorded continuously throughout the maneuvre, while blood
was sampled for catecholamine determinations prior to the start of straining
and again approximately 10 s following the end of straining.
Plasma volume and total body haemoglobin were determined at rest.

Predicted Gold

P

I

I

O

O

O

M

M

P

R

I

M

Table 9: Examples of prediction errors of our model that are related to P/I/O labels. The “Predicted”
column indicates the label predicted by our model for a given sentence. The “Gold” column indicates
the gold label of the sentence.

References
Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
arXiv preprint
tors with subword information.
arXiv:1607.04606.

Florian Boudin, Jian-Yun Nie, Joan C Bartlett, Roland
Grad, Pierre Pluye, and Martin Dawes. 2010a. Com-
bining classiﬁers for robust pico element detec-
tion. BMC medical informatics and decision mak-
ing, 10(1):29.

Florian Boudin, Jian-Yun Nie, and Martin Dawes.
2010b. Clinical information retrieval using docu-
ment and pico structure. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 822–830. Association for
Computational Linguistics.

Florian Boudin, Lixin Shi, and Jian-Yun Nie. 2010c.
Improving medical information retrieval with pico
element detection. In European Conference on In-
formation Retrieval, pages 50–61. Springer.

Grace Y Chung. 2009. Sentence retrieval for abstracts
of randomized controlled trials. BMC medical infor-
matics and decision making, 9(1):10.

Grace Y Chung and Enrico Coiera. 2007. A study of
structured clinical abstracts and the semantic classi-
ﬁcation of sentences. In Proceedings of the Work-
shop on BioNLP 2007: Biological, Translational,
and Clinical Language Processing, pages 121–128.
Association for Computational Linguistics.

Martin Dawes, Pierre Pluye, Laura Shea, Roland
and Jian-Yun Nie.
The identiﬁcation of clinically impor-
journal abstracts:

Grad, Arlene Greenberg,
2007.
tant elements within medical

Figure 2: Transition matrix of label sequence. The
rows represent the label of the previous sentence,
while the columns represent the label of the cur-
rent sentence.

mance of PICO components. And by publishing
our curated dataset for benchmarking, we hope to
encourage competition by other approaches than
ours and that more effective and efﬁcient methods
can be developed in the future.

Acknowledgments

This work was supported by funding grant U54-
HG007963 from National Human Genome Re-
search Institute (NHGRI). Thank Matthew Mc-
Dermott for helping revise the manuscript.

74Patient population problem, exposure intervention,
comparison, outcome, duration and results (pecodr).
Journal of
Innovation in Health Informatics,
15(1):9–16.

W Scott Richardson, Mark C Wilson, Jim Nishikawa,
and Robert SA Hayward. 1995. The well-built clin-
ical question: a key to evidence-based decisions.
ACP journal club, 123(3):A12–A12.

David L Sackett. 1997. Evidence-based Medicine How
to practice and teach EBM. WB Saunders Com-
pany.

David L Sackett, William MC Rosenberg, JA Muir
Gray, R Brian Haynes, and W Scott Richardson.
1996. Evidence based medicine: what it is and what
it isn’t.

Connie Schardt, Martha B Adams, Thomas Owens,
Sheri Keitz, and Paul Fontelo. 2007. Utilization of
the pico framework to improve searching pubmed
for clinical questions. BMC medical informatics and
decision making, 7(1):16.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classiﬁcation.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Eya Znaidi, Lynda Tamine, and Chiraz Latiri. 2015.
a semantic
Answering pico clinical questions:
In Conference on Artiﬁcial
graph-based approach.
Intelligence in Medicine in Europe, pages 232–237.
Springer.

Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering clinical questions with knowledge-based
and statistical techniques. Computational Linguis-
tics, 33(1):63–103.

Franck Dernoncourt,

Ji Young Lee,

and Peter
Szolovits. 2016. Neural networks for joint sen-
tence classiﬁcation in medical paper abstracts. arXiv
preprint arXiv:1612.05251.

G David Forney. 1973. The viterbi algorithm. Pro-

ceedings of the IEEE, 61(3):268–278.

Marie J Hansen, Nana Ø Rasmussen, and Grace Chung.
2008. A method of extracting the number of trial
participants from abstracts describing randomized
controlled trials. Journal of Telemedicine and Tele-
care, 14(7):354–358.

Ke-Chun Huang, I-Jen Chiang, Furen Xiao, Chun-Chih
Liao, Charles Chih-Ho Liu, and Jau-Min Wong.
2013. Pico element detection in medical text with-
out metadata: Are ﬁrst sentences enough? Journal
of biomedical informatics, 46(5):940–946.

Ke-Chun Huang, Charles Chih-Ho Liu, Shung-Shiang
Yang, Furen Xiao, Jau-Min Wong, Chun-Chih Liao,
and I-Jen Chiang. 2011. Classiﬁcation of pico ele-
ments by text features systematically extracted from
In Granular Computing (GrC),
pubmed abstracts.
2011 IEEE International Conference on, pages 279–
283. IEEE.

Su Nam Kim, David Martinez, Lawrence Cavedon, and
Lars Yencken. 2011. Automatic classiﬁcation of
sentences to support evidence based medicine.
In
BMC bioinformatics, volume 12, page S5. BioMed
Central.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

SPFGH Moen and Tapio Salakoski2 Sophia Anani-
adou. 2013. Distributional semantics resources for
In Proceedings of the
biomedical text processing.
5th International Symposium on Languages in Biol-
ogy and Medicine, Tokyo, Japan, pages 39–43.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

75
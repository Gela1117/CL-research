Domain Adaptation for Disease Phrase Matching

with Adversarial Networks

Miaofeng Liu(cid:2)∗, Jialong Han♠, Haisong Zhang♠, and Yan Song♠

(cid:2)University of Science and Technology of China

♠Tencent AI Lab

{water3er,jialonghan}@gmail.com,{hansonzhang,clksong}@tencent.com

Abstract

With the development of medical informa-
tion management, numerous medical data
are being classiﬁed, indexed, and searched
in various systems. Disease phrase match-
ing, i.e., deciding whether two given dis-
ease phrases interpret each other, is a ba-
sic but crucial preprocessing step for the
above tasks. Being capable of relieving the
scarceness of annotations, domain adapta-
tion is generally considered useful in med-
ical systems. However, efforts on applying
it to phrase matching remain limited. This
paper presents a domain-adaptive match-
ing network for disease phrases. Our net-
work achieves domain adaptation by ad-
versarial training, i.e., preferring features
indicating whether the two phrases match,
rather than which domain they come from.
Experiments suggest that our model has
the best performance among the very few
non-adaptive or adaptive methods that can
beneﬁt from out-of-domain annotations.

1

Introduction

In recent years, hospitals depend more on infor-
mation systems to store and retrieve medical data
for diagnosis and treatment. To facilitate reliable
and efﬁcient processing of medical data, disease
phrase matching has been identiﬁed as a crucial
task in those medical systems. Given two disease
phrases, this task requires identifying whether they
are able to interpret each other.

Owing to complicated medical terminologies,
overlapping words or similar syntactic structures
are not reliable cues for disease phrase matching.
Table 1 shows two matching candidates for “La-
tent syphilis, speciﬁed as early or late” (Phrase

∗
Work was done during the internship at Tencent AI Lab.

Phrase 1
Latent syphilis, spec-
iﬁed as early or late
Latent syphilis, spec-
iﬁed as early or late

Phrase 2
Syphilis latent

Late syphilis,
speciﬁed

Label
Yes

No

Table 1: Examples of disease phrase matching.

1). In the ﬁrst one, the absent participial modiﬁer
and the different word order do not prevent the two
phrases from matching. The second one is, how-
ever, a false match, though it shares more words
and similar syntactic structures with Phrase 1.

Given the variability of human languages, su-
pervised phrase or sentence matching is widely ap-
plied in information identiﬁcation (Madnani et al.,
2012; Yin et al., 2016), textual entailment (Marelli
et al., 2014), web search (Li et al., 2014), entity
linking (Traylor et al., 2017), and disease infer-
ence (Nie et al., 2015). As deep learning drew at-
tentions on various tasks (Lecun et al., 2015), ded-
icated neural matching models are also designed
in two types of structures. 1) Siamese-based net-
works (Neculoiu et al., 2016; Mueller and Thya-
garajan, 2016): the input phrases are ﬁrst encoded
by the same network; the encoded vectors are then
used to compute similarities by metrics like Co-
sine. 2) Matching-aggregating networks: ﬁne-
grained units of the two phrases are represented
and matched in word-by-word (Rockt¨aschel et al.,
2015), one-direction (Wang and Jiang, 2016),
or bilateral-multi-perspective (Wang et al., 2017)
manners to produce matching features; the fea-
tures are aggregated into a vector, based on which
the matching label is predicted.

Despite encouraging results in other areas, neu-
ral matching models still face speciﬁc challenges
on medical data. Different medical subﬁelds like
physiology and urology may adopt diverse termi-
nologies. Due to their professional nature, it is
hard to obtain human annotations at scale for a
single subﬁeld. This causes systems on a partic-

ProceedingsoftheBioNLP2018workshop,pages137–141Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics137ular target subﬁeld or domain to have too few an-
notations to learn a complicated neural model. It
may be tempting to involve annotations from one
or more source domains for more training data.
But since all above models assume in-domain an-
notations, the effect of source-domain annotations
remains uncertain on the trained models.

Inspired by domain-adversarial

This paper takes a perspective that is orthogo-
nal to works on designing sophisticated matching
networks. We employ domain adaptation in dis-
ease phrase matching to effectively exploit source
annotations. Based on Bilateral Multi-Perspective
Matching (BiMPM) (Wang et al., 2017), we pro-
pose a Domain-Adaptive BiMPM (DA-BiMPM)
model.
train-
ing (Ganin et al., 2016) on text classiﬁcation (Liu
et al., 2017), relation extraction (Fu et al., 2017),
and paraphrase identiﬁcation (Yu et al., 2018), we
introduce a domain discriminator in addition to
the matching predictor in BiMPM. With such a
discriminator, DA-BiMPM is encouraged to learn
features predictive of the matching labels, while
being least discriminative of which domain the
data comes from.
it is expected
that the learned models distill domain-insensitive
knowledge from source annotations. On two med-
ical datasets from different subﬁelds, we set up
non-adaptive baselines fed with or without source-
domain annotations, as well as an adaptive one.
Experimental results show that, when trivially in-
volving source-domain data, only the strongest
baseline BiMPM can achieve a slight gain. Com-
pared with the adaptive approach, DA-BiMPM is
capable of making more improvement on BiMPM.

In doing so,

2 Preliminaries
Before going into details of DA-BiMPM, we start
with introducing the BiMPM model (Wang et al.,
2017), which is illustrated by components outside
the dotted box in Figure 1. Its encoding, matching,
and aggregation layers are described as follows.
Phrase Encoder. Given a disease phrase P =
(p1, . . . , pn) with n words, BiMPM encode it as
follows. First, it transforms P in to a vector se-
quence P = (p1, . . . , pn). Each word is repre-
sented by concatenating a pre-trained GloVe (Pen-
nington et al., 2014) vector and a character-
BiLSTM-encoded vector. A BiLSTM is then ap-
plied on P to represent context in both directions:
(1)

2, . . . ,

n) =

←−
h P
−→
h P

←−−−
LSTM(P)
−−−→
LSTM(P)

(2)
Phrase Matcher. Given context representations

2, . . . ,

n) =

←−
HP = (
−→
HP = (

←−
h P
1,
−→
h P
1,

←−
h P
−→
h P

Figure 1: The architecture of (DA-)BiMPM.

mP

i = (

−→
i ⊗ −→
HQ,
h P

←−
i ⊗ ←−
HQ)
h P

of P and Q, a phrase matcher compares them with
each time step of one against all of the other’s in
both directions. For example, when comparing
word pi with Q, we generated a matching vector
(3)
Here ⊗ denotes the multi-perspective matching
operation deﬁned in (Wang et al., 2017). We re-
fer readers to this paper for details.
Aggregation Layer. Given all matching vectors
MP = (mP
n) by comparing P to Q, and
MQ vice versa, we apply another BiLSTM layer
to aggregate both of them, respectively. Formally,
(4)

1, . . . , mP

−→
AP,
(
−→
AQ,

←−
AP) = BiLSTM(MP)
←−
AQ) = BiLSTM(MQ)

(

−→a P
n,

(5)
Finally, we concatenate the four ending hidden
←−a P
−→a Q
m,
1,
vectors of the BiLSTM layer, i.e.,
←−a Q
1 , as the matching features F.
and
To decide whether P and Q match, we apply a
fully connected softmax layer on F to produce the
prediction y(F). Denoting all parameters of the
feature extraction layers by φf , and the prediction
layer φy, for ground truth y(k) of the k-th phrase
pair, the instance-level matching loss is
l(k)(φf , φy) = l(y(F(k)), y(k))
3 Domain-Adversarial Training
Given the conﬁgurations of BiMPM,
the net-
work parameters {φf , φy} are optimized to min-
imize the gap between predicted and ground-truth
matching labels. When source-domain training
data is involved, due to the large parameter space
of φf , the model may be satisﬁed with ﬁtting la-
bels in each domain separately instead of ﬁnding
a uniﬁed explanation. This limitation thus causes
the model to miss potential beneﬁts of learning
domain-independent matching features.

(6)

138To fully utilize source-domain annotations, we
apply domain-adversarial training (Ganin et al.,
2016) on BiMPM. As illustrated by the dotted box
in Figure 1, we add a domain discriminator d(.) on
F, i.e., the matching features. The discriminator
is conﬁgured with the same fully-connected and
softmax layers as the matching prediction layer.
Given the domain d(k) where the k-th phrase pair
is from, the domain loss is similarly given as

d

l(k)
d (φf , φd) = ld(d(F(k)), d(k))

(7)
Different from minimizing the matching loss l(k),
we optimize the domain loss l(k)
in the contrary
direction. In other words, we prefer {φf , φd} that
preserve little domain-speciﬁc information.
Formally, given training phrase pairs in the tar-
get domain with indices k ∈ T , and source-
domain data with indices k ∈ S, our joint objec-
tive function is given as follows by interpolating
both the matching and the domain losses:
l(k)(φf , φy)
(cid:2)

L(φf , φy, φd) =

|S ∪ T|

(cid:2)

(cid:2)

(8)

1

(cid:4)

l(k)
d (φf , φd) +

l(k)
d (φf , φd)

− λ

(cid:3) 1|S|

k∈S∪T
1|T|

k∈S

k∈T

When optimizing the objective function, we seek
for a saddle { ˆφf , ˆφy, ˆφd} such that:

ˆφf , ˆφy = arg min
φf ,φy

L(φf , φy, ˆφd)

ˆφd = arg max

φd

L( ˆφf , ˆφy, φd)

(9)

(10)

By considering domain adaptation and matching
label prediction in the joint objective, the training
process pursuits a balance between both aspects.
Interactions between the matching loss and the do-
main loss will force their shared parameters, i.e.,
ˆφf , to be generalizable across domains.

4 Experiments
4.1 Datasets and Baselines
We employ ICD10DATA1 and MIMIC (Johnson
et al., 2016) as the source and target domain
datasets, respectively. ICD10DATA consists of di-
verse disease names from multiple medical sub-
ﬁelds2 and their approximate synonyms. MIMIC
is a public dataset on computational physiology.
The used phrase pairs are composed of terminol-
ogy co-reference pairs of disease entities. Because
both datasets consist of only positive pairs, we
have to generate negative pairs. For each positive
pair (cid:6)P, Q(cid:7), we corrupt Q with a random phrase
1http://www.icd10data.com/. We only used the

ICD-10-CM (diagnosis) subset.

2We uniformly treat them as from one source domain.

Dataset
ICD10DATA
MIMIC

# of Pairs
29,783
22,504

Subﬁeld
Mixed

Physiology

Domain
Source
Target

Table 2: Statistics of source and target datasets.

from all other pairs containing neither P nor Q. We
summarize both datasets in Table 2.

We adopt a training/validation/testing split of
3:1:1 on the target dataset, and conduct 5-fold
cross validation. Average results on the ﬁve test-
ing sets are reported. When involving the source
dataset to help train better classiﬁers for the target
domain, we use all annotations for training. We
compare DA-BiMPM with ﬁve baselines:
Cosine: Phrases are represented by summing their
GloVe (Pennington et al., 2014) word vectors.
Their similarities are measured by Cosine scores.
Support Vector Machine (SVM): An SVM clas-
siﬁer is trained and applied on the concatenation
of the phrase pairs’ GloVe vectors.
Random Forest: Instead of SVM, this baseline
applies random forest to train matching classiﬁers.
Siamese-LSTM: We use an existing implementa-
tion3 of Mueller and Thyagarajan (2016).
BiMPM: This is the matching-aggregating net-
work (Wang et al., 2017) described in Section 2.

In DA-BiMPM, we adopt the same conﬁgura-
tion with that of BiMPM. We empirically set λ in
Equation 8 to 0.5 throughout the experiments.

4.2 Preliminary Results
Figure 2 demonstrates the changes of the three
losses in Equations 6, 7, and 8, respectively. We
observe that, as training proceeds to about 100
iterations, all losses tend to decrease and then
converge. Readers may notice that the domain
loss follows a decreasing trend, which seems in-
consistent with its negative coefﬁcient in Equa-
tion 8. Note that the matching and domain losses
are both functions of the feature extraction param-
eters φf , thus are correlated. As the matching
loss decreases, φf may inevitably capture domain-
dependent information. Therefore, the trade-off
between minimizing the matching loss and maxi-
mizing the domain loss cannot achieve both objec-
tives in positive directions. It can only prevent the
latter loss from decreasing too much. The same
ﬁgure also shows that, after 20 iterations, the vali-
dation accuracy grows quickly and then converges
to 96.04%, yielding a testing accuracy of 96.96%.

3https://github.com/dhwajraj/

deep-siamese-text-similarity

139100

80

s
s
o
L

60

40

20

0

0

20

40

Matching Loss
Domain Loss
Joint Loss
Validation Accuracy

100

90

)

%

(
 
y
c
a
r
u
c
c
A

80

70

60

50

40

100

120

60

80

Iteration

S. + T. T. Only
Model
53.73
48.22
Cosine
80.04
78.54
SVM
86.15
Random Forest
83.61
90.97
Siamese-LSTM 90.75
BiMPM
91.27
91.06
N/A
96.96
DA-BiMPM

Setting
BiMPM (S. Only)
BiMPM (T. Only)
BiMPM (S. + T.)
BiMPM (DDC variant)
DA-BiMPM (unsupervised)
DA-BiMPM (supervised)

Accuracy

90.74
91.06
91.27
92.39
96.12
96.96

Figure 2: Training losses and
validation accuracy.

Table 3: Testing accuracy (%)
w/ or w/o source annotations.

Table 4: (DA-)BiMPM’s testing ac-
curacy (%) w.r.t. different settings.

4.3 Comparative Studies
In Table 3, we report the performance of all ap-
proaches. For each baseline, we train the model by
aggregating both the entire source-domain dataset
and the training set of the target domain. For com-
parison, we also trained them without the source
dataset. We have the following observations.

First, when given the combined training set, the
performance of the ﬁve baselines increases by the
order they are presented. Speciﬁcally, the simplest
Cosine approach is close to random guesses. Su-
pervised methods like SVM and Random Forest,
on the other hand, produce much better results.
Neural network approaches, including Siamese-
LSTM and matching-aggregating-based BiMPM,
have the best performance among all baselines.

Moreover, including the source-domain dataset
for training have different effects on the baselines.
For the ﬁrst four baselines,
this dataset harms
the training process and results in inferior perfor-
mance. In contrast, BiMPM achieves slightly bet-
ter accuracy by involving source-domain annota-
tions. We note that such different effects may be
due to a different model complexity. As a com-
plicated model, BiMPM is able and tends to bene-
ﬁt from larger training data, even if they are from
different domains. In summary, if exploited in a
straight-forward manner, source-domain annota-
tions cannot always guarantee better performance.
Finally, DA-BiMPM achieves more than ﬁve
points of performance gain on top of BiMPM.
Note that BiMPM has already taken advantage
of source-domain annotations. Compared with
BiMPM, DA-BiMPM only accepts domain labels
as additional training information. The match-
ing classiﬁer trained by DA-BiMPM has the same
structure, and requires the same input to make pre-
dictions, with that of BiMPM. This indicates that
DA-BiMPM is making domain-adaptive exploita-
tion of source-domain data from the feature level.
In Table 4, we evaluate DA-BiMPM in the unsu-
pervised setting, i.e., considering only source an-

notations in matching loss. This is done by not in-
volving any target data when updating the predic-
tion layer. We compete with Deep Domain Con-
fusion (DDC) (Tzeng et al., 2014), where an adap-
tation layer based on Maximum Mean Discrep-
ancy (Borgwardt et al., 2006) is applied after the
phrase matcher. We also include (DA-)BiMPM’s
results in other relevant settings for comparison.
It is observed that approaches with more informa-
tion achieve better accuracy. Speciﬁcally, with ac-
cess to the source data and distribution of the target
training set, the unsupervised DA-BiMPM outper-
forms DDC-based BiMPM by nearly four points.

4.4 A Case Study
To further examine the impact of domain adapta-
tion, we study a phrase pair “bleed” and “gun shot
wound to the head” in the target set. When involv-
ing only target data, BiMPM correctly judged the
pair as a mismatch. We ﬁnd that, if involved in
a pair, “bleed” on both sides tends to suggest a
match. The numbers of instances for and against
this feature are 1,401 and 747, respectively.

After trivially accessing source data, BiMPM
achieved a slight gain. However, the above statis-
tics are both 41 on the source set, implying a dif-
ferent data distribution. BiMPM was mislead on
the above pair, and gave a false positive label.
Meanwhile, DA-BiMPM overcomes the domain
difference, and corrected the label to negative.

5 Conclusion

We present DA-BiMPM, a domain-adversarial
network for disease phrase matching.
It outper-
forms the base model BiMPM as well as four other
baselines, with or without source annotations.
Experiments also demonstrate that, when triv-
ially combined with target-domain training data,
source-domain data does not always make posi-
tive impacts. However, DA-BiMPM can better ex-
ploit the source-domain data, even if BiMPM or
its DDC variant have taken advantage of it.

140Liqiang Nie, Meng Wang, Luming Zhang, Shuicheng
Yan, Bo Zhang, and Tat-Seng Chua. 2015. Disease
inference from health-related questions via sparse
deep learning.
IEEE Transactions on Knowledge
and Data Engineering 27(8):2107–2119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664 .

Aaron Traylor, Nicholas Monath, Rajarshi Das, and
Andrew McCallum. 2017. Learning string align-
ments for entity aliases.
In 6th Workshop on Au-
tomated Knowledge Base Construction (AKBC).

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko,
and Trevor Darrell. 2014. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint
arXiv:1412.3474 .

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with lstm. In Proceedings of
NAACL-HLT. pages 1442–1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. arXiv preprint arXiv:1702.03814
.

Wenpeng Yin, Hinrich Sch¨utze, Bing Xiang, and
Bowen Zhou. 2016. ABCNN: attention-based con-
volutional neural network for modeling sentence
pairs. TACL 4:259–272.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang,
Shuangyong Song, Wei Chu, and Haiqing Chen.
2018. Modelling domain relationships for transfer
learning on retrieval-based question answering sys-
tems in e-commerce. In Proceedings of the Eleventh
ACM International Conference on Web Search and
Data Mining. ACM, pages 682–690.

References
Karsten M Borgwardt, Arthur Gretton, Malte J Rasch,
Hans-Peter Kriegel, Bernhard Sch¨olkopf, and Alex J
Smola. 2006. Integrating structured biological data
by kernel maximum mean discrepancy. Bioinfor-
matics 22(14):e49–e57.

Lisheng Fu, Thien Huu Nguyen, Bonan Min, and
Ralph Grishman. 2017. Domain adaptation for re-
lation extraction with domain adversarial neural net-
work.
In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers). volume 2, pages 425–429.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, Franc¸ois Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works.
Journal of Machine Learning Research
17(59):1–35.

Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-
wei H Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G Mark. 2016. Mimic-iii,
a freely accessible critical care database. Scientiﬁc
data 3.

Yann Lecun, Yoshua Bengio, and Geoffrey Hinton.

2015. Deep learning. Nature 521(7553):436–444.

Hang Li, Jun Xu, et al. 2014. Semantic matching in
search. Foundations and Trends R(cid:8) in Information
Retrieval 7(5):343–469.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classiﬁca-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). volume 1, pages 1–10.

Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identiﬁcation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Com-
putational Linguistics, pages 182–190.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In SemEval@ COLING. pages 1–8.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Thirtieth AAAI Conference on Artiﬁcial Intel-
ligence. pages 2786–2792.

Paul Neculoiu, Maarten Versteegh, and Mihai Rotaru.
2016. Learning text similarity with siamese recur-
rent networks. In Repl4nlp Workshop at ACL.

141
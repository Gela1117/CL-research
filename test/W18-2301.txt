Embedding Transfer for Low-Resource Medical Named Entity

Recognition: A Case Study on Patient Mobility

Denis Newman-Grifﬁs1,2 and Ayah Zirikly1

1Rehabilitation Medicine Department, Clinical Center, National Institutes of Health, Bethesda, MD

2Department of Computer Science and Engineering, The Ohio State University, Columbus, OH

{denis.griffis, ayah.zirikly} @nih.gov

Abstract

Functioning is gaining recognition as an
important indicator of global health, but
remains under-studied in medical natural
language processing research. We present
the ﬁrst analysis of automatically extract-
ing descriptions of patient mobility, using
a recently-developed dataset of free text
electronic health records. We frame the
task as a named entity recognition (NER)
problem, and investigate the applicabil-
ity of NER techniques to mobility extrac-
tion. As text corpora focused on patient
functioning are scarce, we explore domain
adaptation of word embeddings for use
in a recurrent neural network NER sys-
tem. We ﬁnd that embeddings trained on
a small in-domain corpus perform nearly
as well as those learned from large out-of-
domain corpora, and that domain adapta-
tion techniques yield additional improve-
ments in both precision and recall. Our
analysis identiﬁes several signiﬁcant chal-
lenges in extracting descriptions of patient
mobility, including the length and com-
plexity of annotated entities and high lin-
guistic variability in mobility descriptions.

Introduction

1
Functioning has recently been recognized as a
leading world health indicator, joining morbid-
ity and mortality (Stucki and Bickenbach, 2017).
Functioning is deﬁned in the International Clas-
siﬁcation of Functioning, Disability, and Health
(ICF; WHO 2001) as the interaction between
health conditions, body functions and structures,
activities and participation, and contextual fac-
tors. Understanding functioning is an important
element in assessing quality of life, and automatic

extraction of patient functioning would serve as
a useful tool for a variety of care decisions, in-
cluding rehabilitation and disability assessment
(Stucki et al., 2017).
In healthcare data, natu-
ral language processing (NLP) techniques have
been successfully used for retrieving information
about health conditions, symptoms and procedures
from unstructured electronic health record (EHR)
text (Soysal et al., 2018; Savova et al., 2010).
As recognition of the importance of functioning
grows, there is a need to investigate the application
of NLP methods to other elements of functioning.
Recently, Thieu et al. (2017) introduced a
dataset of EHR documents annotated for descrip-
tions of patient mobility status, one area of activity
in the ICF. Automatically recognizing these de-
scriptions faces signiﬁcant challenges, including
their length and syntactic complexity and a lack of
terminological resources to draw on. In this study,
we view this task through the lens of named en-
tity recognition (NER), as recent work has illus-
trated the potential of using recurrent neural net-
work (RNN) NER models to address similar issues
in biomedical NLP (Xia et al., 2017; Dernoncourt
et al., 2017b; Habibi et al., 2017).

An additional strength of RNN models is their
ability to leverage pretrained word embeddings,
which capture co-occurrence information about
words from large text corpora. Prior work has
shown that the best improvements come from em-
beddings trained on a corpus related to the target
domain (Pakhomov et al., 2016). However, free
text describing patient functioning is hard to come
by: for example, even the large MIMIC-III corpus
(Johnson et al., 2016) includes only a few hundred
documents from therapy disciplines among its two
million notes. While recent work suggests that us-
ing a training corpus from the target domain can
mitigate a lack of data (Diaz et al., 2016), even
a careful corpus selection may not produce sufﬁ-

ProceedingsoftheBioNLP2018workshop,pages1–11Melbourne,Australia,July19,2018.c(cid:13)2018AssociationforComputationalLinguistics1cient data to train robust word representations.

In this paper, we explore the use of an RNN
model to recognize descriptions of patient mo-
bility. We analyze the impact of initializing the
model with word embeddings trained on a vari-
ety of corpora, ranging from large-scale out-of-
domain data to small, highly-targeted in-domain
documents. We further explore several domain
adaptation techniques for combining word-level
information from both of these data sources, in-
cluding a novel nonlinear embedding transforma-
tion method using a deep neural network.

in both precision and recall.

We ﬁnd that embeddings trained on a very
small set of therapy encounter notes nearly match
the mobility NER performance of representations
trained on millions of out-of-domain documents.
Domain adaptation of input word embeddings
often improves performance on this challenging
dataset,
Finally,
we ﬁnd that simpler adaptation methods such as
concatenation and preinitialization achieve high-
est overall performance, but that nonlinear map-
ping of embeddings yields the most consistent per-
formance across experiments. We achieve a best
performance of 69% exact match and over 83%
token-level match F-1 score on the mobility data,
and identify several trends in system errors that
suggest fruitful directions for further research on
recognizing descriptions of patient functioning.

2 Related work

The extraction of named entities in free text has
been one of the most important tasks in NLP and
information extraction (IE). As a result, this track
of research has matured over the last two decades,
especially in the newswire domain for high re-
source languages such as English. Many of the
successful existing NER systems use a combi-
nation of engineered features trained using con-
ditional random ﬁelds (CRF) model (McCallum
and Li, 2003; Finkel et al., 2005). NER systems
have also been widely studied in medical NLP,
using dictionary lookup methods (Savova et al.,
2010), support vector machine (SVM) classi-
ﬁers (Kazama et al., 2002), and sequential models
(Tsai et al., 2006; Settles, 2004). In recent years,
deep learning models have been used in NER
with successful results in many domains (Col-
lobert et al., 2011). Proposed neural network ar-
chitectures included hybrid convolutional neural
network (CNN) and bi-directional long-short term

Evaluation:

[Scoring: 1=totally dependent,
2=requires assistance,
3=requires appliances, 4=totally
independent]ScoreDeﬁnition.
[Ambulation: 4]Mobility
Observations:
Pt is weight bearing: [she
ambulates independently w/o
use of assistive device]Mobility.
Limited to very brief
examination.

Figure 1: Synthetic document with examples of
ScoreDeﬁnition (in blue) and Mobility (in orange).

memory (Bi-LSTM) as introduced by Chiu and
Nichols (2015). State-of-the-art NER models use
the architecture proposed by Lample et al. (2016),
a stacked bi-directional long-short term memory
(Bi-LSTM) for both character and word, with a
CRF layer on the top of the network.
In the
biomedical domain, Habibi et al. (2017) used this
architecture for chemical and gene name recog-
nition. Liu et al. (2017) and Dernoncourt et al.
(2017a) adapted it for state-of-the-art note deiden-
tiﬁcation. In terms of functioning, Kukafka et al.
(2006) and Skube et al. (2018) investigate the pres-
ence of functioning terminology in clinical data,
but do not evaluate it from an NER perspective.

3 Data

Thieu et al. (2017) presented a dataset of 250 de-
identiﬁed EHR documents collected from Physi-
cal Therapy (PT) encounters at the Clinical Center
of the National Institutes of Health (NIH). These
documents, obtained from the NIH Biomedi-
cal Translational Research Informatics System
(BTRIS; Cimino and Ayres 2010), were anno-
tated for several aspects of patient mobility, a sub-
domain of functioning-related activities deﬁned
by the ICF; we therefore refer to this dataset as
BTRIS-Mobility. We focus on two types of con-
tiguous text spans: descriptions of mobility status,
which we call Mobility entities, and measurement
scales related to mobility activity, which we refer
to as ScoreDeﬁnition entities.

Two major differences stand out in BTRIS-
Mobility as compared with standard NER data.
The entities, deﬁned for this task as contiguous
text spans completely describing an aspect of mo-
bility, tend to be quite long: while prior NER
datasets such as the i2b2/VA 2010 shared task data
(Uzuner et al., 2012) include fairly short entities
(2.1 tokens on average for i2b2), Mobility entities

2Entity
Mobility
ScoreDeﬁnition

Train Valid Test
1,533
947
48

467
24

82

Table 1: Named entity statistics for training, vali-
dation, and test splits of BTRIS-Mobility. Due to
the rarity of ScoreDeﬁnition entities, we use a 2:1
split of training to test data, and hold out 10% of
training data as validation.

are an average of 10 tokens long, and ScoreDeﬁni-
tion average 33.7 tokens. Also, both Mobility and
ScoreDeﬁnition entities tend to be entire clauses
or sentences, in contrast with the constituent noun
phrases that are the meat of most NER. Figure 1
shows example Mobility and ScoreDeﬁnition en-
tities in a short synthetic document. Despite these
challenges, Thieu et al. (2017) show high (> 0.9)
inter-annotator agreement on the text spans, sup-
porting use of the data for training and evaluation.
These characteristics align well with past suc-
cessful applications of recurrent neural models to
challenging NLP problems. For our evaluation on
this dataset, we randomly split BTRIS-Mobility at
document level into training, validation, and test
sets, as described in Table 1.

3.1 Text corpora
In order to learn input word embeddings for NER,
we use a variety of both in-domain and out-of-
domain corpora, deﬁned in terms of whether the
corpus documents include descriptions of func-
tion. For in-domain data, with explicit references
to patient functioning, we use a corpus of 154,967
EHR documents shared with us (under an NIH
Clinical Center Ofﬁce of Human Subjects deter-
mination) from the NIH BTRIS system.1 A large
proportion of these documents comes from the
Rehabilitation Medicine Department of the NIH
Clinical Center, including Physical Therapy (PT),
Occupational Therapy (OT), and other therapeu-
tic records; the remaining documents are sampled
from other departments of the Clinical Center.

Since BTRIS-Mobility is focused on PT docu-
ments, we also use a subset of this corpus con-
sisting of 17,952 PT and OT documents. Despite
this small size, the topical similarity of these doc-
uments makes them a very targeted in-domain cor-
pus. For clarity, we refer to the full corpus as

1There is no overlap between these documents and the
annotated data in BTRIS-Mobility (T. Thieu, personal com-
munication).

BTRIS, and the smaller subset as PT-OT.

3.1.1 Out-of-domain corpora
As the BTRIS corpus is considered a small train-
ing corpus for learning word embeddings, we also
use three larger out-of-domain corpora, which rep-
resent different degrees of difference from the in-
domain data. Our largest data source is pretrained
FastText embeddings from Wikipedia 2017, web
crawl data, and news documents.2

We also make use of two biomedical corpora for
comparison with existing work. PubMed abstracts
have been an extremely useful source of embed-
ding training in biomedical NLP (Chiu et al.,
2016); we use the text of approximately 14.7 mil-
lion abstracts taken from the 2016 PubMed base-
line as a high-resource biomedical corpus. In ad-
dition, we use two million free-text documents
released as part of the MIMIC-III critical care
database (Johnson et al., 2016). Though smaller
than PubMed, the MIMIC corpus is a large sample
of clinical text, which is often difﬁcult to obtain
and shows signiﬁcant linguistic differences with
biomedical literature (Friedman et al., 2002). As
MIMIC is clinical text, it is the closest compari-
son corpus to the BTRIS data; however, as MIMIC
focuses on ICU care, the information in it differs
signiﬁcantly from in-domain BTRIS documents.

4 Methods

We adopt the architecture of Dernoncourt et al.
(2017a), due to its successful NER results on
CoNLL and i2b2 datasets. The architecture, as
depicted in Figure 2, is a stacked LSTM com-
posed of: i) character Bi-LSTM layer that gen-
erates character embeddings. We include this in
our experimentations due to its performance en-
hancement; ii) token Bi-LSTM layer using both
character and pre-trained word embeddings as in-
put; iii) CRF layer to enhance the performance
by taking into account the surrounding tags (Lam-
ple et al., 2016). We use the following values for
the network hyperparameters, as they yielded the
best performance on the validation set: i) hidden
state dimension of 25 for both character and token
layers. In contrast to more common token layer
sizes such as 100 or 200, we found the best val-
idation set performance for our task with 25 di-
mensions; ii) learning rate = 0.005; iii) patience
= 10; iv) optimization with stochastic gradient de-

2fasttext.cc/docs/en/english-vectors

3NER performance. Their method aims to help the
model to differentiate between general and domain
speciﬁc terms, using a signiﬁcance function φ of a
word w. φ is dependent on the deﬁnition of w’s
frequency, where in our implementation it is the
word frequency in the target corpora.

Linear transform However, these approaches
suffer from the same limitations as training BTRIS
embeddings directly: a restricted vocabulary and
minimal training data, both due to the size of the
corpus. We therefore also investigate two meth-
ods for learning a transformation from one set
of embeddings into the same space as another,
based on a reference dictionary. Given an out-of-
domain source embedding set and a target BTRIS
embedding set, we use all words in common be-
tween source and target as our training vocabu-
lary.6 We adapt this to the linear transformation
method successfully applied to bilingual embed-
dings by Artetxe et al. (2016), using this shared
vocabulary as the training dictionary.

Non-linear transform As all of our embed-
dings are in English, but from domains that do
not intuitively seem to have a linear relationship,
we also extend the method of Artetxe et al. to a
non-linear transformation. We randomly divide
the shared vocabulary into ten folds, and train a
feed-forward neural network using nine-tenths of
the data, minimizing mean squared error (MSE)
between the learned projection and the true em-
beddings. After each epoch, we calculate MSE on
the held-out set, and halt when this error stops de-
creasing. Finally, we average the learned projec-
tions from each fold to yield the ﬁnal transforma-
tion function. Following Artetxe et al. (2016), we
apply this function to all source embeddings, al-
lowing us to maintain the original vocabulary size.
Our model is a fully-connected feed-forward
neural network, with the same hidden dimension
as our embeddings. We evaluate with both 1 and
5 hidden layers, and use either tanh or rectiﬁed
linear unit (ReLU) activation throughout. Model
structure is denoted in the result; for example, “5-
layer ReLU” refers to nonlinear mapping using a
5-layer network with ReLU activation. We train
with Adam optimization (Kingma and Ba, 2014)
and a minibatch size of 5.7

6We evaluated using subsets of 1k, 2k, or 10k shared
words most frequent in BTRIS, but the best downstream per-
formance was achieved using all pivot points.

7Source implementation available at

github.com/drgriffis/NeuralVecmap

Figure 2: Bi-LSTM-CRF network architecture

scent (SGD) which showed superior performance
to adaptive moment estimation (Adam) optimiza-
tion technique (Kingma and Ba, 2014).

4.1 Embedding training
We use two popular toolkits for learning word em-
beddings: word2vec3 (Mikolov et al., 2013) and
FastText4 (Bojanowski et al., 2017). We run both
toolkits using skip-gram with negative sampling
to train 300-dimensional embeddings, and use de-
fault settings for all other hyperparameters.5

4.2 Domain adaptation methods
We evaluate several different methods for adapting
out-of-domain embeddings to the BTRIS corpus.
Concatenation In addition to the original
embeddings, we concatenate out-of-domain and
BTRIS/PT-OT embeddings as a baseline, allowing
the model to learn a task-speciﬁc combination of
the two representations.

Preinitialization Recent work has found bene-
ﬁts from retraining learned embeddings on a target
corpus (Yang et al., 2017). We pre-initialize both
word2vec and FastText toolkits with embeddings
learned on each of our three reference corpora,
and retrain on the BTRIS corpus using an ini-
tial learning rate of 0.1. Additionally, we use the
regularization-based domain adaptation approach
introduced by Yang et al. (2017) as another base-
line, due to its successful results in improving

3We use word2vec modiﬁed to support pre-initialization,

from github.com/drgriffis/word2vec-r.

4github.com/facebookresearch/fastText
5For PT-OT embeddings, due to the extremely small cor-
pus size, we use an initial learning rate of 0.05, keep all words
with minimum frequency 2, and train for 25 iterations.

4x1,1x1,	ℓ(1)LSTMLSTMLSTMLSTMVCVT……VCxn,1xn,ℓ(n)enLSTMLSTMLSTMLSTMLSTMVCVTx1……VCe1LSTMLSTMd1a1y1LSTMdnanyn…………xnCharacter layerToken layerCRF layerCorpus

Size

Toolkit

Exact match

Token match

Exact match

Token match

Mobility

ScoreDeﬁnition

Random initialization

WikiNews
PubMed

16B
2.6B

MIMIC

BTRIS

PT-OT

497M

74.6M

4.2M

FT
FT
w2v
FT
w2v
FT
w2v
FT
w2v

Pr
67.7
67.0
68.7
64.9
37.7
71.9
66.8
69.7
68.8
70.8

Rec
61.8
64.0
65.9
64.7
10.6
64.9
63.8
63.7
62.5
63.4

F1
64.6
65.4
67.2
64.8
16.5
68.2
65.3
66.7
65.5
67.0

Pr
84.0
83.0
82.0
77.4
78.9
84.3
80.6
86.0
84.5
85.8

Rec
75.9
80.0
84.5
87.7
21.7
83.0
83.4
79.2
80.2
79.4

F1
79.7
81.5
83.2
82.2
34.0
83.6
82.0
82.4
82.3
82.5

Pr
86.5
83.3
93.6
90.0
86.0
91.7
90.2
88.2
92.0
86.3

Rec
93.4
93.4
91.7
93.8
90.0
91.7
95.8
93.8
95.8
91.7

F1
90.0
88.2
92.6
91.8
87.8
91.7
92.9
90.9
93.9
88.9

Pr
97.7
96.8
98.1
97.8
97.9
96.5
95.9
96.7
97.1
96.3

Rec
98.9
99.3
97.8
99.6
97.7
99.6
99.0
99.9
97.7
98.9

F1
98.3
98.0
97.9
98.7
97.8
98.0
97.4
98.3
97.4
97.6

Table 2: Exact and token-level match results on BTRIS-Mobility, using randomly-initialized embeddings
as a baseline and unmodiﬁed word2vec (w2v) and FastText (FT) embeddings from different corpora. Size
is the number of tokens in the training corpus.

5 Results

We report exact match results, calculated using
CoNLL 2003 named entity recognition shared task
evaluation scoring (Tjong Kim Sang and De Meul-
der, 2003), which requires that all tokens of an en-
tity are correctly recognized. Additionally, given
the long span of Mobility and ScoreDeﬁnition en-
tities (see Section 3), we evaluated partial match
performance using token-level results. For sim-
plicity, we report only performance on the test set;
however, validation set numbers consistently fol-
low the same trends observed in test data. We de-
note embeddings trained using FastText with the
subscript F T , and word2vec with w2v.

5.1 Embedding corpora
Exact and token-level match results for both Mo-
bility and ScoreDeﬁnition entities are given for
embeddings from each corpus in Table 2. By and
large, the in-domain BTRIS and PT-OT embed-
dings yield higher precision than out-of-domain
embeddings, though this comes at the expense of
recall. word2vec embeddings consistently achieve
better NER performance than FastText embed-
dings from the clinical corpora, although this was
reversed with PubMed, suggesting that further re-
search is needed on the strengths of different em-
bedding methods in biomedical data. The un-
usually poor performance of MIMICF T embed-
dings persisted across multiple experiments with
two embedding samples, manifesting primarily in
making very few predictions (less than 30% as
many Mobility entities other embeddings yielded).
Most notably, despite a thousand-fold reduction
in training corpus size, we see that PT-OT embed-
dings match the performance of PubMed embed-

dings on Mobility mentions and achieve the best
overall performance on ScoreDeﬁnition entities.
Together with the overall superior performance of
PT-OT embeddings even to the larger BTRIS cor-
pus, our ﬁndings support the value of using input
embeddings that are highly representative of the
target domain. Nonetheless, MIMIC embeddings
have both the best precision and overall perfor-
mance on Mobility data, despite the domain mis-
match of critical care versus therapeutic encoun-
ters. This indicates that there is a limit to the ben-
eﬁts of in-domain data that can be outweighed by
sufﬁcient data from a different but related domain.
Token-level results follow the same trends as
exact match, with clinical embeddings achiev-
ing highest precision, while PubMed embeddings
yield better recall. As many entity-level errors are
only off by a few tokens, token-level scores are
generally 15-20 absolute points higher than their
corresponding entity-level scores. At the token
level, it is clear that ScoreDeﬁnition entities are ef-
fectively solved in this dataset, with all F1 scores
are above 97.4%. This is primarily due to the reg-
ularity of ScoreDeﬁnition strings:
they typically
consist of a sequence of single numbers followed
by explanatory strings, as shown in Figure 1.

5.2 Mapping methods

Table 3 takes a single representative source/target
pair and compares the different results obtained
on recognizing Mobility entities when the NER
model is initialized with embeddings learned us-
ing different domain adaptation methods. In this
case, as with several other source/target pairs we
evaluated, the concatenated embeddings give the
best overall performance, stemming largely from

5Target

BTRISF T

PT-OTF T

Source
WikiNewsF T
PubMedF T
PubMedw2v
MIMICF T
MIMICw2v
WikiNewsF T
PubMedF T
MIMICw2v

Concat

Rec
65.3
65.8
65.3
10.4
67.6
63.9
65.1
66.1

F1
68.6
67.6
65.3
16.0
67.5
65.6
63.9
65.1

Pr
72.2
69.5
65.3
35.0
67.4
67.5
62.8
64.1

Preinit
Rec
59.2
66.5
65.4
15.5
64.6
57.9
50.2
61.8

F1
57.0
65.4
65.1
22.0
66.5
56.1
55.2
60.8

Pr
55.0
64.2
64.8
37.8
68.5
54.5
61.3
59.9

Linear
Rec
61.9
60
65.8
62.9
60.3
63.8
62.6
54.1

F1
63.5
62.7
68
63.3
63.4
66.2
62.6
55.9

Pr
65.1
65.6
70.3
63.7
66.8
68.9
62.6
57.9

5-layer tanh

Pr
69.3
66.1
66.3
70.3
69.2
68.5
68.3
67.3

Rec
64.2
64.5
62.6
61.3
64.3
63.4
60.1
63.2

F1
66.7
65.3
64.4
65.5
66.7
65.8
63.9
65.1

Table 4: Exact match precision and recall for Mobility entities with word embeddings mapped from each
source to BTRISF T embeddings, using four selected domain adaptation methods. The best-performing
embeddings from each source corpus were also mapped to PT-OTF T embeddings. The best precision,
recall, and F1 achieved with each source/target pair is marked in bold.

Method
Pr
67.0
WikiNewsF T
70.0
BTRISw2v
68.6
Concatenated
66.8
Preinitialized
72.5
Linear
1-layer ReLU 69.2
1-layer tanh
70.6
5-layer ReLU 67.3
5-layer tanh
67.9

Exact match

Token match

Rec
64.0
63.7
66.7
64.5
58.9
63.2
61.0
61.9
62.1

F1
65.4
66.6
67.6
65.6
65
66.0
65.5
64.5
64.9

Pr
83.0
86.0
84.3
78.4
79.1
83.4
84.9
83.5
82.1

Rec
80.0
79.2
81.8
86.4
83
76.9
75.7
76.6
77.0

F1
81.5
81.5
83.0
82.2
81
80.0
80.1
79.9
79.4

Table 3: Comparison of mapping methods, using
WikiNewsF T as source and BTRISw2v as target.
Results are given for exact entity-level match and
token-level match for test set Mobility entities.

an increase in recall over the baselines. How-
ever, we see that the nonlinear mapping methods
tend to yield high precision: all settings improve
over WikiNews embeddings alone, and the 1-layer
tanh mapping beats the BTRIS embeddings as
well. Reﬂecting the earlier observed trends of in-
domain data, this is offset by a drop in recall, often
of several absolute percentage points.

These differences are ﬂeshed out further in Ta-
ble 4, comparing four domain adaptation meth-
ods across several source/target pairs. Concate-
nation typically achieves the best overall perfor-
mance among the adaptation methods, but non-
linear mappings yield highest precision in 6 of
the 8 settings shown. Concatenation is also more
sensitive to noise in the source embeddings, as
shown with MIMICF T results, and preinitializa-
tion varies widely in its performance. By contrast,
linear and nonlinear mapping methods are less af-
fected by the choice of source embeddings, yield-
ing more consistent results than preinitialization or
concatenation for a given target corpus. Nonlinear
mappings exhibit this stability most clearly, pro-
ducing very similar results across all settings. The

Target

Source
Method
WikiNewsF T PT-OTw2v
Preinit
WikiNewsF T BTRISw2v Linear
MIMICw2v
Concat

BTRISF T

Rec

Pr
F1
72.1 66.1 69.0
72.5 58.9
65
67.4 67.6 67.5

Table 5: Best precision, recall, and F1 (exact) for
test set Mobility mentions, with the source/target
pair and domain adaptation method used.

regularization-based domain adaptation method of
Yang et al. (2017) consistently yielded similar
results to preinitialization:
for example, an F1
score of 65% when PubMedw2v embeddings are
adapted to BTRIS, as compared to 65.4% using
pre-initialization with word2vec. We therefore
omit these results for brevity.

Comparing both Tables 3 and 4 to the perfor-
mance of unmodiﬁed embeddings shown in Ta-
ble 2, we see a surprising lack of overall per-
formance improvement or degradation. While
the different adaptation methods exhibit consistent
differences between one another, only 12 of the 32
F1 scores in Table 4 represent improvements over
the relevant unmapped baselines. Many adapta-
tion results achieve notable improvement in preci-
sion or recall individually, suggesting that differ-
ent methods may be more useful for downstream
applications where one metric is emphasized over
the other. However, several of our results indicate
failure to adapt, illustrating the difﬁculty of effec-
tively adapting embeddings for this task.

5.3 Source/target pairs
Table 5 highlights the source/target pairs that
achieved the best exact match precision, recall,
and F1 out of all
the embeddings we evalu-
ated, both unmapped and mapped. Though each
source/target pair produced varying downstream
results among the domain adaptation methods, a

6couple of broad trends emerged from our analy-
sis. The largest performance gains over unmapped
baselines were found when adapting high-resource
WikiNews and PubMed embeddings to in-domain
representations; however, these pairings also had
the highest variability in results. The most consis-
tent gains in precision came from using MIMIC
embeddings as source, and these were mostly
achieved through the nonlinear mapping approach.
There was no clear trend in the domain-adapted
results as to whether word2vec or FastText em-
beddings led to the best downstream performance:
it varied between pairs and adaptation methods.
word2vec embeddings were generally more con-
sistent, but as seen in Tables 4 and 5, FastText em-
beddings often achieved the highest performance.

5.4 Error analysis
Several interesting trends emerge in the NER er-
rors produced in our experiments. Most generally,
punctuation is often falsely considered to bound
an entity. For example, the following string is part
of a continuous Mobility entity:8

supine in bed with elevated leg,
and was left sitting in bed

However, most trained models separated this at the
comma into two Mobility entities. Unsurprisingly,
given the length of Mobility entities, we ﬁnd many
cases where most of the correct entity is tagged by
the model, but the ﬁrst or last few words are left
off, as in

[he exhibits compensatory gait
patterns]P red as a result]Gold

This behavior is illustrated in the large perfor-
mance difference between entity-level and token-
level evaluation discussed in Section 5.1.

We also see that descriptions of physical activity
without speciﬁc evaluative terminology are often
missed by the model. For example, working out
in the yard is a Mobility entity ignored by the
vast majority of our experiments, as is negotiate
six steps to enter the apartment.

5.4.1 Corpus effects
Within correctly predicted entities, we see some
indications of source corpus effect in the results.
Considering just the original, non-adapted em-
beddings as presented in Table 2, we note two
main differences between models trained on out-
of-domain vs in-domain embeddings. In-domain

8Several examples in this section have been edited for dei-

dentiﬁcation purposes and brevity.

embeddings lead to much more conservative mod-
els:
for example, PT-OTw2v only predicts 850
Mobility entities in test data, and BTRISw2v pre-
dicts 863; this is in contrast to 922 predictions
from MIMICw2v and 940 from PubMedw2v. This
carries through to mapped embeddings as well:
adding PT-OT embeddings into the mix decreases
the number of predictions across the board.

Several predictions exhibit some degree of do-
main sensitivity, as well. For example, “fatigue”
is present at the end of several Mobility men-
tions, and both PubMed and MIMIC embeddings
typically end these mentions early. PubMed em-
beddings also append more typical symptomatic
language onto otherwise correct Mobility entities,
such as no areas of pressure noted on skin
and numbness and tingling of arms. MIMIC
and the heterogeneous in-domain BTRIS corpus
append similar language, including and chronic
pain. WikiNews embeddings, by contrast, ap-
pear oversensitive to key words in many Mobility
mentions, tagging false positives such as my wife
(spouses are often referred to as a source of phys-
ical support) and stairs are within range.

5.4.2 Changes from domain adaptation
Domain-adapted embeddings ﬁx some corpus-
based issues, but re-introduce others. Out-of-
domain corpora tend to chain together Mobility
entities separated by only one or two words, as in

[He ambulates w/o ad]M obility, no
walker observed, [antalgic gait
pattern]M obility

While source PubMed and WikiNews embeddings
often collapse these to a single mention, adapting
them to the target domain ﬁxes many such cases.
However, some of the original corpus noise re-
mains: PT-OTw2v correctly ignored and chronic
pain after a Mobility mention, but MIMICw2v
mapped to PT-OTw2v re-introduces this error.

The most consistent

improvement obtained
from domain adaptation was on Mobility en-
tities that are short noun phrases, e.g.
gait
instability, and unsteady gait. Non-adapted
embeddings typically miss such phrases, but
mapped embeddings correctly ﬁnd many of them,
including some that in-domain embeddings miss.

5.4.3 Adaptation method effects
The most striking difference we observe when
comparing different domain adaptation methods
is that preinitialization universally leads to longer

7Figure 3: Number of words in shared vocabulary with different nearest neighbors in source and domain-
adapted embeddings, using BTRISF T as target. Light hatched bars indicate the number of words whose
new nearest neighbor matches BTRISF T . The dashed line indicates shared vocabulary size.

Source set

PubMedF T

WikiNewsF T

Source

ambulating
ambulate
crutches
ambulating
ambulate
extubation

Target

ambulating
ambulate
ambulatory
ambulating
ambulate
ambulatory

Preinit

ambulating
ambulate
walker

pos
76

acuity

Concat

ambulating
ambulate
crutches
ambulating
ambulate
ambulatory

Linear

ambulating
ambulate
crutches

cardiopulmonary

neurosurgical
resuscitation

h1 tanh

ambulating
ambulate
crutch
robotic
overhead
ambulating

h5 tanh
worsening
wearing
complaints
respiratory
sclerotic

acupuncture

Table 6: Top 3 nearest neighbors of ambulation in embeddings mapped to BTRISF T using different
adaptation methods. Source and Target are neighbors in the original source and BTRISF T embeddings.

Mobility entity predictions, by both mean and
variance of entity length. Though preinitialized
embeddings still perform well overall, many pre-
dictions include several extra tokens before or af-
ter the true entity, as in the following example:

(now that her leg is healed [she
is independent with wheelchair
transfer]Gold and using her
shower bench)P red

Preinitialized embeddings also have a strong ten-
dency to collapse sequential Mobility entities.
Both of these trends are reﬂected in the lower
token-level precision numbers in Table 3.

Comparing nonlinear mapping methods, we
ﬁnd that a 1-layer mapping with tanh activa-
tion consistently leads to fewer predicted Mobil-
ity entities than with ReLU (for example, 814
vs 859 with WikiNewsF T mapped to BTRISw2v,
917 vs 968 with MIMICw2v mapped to PT-
this difference disappears
OTw2v). However,
when a 5-layer mapping is used.
Despite
their consistent performance, nonlinear transfor-
mations seem to re-introduce a number of er-
rors related to more general mobility terminology.
For example, he is very active and runs 15
miles per week is correctly recognized by con-
catenated WikiNewsF T
but
missed by several of their nonlinear mappings.

and BTRISw2v,

6 Embedding analysis

To further evaluate the effects of different do-
main adaptation methods, we analyzed the nearest
neighbors by cosine similarity of each word before
and after domain adaptation. We only considered
the words present both in the dataset and in each of
our original sets of embeddings, yielding a vocab-
ulary of 6,201 words. We then took this vocabu-
lary and calculated nearest neighbors within it, us-
ing each set of out-of-domain original embeddings
and each of its domain-adapted transformations.

Figure 3 shows the number of words whose
nearest neighbors changed after adaptation, us-
ing BTRISF T as the target; all other targets dis-
play similar results. We see that
in general,
the neighborhood structure of target embeddings
is well-preserved with concatenation, sometimes
preserved with preinitialization, and completely
disposed of with the nonlinear transformation. In-
terestingly, this reorganization of words to some-
thing different from both source and target does
not lead to the performance degradation we might
expect, as shown in Section 5.

We also qualitatively examined nearest neigh-
bors before and after adaptation. Table 6 shows
nearest neighbors of ambulation, a common Mo-
bility word, for two representative source/target
pairs.
Preinitialization generally reﬂects the
neighborhood structure of the target embeddings,

8but can be noisy:
in WikiNewsF T /BTRISF T ,
other words such as therapy and fatigue share am-
bulation’s less-than-intuitive neighbors.

Reﬂecting the changes seen in Figure 3, the
linear transformation preserves source neighbors
in the biomedical PubMed corpus, but yields a
neighborhood structure different from source or
target with highly out-of-domain WikiNews em-
beddings. Nonlinear transformations sometimes
yield sensible nearest neighbors, as in the single-
layer tanh mapping of PubMedF T to BTRISF T .
More often, however, the learned projection sig-
niﬁcantly shufﬂes neighborhood structure, and ob-
served neighbors may bear only a distant simi-
larity to the query term.
In several cases, large
swathes of the vocabulary are mapped to a single
tight region of the space, yielding the same nearest
neighbors for many disparate words. This occurs
more often when using a ReLU activation, but we
also observe it occasionally with tanh activation.

7 Conclusions

We have conducted an experimental analysis of
recognizing descriptions of patient mobility with
a recurrent neural network, and of the effects of
various domain adaptation methods on recognition
performance. We ﬁnd that a state-of-the-art re-
current neural model is capable of capturing long,
complex descriptions of mobility, and of recogniz-
ing mobility measurement scales nearly perfectly.
Our experiments show that domain adaptation
methods often improve recognition performance
over both in- and out-of-domain baselines, though
such improvements are difﬁcult to achieve con-
sistently. Simpler methods such as preinitializa-
tion and concatenation achieve better performance
gains, but are also susceptible to noise in source
embeddings; more complex methods yield more
consistent performance, but with practical down-
sides such as decreased recall and a non-intuitive
projection of the embedding space. Most strik-
ingly, we see that embeddings trained on a very
small corpus of highly relevant documents nearly
match the performance of embeddings trained on
extremely large out-of-domain corpora, adding to
the recent ﬁndings of Diaz et al. (2016).

To our knowledge, this is the ﬁrst investigation
into automatically recognizing descriptions of pa-
tient functioning. Viewing this problem through
an NER lens provides a robust framework for
model design and evaluation, but is accompanied

by challenges such as effectively evaluating recog-
nition of long text spans and dealing with complex
syntactic structure and punctuation within relevant
mentions. It is our hope that these initial ﬁndings,
along with further research reﬁning the appropri-
ate framework for representing and approaching
the recognition problem, will spur further research
into this complex and important domain.

Acknowledgments

The authors would like to thank Elizabeth Rasch,
Thanh Thieu, and Eric Fosler-Lussier for help-
ful discussions, the NIH Biomedical Translational
Research Information System (BTRIS) for their
support, and our anonymous reviewers for their in-
valuable feedback. This research was supported
in part by the Intramural Research Program of the
National Institutes of Health, Clinical Research
Center and through an Inter-Agency Agreement
with the US Social Security Administration.

References

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294, Austin, Texas. Association for Compu-
tational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. Transactions of the ACL,
5:135–146.

Billy Chiu, Gamal Crichton, Anna Korhonen, and
Sampo Pyysalo. 2016. How to Train Good Word
Embeddings for Biomedical NLP. Proceedings of
the 15th Workshop on Biomedical Natural Language
Processing, pages 166–174.

Jason PC Chiu and Eric Nichols. 2015. Named en-
tity recognition with bidirectional lstm-cnns. arXiv
preprint arXiv:1511.08308.

James J. Cimino and Elaine J. Ayres. 2010. The clin-
ical research data repository of the US National In-
stitutes of Health. Studies in Health Technology and
Informatics, 160(PART 1):1299–1303.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
Journal of Machine Learning Research,
scratch.
12(Aug):2493–2537.

9Franck Dernoncourt,

Ji Young Lee,

and Peter
Szolovits. 2017a. NeuroNER: an easy-to-use pro-
gram for named-entity recognition based on neu-
In Proceedings of the 2017 Confer-
ral networks.
ence on Empirical Methods in Natural Language
Processing: System Demonstrations, pages 97–102,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner,
and Peter Szolovits. 2017b. De-identiﬁcation of pa-
tient notes with recurrent neural networks. Journal
of the American Medical Informatics Association,
24(3):596–606.

Fernando Diaz, Bhaskar Mitra, and Nick Craswell.
2016. Query Expansion with Locally-Trained Word
In Proceedings of the 54th Annual
Embeddings.
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 367–377,
Berlin, Germany. Association for Computational
Linguistics.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005.
Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363–370. Association for Computational
Linguistics.

Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: A description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222–235.

Maryam Habibi, Leon Weber, Mariana Neves,
David Luis Wiegandt, and Ulf Leser. 2017. Deep
learning with word embeddings improves biomed-
Bioinformatics,
ical named entity recognition.
33(14):i37–i48.

Alistair E W Johnson, Tom J Pollard, Lu Shen, Li-
Wei H Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G Mark. 2016. MIMIC-III,
a freely accessible critical care database. Scientiﬁc
data, 3:160035.

Jun’ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun’ichi Tsujii. 2002. Tuning support vector ma-
chines for biomedical named entity recognition. In
Proceedings of the ACL-02 Workshop on Natural
Language Processing in the Biomedical Domain-
Volume 3, pages 1–8. Association for Computational
Linguistics.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Rita Kukafka, Michael E. Bales, Ann Burkhardt, and
Carol Friedman. 2006. Human and Automated Cod-
ing of Rehabilitation Discharge Summaries Accord-
ing to the International Classiﬁcation of Function-
ing, Disability, and Health. Journal of the American
Medical Informatics Association, 13(5):508–515.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Zengjian Liu, Buzhou Tang, Xiaolong Wang, and
Qingcai Chen. 2017. De-identiﬁcation of clinical
notes via recurrent neural network and conditional
random ﬁeld. Journal of Biomedical Informatics,
75:S34–S42.

Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom ﬁelds, feature induction and web-enhanced lex-
icons. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003-
Volume 4, pages 188–191. Association for Compu-
tational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
Efﬁcient Estimation of Word
arXiv preprint

frey Dean. 2013.
Representations in Vector Space.
arXiv:1301.3781, pages 1–12.

Serguei V S Pakhomov, Greg Finley, Reed McEwan,
Yan Wang, and Genevieve B Melton. 2016. Cor-
pus domain effects on distributional semantic mod-
eling of medical terms. Bioinformatics, 32(Au-
gust):btw529.

Guergana K Savova, James J Masanz, Philip V Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-
Schuler, and Christopher G Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction Sys-
tem (cTAKES): architecture, component evaluation
and applications. Journal of the American Medical
Informatics Association : JAMIA, 17(5):507–513.

Burr Settles. 2004. Biomedical named entity recog-
nition using conditional random ﬁelds and rich fea-
the International
ture sets.
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications, pages 104–107.
Association for Computational Linguistics.

In Proceedings of

Steven J Skube, Elizabeth A Lindemann, Elliot G Arso-
niadis, Elizabeth C Wick, and Genevieve B Melton.
2018. Characterizing Functional Health Status of
Surgical Patients in Clinical Notes. In 2018 AMIA
Summit on Clinical Research Informatics. American
Medical Informatics Association.

Ergin Soysal, Jingqi Wang, Min Jiang, Yonghui Wu,
Serguei Pakhomov, Hongfang Liu, and Hua Xu.
2018. CLAMP a toolkit for efﬁciently build-
ing customized clinical natural language processing
pipelines. Journal of the American Medical Infor-
matics Association, 25(3):331–336.

G Stucki, J Bickenbach, and J Melvin. 2017. Strength-
ening Rehabilitation in Health Systems Worldwide
by Integrating Information on Functioning in Na-
tional Health Information Systems. Am J Phys Med
Rehabil, 96(9):677–681.

10Gerold Stucki and Jerome Bickenbach. 2017. Func-
tioning:
the third health indicator in the health
system and the key indicator for rehabilitation.
European Journal of Physical and Rehabilitation
Medicine, 53(1):134–138.

Thanh Thieu, Jonathan Camacho, Pei-Shu Ho, Di-
ane Brandt, Julia Porcino, Denis Newman-Grifﬁs,
Ao Yuan, Min Ding, Lisa Nelson, Elizabeth Rasch,
Chunxiao Zhou, Albert M Lai, and Leighton Chan.
2017.
Inductive identiﬁcation of functional status
information and establishing a gold standard cor-
IEEE
pus A case study on the Mobility domain.
International Conference on Bioinformatics and
Biomedicine (BIBM), pages 2300–2302.

Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the CoNLL-2003 Shared
Task: Language-Independent Named Entity Recog-
nition. In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 142–147.

Richard Tzong-Han Tsai, Cheng-Lung Sung, Hong-Jie
Dai, Hsieh-Chuan Hung, Ting-Yi Sung, and Wen-
Lian Hsu. 2006. Nerbio: using selected word con-
junctions, term normalization, and global patterns to
improve biomedical named entity recognition.
In
BMC Bioinformatics, volume 7, page S11. BioMed
Central.

¨Ozlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2012. 2010 i2b2/VA challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation : JAMIA, 18(5):552–6.

WHO. 2001. International Classiﬁcation of Function-
ing, Disability and Health: ICF. World Health Or-
ganization.

Long Xia, G Alan Wang, and Weiguo Fan. 2017. A
Deep Learning Based Named Entity Recognition
Approach for Adverse Drug Events Identiﬁcation
In Smart
and Extraction in Health Social Media.
Health, pages 237–248, Cham. Springer Interna-
tional Publishing.

Wei Yang, Wei Lu, and Vincent Zheng. 2017. A simple
regularization-based algorithm for learning cross-
In Proceedings of the
domain word embeddings.
2017 Conference on Empirical Methods in Natural
Language Processing, pages 2898–2904.

11